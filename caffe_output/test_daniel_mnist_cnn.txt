I1127 10:40:36.085422  4302 caffe.cpp:184] Using GPUs 0
I1127 10:40:36.498572  4302 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:40:36.498905  4302 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:40:36.499410  4302 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:40:36.499456  4302 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:40:36.499650  4302 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:40:36.499805  4302 layer_factory.hpp:76] Creating layer mnist
I1127 10:40:36.542425  4302 net.cpp:106] Creating Layer mnist
I1127 10:40:36.542610  4302 net.cpp:411] mnist -> data
I1127 10:40:36.542728  4302 net.cpp:411] mnist -> label
I1127 10:40:36.544740  4305 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:40:36.585686  4302 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:40:36.594424  4302 net.cpp:150] Setting up mnist
I1127 10:40:36.594570  4302 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:40:36.594588  4302 net.cpp:157] Top shape: 64 (64)
I1127 10:40:36.594600  4302 net.cpp:165] Memory required for data: 200960
I1127 10:40:36.594630  4302 layer_factory.hpp:76] Creating layer conv1
I1127 10:40:36.594689  4302 net.cpp:106] Creating Layer conv1
I1127 10:40:36.594709  4302 net.cpp:454] conv1 <- data
I1127 10:40:36.594737  4302 net.cpp:411] conv1 -> conv1
I1127 10:40:36.596272  4302 net.cpp:150] Setting up conv1
I1127 10:40:36.596350  4302 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:40:36.596360  4302 net.cpp:165] Memory required for data: 3150080
I1127 10:40:36.596391  4302 layer_factory.hpp:76] Creating layer pool1
I1127 10:40:36.596415  4302 net.cpp:106] Creating Layer pool1
I1127 10:40:36.596424  4302 net.cpp:454] pool1 <- conv1
I1127 10:40:36.596453  4302 net.cpp:411] pool1 -> pool1
I1127 10:40:36.596536  4302 net.cpp:150] Setting up pool1
I1127 10:40:36.596552  4302 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:40:36.596560  4302 net.cpp:165] Memory required for data: 3887360
I1127 10:40:36.596568  4302 layer_factory.hpp:76] Creating layer conv2
I1127 10:40:36.596593  4302 net.cpp:106] Creating Layer conv2
I1127 10:40:36.596601  4302 net.cpp:454] conv2 <- pool1
I1127 10:40:36.596613  4302 net.cpp:411] conv2 -> conv2
I1127 10:40:36.597538  4302 net.cpp:150] Setting up conv2
I1127 10:40:36.597620  4302 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:40:36.597625  4302 net.cpp:165] Memory required for data: 4706560
I1127 10:40:36.597647  4302 layer_factory.hpp:76] Creating layer pool2
I1127 10:40:36.597676  4302 net.cpp:106] Creating Layer pool2
I1127 10:40:36.597682  4302 net.cpp:454] pool2 <- conv2
I1127 10:40:36.597695  4302 net.cpp:411] pool2 -> pool2
I1127 10:40:36.597748  4302 net.cpp:150] Setting up pool2
I1127 10:40:36.597756  4302 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:40:36.597760  4302 net.cpp:165] Memory required for data: 4911360
I1127 10:40:36.597765  4302 layer_factory.hpp:76] Creating layer ip1
I1127 10:40:36.597781  4302 net.cpp:106] Creating Layer ip1
I1127 10:40:36.597786  4302 net.cpp:454] ip1 <- pool2
I1127 10:40:36.597793  4302 net.cpp:411] ip1 -> ip1
I1127 10:40:36.601305  4302 net.cpp:150] Setting up ip1
I1127 10:40:36.601388  4302 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:40:36.601393  4302 net.cpp:165] Memory required for data: 5039360
I1127 10:40:36.601416  4302 layer_factory.hpp:76] Creating layer relu1
I1127 10:40:36.601438  4302 net.cpp:106] Creating Layer relu1
I1127 10:40:36.601447  4302 net.cpp:454] relu1 <- ip1
I1127 10:40:36.601457  4302 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:40:36.601481  4302 net.cpp:150] Setting up relu1
I1127 10:40:36.601488  4302 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:40:36.601492  4302 net.cpp:165] Memory required for data: 5167360
I1127 10:40:36.601496  4302 layer_factory.hpp:76] Creating layer ip2
I1127 10:40:36.601510  4302 net.cpp:106] Creating Layer ip2
I1127 10:40:36.601513  4302 net.cpp:454] ip2 <- ip1
I1127 10:40:36.601522  4302 net.cpp:411] ip2 -> ip2
I1127 10:40:36.602371  4302 net.cpp:150] Setting up ip2
I1127 10:40:36.602450  4302 net.cpp:157] Top shape: 64 10 (640)
I1127 10:40:36.602457  4302 net.cpp:165] Memory required for data: 5169920
I1127 10:40:36.602473  4302 layer_factory.hpp:76] Creating layer loss
I1127 10:40:36.602500  4302 net.cpp:106] Creating Layer loss
I1127 10:40:36.602509  4302 net.cpp:454] loss <- ip2
I1127 10:40:36.602519  4302 net.cpp:454] loss <- label
I1127 10:40:36.602535  4302 net.cpp:411] loss -> loss
I1127 10:40:36.602571  4302 layer_factory.hpp:76] Creating layer loss
I1127 10:40:36.602689  4302 net.cpp:150] Setting up loss
I1127 10:40:36.602701  4302 net.cpp:157] Top shape: (1)
I1127 10:40:36.602705  4302 net.cpp:160]     with loss weight 1
I1127 10:40:36.602736  4302 net.cpp:165] Memory required for data: 5169924
I1127 10:40:36.602741  4302 net.cpp:226] loss needs backward computation.
I1127 10:40:36.602746  4302 net.cpp:226] ip2 needs backward computation.
I1127 10:40:36.602751  4302 net.cpp:226] relu1 needs backward computation.
I1127 10:40:36.602756  4302 net.cpp:226] ip1 needs backward computation.
I1127 10:40:36.602761  4302 net.cpp:226] pool2 needs backward computation.
I1127 10:40:36.602764  4302 net.cpp:226] conv2 needs backward computation.
I1127 10:40:36.602769  4302 net.cpp:226] pool1 needs backward computation.
I1127 10:40:36.602774  4302 net.cpp:226] conv1 needs backward computation.
I1127 10:40:36.602779  4302 net.cpp:228] mnist does not need backward computation.
I1127 10:40:36.602784  4302 net.cpp:270] This network produces output loss
I1127 10:40:36.602800  4302 net.cpp:283] Network initialization done.
I1127 10:40:36.603165  4302 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:40:36.603231  4302 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:40:36.603416  4302 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:40:36.603528  4302 layer_factory.hpp:76] Creating layer mnist
I1127 10:40:36.603694  4302 net.cpp:106] Creating Layer mnist
I1127 10:40:36.603709  4302 net.cpp:411] mnist -> data
I1127 10:40:36.603734  4302 net.cpp:411] mnist -> label
I1127 10:40:36.605867  4307 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:40:36.606041  4302 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:40:36.612763  4302 net.cpp:150] Setting up mnist
I1127 10:40:36.612861  4302 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:40:36.612869  4302 net.cpp:157] Top shape: 100 (100)
I1127 10:40:36.612874  4302 net.cpp:165] Memory required for data: 314000
I1127 10:40:36.612886  4302 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:40:36.612918  4302 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:40:36.612927  4302 net.cpp:454] label_mnist_1_split <- label
I1127 10:40:36.612939  4302 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:40:36.612959  4302 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:40:36.613062  4302 net.cpp:150] Setting up label_mnist_1_split
I1127 10:40:36.613072  4302 net.cpp:157] Top shape: 100 (100)
I1127 10:40:36.613077  4302 net.cpp:157] Top shape: 100 (100)
I1127 10:40:36.613081  4302 net.cpp:165] Memory required for data: 314800
I1127 10:40:36.613086  4302 layer_factory.hpp:76] Creating layer conv1
I1127 10:40:36.613106  4302 net.cpp:106] Creating Layer conv1
I1127 10:40:36.613111  4302 net.cpp:454] conv1 <- data
I1127 10:40:36.613119  4302 net.cpp:411] conv1 -> conv1
I1127 10:40:36.613288  4302 net.cpp:150] Setting up conv1
I1127 10:40:36.613297  4302 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:40:36.613301  4302 net.cpp:165] Memory required for data: 4922800
I1127 10:40:36.613312  4302 layer_factory.hpp:76] Creating layer pool1
I1127 10:40:36.613322  4302 net.cpp:106] Creating Layer pool1
I1127 10:40:36.613342  4302 net.cpp:454] pool1 <- conv1
I1127 10:40:36.613368  4302 net.cpp:411] pool1 -> pool1
I1127 10:40:36.613399  4302 net.cpp:150] Setting up pool1
I1127 10:40:36.613405  4302 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:40:36.613409  4302 net.cpp:165] Memory required for data: 6074800
I1127 10:40:36.613415  4302 layer_factory.hpp:76] Creating layer conv2
I1127 10:40:36.613423  4302 net.cpp:106] Creating Layer conv2
I1127 10:40:36.613428  4302 net.cpp:454] conv2 <- pool1
I1127 10:40:36.613440  4302 net.cpp:411] conv2 -> conv2
I1127 10:40:36.613726  4302 net.cpp:150] Setting up conv2
I1127 10:40:36.613739  4302 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:40:36.613744  4302 net.cpp:165] Memory required for data: 7354800
I1127 10:40:36.613752  4302 layer_factory.hpp:76] Creating layer pool2
I1127 10:40:36.613760  4302 net.cpp:106] Creating Layer pool2
I1127 10:40:36.613765  4302 net.cpp:454] pool2 <- conv2
I1127 10:40:36.613771  4302 net.cpp:411] pool2 -> pool2
I1127 10:40:36.613800  4302 net.cpp:150] Setting up pool2
I1127 10:40:36.613808  4302 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:40:36.613812  4302 net.cpp:165] Memory required for data: 7674800
I1127 10:40:36.613816  4302 layer_factory.hpp:76] Creating layer ip1
I1127 10:40:36.613826  4302 net.cpp:106] Creating Layer ip1
I1127 10:40:36.613831  4302 net.cpp:454] ip1 <- pool2
I1127 10:40:36.613836  4302 net.cpp:411] ip1 -> ip1
I1127 10:40:36.617030  4302 net.cpp:150] Setting up ip1
I1127 10:40:36.617072  4302 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:40:36.617077  4302 net.cpp:165] Memory required for data: 7874800
I1127 10:40:36.617092  4302 layer_factory.hpp:76] Creating layer relu1
I1127 10:40:36.617110  4302 net.cpp:106] Creating Layer relu1
I1127 10:40:36.617118  4302 net.cpp:454] relu1 <- ip1
I1127 10:40:36.617128  4302 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:40:36.617144  4302 net.cpp:150] Setting up relu1
I1127 10:40:36.617151  4302 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:40:36.617154  4302 net.cpp:165] Memory required for data: 8074800
I1127 10:40:36.617159  4302 layer_factory.hpp:76] Creating layer ip2
I1127 10:40:36.617177  4302 net.cpp:106] Creating Layer ip2
I1127 10:40:36.617183  4302 net.cpp:454] ip2 <- ip1
I1127 10:40:36.617193  4302 net.cpp:411] ip2 -> ip2
I1127 10:40:36.617346  4302 net.cpp:150] Setting up ip2
I1127 10:40:36.617359  4302 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:40:36.617363  4302 net.cpp:165] Memory required for data: 8078800
I1127 10:40:36.617372  4302 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:40:36.617384  4302 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:40:36.617390  4302 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:40:36.617398  4302 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:40:36.617408  4302 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:40:36.617449  4302 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:40:36.617457  4302 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:40:36.617463  4302 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:40:36.617467  4302 net.cpp:165] Memory required for data: 8086800
I1127 10:40:36.617472  4302 layer_factory.hpp:76] Creating layer accuracy
I1127 10:40:36.617485  4302 net.cpp:106] Creating Layer accuracy
I1127 10:40:36.617490  4302 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:40:36.617496  4302 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:40:36.617501  4302 net.cpp:411] accuracy -> accuracy
I1127 10:40:36.617514  4302 net.cpp:150] Setting up accuracy
I1127 10:40:36.617522  4302 net.cpp:157] Top shape: (1)
I1127 10:40:36.617525  4302 net.cpp:165] Memory required for data: 8086804
I1127 10:40:36.617532  4302 layer_factory.hpp:76] Creating layer loss
I1127 10:40:36.617542  4302 net.cpp:106] Creating Layer loss
I1127 10:40:36.617547  4302 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:40:36.617552  4302 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:40:36.617559  4302 net.cpp:411] loss -> loss
I1127 10:40:36.617578  4302 layer_factory.hpp:76] Creating layer loss
I1127 10:40:36.617666  4302 net.cpp:150] Setting up loss
I1127 10:40:36.617674  4302 net.cpp:157] Top shape: (1)
I1127 10:40:36.617678  4302 net.cpp:160]     with loss weight 1
I1127 10:40:36.617689  4302 net.cpp:165] Memory required for data: 8086808
I1127 10:40:36.617694  4302 net.cpp:226] loss needs backward computation.
I1127 10:40:36.617704  4302 net.cpp:228] accuracy does not need backward computation.
I1127 10:40:36.617709  4302 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:40:36.617713  4302 net.cpp:226] ip2 needs backward computation.
I1127 10:40:36.617718  4302 net.cpp:226] relu1 needs backward computation.
I1127 10:40:36.617722  4302 net.cpp:226] ip1 needs backward computation.
I1127 10:40:36.617727  4302 net.cpp:226] pool2 needs backward computation.
I1127 10:40:36.617732  4302 net.cpp:226] conv2 needs backward computation.
I1127 10:40:36.617735  4302 net.cpp:226] pool1 needs backward computation.
I1127 10:40:36.617739  4302 net.cpp:226] conv1 needs backward computation.
I1127 10:40:36.617744  4302 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:40:36.617749  4302 net.cpp:228] mnist does not need backward computation.
I1127 10:40:36.617753  4302 net.cpp:270] This network produces output accuracy
I1127 10:40:36.617758  4302 net.cpp:270] This network produces output loss
I1127 10:40:36.617769  4302 net.cpp:283] Network initialization done.
I1127 10:40:36.617815  4302 solver.cpp:59] Solver scaffolding done.
I1127 10:40:36.618018  4302 caffe.cpp:212] Starting Optimization
I1127 10:40:36.618029  4302 solver.cpp:287] Solving LeNet
I1127 10:40:36.618033  4302 solver.cpp:288] Learning Rate Policy: inv
I1127 10:40:36.618631  4302 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:40:38.351589  4302 solver.cpp:408]     Test net output #0: accuracy = 0.1038
I1127 10:40:38.351640  4302 solver.cpp:408]     Test net output #1: loss = 2.34682 (* 1 = 2.34682 loss)
I1127 10:40:38.362599  4302 solver.cpp:236] Iteration 0, loss = 2.35514
I1127 10:40:38.362701  4302 solver.cpp:252]     Train net output #0: loss = 2.35514 (* 1 = 2.35514 loss)
I1127 10:40:38.362771  4302 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:40:49.891474  4302 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:40:52.700305  4302 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:40:52.832736  4302 solver.cpp:408]     Test net output #0: accuracy = 0.9704
I1127 10:40:52.832916  4302 solver.cpp:408]     Test net output #1: loss = 0.090009 (* 1 = 0.090009 loss)
I1127 10:40:52.845091  4302 solver.cpp:236] Iteration 500, loss = 0.115938
I1127 10:40:52.845214  4302 solver.cpp:252]     Train net output #0: loss = 0.115938 (* 1 = 0.115938 loss)
I1127 10:40:52.845233  4302 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:41:06.160687  4302 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:41:06.174093  4302 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:41:06.184293  4302 solver.cpp:320] Iteration 1000, loss = 0.0820013
I1127 10:41:06.184393  4302 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:41:07.268698  4302 solver.cpp:408]     Test net output #0: accuracy = 0.982
I1127 10:41:07.268848  4302 solver.cpp:408]     Test net output #1: loss = 0.056462 (* 1 = 0.056462 loss)
I1127 10:41:07.268904  4302 solver.cpp:325] Optimization Done.
I1127 10:41:07.268934  4302 caffe.cpp:215] Optimization Done.
I1127 10:41:07.365939  4388 caffe.cpp:184] Using GPUs 0
I1127 10:41:07.805673  4388 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:41:07.805785  4388 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:41:07.806051  4388 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:41:07.806067  4388 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:41:07.806175  4388 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:41:07.806243  4388 layer_factory.hpp:76] Creating layer mnist
I1127 10:41:07.806560  4388 net.cpp:106] Creating Layer mnist
I1127 10:41:07.806582  4388 net.cpp:411] mnist -> data
I1127 10:41:07.806607  4388 net.cpp:411] mnist -> label
I1127 10:41:07.807335  4392 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:41:07.843183  4388 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:41:07.849941  4388 net.cpp:150] Setting up mnist
I1127 10:41:07.849961  4388 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:41:07.849967  4388 net.cpp:157] Top shape: 64 (64)
I1127 10:41:07.849970  4388 net.cpp:165] Memory required for data: 200960
I1127 10:41:07.849979  4388 layer_factory.hpp:76] Creating layer conv1
I1127 10:41:07.849997  4388 net.cpp:106] Creating Layer conv1
I1127 10:41:07.850004  4388 net.cpp:454] conv1 <- data
I1127 10:41:07.850014  4388 net.cpp:411] conv1 -> conv1
I1127 10:41:07.850668  4388 net.cpp:150] Setting up conv1
I1127 10:41:07.850680  4388 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:41:07.850685  4388 net.cpp:165] Memory required for data: 3150080
I1127 10:41:07.850697  4388 layer_factory.hpp:76] Creating layer pool1
I1127 10:41:07.850708  4388 net.cpp:106] Creating Layer pool1
I1127 10:41:07.850713  4388 net.cpp:454] pool1 <- conv1
I1127 10:41:07.850718  4388 net.cpp:411] pool1 -> pool1
I1127 10:41:07.850764  4388 net.cpp:150] Setting up pool1
I1127 10:41:07.850771  4388 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:41:07.850775  4388 net.cpp:165] Memory required for data: 3887360
I1127 10:41:07.850780  4388 layer_factory.hpp:76] Creating layer conv2
I1127 10:41:07.850788  4388 net.cpp:106] Creating Layer conv2
I1127 10:41:07.850793  4388 net.cpp:454] conv2 <- pool1
I1127 10:41:07.850800  4388 net.cpp:411] conv2 -> conv2
I1127 10:41:07.851270  4388 net.cpp:150] Setting up conv2
I1127 10:41:07.851280  4388 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:41:07.851284  4388 net.cpp:165] Memory required for data: 4706560
I1127 10:41:07.851292  4388 layer_factory.hpp:76] Creating layer pool2
I1127 10:41:07.851300  4388 net.cpp:106] Creating Layer pool2
I1127 10:41:07.851305  4388 net.cpp:454] pool2 <- conv2
I1127 10:41:07.851311  4388 net.cpp:411] pool2 -> pool2
I1127 10:41:07.851339  4388 net.cpp:150] Setting up pool2
I1127 10:41:07.851346  4388 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:41:07.851351  4388 net.cpp:165] Memory required for data: 4911360
I1127 10:41:07.851354  4388 layer_factory.hpp:76] Creating layer ip1
I1127 10:41:07.851364  4388 net.cpp:106] Creating Layer ip1
I1127 10:41:07.851368  4388 net.cpp:454] ip1 <- pool2
I1127 10:41:07.851375  4388 net.cpp:411] ip1 -> ip1
I1127 10:41:07.853456  4388 net.cpp:150] Setting up ip1
I1127 10:41:07.853466  4388 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:41:07.853471  4388 net.cpp:165] Memory required for data: 5039360
I1127 10:41:07.853479  4388 layer_factory.hpp:76] Creating layer relu1
I1127 10:41:07.853487  4388 net.cpp:106] Creating Layer relu1
I1127 10:41:07.853492  4388 net.cpp:454] relu1 <- ip1
I1127 10:41:07.853497  4388 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:41:07.853505  4388 net.cpp:150] Setting up relu1
I1127 10:41:07.853510  4388 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:41:07.853514  4388 net.cpp:165] Memory required for data: 5167360
I1127 10:41:07.853518  4388 layer_factory.hpp:76] Creating layer ip2
I1127 10:41:07.853526  4388 net.cpp:106] Creating Layer ip2
I1127 10:41:07.853531  4388 net.cpp:454] ip2 <- ip1
I1127 10:41:07.853538  4388 net.cpp:411] ip2 -> ip2
I1127 10:41:07.853915  4388 net.cpp:150] Setting up ip2
I1127 10:41:07.853925  4388 net.cpp:157] Top shape: 64 10 (640)
I1127 10:41:07.853929  4388 net.cpp:165] Memory required for data: 5169920
I1127 10:41:07.853936  4388 layer_factory.hpp:76] Creating layer loss
I1127 10:41:07.853945  4388 net.cpp:106] Creating Layer loss
I1127 10:41:07.853950  4388 net.cpp:454] loss <- ip2
I1127 10:41:07.853955  4388 net.cpp:454] loss <- label
I1127 10:41:07.853962  4388 net.cpp:411] loss -> loss
I1127 10:41:07.853973  4388 layer_factory.hpp:76] Creating layer loss
I1127 10:41:07.854038  4388 net.cpp:150] Setting up loss
I1127 10:41:07.854045  4388 net.cpp:157] Top shape: (1)
I1127 10:41:07.854049  4388 net.cpp:160]     with loss weight 1
I1127 10:41:07.854064  4388 net.cpp:165] Memory required for data: 5169924
I1127 10:41:07.854069  4388 net.cpp:226] loss needs backward computation.
I1127 10:41:07.854074  4388 net.cpp:226] ip2 needs backward computation.
I1127 10:41:07.854079  4388 net.cpp:226] relu1 needs backward computation.
I1127 10:41:07.854082  4388 net.cpp:226] ip1 needs backward computation.
I1127 10:41:07.854086  4388 net.cpp:226] pool2 needs backward computation.
I1127 10:41:07.854090  4388 net.cpp:226] conv2 needs backward computation.
I1127 10:41:07.854094  4388 net.cpp:226] pool1 needs backward computation.
I1127 10:41:07.854099  4388 net.cpp:226] conv1 needs backward computation.
I1127 10:41:07.854104  4388 net.cpp:228] mnist does not need backward computation.
I1127 10:41:07.854107  4388 net.cpp:270] This network produces output loss
I1127 10:41:07.854115  4388 net.cpp:283] Network initialization done.
I1127 10:41:07.854353  4388 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:41:07.854375  4388 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:41:07.854485  4388 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:41:07.854543  4388 layer_factory.hpp:76] Creating layer mnist
I1127 10:41:07.854643  4388 net.cpp:106] Creating Layer mnist
I1127 10:41:07.854655  4388 net.cpp:411] mnist -> data
I1127 10:41:07.854666  4388 net.cpp:411] mnist -> label
I1127 10:41:07.855355  4401 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:41:07.855450  4388 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:41:07.859145  4388 net.cpp:150] Setting up mnist
I1127 10:41:07.859158  4388 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:41:07.859163  4388 net.cpp:157] Top shape: 100 (100)
I1127 10:41:07.859168  4388 net.cpp:165] Memory required for data: 314000
I1127 10:41:07.859172  4388 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:41:07.859179  4388 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:41:07.859184  4388 net.cpp:454] label_mnist_1_split <- label
I1127 10:41:07.859190  4388 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:41:07.859199  4388 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:41:07.859230  4388 net.cpp:150] Setting up label_mnist_1_split
I1127 10:41:07.859237  4388 net.cpp:157] Top shape: 100 (100)
I1127 10:41:07.859242  4388 net.cpp:157] Top shape: 100 (100)
I1127 10:41:07.859246  4388 net.cpp:165] Memory required for data: 314800
I1127 10:41:07.859251  4388 layer_factory.hpp:76] Creating layer conv1
I1127 10:41:07.859261  4388 net.cpp:106] Creating Layer conv1
I1127 10:41:07.859266  4388 net.cpp:454] conv1 <- data
I1127 10:41:07.859272  4388 net.cpp:411] conv1 -> conv1
I1127 10:41:07.859412  4388 net.cpp:150] Setting up conv1
I1127 10:41:07.859421  4388 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:41:07.859426  4388 net.cpp:165] Memory required for data: 4922800
I1127 10:41:07.859434  4388 layer_factory.hpp:76] Creating layer pool1
I1127 10:41:07.859441  4388 net.cpp:106] Creating Layer pool1
I1127 10:41:07.859446  4388 net.cpp:454] pool1 <- conv1
I1127 10:41:07.859457  4388 net.cpp:411] pool1 -> pool1
I1127 10:41:07.859488  4388 net.cpp:150] Setting up pool1
I1127 10:41:07.859494  4388 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:41:07.859498  4388 net.cpp:165] Memory required for data: 6074800
I1127 10:41:07.859503  4388 layer_factory.hpp:76] Creating layer conv2
I1127 10:41:07.859511  4388 net.cpp:106] Creating Layer conv2
I1127 10:41:07.859519  4388 net.cpp:454] conv2 <- pool1
I1127 10:41:07.859525  4388 net.cpp:411] conv2 -> conv2
I1127 10:41:07.859769  4388 net.cpp:150] Setting up conv2
I1127 10:41:07.859777  4388 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:41:07.859781  4388 net.cpp:165] Memory required for data: 7354800
I1127 10:41:07.859789  4388 layer_factory.hpp:76] Creating layer pool2
I1127 10:41:07.859797  4388 net.cpp:106] Creating Layer pool2
I1127 10:41:07.859800  4388 net.cpp:454] pool2 <- conv2
I1127 10:41:07.859805  4388 net.cpp:411] pool2 -> pool2
I1127 10:41:07.859833  4388 net.cpp:150] Setting up pool2
I1127 10:41:07.859839  4388 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:41:07.859843  4388 net.cpp:165] Memory required for data: 7674800
I1127 10:41:07.859848  4388 layer_factory.hpp:76] Creating layer ip1
I1127 10:41:07.859853  4388 net.cpp:106] Creating Layer ip1
I1127 10:41:07.859858  4388 net.cpp:454] ip1 <- pool2
I1127 10:41:07.859869  4388 net.cpp:411] ip1 -> ip1
I1127 10:41:07.861965  4388 net.cpp:150] Setting up ip1
I1127 10:41:07.861979  4388 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:41:07.861984  4388 net.cpp:165] Memory required for data: 7874800
I1127 10:41:07.861991  4388 layer_factory.hpp:76] Creating layer relu1
I1127 10:41:07.861999  4388 net.cpp:106] Creating Layer relu1
I1127 10:41:07.862004  4388 net.cpp:454] relu1 <- ip1
I1127 10:41:07.862009  4388 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:41:07.862015  4388 net.cpp:150] Setting up relu1
I1127 10:41:07.862021  4388 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:41:07.862025  4388 net.cpp:165] Memory required for data: 8074800
I1127 10:41:07.862030  4388 layer_factory.hpp:76] Creating layer ip2
I1127 10:41:07.862040  4388 net.cpp:106] Creating Layer ip2
I1127 10:41:07.862045  4388 net.cpp:454] ip2 <- ip1
I1127 10:41:07.862051  4388 net.cpp:411] ip2 -> ip2
I1127 10:41:07.862146  4388 net.cpp:150] Setting up ip2
I1127 10:41:07.862155  4388 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:41:07.862159  4388 net.cpp:165] Memory required for data: 8078800
I1127 10:41:07.862165  4388 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:41:07.862172  4388 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:41:07.862176  4388 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:41:07.862181  4388 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:41:07.862188  4388 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:41:07.862215  4388 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:41:07.862221  4388 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:41:07.862226  4388 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:41:07.862231  4388 net.cpp:165] Memory required for data: 8086800
I1127 10:41:07.862236  4388 layer_factory.hpp:76] Creating layer accuracy
I1127 10:41:07.862241  4388 net.cpp:106] Creating Layer accuracy
I1127 10:41:07.862246  4388 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:41:07.862251  4388 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:41:07.862257  4388 net.cpp:411] accuracy -> accuracy
I1127 10:41:07.862267  4388 net.cpp:150] Setting up accuracy
I1127 10:41:07.862273  4388 net.cpp:157] Top shape: (1)
I1127 10:41:07.862283  4388 net.cpp:165] Memory required for data: 8086804
I1127 10:41:07.862287  4388 layer_factory.hpp:76] Creating layer loss
I1127 10:41:07.862293  4388 net.cpp:106] Creating Layer loss
I1127 10:41:07.862298  4388 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:41:07.862303  4388 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:41:07.862309  4388 net.cpp:411] loss -> loss
I1127 10:41:07.862318  4388 layer_factory.hpp:76] Creating layer loss
I1127 10:41:07.862385  4388 net.cpp:150] Setting up loss
I1127 10:41:07.862396  4388 net.cpp:157] Top shape: (1)
I1127 10:41:07.862399  4388 net.cpp:160]     with loss weight 1
I1127 10:41:07.862407  4388 net.cpp:165] Memory required for data: 8086808
I1127 10:41:07.862412  4388 net.cpp:226] loss needs backward computation.
I1127 10:41:07.862418  4388 net.cpp:228] accuracy does not need backward computation.
I1127 10:41:07.862423  4388 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:41:07.862432  4388 net.cpp:226] ip2 needs backward computation.
I1127 10:41:07.862435  4388 net.cpp:226] relu1 needs backward computation.
I1127 10:41:07.862439  4388 net.cpp:226] ip1 needs backward computation.
I1127 10:41:07.862443  4388 net.cpp:226] pool2 needs backward computation.
I1127 10:41:07.862457  4388 net.cpp:226] conv2 needs backward computation.
I1127 10:41:07.862460  4388 net.cpp:226] pool1 needs backward computation.
I1127 10:41:07.862464  4388 net.cpp:226] conv1 needs backward computation.
I1127 10:41:07.862469  4388 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:41:07.862474  4388 net.cpp:228] mnist does not need backward computation.
I1127 10:41:07.862478  4388 net.cpp:270] This network produces output accuracy
I1127 10:41:07.862483  4388 net.cpp:270] This network produces output loss
I1127 10:41:07.862491  4388 net.cpp:283] Network initialization done.
I1127 10:41:07.862524  4388 solver.cpp:59] Solver scaffolding done.
I1127 10:41:07.862711  4388 caffe.cpp:212] Starting Optimization
I1127 10:41:07.862718  4388 solver.cpp:287] Solving LeNet
I1127 10:41:07.862722  4388 solver.cpp:288] Learning Rate Policy: inv
I1127 10:41:07.863018  4388 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:41:10.531008  4388 solver.cpp:408]     Test net output #0: accuracy = 0.1094
I1127 10:41:10.531065  4388 solver.cpp:408]     Test net output #1: loss = 2.35312 (* 1 = 2.35312 loss)
I1127 10:41:10.544395  4388 solver.cpp:236] Iteration 0, loss = 2.3972
I1127 10:41:10.544441  4388 solver.cpp:252]     Train net output #0: loss = 2.3972 (* 1 = 2.3972 loss)
I1127 10:41:10.544463  4388 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:41:22.542357  4388 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:41:25.056818  4388 solver.cpp:408]     Test net output #0: accuracy = 0.9725
I1127 10:41:25.056927  4388 solver.cpp:408]     Test net output #1: loss = 0.0852766 (* 1 = 0.0852766 loss)
I1127 10:41:25.066479  4388 solver.cpp:236] Iteration 500, loss = 0.104811
I1127 10:41:25.066551  4388 solver.cpp:252]     Train net output #0: loss = 0.104811 (* 1 = 0.104811 loss)
I1127 10:41:25.066567  4388 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:41:37.322370  4388 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:41:37.346705  4388 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:41:37.374531  4388 solver.cpp:320] Iteration 1000, loss = 0.0819178
I1127 10:41:37.374765  4388 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:41:39.567914  4388 solver.cpp:408]     Test net output #0: accuracy = 0.9811
I1127 10:41:39.567984  4388 solver.cpp:408]     Test net output #1: loss = 0.0584263 (* 1 = 0.0584263 loss)
I1127 10:41:39.568006  4388 solver.cpp:325] Optimization Done.
I1127 10:41:39.568014  4388 caffe.cpp:215] Optimization Done.
I1127 10:41:39.683634  4444 caffe.cpp:184] Using GPUs 0
I1127 10:41:39.932018  4444 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:41:39.932241  4444 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:41:39.932674  4444 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:41:39.932716  4444 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:41:39.932886  4444 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:41:39.933038  4444 layer_factory.hpp:76] Creating layer mnist
I1127 10:41:39.933665  4444 net.cpp:106] Creating Layer mnist
I1127 10:41:39.933697  4444 net.cpp:411] mnist -> data
I1127 10:41:39.933753  4444 net.cpp:411] mnist -> label
I1127 10:41:39.935020  4447 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:41:39.948083  4444 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:41:39.949616  4444 net.cpp:150] Setting up mnist
I1127 10:41:39.949663  4444 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:41:39.949676  4444 net.cpp:157] Top shape: 64 (64)
I1127 10:41:39.949684  4444 net.cpp:165] Memory required for data: 200960
I1127 10:41:39.949698  4444 layer_factory.hpp:76] Creating layer conv1
I1127 10:41:39.949726  4444 net.cpp:106] Creating Layer conv1
I1127 10:41:39.949738  4444 net.cpp:454] conv1 <- data
I1127 10:41:39.949756  4444 net.cpp:411] conv1 -> conv1
I1127 10:41:39.952468  4444 net.cpp:150] Setting up conv1
I1127 10:41:39.952569  4444 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:41:39.952584  4444 net.cpp:165] Memory required for data: 3150080
I1127 10:41:39.952631  4444 layer_factory.hpp:76] Creating layer pool1
I1127 10:41:39.952672  4444 net.cpp:106] Creating Layer pool1
I1127 10:41:39.952687  4444 net.cpp:454] pool1 <- conv1
I1127 10:41:39.952708  4444 net.cpp:411] pool1 -> pool1
I1127 10:41:39.952882  4444 net.cpp:150] Setting up pool1
I1127 10:41:39.952899  4444 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:41:39.952910  4444 net.cpp:165] Memory required for data: 3887360
I1127 10:41:39.952920  4444 layer_factory.hpp:76] Creating layer conv2
I1127 10:41:39.952950  4444 net.cpp:106] Creating Layer conv2
I1127 10:41:39.952961  4444 net.cpp:454] conv2 <- pool1
I1127 10:41:39.952976  4444 net.cpp:411] conv2 -> conv2
I1127 10:41:39.953475  4444 net.cpp:150] Setting up conv2
I1127 10:41:39.953497  4444 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:41:39.953506  4444 net.cpp:165] Memory required for data: 4706560
I1127 10:41:39.953523  4444 layer_factory.hpp:76] Creating layer pool2
I1127 10:41:39.953541  4444 net.cpp:106] Creating Layer pool2
I1127 10:41:39.953550  4444 net.cpp:454] pool2 <- conv2
I1127 10:41:39.953562  4444 net.cpp:411] pool2 -> pool2
I1127 10:41:39.953613  4444 net.cpp:150] Setting up pool2
I1127 10:41:39.953625  4444 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:41:39.953644  4444 net.cpp:165] Memory required for data: 4911360
I1127 10:41:39.953652  4444 layer_factory.hpp:76] Creating layer ip1
I1127 10:41:39.953667  4444 net.cpp:106] Creating Layer ip1
I1127 10:41:39.953676  4444 net.cpp:454] ip1 <- pool2
I1127 10:41:39.953690  4444 net.cpp:411] ip1 -> ip1
I1127 10:41:39.957809  4444 net.cpp:150] Setting up ip1
I1127 10:41:39.957895  4444 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:41:39.957908  4444 net.cpp:165] Memory required for data: 5039360
I1127 10:41:39.957936  4444 layer_factory.hpp:76] Creating layer relu1
I1127 10:41:39.957957  4444 net.cpp:106] Creating Layer relu1
I1127 10:41:39.957968  4444 net.cpp:454] relu1 <- ip1
I1127 10:41:39.957988  4444 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:41:39.958009  4444 net.cpp:150] Setting up relu1
I1127 10:41:39.958021  4444 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:41:39.958029  4444 net.cpp:165] Memory required for data: 5167360
I1127 10:41:39.958036  4444 layer_factory.hpp:76] Creating layer ip2
I1127 10:41:39.958052  4444 net.cpp:106] Creating Layer ip2
I1127 10:41:39.958061  4444 net.cpp:454] ip2 <- ip1
I1127 10:41:39.958075  4444 net.cpp:411] ip2 -> ip2
I1127 10:41:39.958871  4444 net.cpp:150] Setting up ip2
I1127 10:41:39.958904  4444 net.cpp:157] Top shape: 64 10 (640)
I1127 10:41:39.958912  4444 net.cpp:165] Memory required for data: 5169920
I1127 10:41:39.958932  4444 layer_factory.hpp:76] Creating layer loss
I1127 10:41:39.958951  4444 net.cpp:106] Creating Layer loss
I1127 10:41:39.958961  4444 net.cpp:454] loss <- ip2
I1127 10:41:39.958973  4444 net.cpp:454] loss <- label
I1127 10:41:39.958987  4444 net.cpp:411] loss -> loss
I1127 10:41:39.959014  4444 layer_factory.hpp:76] Creating layer loss
I1127 10:41:39.959127  4444 net.cpp:150] Setting up loss
I1127 10:41:39.959141  4444 net.cpp:157] Top shape: (1)
I1127 10:41:39.959151  4444 net.cpp:160]     with loss weight 1
I1127 10:41:39.959189  4444 net.cpp:165] Memory required for data: 5169924
I1127 10:41:39.959198  4444 net.cpp:226] loss needs backward computation.
I1127 10:41:39.959208  4444 net.cpp:226] ip2 needs backward computation.
I1127 10:41:39.959218  4444 net.cpp:226] relu1 needs backward computation.
I1127 10:41:39.959226  4444 net.cpp:226] ip1 needs backward computation.
I1127 10:41:39.959239  4444 net.cpp:226] pool2 needs backward computation.
I1127 10:41:39.959249  4444 net.cpp:226] conv2 needs backward computation.
I1127 10:41:39.959259  4444 net.cpp:226] pool1 needs backward computation.
I1127 10:41:39.959266  4444 net.cpp:226] conv1 needs backward computation.
I1127 10:41:39.959275  4444 net.cpp:228] mnist does not need backward computation.
I1127 10:41:39.959283  4444 net.cpp:270] This network produces output loss
I1127 10:41:39.959300  4444 net.cpp:283] Network initialization done.
I1127 10:41:39.959740  4444 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:41:39.959779  4444 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:41:39.959961  4444 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:41:39.960062  4444 layer_factory.hpp:76] Creating layer mnist
I1127 10:41:39.960201  4444 net.cpp:106] Creating Layer mnist
I1127 10:41:39.960216  4444 net.cpp:411] mnist -> data
I1127 10:41:39.960234  4444 net.cpp:411] mnist -> label
I1127 10:41:39.961307  4449 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:41:39.961609  4444 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:41:39.963419  4444 net.cpp:150] Setting up mnist
I1127 10:41:39.963476  4444 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:41:39.963490  4444 net.cpp:157] Top shape: 100 (100)
I1127 10:41:39.963498  4444 net.cpp:165] Memory required for data: 314000
I1127 10:41:39.963511  4444 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:41:39.963536  4444 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:41:39.963547  4444 net.cpp:454] label_mnist_1_split <- label
I1127 10:41:39.963564  4444 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:41:39.963587  4444 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:41:39.963650  4444 net.cpp:150] Setting up label_mnist_1_split
I1127 10:41:39.963665  4444 net.cpp:157] Top shape: 100 (100)
I1127 10:41:39.963675  4444 net.cpp:157] Top shape: 100 (100)
I1127 10:41:39.963682  4444 net.cpp:165] Memory required for data: 314800
I1127 10:41:39.963691  4444 layer_factory.hpp:76] Creating layer conv1
I1127 10:41:39.963712  4444 net.cpp:106] Creating Layer conv1
I1127 10:41:39.963722  4444 net.cpp:454] conv1 <- data
I1127 10:41:39.963734  4444 net.cpp:411] conv1 -> conv1
I1127 10:41:39.964002  4444 net.cpp:150] Setting up conv1
I1127 10:41:39.964020  4444 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:41:39.964028  4444 net.cpp:165] Memory required for data: 4922800
I1127 10:41:39.964047  4444 layer_factory.hpp:76] Creating layer pool1
I1127 10:41:39.964063  4444 net.cpp:106] Creating Layer pool1
I1127 10:41:39.964073  4444 net.cpp:454] pool1 <- conv1
I1127 10:41:39.964109  4444 net.cpp:411] pool1 -> pool1
I1127 10:41:39.964161  4444 net.cpp:150] Setting up pool1
I1127 10:41:39.964174  4444 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:41:39.964184  4444 net.cpp:165] Memory required for data: 6074800
I1127 10:41:39.964190  4444 layer_factory.hpp:76] Creating layer conv2
I1127 10:41:39.964234  4444 net.cpp:106] Creating Layer conv2
I1127 10:41:39.964247  4444 net.cpp:454] conv2 <- pool1
I1127 10:41:39.964259  4444 net.cpp:411] conv2 -> conv2
I1127 10:41:39.964711  4444 net.cpp:150] Setting up conv2
I1127 10:41:39.964730  4444 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:41:39.964738  4444 net.cpp:165] Memory required for data: 7354800
I1127 10:41:39.964756  4444 layer_factory.hpp:76] Creating layer pool2
I1127 10:41:39.964769  4444 net.cpp:106] Creating Layer pool2
I1127 10:41:39.964778  4444 net.cpp:454] pool2 <- conv2
I1127 10:41:39.964789  4444 net.cpp:411] pool2 -> pool2
I1127 10:41:39.964848  4444 net.cpp:150] Setting up pool2
I1127 10:41:39.964861  4444 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:41:39.964869  4444 net.cpp:165] Memory required for data: 7674800
I1127 10:41:39.964877  4444 layer_factory.hpp:76] Creating layer ip1
I1127 10:41:39.964893  4444 net.cpp:106] Creating Layer ip1
I1127 10:41:39.964902  4444 net.cpp:454] ip1 <- pool2
I1127 10:41:39.964916  4444 net.cpp:411] ip1 -> ip1
I1127 10:41:39.968757  4444 net.cpp:150] Setting up ip1
I1127 10:41:39.968807  4444 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:41:39.968823  4444 net.cpp:165] Memory required for data: 7874800
I1127 10:41:39.968848  4444 layer_factory.hpp:76] Creating layer relu1
I1127 10:41:39.968869  4444 net.cpp:106] Creating Layer relu1
I1127 10:41:39.968881  4444 net.cpp:454] relu1 <- ip1
I1127 10:41:39.968896  4444 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:41:39.968914  4444 net.cpp:150] Setting up relu1
I1127 10:41:39.968927  4444 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:41:39.968935  4444 net.cpp:165] Memory required for data: 8074800
I1127 10:41:39.968942  4444 layer_factory.hpp:76] Creating layer ip2
I1127 10:41:39.968961  4444 net.cpp:106] Creating Layer ip2
I1127 10:41:39.968971  4444 net.cpp:454] ip2 <- ip1
I1127 10:41:39.968982  4444 net.cpp:411] ip2 -> ip2
I1127 10:41:39.969151  4444 net.cpp:150] Setting up ip2
I1127 10:41:39.969166  4444 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:41:39.969173  4444 net.cpp:165] Memory required for data: 8078800
I1127 10:41:39.969184  4444 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:41:39.969194  4444 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:41:39.969203  4444 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:41:39.969215  4444 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:41:39.969230  4444 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:41:39.969279  4444 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:41:39.969291  4444 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:41:39.969301  4444 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:41:39.969310  4444 net.cpp:165] Memory required for data: 8086800
I1127 10:41:39.969317  4444 layer_factory.hpp:76] Creating layer accuracy
I1127 10:41:39.969332  4444 net.cpp:106] Creating Layer accuracy
I1127 10:41:39.969341  4444 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:41:39.969352  4444 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:41:39.969364  4444 net.cpp:411] accuracy -> accuracy
I1127 10:41:39.969382  4444 net.cpp:150] Setting up accuracy
I1127 10:41:39.969393  4444 net.cpp:157] Top shape: (1)
I1127 10:41:39.969401  4444 net.cpp:165] Memory required for data: 8086804
I1127 10:41:39.969409  4444 layer_factory.hpp:76] Creating layer loss
I1127 10:41:39.969421  4444 net.cpp:106] Creating Layer loss
I1127 10:41:39.969430  4444 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:41:39.969440  4444 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:41:39.969451  4444 net.cpp:411] loss -> loss
I1127 10:41:39.969470  4444 layer_factory.hpp:76] Creating layer loss
I1127 10:41:39.969647  4444 net.cpp:150] Setting up loss
I1127 10:41:39.969661  4444 net.cpp:157] Top shape: (1)
I1127 10:41:39.969671  4444 net.cpp:160]     with loss weight 1
I1127 10:41:39.969694  4444 net.cpp:165] Memory required for data: 8086808
I1127 10:41:39.969707  4444 net.cpp:226] loss needs backward computation.
I1127 10:41:39.969723  4444 net.cpp:228] accuracy does not need backward computation.
I1127 10:41:39.969734  4444 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:41:39.969745  4444 net.cpp:226] ip2 needs backward computation.
I1127 10:41:39.969753  4444 net.cpp:226] relu1 needs backward computation.
I1127 10:41:39.969760  4444 net.cpp:226] ip1 needs backward computation.
I1127 10:41:39.969769  4444 net.cpp:226] pool2 needs backward computation.
I1127 10:41:39.969777  4444 net.cpp:226] conv2 needs backward computation.
I1127 10:41:39.969786  4444 net.cpp:226] pool1 needs backward computation.
I1127 10:41:39.969795  4444 net.cpp:226] conv1 needs backward computation.
I1127 10:41:39.969815  4444 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:41:39.969825  4444 net.cpp:228] mnist does not need backward computation.
I1127 10:41:39.969832  4444 net.cpp:270] This network produces output accuracy
I1127 10:41:39.969841  4444 net.cpp:270] This network produces output loss
I1127 10:41:39.969863  4444 net.cpp:283] Network initialization done.
I1127 10:41:39.970010  4444 solver.cpp:59] Solver scaffolding done.
I1127 10:41:39.970432  4444 caffe.cpp:212] Starting Optimization
I1127 10:41:39.970448  4444 solver.cpp:287] Solving LeNet
I1127 10:41:39.970454  4444 solver.cpp:288] Learning Rate Policy: inv
I1127 10:41:39.971379  4444 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:41:39.972347  4444 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:41:42.110772  4444 solver.cpp:408]     Test net output #0: accuracy = 0.1123
I1127 10:41:42.110896  4444 solver.cpp:408]     Test net output #1: loss = 2.37942 (* 1 = 2.37942 loss)
I1127 10:41:42.146342  4444 solver.cpp:236] Iteration 0, loss = 2.33811
I1127 10:41:42.146461  4444 solver.cpp:252]     Train net output #0: loss = 2.33811 (* 1 = 2.33811 loss)
I1127 10:41:42.146491  4444 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:41:55.011211  4444 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:41:57.587728  4444 solver.cpp:408]     Test net output #0: accuracy = 0.9736
I1127 10:41:57.587848  4444 solver.cpp:408]     Test net output #1: loss = 0.0843104 (* 1 = 0.0843104 loss)
I1127 10:41:57.597620  4444 solver.cpp:236] Iteration 500, loss = 0.10947
I1127 10:41:57.597666  4444 solver.cpp:252]     Train net output #0: loss = 0.10947 (* 1 = 0.10947 loss)
I1127 10:41:57.597676  4444 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:42:09.049202  4444 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:42:09.067258  4444 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:42:09.080137  4444 solver.cpp:320] Iteration 1000, loss = 0.0914424
I1127 10:42:09.080235  4444 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:42:11.682553  4444 solver.cpp:408]     Test net output #0: accuracy = 0.9805
I1127 10:42:11.694103  4444 solver.cpp:408]     Test net output #1: loss = 0.0573552 (* 1 = 0.0573552 loss)
I1127 10:42:11.694110  4444 solver.cpp:325] Optimization Done.
I1127 10:42:11.694114  4444 caffe.cpp:215] Optimization Done.
I1127 10:42:11.759647  4471 caffe.cpp:184] Using GPUs 0
I1127 10:42:12.244432  4471 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:42:12.244760  4471 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:42:12.245331  4471 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:42:12.245393  4471 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:42:12.245630  4471 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:42:12.245841  4471 layer_factory.hpp:76] Creating layer mnist
I1127 10:42:12.246543  4471 net.cpp:106] Creating Layer mnist
I1127 10:42:12.246604  4471 net.cpp:411] mnist -> data
I1127 10:42:12.246646  4471 net.cpp:411] mnist -> label
I1127 10:42:12.248023  4474 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:42:12.259924  4471 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:42:12.262048  4471 net.cpp:150] Setting up mnist
I1127 10:42:12.262133  4471 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:42:12.262169  4471 net.cpp:157] Top shape: 64 (64)
I1127 10:42:12.262181  4471 net.cpp:165] Memory required for data: 200960
I1127 10:42:12.262203  4471 layer_factory.hpp:76] Creating layer conv1
I1127 10:42:12.262248  4471 net.cpp:106] Creating Layer conv1
I1127 10:42:12.262264  4471 net.cpp:454] conv1 <- data
I1127 10:42:12.262290  4471 net.cpp:411] conv1 -> conv1
I1127 10:42:12.263600  4471 net.cpp:150] Setting up conv1
I1127 10:42:12.263705  4471 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:42:12.263725  4471 net.cpp:165] Memory required for data: 3150080
I1127 10:42:12.263774  4471 layer_factory.hpp:76] Creating layer pool1
I1127 10:42:12.263810  4471 net.cpp:106] Creating Layer pool1
I1127 10:42:12.263825  4471 net.cpp:454] pool1 <- conv1
I1127 10:42:12.263850  4471 net.cpp:411] pool1 -> pool1
I1127 10:42:12.264011  4471 net.cpp:150] Setting up pool1
I1127 10:42:12.264034  4471 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:42:12.264041  4471 net.cpp:165] Memory required for data: 3887360
I1127 10:42:12.264050  4471 layer_factory.hpp:76] Creating layer conv2
I1127 10:42:12.264070  4471 net.cpp:106] Creating Layer conv2
I1127 10:42:12.264081  4471 net.cpp:454] conv2 <- pool1
I1127 10:42:12.264094  4471 net.cpp:411] conv2 -> conv2
I1127 10:42:12.264688  4471 net.cpp:150] Setting up conv2
I1127 10:42:12.264722  4471 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:42:12.264729  4471 net.cpp:165] Memory required for data: 4706560
I1127 10:42:12.264745  4471 layer_factory.hpp:76] Creating layer pool2
I1127 10:42:12.264766  4471 net.cpp:106] Creating Layer pool2
I1127 10:42:12.264775  4471 net.cpp:454] pool2 <- conv2
I1127 10:42:12.264786  4471 net.cpp:411] pool2 -> pool2
I1127 10:42:12.264830  4471 net.cpp:150] Setting up pool2
I1127 10:42:12.264840  4471 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:42:12.264847  4471 net.cpp:165] Memory required for data: 4911360
I1127 10:42:12.264853  4471 layer_factory.hpp:76] Creating layer ip1
I1127 10:42:12.264868  4471 net.cpp:106] Creating Layer ip1
I1127 10:42:12.264875  4471 net.cpp:454] ip1 <- pool2
I1127 10:42:12.264886  4471 net.cpp:411] ip1 -> ip1
I1127 10:42:12.269789  4471 net.cpp:150] Setting up ip1
I1127 10:42:12.269845  4471 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:42:12.269853  4471 net.cpp:165] Memory required for data: 5039360
I1127 10:42:12.269887  4471 layer_factory.hpp:76] Creating layer relu1
I1127 10:42:12.269906  4471 net.cpp:106] Creating Layer relu1
I1127 10:42:12.269913  4471 net.cpp:454] relu1 <- ip1
I1127 10:42:12.269923  4471 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:42:12.269940  4471 net.cpp:150] Setting up relu1
I1127 10:42:12.269949  4471 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:42:12.269955  4471 net.cpp:165] Memory required for data: 5167360
I1127 10:42:12.269961  4471 layer_factory.hpp:76] Creating layer ip2
I1127 10:42:12.269974  4471 net.cpp:106] Creating Layer ip2
I1127 10:42:12.269981  4471 net.cpp:454] ip2 <- ip1
I1127 10:42:12.269990  4471 net.cpp:411] ip2 -> ip2
I1127 10:42:12.270680  4471 net.cpp:150] Setting up ip2
I1127 10:42:12.270706  4471 net.cpp:157] Top shape: 64 10 (640)
I1127 10:42:12.270714  4471 net.cpp:165] Memory required for data: 5169920
I1127 10:42:12.270732  4471 layer_factory.hpp:76] Creating layer loss
I1127 10:42:12.270757  4471 net.cpp:106] Creating Layer loss
I1127 10:42:12.270776  4471 net.cpp:454] loss <- ip2
I1127 10:42:12.270795  4471 net.cpp:454] loss <- label
I1127 10:42:12.270824  4471 net.cpp:411] loss -> loss
I1127 10:42:12.270872  4471 layer_factory.hpp:76] Creating layer loss
I1127 10:42:12.271049  4471 net.cpp:150] Setting up loss
I1127 10:42:12.271067  4471 net.cpp:157] Top shape: (1)
I1127 10:42:12.271075  4471 net.cpp:160]     with loss weight 1
I1127 10:42:12.271136  4471 net.cpp:165] Memory required for data: 5169924
I1127 10:42:12.271142  4471 net.cpp:226] loss needs backward computation.
I1127 10:42:12.271150  4471 net.cpp:226] ip2 needs backward computation.
I1127 10:42:12.271158  4471 net.cpp:226] relu1 needs backward computation.
I1127 10:42:12.271167  4471 net.cpp:226] ip1 needs backward computation.
I1127 10:42:12.271174  4471 net.cpp:226] pool2 needs backward computation.
I1127 10:42:12.271183  4471 net.cpp:226] conv2 needs backward computation.
I1127 10:42:12.271191  4471 net.cpp:226] pool1 needs backward computation.
I1127 10:42:12.271200  4471 net.cpp:226] conv1 needs backward computation.
I1127 10:42:12.271209  4471 net.cpp:228] mnist does not need backward computation.
I1127 10:42:12.271216  4471 net.cpp:270] This network produces output loss
I1127 10:42:12.271234  4471 net.cpp:283] Network initialization done.
I1127 10:42:12.271756  4471 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:42:12.271826  4471 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:42:12.272066  4471 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:42:12.272192  4471 layer_factory.hpp:76] Creating layer mnist
I1127 10:42:12.272353  4471 net.cpp:106] Creating Layer mnist
I1127 10:42:12.272368  4471 net.cpp:411] mnist -> data
I1127 10:42:12.272388  4471 net.cpp:411] mnist -> label
I1127 10:42:12.273764  4476 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:42:12.274047  4471 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:42:12.276099  4471 net.cpp:150] Setting up mnist
I1127 10:42:12.276140  4471 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:42:12.276151  4471 net.cpp:157] Top shape: 100 (100)
I1127 10:42:12.276157  4471 net.cpp:165] Memory required for data: 314000
I1127 10:42:12.276170  4471 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:42:12.276193  4471 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:42:12.276201  4471 net.cpp:454] label_mnist_1_split <- label
I1127 10:42:12.276214  4471 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:42:12.276233  4471 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:42:12.276298  4471 net.cpp:150] Setting up label_mnist_1_split
I1127 10:42:12.276311  4471 net.cpp:157] Top shape: 100 (100)
I1127 10:42:12.276319  4471 net.cpp:157] Top shape: 100 (100)
I1127 10:42:12.276326  4471 net.cpp:165] Memory required for data: 314800
I1127 10:42:12.276334  4471 layer_factory.hpp:76] Creating layer conv1
I1127 10:42:12.276351  4471 net.cpp:106] Creating Layer conv1
I1127 10:42:12.276360  4471 net.cpp:454] conv1 <- data
I1127 10:42:12.276371  4471 net.cpp:411] conv1 -> conv1
I1127 10:42:12.276602  4471 net.cpp:150] Setting up conv1
I1127 10:42:12.276618  4471 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:42:12.276624  4471 net.cpp:165] Memory required for data: 4922800
I1127 10:42:12.276639  4471 layer_factory.hpp:76] Creating layer pool1
I1127 10:42:12.276651  4471 net.cpp:106] Creating Layer pool1
I1127 10:42:12.276659  4471 net.cpp:454] pool1 <- conv1
I1127 10:42:12.276684  4471 net.cpp:411] pool1 -> pool1
I1127 10:42:12.276726  4471 net.cpp:150] Setting up pool1
I1127 10:42:12.276737  4471 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:42:12.276743  4471 net.cpp:165] Memory required for data: 6074800
I1127 10:42:12.276751  4471 layer_factory.hpp:76] Creating layer conv2
I1127 10:42:12.276765  4471 net.cpp:106] Creating Layer conv2
I1127 10:42:12.276772  4471 net.cpp:454] conv2 <- pool1
I1127 10:42:12.276782  4471 net.cpp:411] conv2 -> conv2
I1127 10:42:12.277470  4471 net.cpp:150] Setting up conv2
I1127 10:42:12.277554  4471 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:42:12.277565  4471 net.cpp:165] Memory required for data: 7354800
I1127 10:42:12.277595  4471 layer_factory.hpp:76] Creating layer pool2
I1127 10:42:12.277627  4471 net.cpp:106] Creating Layer pool2
I1127 10:42:12.277647  4471 net.cpp:454] pool2 <- conv2
I1127 10:42:12.277673  4471 net.cpp:411] pool2 -> pool2
I1127 10:42:12.277739  4471 net.cpp:150] Setting up pool2
I1127 10:42:12.277752  4471 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:42:12.277760  4471 net.cpp:165] Memory required for data: 7674800
I1127 10:42:12.277770  4471 layer_factory.hpp:76] Creating layer ip1
I1127 10:42:12.277796  4471 net.cpp:106] Creating Layer ip1
I1127 10:42:12.277807  4471 net.cpp:454] ip1 <- pool2
I1127 10:42:12.277828  4471 net.cpp:411] ip1 -> ip1
I1127 10:42:12.283048  4471 net.cpp:150] Setting up ip1
I1127 10:42:12.283190  4471 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:42:12.283203  4471 net.cpp:165] Memory required for data: 7874800
I1127 10:42:12.283251  4471 layer_factory.hpp:76] Creating layer relu1
I1127 10:42:12.283298  4471 net.cpp:106] Creating Layer relu1
I1127 10:42:12.283325  4471 net.cpp:454] relu1 <- ip1
I1127 10:42:12.283363  4471 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:42:12.283407  4471 net.cpp:150] Setting up relu1
I1127 10:42:12.283429  4471 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:42:12.283444  4471 net.cpp:165] Memory required for data: 8074800
I1127 10:42:12.283457  4471 layer_factory.hpp:76] Creating layer ip2
I1127 10:42:12.283491  4471 net.cpp:106] Creating Layer ip2
I1127 10:42:12.283507  4471 net.cpp:454] ip2 <- ip1
I1127 10:42:12.283536  4471 net.cpp:411] ip2 -> ip2
I1127 10:42:12.283942  4471 net.cpp:150] Setting up ip2
I1127 10:42:12.283967  4471 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:42:12.283975  4471 net.cpp:165] Memory required for data: 8078800
I1127 10:42:12.283993  4471 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:42:12.284010  4471 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:42:12.284019  4471 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:42:12.284029  4471 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:42:12.284044  4471 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:42:12.284102  4471 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:42:12.284118  4471 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:42:12.284128  4471 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:42:12.284137  4471 net.cpp:165] Memory required for data: 8086800
I1127 10:42:12.284144  4471 layer_factory.hpp:76] Creating layer accuracy
I1127 10:42:12.284162  4471 net.cpp:106] Creating Layer accuracy
I1127 10:42:12.284171  4471 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:42:12.284180  4471 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:42:12.284191  4471 net.cpp:411] accuracy -> accuracy
I1127 10:42:12.284210  4471 net.cpp:150] Setting up accuracy
I1127 10:42:12.284224  4471 net.cpp:157] Top shape: (1)
I1127 10:42:12.284231  4471 net.cpp:165] Memory required for data: 8086804
I1127 10:42:12.284240  4471 layer_factory.hpp:76] Creating layer loss
I1127 10:42:12.284255  4471 net.cpp:106] Creating Layer loss
I1127 10:42:12.284266  4471 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:42:12.284276  4471 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:42:12.284287  4471 net.cpp:411] loss -> loss
I1127 10:42:12.284307  4471 layer_factory.hpp:76] Creating layer loss
I1127 10:42:12.284508  4471 net.cpp:150] Setting up loss
I1127 10:42:12.284528  4471 net.cpp:157] Top shape: (1)
I1127 10:42:12.284534  4471 net.cpp:160]     with loss weight 1
I1127 10:42:12.284565  4471 net.cpp:165] Memory required for data: 8086808
I1127 10:42:12.284574  4471 net.cpp:226] loss needs backward computation.
I1127 10:42:12.284586  4471 net.cpp:228] accuracy does not need backward computation.
I1127 10:42:12.284595  4471 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:42:12.284603  4471 net.cpp:226] ip2 needs backward computation.
I1127 10:42:12.284611  4471 net.cpp:226] relu1 needs backward computation.
I1127 10:42:12.284621  4471 net.cpp:226] ip1 needs backward computation.
I1127 10:42:12.284628  4471 net.cpp:226] pool2 needs backward computation.
I1127 10:42:12.284639  4471 net.cpp:226] conv2 needs backward computation.
I1127 10:42:12.284648  4471 net.cpp:226] pool1 needs backward computation.
I1127 10:42:12.284657  4471 net.cpp:226] conv1 needs backward computation.
I1127 10:42:12.284668  4471 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:42:12.284677  4471 net.cpp:228] mnist does not need backward computation.
I1127 10:42:12.284685  4471 net.cpp:270] This network produces output accuracy
I1127 10:42:12.284694  4471 net.cpp:270] This network produces output loss
I1127 10:42:12.284716  4471 net.cpp:283] Network initialization done.
I1127 10:42:12.284842  4471 solver.cpp:59] Solver scaffolding done.
I1127 10:42:12.285202  4471 caffe.cpp:212] Starting Optimization
I1127 10:42:12.285223  4471 solver.cpp:287] Solving LeNet
I1127 10:42:12.285233  4471 solver.cpp:288] Learning Rate Policy: inv
I1127 10:42:12.286273  4471 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:42:13.374979  4471 solver.cpp:408]     Test net output #0: accuracy = 0.1197
I1127 10:42:13.375030  4471 solver.cpp:408]     Test net output #1: loss = 2.29893 (* 1 = 2.29893 loss)
I1127 10:42:13.385499  4471 solver.cpp:236] Iteration 0, loss = 2.31112
I1127 10:42:13.385552  4471 solver.cpp:252]     Train net output #0: loss = 2.31112 (* 1 = 2.31112 loss)
I1127 10:42:13.385570  4471 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:42:26.796988  4471 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:42:29.937779  4471 solver.cpp:408]     Test net output #0: accuracy = 0.9716
I1127 10:42:29.937870  4471 solver.cpp:408]     Test net output #1: loss = 0.0873183 (* 1 = 0.0873183 loss)
I1127 10:42:29.950925  4471 solver.cpp:236] Iteration 500, loss = 0.10446
I1127 10:42:29.951056  4471 solver.cpp:252]     Train net output #0: loss = 0.10446 (* 1 = 0.10446 loss)
I1127 10:42:29.951078  4471 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:42:41.455657  4471 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:42:41.510992  4471 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:42:41.521767  4471 solver.cpp:320] Iteration 1000, loss = 0.130386
I1127 10:42:41.521878  4471 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:42:41.859021  4471 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:42:44.170364  4471 solver.cpp:408]     Test net output #0: accuracy = 0.9797
I1127 10:42:44.170404  4471 solver.cpp:408]     Test net output #1: loss = 0.0599055 (* 1 = 0.0599055 loss)
I1127 10:42:44.170411  4471 solver.cpp:325] Optimization Done.
I1127 10:42:44.170416  4471 caffe.cpp:215] Optimization Done.
I1127 10:42:44.235879  4499 caffe.cpp:184] Using GPUs 0
I1127 10:42:44.698942  4499 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:42:44.699149  4499 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:42:44.699565  4499 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:42:44.699586  4499 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:42:44.699687  4499 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:42:44.699754  4499 layer_factory.hpp:76] Creating layer mnist
I1127 10:42:44.700155  4499 net.cpp:106] Creating Layer mnist
I1127 10:42:44.700171  4499 net.cpp:411] mnist -> data
I1127 10:42:44.700198  4499 net.cpp:411] mnist -> label
I1127 10:42:44.701284  4502 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:42:44.711083  4499 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:42:44.713248  4499 net.cpp:150] Setting up mnist
I1127 10:42:44.713369  4499 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:42:44.713425  4499 net.cpp:157] Top shape: 64 (64)
I1127 10:42:44.713457  4499 net.cpp:165] Memory required for data: 200960
I1127 10:42:44.713505  4499 layer_factory.hpp:76] Creating layer conv1
I1127 10:42:44.713584  4499 net.cpp:106] Creating Layer conv1
I1127 10:42:44.713629  4499 net.cpp:454] conv1 <- data
I1127 10:42:44.713682  4499 net.cpp:411] conv1 -> conv1
I1127 10:42:44.715646  4499 net.cpp:150] Setting up conv1
I1127 10:42:44.715737  4499 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:42:44.715764  4499 net.cpp:165] Memory required for data: 3150080
I1127 10:42:44.715816  4499 layer_factory.hpp:76] Creating layer pool1
I1127 10:42:44.715867  4499 net.cpp:106] Creating Layer pool1
I1127 10:42:44.715890  4499 net.cpp:454] pool1 <- conv1
I1127 10:42:44.715919  4499 net.cpp:411] pool1 -> pool1
I1127 10:42:44.716096  4499 net.cpp:150] Setting up pool1
I1127 10:42:44.716116  4499 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:42:44.716125  4499 net.cpp:165] Memory required for data: 3887360
I1127 10:42:44.716133  4499 layer_factory.hpp:76] Creating layer conv2
I1127 10:42:44.716153  4499 net.cpp:106] Creating Layer conv2
I1127 10:42:44.716163  4499 net.cpp:454] conv2 <- pool1
I1127 10:42:44.716179  4499 net.cpp:411] conv2 -> conv2
I1127 10:42:44.716809  4499 net.cpp:150] Setting up conv2
I1127 10:42:44.716847  4499 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:42:44.716859  4499 net.cpp:165] Memory required for data: 4706560
I1127 10:42:44.716883  4499 layer_factory.hpp:76] Creating layer pool2
I1127 10:42:44.716902  4499 net.cpp:106] Creating Layer pool2
I1127 10:42:44.716912  4499 net.cpp:454] pool2 <- conv2
I1127 10:42:44.716924  4499 net.cpp:411] pool2 -> pool2
I1127 10:42:44.716990  4499 net.cpp:150] Setting up pool2
I1127 10:42:44.717005  4499 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:42:44.717011  4499 net.cpp:165] Memory required for data: 4911360
I1127 10:42:44.717020  4499 layer_factory.hpp:76] Creating layer ip1
I1127 10:42:44.717042  4499 net.cpp:106] Creating Layer ip1
I1127 10:42:44.717051  4499 net.cpp:454] ip1 <- pool2
I1127 10:42:44.717064  4499 net.cpp:411] ip1 -> ip1
I1127 10:42:44.720441  4499 net.cpp:150] Setting up ip1
I1127 10:42:44.720497  4499 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:42:44.720510  4499 net.cpp:165] Memory required for data: 5039360
I1127 10:42:44.720535  4499 layer_factory.hpp:76] Creating layer relu1
I1127 10:42:44.720554  4499 net.cpp:106] Creating Layer relu1
I1127 10:42:44.720566  4499 net.cpp:454] relu1 <- ip1
I1127 10:42:44.720582  4499 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:42:44.720602  4499 net.cpp:150] Setting up relu1
I1127 10:42:44.720610  4499 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:42:44.720616  4499 net.cpp:165] Memory required for data: 5167360
I1127 10:42:44.720633  4499 layer_factory.hpp:76] Creating layer ip2
I1127 10:42:44.720644  4499 net.cpp:106] Creating Layer ip2
I1127 10:42:44.720650  4499 net.cpp:454] ip2 <- ip1
I1127 10:42:44.720659  4499 net.cpp:411] ip2 -> ip2
I1127 10:42:44.721748  4499 net.cpp:150] Setting up ip2
I1127 10:42:44.721794  4499 net.cpp:157] Top shape: 64 10 (640)
I1127 10:42:44.721807  4499 net.cpp:165] Memory required for data: 5169920
I1127 10:42:44.721819  4499 layer_factory.hpp:76] Creating layer loss
I1127 10:42:44.721835  4499 net.cpp:106] Creating Layer loss
I1127 10:42:44.721843  4499 net.cpp:454] loss <- ip2
I1127 10:42:44.721851  4499 net.cpp:454] loss <- label
I1127 10:42:44.721861  4499 net.cpp:411] loss -> loss
I1127 10:42:44.721884  4499 layer_factory.hpp:76] Creating layer loss
I1127 10:42:44.722008  4499 net.cpp:150] Setting up loss
I1127 10:42:44.722018  4499 net.cpp:157] Top shape: (1)
I1127 10:42:44.722023  4499 net.cpp:160]     with loss weight 1
I1127 10:42:44.722044  4499 net.cpp:165] Memory required for data: 5169924
I1127 10:42:44.722050  4499 net.cpp:226] loss needs backward computation.
I1127 10:42:44.722056  4499 net.cpp:226] ip2 needs backward computation.
I1127 10:42:44.722061  4499 net.cpp:226] relu1 needs backward computation.
I1127 10:42:44.722066  4499 net.cpp:226] ip1 needs backward computation.
I1127 10:42:44.722071  4499 net.cpp:226] pool2 needs backward computation.
I1127 10:42:44.722075  4499 net.cpp:226] conv2 needs backward computation.
I1127 10:42:44.722081  4499 net.cpp:226] pool1 needs backward computation.
I1127 10:42:44.722086  4499 net.cpp:226] conv1 needs backward computation.
I1127 10:42:44.722091  4499 net.cpp:228] mnist does not need backward computation.
I1127 10:42:44.722095  4499 net.cpp:270] This network produces output loss
I1127 10:42:44.722107  4499 net.cpp:283] Network initialization done.
I1127 10:42:44.722460  4499 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:42:44.722503  4499 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:42:44.722674  4499 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:42:44.722753  4499 layer_factory.hpp:76] Creating layer mnist
I1127 10:42:44.722856  4499 net.cpp:106] Creating Layer mnist
I1127 10:42:44.722865  4499 net.cpp:411] mnist -> data
I1127 10:42:44.722877  4499 net.cpp:411] mnist -> label
I1127 10:42:44.727865  4504 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:42:44.730597  4499 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:42:44.732812  4499 net.cpp:150] Setting up mnist
I1127 10:42:44.732916  4499 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:42:44.732951  4499 net.cpp:157] Top shape: 100 (100)
I1127 10:42:44.732971  4499 net.cpp:165] Memory required for data: 314000
I1127 10:42:44.733000  4499 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:42:44.733047  4499 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:42:44.733072  4499 net.cpp:454] label_mnist_1_split <- label
I1127 10:42:44.733098  4499 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:42:44.733130  4499 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:42:44.733232  4499 net.cpp:150] Setting up label_mnist_1_split
I1127 10:42:44.733263  4499 net.cpp:157] Top shape: 100 (100)
I1127 10:42:44.733274  4499 net.cpp:157] Top shape: 100 (100)
I1127 10:42:44.733285  4499 net.cpp:165] Memory required for data: 314800
I1127 10:42:44.733294  4499 layer_factory.hpp:76] Creating layer conv1
I1127 10:42:44.733321  4499 net.cpp:106] Creating Layer conv1
I1127 10:42:44.733331  4499 net.cpp:454] conv1 <- data
I1127 10:42:44.733346  4499 net.cpp:411] conv1 -> conv1
I1127 10:42:44.733652  4499 net.cpp:150] Setting up conv1
I1127 10:42:44.733670  4499 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:42:44.733677  4499 net.cpp:165] Memory required for data: 4922800
I1127 10:42:44.733695  4499 layer_factory.hpp:76] Creating layer pool1
I1127 10:42:44.733710  4499 net.cpp:106] Creating Layer pool1
I1127 10:42:44.733716  4499 net.cpp:454] pool1 <- conv1
I1127 10:42:44.733748  4499 net.cpp:411] pool1 -> pool1
I1127 10:42:44.733798  4499 net.cpp:150] Setting up pool1
I1127 10:42:44.733809  4499 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:42:44.733816  4499 net.cpp:165] Memory required for data: 6074800
I1127 10:42:44.733824  4499 layer_factory.hpp:76] Creating layer conv2
I1127 10:42:44.733840  4499 net.cpp:106] Creating Layer conv2
I1127 10:42:44.733850  4499 net.cpp:454] conv2 <- pool1
I1127 10:42:44.733860  4499 net.cpp:411] conv2 -> conv2
I1127 10:42:44.739302  4499 net.cpp:150] Setting up conv2
I1127 10:42:44.739368  4499 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:42:44.739384  4499 net.cpp:165] Memory required for data: 7354800
I1127 10:42:44.739410  4499 layer_factory.hpp:76] Creating layer pool2
I1127 10:42:44.739435  4499 net.cpp:106] Creating Layer pool2
I1127 10:42:44.739446  4499 net.cpp:454] pool2 <- conv2
I1127 10:42:44.739461  4499 net.cpp:411] pool2 -> pool2
I1127 10:42:44.739547  4499 net.cpp:150] Setting up pool2
I1127 10:42:44.739565  4499 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:42:44.739573  4499 net.cpp:165] Memory required for data: 7674800
I1127 10:42:44.739581  4499 layer_factory.hpp:76] Creating layer ip1
I1127 10:42:44.739599  4499 net.cpp:106] Creating Layer ip1
I1127 10:42:44.739609  4499 net.cpp:454] ip1 <- pool2
I1127 10:42:44.739626  4499 net.cpp:411] ip1 -> ip1
I1127 10:42:44.744225  4499 net.cpp:150] Setting up ip1
I1127 10:42:44.744324  4499 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:42:44.744348  4499 net.cpp:165] Memory required for data: 7874800
I1127 10:42:44.744388  4499 layer_factory.hpp:76] Creating layer relu1
I1127 10:42:44.744423  4499 net.cpp:106] Creating Layer relu1
I1127 10:42:44.744441  4499 net.cpp:454] relu1 <- ip1
I1127 10:42:44.744458  4499 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:42:44.744485  4499 net.cpp:150] Setting up relu1
I1127 10:42:44.744503  4499 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:42:44.744527  4499 net.cpp:165] Memory required for data: 8074800
I1127 10:42:44.744536  4499 layer_factory.hpp:76] Creating layer ip2
I1127 10:42:44.744560  4499 net.cpp:106] Creating Layer ip2
I1127 10:42:44.744570  4499 net.cpp:454] ip2 <- ip1
I1127 10:42:44.744583  4499 net.cpp:411] ip2 -> ip2
I1127 10:42:44.744904  4499 net.cpp:150] Setting up ip2
I1127 10:42:44.744930  4499 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:42:44.744938  4499 net.cpp:165] Memory required for data: 8078800
I1127 10:42:44.744957  4499 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:42:44.744977  4499 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:42:44.744989  4499 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:42:44.745003  4499 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:42:44.745019  4499 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:42:44.745126  4499 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:42:44.745146  4499 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:42:44.745154  4499 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:42:44.745159  4499 net.cpp:165] Memory required for data: 8086800
I1127 10:42:44.745167  4499 layer_factory.hpp:76] Creating layer accuracy
I1127 10:42:44.745180  4499 net.cpp:106] Creating Layer accuracy
I1127 10:42:44.745187  4499 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:42:44.745196  4499 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:42:44.745209  4499 net.cpp:411] accuracy -> accuracy
I1127 10:42:44.745228  4499 net.cpp:150] Setting up accuracy
I1127 10:42:44.745236  4499 net.cpp:157] Top shape: (1)
I1127 10:42:44.745241  4499 net.cpp:165] Memory required for data: 8086804
I1127 10:42:44.745247  4499 layer_factory.hpp:76] Creating layer loss
I1127 10:42:44.745265  4499 net.cpp:106] Creating Layer loss
I1127 10:42:44.745271  4499 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:42:44.745278  4499 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:42:44.745285  4499 net.cpp:411] loss -> loss
I1127 10:42:44.745299  4499 layer_factory.hpp:76] Creating layer loss
I1127 10:42:44.745499  4499 net.cpp:150] Setting up loss
I1127 10:42:44.745517  4499 net.cpp:157] Top shape: (1)
I1127 10:42:44.745522  4499 net.cpp:160]     with loss weight 1
I1127 10:42:44.745544  4499 net.cpp:165] Memory required for data: 8086808
I1127 10:42:44.745550  4499 net.cpp:226] loss needs backward computation.
I1127 10:42:44.745563  4499 net.cpp:228] accuracy does not need backward computation.
I1127 10:42:44.745570  4499 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:42:44.745576  4499 net.cpp:226] ip2 needs backward computation.
I1127 10:42:44.745580  4499 net.cpp:226] relu1 needs backward computation.
I1127 10:42:44.745585  4499 net.cpp:226] ip1 needs backward computation.
I1127 10:42:44.745591  4499 net.cpp:226] pool2 needs backward computation.
I1127 10:42:44.745597  4499 net.cpp:226] conv2 needs backward computation.
I1127 10:42:44.745602  4499 net.cpp:226] pool1 needs backward computation.
I1127 10:42:44.745607  4499 net.cpp:226] conv1 needs backward computation.
I1127 10:42:44.745614  4499 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:42:44.745620  4499 net.cpp:228] mnist does not need backward computation.
I1127 10:42:44.745625  4499 net.cpp:270] This network produces output accuracy
I1127 10:42:44.745630  4499 net.cpp:270] This network produces output loss
I1127 10:42:44.745651  4499 net.cpp:283] Network initialization done.
I1127 10:42:44.745749  4499 solver.cpp:59] Solver scaffolding done.
I1127 10:42:44.746106  4499 caffe.cpp:212] Starting Optimization
I1127 10:42:44.746125  4499 solver.cpp:287] Solving LeNet
I1127 10:42:44.746131  4499 solver.cpp:288] Learning Rate Policy: inv
I1127 10:42:44.747534  4499 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:42:45.830454  4499 solver.cpp:408]     Test net output #0: accuracy = 0.0785
I1127 10:42:45.830605  4499 solver.cpp:408]     Test net output #1: loss = 2.3636 (* 1 = 2.3636 loss)
I1127 10:42:45.842824  4499 solver.cpp:236] Iteration 0, loss = 2.38653
I1127 10:42:45.842947  4499 solver.cpp:252]     Train net output #0: loss = 2.38653 (* 1 = 2.38653 loss)
I1127 10:42:45.842989  4499 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:42:59.273440  4499 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:43:02.282112  4499 solver.cpp:408]     Test net output #0: accuracy = 0.9736
I1127 10:43:02.282232  4499 solver.cpp:408]     Test net output #1: loss = 0.0843553 (* 1 = 0.0843553 loss)
I1127 10:43:02.311875  4499 solver.cpp:236] Iteration 500, loss = 0.0961055
I1127 10:43:02.311940  4499 solver.cpp:252]     Train net output #0: loss = 0.0961055 (* 1 = 0.0961055 loss)
I1127 10:43:02.311951  4499 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:43:08.067962  4499 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:43:13.938925  4499 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:43:13.953043  4499 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:43:13.963693  4499 solver.cpp:320] Iteration 1000, loss = 0.115746
I1127 10:43:13.963739  4499 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:43:16.150002  4499 solver.cpp:408]     Test net output #0: accuracy = 0.9811
I1127 10:43:16.150105  4499 solver.cpp:408]     Test net output #1: loss = 0.0612965 (* 1 = 0.0612965 loss)
I1127 10:43:16.150113  4499 solver.cpp:325] Optimization Done.
I1127 10:43:16.150118  4499 caffe.cpp:215] Optimization Done.
I1127 10:43:16.215971  4525 caffe.cpp:184] Using GPUs 0
I1127 10:43:16.600471  4525 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:43:16.600611  4525 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:43:16.600996  4525 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:43:16.601019  4525 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:43:16.601155  4525 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:43:16.601238  4525 layer_factory.hpp:76] Creating layer mnist
I1127 10:43:16.601704  4525 net.cpp:106] Creating Layer mnist
I1127 10:43:16.601721  4525 net.cpp:411] mnist -> data
I1127 10:43:16.601752  4525 net.cpp:411] mnist -> label
I1127 10:43:16.602502  4528 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:43:16.636417  4525 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:43:16.643589  4525 net.cpp:150] Setting up mnist
I1127 10:43:16.643615  4525 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:43:16.643626  4525 net.cpp:157] Top shape: 64 (64)
I1127 10:43:16.643635  4525 net.cpp:165] Memory required for data: 200960
I1127 10:43:16.643648  4525 layer_factory.hpp:76] Creating layer conv1
I1127 10:43:16.643669  4525 net.cpp:106] Creating Layer conv1
I1127 10:43:16.643681  4525 net.cpp:454] conv1 <- data
I1127 10:43:16.643699  4525 net.cpp:411] conv1 -> conv1
I1127 10:43:16.644496  4525 net.cpp:150] Setting up conv1
I1127 10:43:16.644512  4525 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:43:16.644521  4525 net.cpp:165] Memory required for data: 3150080
I1127 10:43:16.644541  4525 layer_factory.hpp:76] Creating layer pool1
I1127 10:43:16.644554  4525 net.cpp:106] Creating Layer pool1
I1127 10:43:16.644563  4525 net.cpp:454] pool1 <- conv1
I1127 10:43:16.644573  4525 net.cpp:411] pool1 -> pool1
I1127 10:43:16.644773  4525 net.cpp:150] Setting up pool1
I1127 10:43:16.644786  4525 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:43:16.644795  4525 net.cpp:165] Memory required for data: 3887360
I1127 10:43:16.644811  4525 layer_factory.hpp:76] Creating layer conv2
I1127 10:43:16.644824  4525 net.cpp:106] Creating Layer conv2
I1127 10:43:16.644839  4525 net.cpp:454] conv2 <- pool1
I1127 10:43:16.644855  4525 net.cpp:411] conv2 -> conv2
I1127 10:43:16.645277  4525 net.cpp:150] Setting up conv2
I1127 10:43:16.645295  4525 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:43:16.645304  4525 net.cpp:165] Memory required for data: 4706560
I1127 10:43:16.645323  4525 layer_factory.hpp:76] Creating layer pool2
I1127 10:43:16.645336  4525 net.cpp:106] Creating Layer pool2
I1127 10:43:16.645345  4525 net.cpp:454] pool2 <- conv2
I1127 10:43:16.645362  4525 net.cpp:411] pool2 -> pool2
I1127 10:43:16.645408  4525 net.cpp:150] Setting up pool2
I1127 10:43:16.645421  4525 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:43:16.645428  4525 net.cpp:165] Memory required for data: 4911360
I1127 10:43:16.645437  4525 layer_factory.hpp:76] Creating layer ip1
I1127 10:43:16.645450  4525 net.cpp:106] Creating Layer ip1
I1127 10:43:16.645459  4525 net.cpp:454] ip1 <- pool2
I1127 10:43:16.645473  4525 net.cpp:411] ip1 -> ip1
I1127 10:43:16.649175  4525 net.cpp:150] Setting up ip1
I1127 10:43:16.649193  4525 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:43:16.649201  4525 net.cpp:165] Memory required for data: 5039360
I1127 10:43:16.649216  4525 layer_factory.hpp:76] Creating layer relu1
I1127 10:43:16.649231  4525 net.cpp:106] Creating Layer relu1
I1127 10:43:16.649240  4525 net.cpp:454] relu1 <- ip1
I1127 10:43:16.649251  4525 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:43:16.649266  4525 net.cpp:150] Setting up relu1
I1127 10:43:16.649276  4525 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:43:16.649284  4525 net.cpp:165] Memory required for data: 5167360
I1127 10:43:16.649292  4525 layer_factory.hpp:76] Creating layer ip2
I1127 10:43:16.649307  4525 net.cpp:106] Creating Layer ip2
I1127 10:43:16.649317  4525 net.cpp:454] ip2 <- ip1
I1127 10:43:16.649327  4525 net.cpp:411] ip2 -> ip2
I1127 10:43:16.649920  4525 net.cpp:150] Setting up ip2
I1127 10:43:16.649935  4525 net.cpp:157] Top shape: 64 10 (640)
I1127 10:43:16.649945  4525 net.cpp:165] Memory required for data: 5169920
I1127 10:43:16.649957  4525 layer_factory.hpp:76] Creating layer loss
I1127 10:43:16.649978  4525 net.cpp:106] Creating Layer loss
I1127 10:43:16.649988  4525 net.cpp:454] loss <- ip2
I1127 10:43:16.649997  4525 net.cpp:454] loss <- label
I1127 10:43:16.650012  4525 net.cpp:411] loss -> loss
I1127 10:43:16.650032  4525 layer_factory.hpp:76] Creating layer loss
I1127 10:43:16.650137  4525 net.cpp:150] Setting up loss
I1127 10:43:16.650157  4525 net.cpp:157] Top shape: (1)
I1127 10:43:16.650166  4525 net.cpp:160]     with loss weight 1
I1127 10:43:16.650189  4525 net.cpp:165] Memory required for data: 5169924
I1127 10:43:16.650198  4525 net.cpp:226] loss needs backward computation.
I1127 10:43:16.650207  4525 net.cpp:226] ip2 needs backward computation.
I1127 10:43:16.650214  4525 net.cpp:226] relu1 needs backward computation.
I1127 10:43:16.650223  4525 net.cpp:226] ip1 needs backward computation.
I1127 10:43:16.650229  4525 net.cpp:226] pool2 needs backward computation.
I1127 10:43:16.650238  4525 net.cpp:226] conv2 needs backward computation.
I1127 10:43:16.650245  4525 net.cpp:226] pool1 needs backward computation.
I1127 10:43:16.650254  4525 net.cpp:226] conv1 needs backward computation.
I1127 10:43:16.650261  4525 net.cpp:228] mnist does not need backward computation.
I1127 10:43:16.650269  4525 net.cpp:270] This network produces output loss
I1127 10:43:16.650282  4525 net.cpp:283] Network initialization done.
I1127 10:43:16.650650  4525 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:43:16.650691  4525 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:43:16.650861  4525 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:43:16.650954  4525 layer_factory.hpp:76] Creating layer mnist
I1127 10:43:16.651077  4525 net.cpp:106] Creating Layer mnist
I1127 10:43:16.651093  4525 net.cpp:411] mnist -> data
I1127 10:43:16.651108  4525 net.cpp:411] mnist -> label
I1127 10:43:16.651806  4530 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:43:16.651912  4525 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:43:16.655920  4525 net.cpp:150] Setting up mnist
I1127 10:43:16.655937  4525 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:43:16.655947  4525 net.cpp:157] Top shape: 100 (100)
I1127 10:43:16.655956  4525 net.cpp:165] Memory required for data: 314000
I1127 10:43:16.655963  4525 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:43:16.655978  4525 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:43:16.655987  4525 net.cpp:454] label_mnist_1_split <- label
I1127 10:43:16.655997  4525 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:43:16.656011  4525 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:43:16.656059  4525 net.cpp:150] Setting up label_mnist_1_split
I1127 10:43:16.656071  4525 net.cpp:157] Top shape: 100 (100)
I1127 10:43:16.656081  4525 net.cpp:157] Top shape: 100 (100)
I1127 10:43:16.656088  4525 net.cpp:165] Memory required for data: 314800
I1127 10:43:16.656096  4525 layer_factory.hpp:76] Creating layer conv1
I1127 10:43:16.656111  4525 net.cpp:106] Creating Layer conv1
I1127 10:43:16.656119  4525 net.cpp:454] conv1 <- data
I1127 10:43:16.656133  4525 net.cpp:411] conv1 -> conv1
I1127 10:43:16.656366  4525 net.cpp:150] Setting up conv1
I1127 10:43:16.656380  4525 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:43:16.656389  4525 net.cpp:165] Memory required for data: 4922800
I1127 10:43:16.656404  4525 layer_factory.hpp:76] Creating layer pool1
I1127 10:43:16.656419  4525 net.cpp:106] Creating Layer pool1
I1127 10:43:16.656429  4525 net.cpp:454] pool1 <- conv1
I1127 10:43:16.656447  4525 net.cpp:411] pool1 -> pool1
I1127 10:43:16.656492  4525 net.cpp:150] Setting up pool1
I1127 10:43:16.656505  4525 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:43:16.656513  4525 net.cpp:165] Memory required for data: 6074800
I1127 10:43:16.656520  4525 layer_factory.hpp:76] Creating layer conv2
I1127 10:43:16.656535  4525 net.cpp:106] Creating Layer conv2
I1127 10:43:16.656544  4525 net.cpp:454] conv2 <- pool1
I1127 10:43:16.656555  4525 net.cpp:411] conv2 -> conv2
I1127 10:43:16.657486  4525 net.cpp:150] Setting up conv2
I1127 10:43:16.657501  4525 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:43:16.657510  4525 net.cpp:165] Memory required for data: 7354800
I1127 10:43:16.657526  4525 layer_factory.hpp:76] Creating layer pool2
I1127 10:43:16.657537  4525 net.cpp:106] Creating Layer pool2
I1127 10:43:16.657546  4525 net.cpp:454] pool2 <- conv2
I1127 10:43:16.657557  4525 net.cpp:411] pool2 -> pool2
I1127 10:43:16.657672  4525 net.cpp:150] Setting up pool2
I1127 10:43:16.657686  4525 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:43:16.657694  4525 net.cpp:165] Memory required for data: 7674800
I1127 10:43:16.657702  4525 layer_factory.hpp:76] Creating layer ip1
I1127 10:43:16.657716  4525 net.cpp:106] Creating Layer ip1
I1127 10:43:16.657726  4525 net.cpp:454] ip1 <- pool2
I1127 10:43:16.657737  4525 net.cpp:411] ip1 -> ip1
I1127 10:43:16.661345  4525 net.cpp:150] Setting up ip1
I1127 10:43:16.661362  4525 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:43:16.661371  4525 net.cpp:165] Memory required for data: 7874800
I1127 10:43:16.661386  4525 layer_factory.hpp:76] Creating layer relu1
I1127 10:43:16.661397  4525 net.cpp:106] Creating Layer relu1
I1127 10:43:16.661407  4525 net.cpp:454] relu1 <- ip1
I1127 10:43:16.661420  4525 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:43:16.661433  4525 net.cpp:150] Setting up relu1
I1127 10:43:16.661444  4525 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:43:16.661453  4525 net.cpp:165] Memory required for data: 8074800
I1127 10:43:16.661460  4525 layer_factory.hpp:76] Creating layer ip2
I1127 10:43:16.661473  4525 net.cpp:106] Creating Layer ip2
I1127 10:43:16.661484  4525 net.cpp:454] ip2 <- ip1
I1127 10:43:16.661496  4525 net.cpp:411] ip2 -> ip2
I1127 10:43:16.661648  4525 net.cpp:150] Setting up ip2
I1127 10:43:16.661662  4525 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:43:16.661675  4525 net.cpp:165] Memory required for data: 8078800
I1127 10:43:16.661690  4525 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:43:16.661701  4525 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:43:16.661710  4525 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:43:16.661723  4525 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:43:16.661737  4525 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:43:16.661782  4525 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:43:16.661793  4525 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:43:16.661803  4525 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:43:16.661811  4525 net.cpp:165] Memory required for data: 8086800
I1127 10:43:16.661819  4525 layer_factory.hpp:76] Creating layer accuracy
I1127 10:43:16.661833  4525 net.cpp:106] Creating Layer accuracy
I1127 10:43:16.661841  4525 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:43:16.661850  4525 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:43:16.661864  4525 net.cpp:411] accuracy -> accuracy
I1127 10:43:16.661878  4525 net.cpp:150] Setting up accuracy
I1127 10:43:16.661890  4525 net.cpp:157] Top shape: (1)
I1127 10:43:16.661897  4525 net.cpp:165] Memory required for data: 8086804
I1127 10:43:16.661906  4525 layer_factory.hpp:76] Creating layer loss
I1127 10:43:16.661916  4525 net.cpp:106] Creating Layer loss
I1127 10:43:16.661924  4525 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:43:16.661933  4525 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:43:16.661943  4525 net.cpp:411] loss -> loss
I1127 10:43:16.661958  4525 layer_factory.hpp:76] Creating layer loss
I1127 10:43:16.662070  4525 net.cpp:150] Setting up loss
I1127 10:43:16.662082  4525 net.cpp:157] Top shape: (1)
I1127 10:43:16.662091  4525 net.cpp:160]     with loss weight 1
I1127 10:43:16.662104  4525 net.cpp:165] Memory required for data: 8086808
I1127 10:43:16.662112  4525 net.cpp:226] loss needs backward computation.
I1127 10:43:16.662122  4525 net.cpp:228] accuracy does not need backward computation.
I1127 10:43:16.662132  4525 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:43:16.662139  4525 net.cpp:226] ip2 needs backward computation.
I1127 10:43:16.662153  4525 net.cpp:226] relu1 needs backward computation.
I1127 10:43:16.662161  4525 net.cpp:226] ip1 needs backward computation.
I1127 10:43:16.662169  4525 net.cpp:226] pool2 needs backward computation.
I1127 10:43:16.662178  4525 net.cpp:226] conv2 needs backward computation.
I1127 10:43:16.662185  4525 net.cpp:226] pool1 needs backward computation.
I1127 10:43:16.662194  4525 net.cpp:226] conv1 needs backward computation.
I1127 10:43:16.662202  4525 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:43:16.662210  4525 net.cpp:228] mnist does not need backward computation.
I1127 10:43:16.662217  4525 net.cpp:270] This network produces output accuracy
I1127 10:43:16.662225  4525 net.cpp:270] This network produces output loss
I1127 10:43:16.662245  4525 net.cpp:283] Network initialization done.
I1127 10:43:16.662298  4525 solver.cpp:59] Solver scaffolding done.
I1127 10:43:16.662616  4525 caffe.cpp:212] Starting Optimization
I1127 10:43:16.662626  4525 solver.cpp:287] Solving LeNet
I1127 10:43:16.662634  4525 solver.cpp:288] Learning Rate Policy: inv
I1127 10:43:16.663100  4525 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:43:18.177502  4525 solver.cpp:408]     Test net output #0: accuracy = 0.1022
I1127 10:43:18.177634  4525 solver.cpp:408]     Test net output #1: loss = 2.40337 (* 1 = 2.40337 loss)
I1127 10:43:18.193377  4525 solver.cpp:236] Iteration 0, loss = 2.31547
I1127 10:43:18.193483  4525 solver.cpp:252]     Train net output #0: loss = 2.31547 (* 1 = 2.31547 loss)
I1127 10:43:18.193513  4525 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:43:31.629987  4525 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:43:33.632419  4525 solver.cpp:408]     Test net output #0: accuracy = 0.9733
I1127 10:43:33.632499  4525 solver.cpp:408]     Test net output #1: loss = 0.0851584 (* 1 = 0.0851584 loss)
I1127 10:43:33.661435  4525 solver.cpp:236] Iteration 500, loss = 0.117369
I1127 10:43:33.661602  4525 solver.cpp:252]     Train net output #0: loss = 0.117369 (* 1 = 0.117369 loss)
I1127 10:43:33.661628  4525 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:43:35.586706  4525 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:43:46.243806  4525 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:43:46.255182  4525 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:43:46.267057  4525 solver.cpp:320] Iteration 1000, loss = 0.0863219
I1127 10:43:46.267159  4525 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:43:48.987507  4525 solver.cpp:408]     Test net output #0: accuracy = 0.9805
I1127 10:43:48.987547  4525 solver.cpp:408]     Test net output #1: loss = 0.0590967 (* 1 = 0.0590967 loss)
I1127 10:43:48.987553  4525 solver.cpp:325] Optimization Done.
I1127 10:43:48.987558  4525 caffe.cpp:215] Optimization Done.
I1127 10:43:49.053025  4554 caffe.cpp:184] Using GPUs 0
I1127 10:43:49.489627  4554 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:43:49.489868  4554 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:43:49.490391  4554 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:43:49.490437  4554 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:43:49.490633  4554 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:43:49.490779  4554 layer_factory.hpp:76] Creating layer mnist
I1127 10:43:49.491408  4554 net.cpp:106] Creating Layer mnist
I1127 10:43:49.491441  4554 net.cpp:411] mnist -> data
I1127 10:43:49.491487  4554 net.cpp:411] mnist -> label
I1127 10:43:49.492588  4557 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:43:49.502888  4554 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:43:49.504299  4554 net.cpp:150] Setting up mnist
I1127 10:43:49.504400  4554 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:43:49.504413  4554 net.cpp:157] Top shape: 64 (64)
I1127 10:43:49.504421  4554 net.cpp:165] Memory required for data: 200960
I1127 10:43:49.504443  4554 layer_factory.hpp:76] Creating layer conv1
I1127 10:43:49.504482  4554 net.cpp:106] Creating Layer conv1
I1127 10:43:49.504494  4554 net.cpp:454] conv1 <- data
I1127 10:43:49.504515  4554 net.cpp:411] conv1 -> conv1
I1127 10:43:49.505810  4554 net.cpp:150] Setting up conv1
I1127 10:43:49.505883  4554 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:43:49.505905  4554 net.cpp:165] Memory required for data: 3150080
I1127 10:43:49.505954  4554 layer_factory.hpp:76] Creating layer pool1
I1127 10:43:49.505988  4554 net.cpp:106] Creating Layer pool1
I1127 10:43:49.506000  4554 net.cpp:454] pool1 <- conv1
I1127 10:43:49.506016  4554 net.cpp:411] pool1 -> pool1
I1127 10:43:49.506160  4554 net.cpp:150] Setting up pool1
I1127 10:43:49.506181  4554 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:43:49.506188  4554 net.cpp:165] Memory required for data: 3887360
I1127 10:43:49.506196  4554 layer_factory.hpp:76] Creating layer conv2
I1127 10:43:49.506217  4554 net.cpp:106] Creating Layer conv2
I1127 10:43:49.506230  4554 net.cpp:454] conv2 <- pool1
I1127 10:43:49.506247  4554 net.cpp:411] conv2 -> conv2
I1127 10:43:49.506717  4554 net.cpp:150] Setting up conv2
I1127 10:43:49.506733  4554 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:43:49.506741  4554 net.cpp:165] Memory required for data: 4706560
I1127 10:43:49.506762  4554 layer_factory.hpp:76] Creating layer pool2
I1127 10:43:49.506772  4554 net.cpp:106] Creating Layer pool2
I1127 10:43:49.506778  4554 net.cpp:454] pool2 <- conv2
I1127 10:43:49.506788  4554 net.cpp:411] pool2 -> pool2
I1127 10:43:49.506835  4554 net.cpp:150] Setting up pool2
I1127 10:43:49.506844  4554 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:43:49.506849  4554 net.cpp:165] Memory required for data: 4911360
I1127 10:43:49.506855  4554 layer_factory.hpp:76] Creating layer ip1
I1127 10:43:49.506872  4554 net.cpp:106] Creating Layer ip1
I1127 10:43:49.506880  4554 net.cpp:454] ip1 <- pool2
I1127 10:43:49.506889  4554 net.cpp:411] ip1 -> ip1
I1127 10:43:49.510609  4554 net.cpp:150] Setting up ip1
I1127 10:43:49.510664  4554 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:43:49.510673  4554 net.cpp:165] Memory required for data: 5039360
I1127 10:43:49.510691  4554 layer_factory.hpp:76] Creating layer relu1
I1127 10:43:49.510705  4554 net.cpp:106] Creating Layer relu1
I1127 10:43:49.510712  4554 net.cpp:454] relu1 <- ip1
I1127 10:43:49.510723  4554 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:43:49.510738  4554 net.cpp:150] Setting up relu1
I1127 10:43:49.510747  4554 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:43:49.510753  4554 net.cpp:165] Memory required for data: 5167360
I1127 10:43:49.510759  4554 layer_factory.hpp:76] Creating layer ip2
I1127 10:43:49.510772  4554 net.cpp:106] Creating Layer ip2
I1127 10:43:49.510781  4554 net.cpp:454] ip2 <- ip1
I1127 10:43:49.510792  4554 net.cpp:411] ip2 -> ip2
I1127 10:43:49.511519  4554 net.cpp:150] Setting up ip2
I1127 10:43:49.511550  4554 net.cpp:157] Top shape: 64 10 (640)
I1127 10:43:49.511559  4554 net.cpp:165] Memory required for data: 5169920
I1127 10:43:49.511572  4554 layer_factory.hpp:76] Creating layer loss
I1127 10:43:49.511590  4554 net.cpp:106] Creating Layer loss
I1127 10:43:49.511597  4554 net.cpp:454] loss <- ip2
I1127 10:43:49.511606  4554 net.cpp:454] loss <- label
I1127 10:43:49.511620  4554 net.cpp:411] loss -> loss
I1127 10:43:49.511642  4554 layer_factory.hpp:76] Creating layer loss
I1127 10:43:49.511746  4554 net.cpp:150] Setting up loss
I1127 10:43:49.511759  4554 net.cpp:157] Top shape: (1)
I1127 10:43:49.511767  4554 net.cpp:160]     with loss weight 1
I1127 10:43:49.511814  4554 net.cpp:165] Memory required for data: 5169924
I1127 10:43:49.511824  4554 net.cpp:226] loss needs backward computation.
I1127 10:43:49.511832  4554 net.cpp:226] ip2 needs backward computation.
I1127 10:43:49.511839  4554 net.cpp:226] relu1 needs backward computation.
I1127 10:43:49.511844  4554 net.cpp:226] ip1 needs backward computation.
I1127 10:43:49.511849  4554 net.cpp:226] pool2 needs backward computation.
I1127 10:43:49.511855  4554 net.cpp:226] conv2 needs backward computation.
I1127 10:43:49.511863  4554 net.cpp:226] pool1 needs backward computation.
I1127 10:43:49.511870  4554 net.cpp:226] conv1 needs backward computation.
I1127 10:43:49.511879  4554 net.cpp:228] mnist does not need backward computation.
I1127 10:43:49.511886  4554 net.cpp:270] This network produces output loss
I1127 10:43:49.511901  4554 net.cpp:283] Network initialization done.
I1127 10:43:49.512302  4554 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:43:49.512349  4554 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:43:49.512528  4554 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:43:49.512624  4554 layer_factory.hpp:76] Creating layer mnist
I1127 10:43:49.512789  4554 net.cpp:106] Creating Layer mnist
I1127 10:43:49.512809  4554 net.cpp:411] mnist -> data
I1127 10:43:49.512835  4554 net.cpp:411] mnist -> label
I1127 10:43:49.514137  4559 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:43:49.514425  4554 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:43:49.516147  4554 net.cpp:150] Setting up mnist
I1127 10:43:49.516201  4554 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:43:49.516211  4554 net.cpp:157] Top shape: 100 (100)
I1127 10:43:49.516217  4554 net.cpp:165] Memory required for data: 314000
I1127 10:43:49.516229  4554 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:43:49.516250  4554 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:43:49.516268  4554 net.cpp:454] label_mnist_1_split <- label
I1127 10:43:49.516280  4554 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:43:49.516297  4554 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:43:49.516347  4554 net.cpp:150] Setting up label_mnist_1_split
I1127 10:43:49.516360  4554 net.cpp:157] Top shape: 100 (100)
I1127 10:43:49.516369  4554 net.cpp:157] Top shape: 100 (100)
I1127 10:43:49.516376  4554 net.cpp:165] Memory required for data: 314800
I1127 10:43:49.516383  4554 layer_factory.hpp:76] Creating layer conv1
I1127 10:43:49.516399  4554 net.cpp:106] Creating Layer conv1
I1127 10:43:49.516408  4554 net.cpp:454] conv1 <- data
I1127 10:43:49.516420  4554 net.cpp:411] conv1 -> conv1
I1127 10:43:49.516660  4554 net.cpp:150] Setting up conv1
I1127 10:43:49.516674  4554 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:43:49.516681  4554 net.cpp:165] Memory required for data: 4922800
I1127 10:43:49.516695  4554 layer_factory.hpp:76] Creating layer pool1
I1127 10:43:49.516708  4554 net.cpp:106] Creating Layer pool1
I1127 10:43:49.516716  4554 net.cpp:454] pool1 <- conv1
I1127 10:43:49.516737  4554 net.cpp:411] pool1 -> pool1
I1127 10:43:49.516783  4554 net.cpp:150] Setting up pool1
I1127 10:43:49.516795  4554 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:43:49.516803  4554 net.cpp:165] Memory required for data: 6074800
I1127 10:43:49.516808  4554 layer_factory.hpp:76] Creating layer conv2
I1127 10:43:49.516823  4554 net.cpp:106] Creating Layer conv2
I1127 10:43:49.516830  4554 net.cpp:454] conv2 <- pool1
I1127 10:43:49.516842  4554 net.cpp:411] conv2 -> conv2
I1127 10:43:49.517386  4554 net.cpp:150] Setting up conv2
I1127 10:43:49.517407  4554 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:43:49.517416  4554 net.cpp:165] Memory required for data: 7354800
I1127 10:43:49.517431  4554 layer_factory.hpp:76] Creating layer pool2
I1127 10:43:49.517441  4554 net.cpp:106] Creating Layer pool2
I1127 10:43:49.517449  4554 net.cpp:454] pool2 <- conv2
I1127 10:43:49.517458  4554 net.cpp:411] pool2 -> pool2
I1127 10:43:49.517500  4554 net.cpp:150] Setting up pool2
I1127 10:43:49.517513  4554 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:43:49.517519  4554 net.cpp:165] Memory required for data: 7674800
I1127 10:43:49.517526  4554 layer_factory.hpp:76] Creating layer ip1
I1127 10:43:49.517539  4554 net.cpp:106] Creating Layer ip1
I1127 10:43:49.517545  4554 net.cpp:454] ip1 <- pool2
I1127 10:43:49.517556  4554 net.cpp:411] ip1 -> ip1
I1127 10:43:49.520475  4554 net.cpp:150] Setting up ip1
I1127 10:43:49.520514  4554 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:43:49.520519  4554 net.cpp:165] Memory required for data: 7874800
I1127 10:43:49.520531  4554 layer_factory.hpp:76] Creating layer relu1
I1127 10:43:49.520545  4554 net.cpp:106] Creating Layer relu1
I1127 10:43:49.520550  4554 net.cpp:454] relu1 <- ip1
I1127 10:43:49.520556  4554 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:43:49.520566  4554 net.cpp:150] Setting up relu1
I1127 10:43:49.520571  4554 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:43:49.520576  4554 net.cpp:165] Memory required for data: 8074800
I1127 10:43:49.520580  4554 layer_factory.hpp:76] Creating layer ip2
I1127 10:43:49.520589  4554 net.cpp:106] Creating Layer ip2
I1127 10:43:49.520594  4554 net.cpp:454] ip2 <- ip1
I1127 10:43:49.520601  4554 net.cpp:411] ip2 -> ip2
I1127 10:43:49.520711  4554 net.cpp:150] Setting up ip2
I1127 10:43:49.520720  4554 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:43:49.520725  4554 net.cpp:165] Memory required for data: 8078800
I1127 10:43:49.520731  4554 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:43:49.520738  4554 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:43:49.520742  4554 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:43:49.520747  4554 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:43:49.520755  4554 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:43:49.520782  4554 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:43:49.520798  4554 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:43:49.520803  4554 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:43:49.520807  4554 net.cpp:165] Memory required for data: 8086800
I1127 10:43:49.520812  4554 layer_factory.hpp:76] Creating layer accuracy
I1127 10:43:49.520820  4554 net.cpp:106] Creating Layer accuracy
I1127 10:43:49.520824  4554 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:43:49.520829  4554 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:43:49.520836  4554 net.cpp:411] accuracy -> accuracy
I1127 10:43:49.520845  4554 net.cpp:150] Setting up accuracy
I1127 10:43:49.520851  4554 net.cpp:157] Top shape: (1)
I1127 10:43:49.520855  4554 net.cpp:165] Memory required for data: 8086804
I1127 10:43:49.520859  4554 layer_factory.hpp:76] Creating layer loss
I1127 10:43:49.520865  4554 net.cpp:106] Creating Layer loss
I1127 10:43:49.520869  4554 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:43:49.520874  4554 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:43:49.520884  4554 net.cpp:411] loss -> loss
I1127 10:43:49.520892  4554 layer_factory.hpp:76] Creating layer loss
I1127 10:43:49.520966  4554 net.cpp:150] Setting up loss
I1127 10:43:49.520973  4554 net.cpp:157] Top shape: (1)
I1127 10:43:49.520977  4554 net.cpp:160]     with loss weight 1
I1127 10:43:49.520989  4554 net.cpp:165] Memory required for data: 8086808
I1127 10:43:49.520994  4554 net.cpp:226] loss needs backward computation.
I1127 10:43:49.521001  4554 net.cpp:228] accuracy does not need backward computation.
I1127 10:43:49.521006  4554 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:43:49.521010  4554 net.cpp:226] ip2 needs backward computation.
I1127 10:43:49.521014  4554 net.cpp:226] relu1 needs backward computation.
I1127 10:43:49.521018  4554 net.cpp:226] ip1 needs backward computation.
I1127 10:43:49.521023  4554 net.cpp:226] pool2 needs backward computation.
I1127 10:43:49.521028  4554 net.cpp:226] conv2 needs backward computation.
I1127 10:43:49.521031  4554 net.cpp:226] pool1 needs backward computation.
I1127 10:43:49.521035  4554 net.cpp:226] conv1 needs backward computation.
I1127 10:43:49.521040  4554 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:43:49.521045  4554 net.cpp:228] mnist does not need backward computation.
I1127 10:43:49.521050  4554 net.cpp:270] This network produces output accuracy
I1127 10:43:49.521054  4554 net.cpp:270] This network produces output loss
I1127 10:43:49.521064  4554 net.cpp:283] Network initialization done.
I1127 10:43:49.521106  4554 solver.cpp:59] Solver scaffolding done.
I1127 10:43:49.521302  4554 caffe.cpp:212] Starting Optimization
I1127 10:43:49.521311  4554 solver.cpp:287] Solving LeNet
I1127 10:43:49.521313  4554 solver.cpp:288] Learning Rate Policy: inv
I1127 10:43:49.521764  4554 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:43:50.640658  4554 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:43:50.654904  4554 solver.cpp:408]     Test net output #0: accuracy = 0.1172
I1127 10:43:50.654989  4554 solver.cpp:408]     Test net output #1: loss = 2.32819 (* 1 = 2.32819 loss)
I1127 10:43:50.667510  4554 solver.cpp:236] Iteration 0, loss = 2.3791
I1127 10:43:50.667567  4554 solver.cpp:252]     Train net output #0: loss = 2.3791 (* 1 = 2.3791 loss)
I1127 10:43:50.667590  4554 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:44:04.127867  4554 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:44:05.213480  4554 solver.cpp:408]     Test net output #0: accuracy = 0.972
I1127 10:44:05.213547  4554 solver.cpp:408]     Test net output #1: loss = 0.0846399 (* 1 = 0.0846399 loss)
I1127 10:44:05.222669  4554 solver.cpp:236] Iteration 500, loss = 0.0801272
I1127 10:44:05.222713  4554 solver.cpp:252]     Train net output #0: loss = 0.0801272 (* 1 = 0.0801272 loss)
I1127 10:44:05.222724  4554 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:44:18.678016  4554 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:44:18.692674  4554 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:44:18.702528  4554 solver.cpp:320] Iteration 1000, loss = 0.0653348
I1127 10:44:18.702602  4554 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:44:21.108017  4554 solver.cpp:408]     Test net output #0: accuracy = 0.9797
I1127 10:44:21.108126  4554 solver.cpp:408]     Test net output #1: loss = 0.0598344 (* 1 = 0.0598344 loss)
I1127 10:44:21.108134  4554 solver.cpp:325] Optimization Done.
I1127 10:44:21.108139  4554 caffe.cpp:215] Optimization Done.
I1127 10:44:21.183327  4609 caffe.cpp:184] Using GPUs 0
I1127 10:44:21.648156  4609 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:44:21.648268  4609 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:44:21.648524  4609 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:44:21.648537  4609 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:44:21.648622  4609 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:44:21.648675  4609 layer_factory.hpp:76] Creating layer mnist
I1127 10:44:21.648984  4609 net.cpp:106] Creating Layer mnist
I1127 10:44:21.648998  4609 net.cpp:411] mnist -> data
I1127 10:44:21.649019  4609 net.cpp:411] mnist -> label
I1127 10:44:21.649729  4613 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:44:21.685209  4609 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:44:21.742600  4609 net.cpp:150] Setting up mnist
I1127 10:44:21.742671  4609 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:44:21.742679  4609 net.cpp:157] Top shape: 64 (64)
I1127 10:44:21.742684  4609 net.cpp:165] Memory required for data: 200960
I1127 10:44:21.742702  4609 layer_factory.hpp:76] Creating layer conv1
I1127 10:44:21.742743  4609 net.cpp:106] Creating Layer conv1
I1127 10:44:21.742753  4609 net.cpp:454] conv1 <- data
I1127 10:44:21.742764  4609 net.cpp:411] conv1 -> conv1
I1127 10:44:21.743573  4609 net.cpp:150] Setting up conv1
I1127 10:44:21.743607  4609 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:44:21.743612  4609 net.cpp:165] Memory required for data: 3150080
I1127 10:44:21.743625  4609 layer_factory.hpp:76] Creating layer pool1
I1127 10:44:21.743638  4609 net.cpp:106] Creating Layer pool1
I1127 10:44:21.743644  4609 net.cpp:454] pool1 <- conv1
I1127 10:44:21.743651  4609 net.cpp:411] pool1 -> pool1
I1127 10:44:21.743703  4609 net.cpp:150] Setting up pool1
I1127 10:44:21.743711  4609 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:44:21.743716  4609 net.cpp:165] Memory required for data: 3887360
I1127 10:44:21.743721  4609 layer_factory.hpp:76] Creating layer conv2
I1127 10:44:21.743731  4609 net.cpp:106] Creating Layer conv2
I1127 10:44:21.743736  4609 net.cpp:454] conv2 <- pool1
I1127 10:44:21.743741  4609 net.cpp:411] conv2 -> conv2
I1127 10:44:21.744081  4609 net.cpp:150] Setting up conv2
I1127 10:44:21.744096  4609 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:44:21.744101  4609 net.cpp:165] Memory required for data: 4706560
I1127 10:44:21.744113  4609 layer_factory.hpp:76] Creating layer pool2
I1127 10:44:21.744122  4609 net.cpp:106] Creating Layer pool2
I1127 10:44:21.744127  4609 net.cpp:454] pool2 <- conv2
I1127 10:44:21.744135  4609 net.cpp:411] pool2 -> pool2
I1127 10:44:21.744186  4609 net.cpp:150] Setting up pool2
I1127 10:44:21.744197  4609 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:44:21.744204  4609 net.cpp:165] Memory required for data: 4911360
I1127 10:44:21.744210  4609 layer_factory.hpp:76] Creating layer ip1
I1127 10:44:21.744223  4609 net.cpp:106] Creating Layer ip1
I1127 10:44:21.744231  4609 net.cpp:454] ip1 <- pool2
I1127 10:44:21.744240  4609 net.cpp:411] ip1 -> ip1
I1127 10:44:21.746492  4609 net.cpp:150] Setting up ip1
I1127 10:44:21.746523  4609 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:44:21.746528  4609 net.cpp:165] Memory required for data: 5039360
I1127 10:44:21.746541  4609 layer_factory.hpp:76] Creating layer relu1
I1127 10:44:21.746551  4609 net.cpp:106] Creating Layer relu1
I1127 10:44:21.746557  4609 net.cpp:454] relu1 <- ip1
I1127 10:44:21.746565  4609 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:44:21.746578  4609 net.cpp:150] Setting up relu1
I1127 10:44:21.746583  4609 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:44:21.746587  4609 net.cpp:165] Memory required for data: 5167360
I1127 10:44:21.746592  4609 layer_factory.hpp:76] Creating layer ip2
I1127 10:44:21.746603  4609 net.cpp:106] Creating Layer ip2
I1127 10:44:21.746606  4609 net.cpp:454] ip2 <- ip1
I1127 10:44:21.746614  4609 net.cpp:411] ip2 -> ip2
I1127 10:44:21.747114  4609 net.cpp:150] Setting up ip2
I1127 10:44:21.747130  4609 net.cpp:157] Top shape: 64 10 (640)
I1127 10:44:21.747134  4609 net.cpp:165] Memory required for data: 5169920
I1127 10:44:21.747143  4609 layer_factory.hpp:76] Creating layer loss
I1127 10:44:21.747156  4609 net.cpp:106] Creating Layer loss
I1127 10:44:21.747161  4609 net.cpp:454] loss <- ip2
I1127 10:44:21.747166  4609 net.cpp:454] loss <- label
I1127 10:44:21.747175  4609 net.cpp:411] loss -> loss
I1127 10:44:21.747189  4609 layer_factory.hpp:76] Creating layer loss
I1127 10:44:21.747259  4609 net.cpp:150] Setting up loss
I1127 10:44:21.747267  4609 net.cpp:157] Top shape: (1)
I1127 10:44:21.747272  4609 net.cpp:160]     with loss weight 1
I1127 10:44:21.747294  4609 net.cpp:165] Memory required for data: 5169924
I1127 10:44:21.747299  4609 net.cpp:226] loss needs backward computation.
I1127 10:44:21.747304  4609 net.cpp:226] ip2 needs backward computation.
I1127 10:44:21.747308  4609 net.cpp:226] relu1 needs backward computation.
I1127 10:44:21.747313  4609 net.cpp:226] ip1 needs backward computation.
I1127 10:44:21.747318  4609 net.cpp:226] pool2 needs backward computation.
I1127 10:44:21.747323  4609 net.cpp:226] conv2 needs backward computation.
I1127 10:44:21.747335  4609 net.cpp:226] pool1 needs backward computation.
I1127 10:44:21.747340  4609 net.cpp:226] conv1 needs backward computation.
I1127 10:44:21.747345  4609 net.cpp:228] mnist does not need backward computation.
I1127 10:44:21.747350  4609 net.cpp:270] This network produces output loss
I1127 10:44:21.747359  4609 net.cpp:283] Network initialization done.
I1127 10:44:21.747611  4609 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:44:21.747635  4609 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:44:21.747750  4609 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:44:21.747809  4609 layer_factory.hpp:76] Creating layer mnist
I1127 10:44:21.747906  4609 net.cpp:106] Creating Layer mnist
I1127 10:44:21.747916  4609 net.cpp:411] mnist -> data
I1127 10:44:21.747925  4609 net.cpp:411] mnist -> label
I1127 10:44:21.748664  4615 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:44:21.748821  4609 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:44:21.752007  4609 net.cpp:150] Setting up mnist
I1127 10:44:21.752043  4609 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:44:21.752050  4609 net.cpp:157] Top shape: 100 (100)
I1127 10:44:21.752054  4609 net.cpp:165] Memory required for data: 314000
I1127 10:44:21.752061  4609 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:44:21.752074  4609 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:44:21.752080  4609 net.cpp:454] label_mnist_1_split <- label
I1127 10:44:21.752087  4609 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:44:21.752099  4609 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:44:21.752141  4609 net.cpp:150] Setting up label_mnist_1_split
I1127 10:44:21.752148  4609 net.cpp:157] Top shape: 100 (100)
I1127 10:44:21.752153  4609 net.cpp:157] Top shape: 100 (100)
I1127 10:44:21.752166  4609 net.cpp:165] Memory required for data: 314800
I1127 10:44:21.752171  4609 layer_factory.hpp:76] Creating layer conv1
I1127 10:44:21.752183  4609 net.cpp:106] Creating Layer conv1
I1127 10:44:21.752188  4609 net.cpp:454] conv1 <- data
I1127 10:44:21.752195  4609 net.cpp:411] conv1 -> conv1
I1127 10:44:21.752352  4609 net.cpp:150] Setting up conv1
I1127 10:44:21.752362  4609 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:44:21.752367  4609 net.cpp:165] Memory required for data: 4922800
I1127 10:44:21.752377  4609 layer_factory.hpp:76] Creating layer pool1
I1127 10:44:21.752384  4609 net.cpp:106] Creating Layer pool1
I1127 10:44:21.752388  4609 net.cpp:454] pool1 <- conv1
I1127 10:44:21.752403  4609 net.cpp:411] pool1 -> pool1
I1127 10:44:21.752434  4609 net.cpp:150] Setting up pool1
I1127 10:44:21.752440  4609 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:44:21.752445  4609 net.cpp:165] Memory required for data: 6074800
I1127 10:44:21.752449  4609 layer_factory.hpp:76] Creating layer conv2
I1127 10:44:21.752460  4609 net.cpp:106] Creating Layer conv2
I1127 10:44:21.752465  4609 net.cpp:454] conv2 <- pool1
I1127 10:44:21.752471  4609 net.cpp:411] conv2 -> conv2
I1127 10:44:21.752722  4609 net.cpp:150] Setting up conv2
I1127 10:44:21.752730  4609 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:44:21.752734  4609 net.cpp:165] Memory required for data: 7354800
I1127 10:44:21.752743  4609 layer_factory.hpp:76] Creating layer pool2
I1127 10:44:21.752750  4609 net.cpp:106] Creating Layer pool2
I1127 10:44:21.752755  4609 net.cpp:454] pool2 <- conv2
I1127 10:44:21.752760  4609 net.cpp:411] pool2 -> pool2
I1127 10:44:21.753362  4609 net.cpp:150] Setting up pool2
I1127 10:44:21.753381  4609 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:44:21.753387  4609 net.cpp:165] Memory required for data: 7674800
I1127 10:44:21.753392  4609 layer_factory.hpp:76] Creating layer ip1
I1127 10:44:21.753402  4609 net.cpp:106] Creating Layer ip1
I1127 10:44:21.753407  4609 net.cpp:454] ip1 <- pool2
I1127 10:44:21.753415  4609 net.cpp:411] ip1 -> ip1
I1127 10:44:21.756870  4609 net.cpp:150] Setting up ip1
I1127 10:44:21.756942  4609 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:44:21.756953  4609 net.cpp:165] Memory required for data: 7874800
I1127 10:44:21.756985  4609 layer_factory.hpp:76] Creating layer relu1
I1127 10:44:21.757015  4609 net.cpp:106] Creating Layer relu1
I1127 10:44:21.757030  4609 net.cpp:454] relu1 <- ip1
I1127 10:44:21.757047  4609 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:44:21.757071  4609 net.cpp:150] Setting up relu1
I1127 10:44:21.757084  4609 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:44:21.757092  4609 net.cpp:165] Memory required for data: 8074800
I1127 10:44:21.757099  4609 layer_factory.hpp:76] Creating layer ip2
I1127 10:44:21.757123  4609 net.cpp:106] Creating Layer ip2
I1127 10:44:21.757133  4609 net.cpp:454] ip2 <- ip1
I1127 10:44:21.757146  4609 net.cpp:411] ip2 -> ip2
I1127 10:44:21.757370  4609 net.cpp:150] Setting up ip2
I1127 10:44:21.757385  4609 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:44:21.757392  4609 net.cpp:165] Memory required for data: 8078800
I1127 10:44:21.757405  4609 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:44:21.757416  4609 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:44:21.757424  4609 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:44:21.757434  4609 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:44:21.757446  4609 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:44:21.757494  4609 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:44:21.757508  4609 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:44:21.757519  4609 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:44:21.757529  4609 net.cpp:165] Memory required for data: 8086800
I1127 10:44:21.757541  4609 layer_factory.hpp:76] Creating layer accuracy
I1127 10:44:21.757561  4609 net.cpp:106] Creating Layer accuracy
I1127 10:44:21.757573  4609 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:44:21.757586  4609 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:44:21.757627  4609 net.cpp:411] accuracy -> accuracy
I1127 10:44:21.757650  4609 net.cpp:150] Setting up accuracy
I1127 10:44:21.757660  4609 net.cpp:157] Top shape: (1)
I1127 10:44:21.757666  4609 net.cpp:165] Memory required for data: 8086804
I1127 10:44:21.757673  4609 layer_factory.hpp:76] Creating layer loss
I1127 10:44:21.757684  4609 net.cpp:106] Creating Layer loss
I1127 10:44:21.757691  4609 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:44:21.757700  4609 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:44:21.757714  4609 net.cpp:411] loss -> loss
I1127 10:44:21.757733  4609 layer_factory.hpp:76] Creating layer loss
I1127 10:44:21.757880  4609 net.cpp:150] Setting up loss
I1127 10:44:21.757899  4609 net.cpp:157] Top shape: (1)
I1127 10:44:21.757908  4609 net.cpp:160]     with loss weight 1
I1127 10:44:21.757935  4609 net.cpp:165] Memory required for data: 8086808
I1127 10:44:21.757944  4609 net.cpp:226] loss needs backward computation.
I1127 10:44:21.757963  4609 net.cpp:228] accuracy does not need backward computation.
I1127 10:44:21.757974  4609 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:44:21.757982  4609 net.cpp:226] ip2 needs backward computation.
I1127 10:44:21.757989  4609 net.cpp:226] relu1 needs backward computation.
I1127 10:44:21.757997  4609 net.cpp:226] ip1 needs backward computation.
I1127 10:44:21.758005  4609 net.cpp:226] pool2 needs backward computation.
I1127 10:44:21.758013  4609 net.cpp:226] conv2 needs backward computation.
I1127 10:44:21.758021  4609 net.cpp:226] pool1 needs backward computation.
I1127 10:44:21.758029  4609 net.cpp:226] conv1 needs backward computation.
I1127 10:44:21.758038  4609 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:44:21.758050  4609 net.cpp:228] mnist does not need backward computation.
I1127 10:44:21.758057  4609 net.cpp:270] This network produces output accuracy
I1127 10:44:21.758064  4609 net.cpp:270] This network produces output loss
I1127 10:44:21.758080  4609 net.cpp:283] Network initialization done.
I1127 10:44:21.758225  4609 solver.cpp:59] Solver scaffolding done.
I1127 10:44:21.758555  4609 caffe.cpp:212] Starting Optimization
I1127 10:44:21.758569  4609 solver.cpp:287] Solving LeNet
I1127 10:44:21.758577  4609 solver.cpp:288] Learning Rate Policy: inv
I1127 10:44:21.759152  4609 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:44:22.939061  4609 solver.cpp:408]     Test net output #0: accuracy = 0.0401
I1127 10:44:22.939139  4609 solver.cpp:408]     Test net output #1: loss = 2.45176 (* 1 = 2.45176 loss)
I1127 10:44:22.953028  4609 solver.cpp:236] Iteration 0, loss = 2.47424
I1127 10:44:22.953138  4609 solver.cpp:252]     Train net output #0: loss = 2.47424 (* 1 = 2.47424 loss)
I1127 10:44:22.953186  4609 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:44:36.449360  4609 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:44:37.553864  4609 solver.cpp:408]     Test net output #0: accuracy = 0.9739
I1127 10:44:37.553961  4609 solver.cpp:408]     Test net output #1: loss = 0.0846351 (* 1 = 0.0846351 loss)
I1127 10:44:37.563717  4609 solver.cpp:236] Iteration 500, loss = 0.0817534
I1127 10:44:37.563819  4609 solver.cpp:252]     Train net output #0: loss = 0.0817534 (* 1 = 0.0817534 loss)
I1127 10:44:37.563838  4609 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:44:51.027973  4609 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:44:51.041597  4609 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:44:51.052693  4609 solver.cpp:320] Iteration 1000, loss = 0.0845155
I1127 10:44:51.052767  4609 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:44:52.157696  4609 solver.cpp:408]     Test net output #0: accuracy = 0.9819
I1127 10:44:52.157837  4609 solver.cpp:408]     Test net output #1: loss = 0.0571261 (* 1 = 0.0571261 loss)
I1127 10:44:52.157846  4609 solver.cpp:325] Optimization Done.
I1127 10:44:52.157851  4609 caffe.cpp:215] Optimization Done.
I1127 10:44:52.281153  4690 caffe.cpp:184] Using GPUs 0
I1127 10:44:52.629181  4690 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:44:52.629292  4690 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:44:52.629540  4690 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:44:52.629554  4690 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:44:52.629637  4690 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:44:52.629689  4690 layer_factory.hpp:76] Creating layer mnist
I1127 10:44:52.630000  4690 net.cpp:106] Creating Layer mnist
I1127 10:44:52.630012  4690 net.cpp:411] mnist -> data
I1127 10:44:52.630033  4690 net.cpp:411] mnist -> label
I1127 10:44:52.630811  4695 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:44:52.664213  4690 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:44:52.670521  4690 net.cpp:150] Setting up mnist
I1127 10:44:52.670541  4690 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:44:52.670547  4690 net.cpp:157] Top shape: 64 (64)
I1127 10:44:52.670552  4690 net.cpp:165] Memory required for data: 200960
I1127 10:44:52.670562  4690 layer_factory.hpp:76] Creating layer conv1
I1127 10:44:52.670577  4690 net.cpp:106] Creating Layer conv1
I1127 10:44:52.670583  4690 net.cpp:454] conv1 <- data
I1127 10:44:52.670594  4690 net.cpp:411] conv1 -> conv1
I1127 10:44:52.671205  4690 net.cpp:150] Setting up conv1
I1127 10:44:52.671216  4690 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:44:52.671219  4690 net.cpp:165] Memory required for data: 3150080
I1127 10:44:52.671232  4690 layer_factory.hpp:76] Creating layer pool1
I1127 10:44:52.671241  4690 net.cpp:106] Creating Layer pool1
I1127 10:44:52.671252  4690 net.cpp:454] pool1 <- conv1
I1127 10:44:52.671259  4690 net.cpp:411] pool1 -> pool1
I1127 10:44:52.671303  4690 net.cpp:150] Setting up pool1
I1127 10:44:52.671311  4690 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:44:52.671315  4690 net.cpp:165] Memory required for data: 3887360
I1127 10:44:52.671319  4690 layer_factory.hpp:76] Creating layer conv2
I1127 10:44:52.671329  4690 net.cpp:106] Creating Layer conv2
I1127 10:44:52.671334  4690 net.cpp:454] conv2 <- pool1
I1127 10:44:52.671341  4690 net.cpp:411] conv2 -> conv2
I1127 10:44:52.671738  4690 net.cpp:150] Setting up conv2
I1127 10:44:52.671748  4690 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:44:52.671752  4690 net.cpp:165] Memory required for data: 4706560
I1127 10:44:52.671761  4690 layer_factory.hpp:76] Creating layer pool2
I1127 10:44:52.671768  4690 net.cpp:106] Creating Layer pool2
I1127 10:44:52.671773  4690 net.cpp:454] pool2 <- conv2
I1127 10:44:52.671778  4690 net.cpp:411] pool2 -> pool2
I1127 10:44:52.671805  4690 net.cpp:150] Setting up pool2
I1127 10:44:52.671813  4690 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:44:52.671816  4690 net.cpp:165] Memory required for data: 4911360
I1127 10:44:52.671820  4690 layer_factory.hpp:76] Creating layer ip1
I1127 10:44:52.671828  4690 net.cpp:106] Creating Layer ip1
I1127 10:44:52.671833  4690 net.cpp:454] ip1 <- pool2
I1127 10:44:52.671840  4690 net.cpp:411] ip1 -> ip1
I1127 10:44:52.673941  4690 net.cpp:150] Setting up ip1
I1127 10:44:52.673952  4690 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:44:52.673956  4690 net.cpp:165] Memory required for data: 5039360
I1127 10:44:52.673965  4690 layer_factory.hpp:76] Creating layer relu1
I1127 10:44:52.673972  4690 net.cpp:106] Creating Layer relu1
I1127 10:44:52.673977  4690 net.cpp:454] relu1 <- ip1
I1127 10:44:52.673985  4690 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:44:52.673992  4690 net.cpp:150] Setting up relu1
I1127 10:44:52.673998  4690 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:44:52.674001  4690 net.cpp:165] Memory required for data: 5167360
I1127 10:44:52.674006  4690 layer_factory.hpp:76] Creating layer ip2
I1127 10:44:52.674012  4690 net.cpp:106] Creating Layer ip2
I1127 10:44:52.674016  4690 net.cpp:454] ip2 <- ip1
I1127 10:44:52.674023  4690 net.cpp:411] ip2 -> ip2
I1127 10:44:52.674429  4690 net.cpp:150] Setting up ip2
I1127 10:44:52.674439  4690 net.cpp:157] Top shape: 64 10 (640)
I1127 10:44:52.674444  4690 net.cpp:165] Memory required for data: 5169920
I1127 10:44:52.674451  4690 layer_factory.hpp:76] Creating layer loss
I1127 10:44:52.674458  4690 net.cpp:106] Creating Layer loss
I1127 10:44:52.674463  4690 net.cpp:454] loss <- ip2
I1127 10:44:52.674468  4690 net.cpp:454] loss <- label
I1127 10:44:52.674475  4690 net.cpp:411] loss -> loss
I1127 10:44:52.674486  4690 layer_factory.hpp:76] Creating layer loss
I1127 10:44:52.674553  4690 net.cpp:150] Setting up loss
I1127 10:44:52.674561  4690 net.cpp:157] Top shape: (1)
I1127 10:44:52.674564  4690 net.cpp:160]     with loss weight 1
I1127 10:44:52.674581  4690 net.cpp:165] Memory required for data: 5169924
I1127 10:44:52.674584  4690 net.cpp:226] loss needs backward computation.
I1127 10:44:52.674589  4690 net.cpp:226] ip2 needs backward computation.
I1127 10:44:52.674593  4690 net.cpp:226] relu1 needs backward computation.
I1127 10:44:52.674597  4690 net.cpp:226] ip1 needs backward computation.
I1127 10:44:52.674602  4690 net.cpp:226] pool2 needs backward computation.
I1127 10:44:52.674607  4690 net.cpp:226] conv2 needs backward computation.
I1127 10:44:52.674610  4690 net.cpp:226] pool1 needs backward computation.
I1127 10:44:52.674614  4690 net.cpp:226] conv1 needs backward computation.
I1127 10:44:52.674618  4690 net.cpp:228] mnist does not need backward computation.
I1127 10:44:52.674623  4690 net.cpp:270] This network produces output loss
I1127 10:44:52.674633  4690 net.cpp:283] Network initialization done.
I1127 10:44:52.674862  4690 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:44:52.674890  4690 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:44:52.674998  4690 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:44:52.675055  4690 layer_factory.hpp:76] Creating layer mnist
I1127 10:44:52.675138  4690 net.cpp:106] Creating Layer mnist
I1127 10:44:52.675148  4690 net.cpp:411] mnist -> data
I1127 10:44:52.675155  4690 net.cpp:411] mnist -> label
I1127 10:44:52.675858  4697 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:44:52.675933  4690 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:44:52.679450  4690 net.cpp:150] Setting up mnist
I1127 10:44:52.679463  4690 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:44:52.679469  4690 net.cpp:157] Top shape: 100 (100)
I1127 10:44:52.679473  4690 net.cpp:165] Memory required for data: 314000
I1127 10:44:52.679478  4690 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:44:52.679486  4690 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:44:52.679491  4690 net.cpp:454] label_mnist_1_split <- label
I1127 10:44:52.679497  4690 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:44:52.679504  4690 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:44:52.679538  4690 net.cpp:150] Setting up label_mnist_1_split
I1127 10:44:52.679546  4690 net.cpp:157] Top shape: 100 (100)
I1127 10:44:52.679551  4690 net.cpp:157] Top shape: 100 (100)
I1127 10:44:52.679555  4690 net.cpp:165] Memory required for data: 314800
I1127 10:44:52.679559  4690 layer_factory.hpp:76] Creating layer conv1
I1127 10:44:52.679568  4690 net.cpp:106] Creating Layer conv1
I1127 10:44:52.679572  4690 net.cpp:454] conv1 <- data
I1127 10:44:52.679579  4690 net.cpp:411] conv1 -> conv1
I1127 10:44:52.679721  4690 net.cpp:150] Setting up conv1
I1127 10:44:52.679730  4690 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:44:52.679734  4690 net.cpp:165] Memory required for data: 4922800
I1127 10:44:52.679744  4690 layer_factory.hpp:76] Creating layer pool1
I1127 10:44:52.679754  4690 net.cpp:106] Creating Layer pool1
I1127 10:44:52.679759  4690 net.cpp:454] pool1 <- conv1
I1127 10:44:52.679771  4690 net.cpp:411] pool1 -> pool1
I1127 10:44:52.679798  4690 net.cpp:150] Setting up pool1
I1127 10:44:52.679805  4690 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:44:52.679810  4690 net.cpp:165] Memory required for data: 6074800
I1127 10:44:52.679813  4690 layer_factory.hpp:76] Creating layer conv2
I1127 10:44:52.679822  4690 net.cpp:106] Creating Layer conv2
I1127 10:44:52.679826  4690 net.cpp:454] conv2 <- pool1
I1127 10:44:52.679834  4690 net.cpp:411] conv2 -> conv2
I1127 10:44:52.680074  4690 net.cpp:150] Setting up conv2
I1127 10:44:52.680083  4690 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:44:52.680088  4690 net.cpp:165] Memory required for data: 7354800
I1127 10:44:52.680095  4690 layer_factory.hpp:76] Creating layer pool2
I1127 10:44:52.680102  4690 net.cpp:106] Creating Layer pool2
I1127 10:44:52.680107  4690 net.cpp:454] pool2 <- conv2
I1127 10:44:52.680112  4690 net.cpp:411] pool2 -> pool2
I1127 10:44:52.680137  4690 net.cpp:150] Setting up pool2
I1127 10:44:52.680145  4690 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:44:52.680148  4690 net.cpp:165] Memory required for data: 7674800
I1127 10:44:52.680152  4690 layer_factory.hpp:76] Creating layer ip1
I1127 10:44:52.680160  4690 net.cpp:106] Creating Layer ip1
I1127 10:44:52.680165  4690 net.cpp:454] ip1 <- pool2
I1127 10:44:52.680171  4690 net.cpp:411] ip1 -> ip1
I1127 10:44:52.682309  4690 net.cpp:150] Setting up ip1
I1127 10:44:52.682322  4690 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:44:52.682327  4690 net.cpp:165] Memory required for data: 7874800
I1127 10:44:52.682335  4690 layer_factory.hpp:76] Creating layer relu1
I1127 10:44:52.682343  4690 net.cpp:106] Creating Layer relu1
I1127 10:44:52.682348  4690 net.cpp:454] relu1 <- ip1
I1127 10:44:52.682360  4690 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:44:52.682368  4690 net.cpp:150] Setting up relu1
I1127 10:44:52.682374  4690 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:44:52.682377  4690 net.cpp:165] Memory required for data: 8074800
I1127 10:44:52.682381  4690 layer_factory.hpp:76] Creating layer ip2
I1127 10:44:52.682390  4690 net.cpp:106] Creating Layer ip2
I1127 10:44:52.682394  4690 net.cpp:454] ip2 <- ip1
I1127 10:44:52.682400  4690 net.cpp:411] ip2 -> ip2
I1127 10:44:52.682490  4690 net.cpp:150] Setting up ip2
I1127 10:44:52.682498  4690 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:44:52.682503  4690 net.cpp:165] Memory required for data: 8078800
I1127 10:44:52.682509  4690 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:44:52.682517  4690 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:44:52.682521  4690 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:44:52.682526  4690 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:44:52.682533  4690 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:44:52.682559  4690 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:44:52.682566  4690 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:44:52.682571  4690 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:44:52.682575  4690 net.cpp:165] Memory required for data: 8086800
I1127 10:44:52.682580  4690 layer_factory.hpp:76] Creating layer accuracy
I1127 10:44:52.682586  4690 net.cpp:106] Creating Layer accuracy
I1127 10:44:52.682591  4690 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:44:52.682596  4690 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:44:52.682603  4690 net.cpp:411] accuracy -> accuracy
I1127 10:44:52.682612  4690 net.cpp:150] Setting up accuracy
I1127 10:44:52.682618  4690 net.cpp:157] Top shape: (1)
I1127 10:44:52.682622  4690 net.cpp:165] Memory required for data: 8086804
I1127 10:44:52.682626  4690 layer_factory.hpp:76] Creating layer loss
I1127 10:44:52.682631  4690 net.cpp:106] Creating Layer loss
I1127 10:44:52.682636  4690 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:44:52.682641  4690 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:44:52.682651  4690 net.cpp:411] loss -> loss
I1127 10:44:52.682660  4690 layer_factory.hpp:76] Creating layer loss
I1127 10:44:52.682725  4690 net.cpp:150] Setting up loss
I1127 10:44:52.682734  4690 net.cpp:157] Top shape: (1)
I1127 10:44:52.682741  4690 net.cpp:160]     with loss weight 1
I1127 10:44:52.682749  4690 net.cpp:165] Memory required for data: 8086808
I1127 10:44:52.682754  4690 net.cpp:226] loss needs backward computation.
I1127 10:44:52.682760  4690 net.cpp:228] accuracy does not need backward computation.
I1127 10:44:52.682765  4690 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:44:52.682770  4690 net.cpp:226] ip2 needs backward computation.
I1127 10:44:52.682775  4690 net.cpp:226] relu1 needs backward computation.
I1127 10:44:52.682777  4690 net.cpp:226] ip1 needs backward computation.
I1127 10:44:52.682782  4690 net.cpp:226] pool2 needs backward computation.
I1127 10:44:52.682786  4690 net.cpp:226] conv2 needs backward computation.
I1127 10:44:52.682790  4690 net.cpp:226] pool1 needs backward computation.
I1127 10:44:52.682796  4690 net.cpp:226] conv1 needs backward computation.
I1127 10:44:52.682799  4690 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:44:52.682806  4690 net.cpp:228] mnist does not need backward computation.
I1127 10:44:52.682811  4690 net.cpp:270] This network produces output accuracy
I1127 10:44:52.682814  4690 net.cpp:270] This network produces output loss
I1127 10:44:52.682826  4690 net.cpp:283] Network initialization done.
I1127 10:44:52.682857  4690 solver.cpp:59] Solver scaffolding done.
I1127 10:44:52.683071  4690 caffe.cpp:212] Starting Optimization
I1127 10:44:52.683079  4690 solver.cpp:287] Solving LeNet
I1127 10:44:52.683081  4690 solver.cpp:288] Learning Rate Policy: inv
I1127 10:44:52.683392  4690 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:44:55.733656  4690 solver.cpp:408]     Test net output #0: accuracy = 0.1167
I1127 10:44:55.733794  4690 solver.cpp:408]     Test net output #1: loss = 2.29572 (* 1 = 2.29572 loss)
I1127 10:44:55.750891  4690 solver.cpp:236] Iteration 0, loss = 2.3028
I1127 10:44:55.750998  4690 solver.cpp:252]     Train net output #0: loss = 2.3028 (* 1 = 2.3028 loss)
I1127 10:44:55.751047  4690 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:45:09.024895  4690 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:45:09.922428  4690 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:45:10.293334  4690 solver.cpp:408]     Test net output #0: accuracy = 0.9715
I1127 10:45:10.293493  4690 solver.cpp:408]     Test net output #1: loss = 0.0885661 (* 1 = 0.0885661 loss)
I1127 10:45:10.304507  4690 solver.cpp:236] Iteration 500, loss = 0.136216
I1127 10:45:10.304582  4690 solver.cpp:252]     Train net output #0: loss = 0.136216 (* 1 = 0.136216 loss)
I1127 10:45:10.304596  4690 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:45:23.792199  4690 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:45:23.804716  4690 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:45:23.815347  4690 solver.cpp:320] Iteration 1000, loss = 0.124729
I1127 10:45:23.815399  4690 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:45:25.010025  4690 solver.cpp:408]     Test net output #0: accuracy = 0.9816
I1127 10:45:25.010071  4690 solver.cpp:408]     Test net output #1: loss = 0.0577241 (* 1 = 0.0577241 loss)
I1127 10:45:25.010078  4690 solver.cpp:325] Optimization Done.
I1127 10:45:25.010083  4690 caffe.cpp:215] Optimization Done.
I1127 10:45:25.076637  4732 caffe.cpp:184] Using GPUs 0
I1127 10:45:25.525275  4732 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:45:25.525418  4732 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:45:25.525843  4732 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:45:25.525866  4732 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:45:25.525997  4732 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:45:25.526068  4732 layer_factory.hpp:76] Creating layer mnist
I1127 10:45:25.526551  4732 net.cpp:106] Creating Layer mnist
I1127 10:45:25.526569  4732 net.cpp:411] mnist -> data
I1127 10:45:25.526602  4732 net.cpp:411] mnist -> label
I1127 10:45:25.527328  4735 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:45:25.562830  4732 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:45:25.570108  4732 net.cpp:150] Setting up mnist
I1127 10:45:25.570138  4732 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:45:25.570154  4732 net.cpp:157] Top shape: 64 (64)
I1127 10:45:25.570161  4732 net.cpp:165] Memory required for data: 200960
I1127 10:45:25.570173  4732 layer_factory.hpp:76] Creating layer conv1
I1127 10:45:25.570195  4732 net.cpp:106] Creating Layer conv1
I1127 10:45:25.570204  4732 net.cpp:454] conv1 <- data
I1127 10:45:25.570219  4732 net.cpp:411] conv1 -> conv1
I1127 10:45:25.570909  4732 net.cpp:150] Setting up conv1
I1127 10:45:25.570924  4732 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:45:25.570931  4732 net.cpp:165] Memory required for data: 3150080
I1127 10:45:25.570950  4732 layer_factory.hpp:76] Creating layer pool1
I1127 10:45:25.570963  4732 net.cpp:106] Creating Layer pool1
I1127 10:45:25.570971  4732 net.cpp:454] pool1 <- conv1
I1127 10:45:25.570981  4732 net.cpp:411] pool1 -> pool1
I1127 10:45:25.571435  4732 net.cpp:150] Setting up pool1
I1127 10:45:25.571449  4732 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:45:25.571455  4732 net.cpp:165] Memory required for data: 3887360
I1127 10:45:25.571462  4732 layer_factory.hpp:76] Creating layer conv2
I1127 10:45:25.571475  4732 net.cpp:106] Creating Layer conv2
I1127 10:45:25.571485  4732 net.cpp:454] conv2 <- pool1
I1127 10:45:25.571501  4732 net.cpp:411] conv2 -> conv2
I1127 10:45:25.571902  4732 net.cpp:150] Setting up conv2
I1127 10:45:25.571914  4732 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:45:25.571923  4732 net.cpp:165] Memory required for data: 4706560
I1127 10:45:25.571936  4732 layer_factory.hpp:76] Creating layer pool2
I1127 10:45:25.571951  4732 net.cpp:106] Creating Layer pool2
I1127 10:45:25.571960  4732 net.cpp:454] pool2 <- conv2
I1127 10:45:25.571969  4732 net.cpp:411] pool2 -> pool2
I1127 10:45:25.572011  4732 net.cpp:150] Setting up pool2
I1127 10:45:25.572022  4732 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:45:25.572028  4732 net.cpp:165] Memory required for data: 4911360
I1127 10:45:25.572036  4732 layer_factory.hpp:76] Creating layer ip1
I1127 10:45:25.572048  4732 net.cpp:106] Creating Layer ip1
I1127 10:45:25.572055  4732 net.cpp:454] ip1 <- pool2
I1127 10:45:25.572064  4732 net.cpp:411] ip1 -> ip1
I1127 10:45:25.575728  4732 net.cpp:150] Setting up ip1
I1127 10:45:25.575743  4732 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:45:25.575750  4732 net.cpp:165] Memory required for data: 5039360
I1127 10:45:25.575764  4732 layer_factory.hpp:76] Creating layer relu1
I1127 10:45:25.575774  4732 net.cpp:106] Creating Layer relu1
I1127 10:45:25.575783  4732 net.cpp:454] relu1 <- ip1
I1127 10:45:25.575793  4732 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:45:25.575805  4732 net.cpp:150] Setting up relu1
I1127 10:45:25.575814  4732 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:45:25.575820  4732 net.cpp:165] Memory required for data: 5167360
I1127 10:45:25.575827  4732 layer_factory.hpp:76] Creating layer ip2
I1127 10:45:25.575837  4732 net.cpp:106] Creating Layer ip2
I1127 10:45:25.575845  4732 net.cpp:454] ip2 <- ip1
I1127 10:45:25.575856  4732 net.cpp:411] ip2 -> ip2
I1127 10:45:25.576421  4732 net.cpp:150] Setting up ip2
I1127 10:45:25.576436  4732 net.cpp:157] Top shape: 64 10 (640)
I1127 10:45:25.576442  4732 net.cpp:165] Memory required for data: 5169920
I1127 10:45:25.576453  4732 layer_factory.hpp:76] Creating layer loss
I1127 10:45:25.576467  4732 net.cpp:106] Creating Layer loss
I1127 10:45:25.576473  4732 net.cpp:454] loss <- ip2
I1127 10:45:25.576483  4732 net.cpp:454] loss <- label
I1127 10:45:25.576493  4732 net.cpp:411] loss -> loss
I1127 10:45:25.576508  4732 layer_factory.hpp:76] Creating layer loss
I1127 10:45:25.576606  4732 net.cpp:150] Setting up loss
I1127 10:45:25.576617  4732 net.cpp:157] Top shape: (1)
I1127 10:45:25.576624  4732 net.cpp:160]     with loss weight 1
I1127 10:45:25.576642  4732 net.cpp:165] Memory required for data: 5169924
I1127 10:45:25.576649  4732 net.cpp:226] loss needs backward computation.
I1127 10:45:25.576658  4732 net.cpp:226] ip2 needs backward computation.
I1127 10:45:25.576663  4732 net.cpp:226] relu1 needs backward computation.
I1127 10:45:25.576670  4732 net.cpp:226] ip1 needs backward computation.
I1127 10:45:25.576678  4732 net.cpp:226] pool2 needs backward computation.
I1127 10:45:25.576684  4732 net.cpp:226] conv2 needs backward computation.
I1127 10:45:25.576691  4732 net.cpp:226] pool1 needs backward computation.
I1127 10:45:25.576697  4732 net.cpp:226] conv1 needs backward computation.
I1127 10:45:25.576705  4732 net.cpp:228] mnist does not need backward computation.
I1127 10:45:25.576711  4732 net.cpp:270] This network produces output loss
I1127 10:45:25.576725  4732 net.cpp:283] Network initialization done.
I1127 10:45:25.577075  4732 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:45:25.577107  4732 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:45:25.577273  4732 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:45:25.577364  4732 layer_factory.hpp:76] Creating layer mnist
I1127 10:45:25.577481  4732 net.cpp:106] Creating Layer mnist
I1127 10:45:25.577492  4732 net.cpp:411] mnist -> data
I1127 10:45:25.577505  4732 net.cpp:411] mnist -> label
I1127 10:45:25.578214  4737 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:45:25.578316  4732 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:45:25.691274  4732 net.cpp:150] Setting up mnist
I1127 10:45:25.691318  4732 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:45:25.691330  4732 net.cpp:157] Top shape: 100 (100)
I1127 10:45:25.691337  4732 net.cpp:165] Memory required for data: 314000
I1127 10:45:25.691349  4732 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:45:25.691366  4732 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:45:25.691376  4732 net.cpp:454] label_mnist_1_split <- label
I1127 10:45:25.691390  4732 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:45:25.691406  4732 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:45:25.691501  4732 net.cpp:150] Setting up label_mnist_1_split
I1127 10:45:25.691514  4732 net.cpp:157] Top shape: 100 (100)
I1127 10:45:25.691521  4732 net.cpp:157] Top shape: 100 (100)
I1127 10:45:25.691529  4732 net.cpp:165] Memory required for data: 314800
I1127 10:45:25.691535  4732 layer_factory.hpp:76] Creating layer conv1
I1127 10:45:25.691555  4732 net.cpp:106] Creating Layer conv1
I1127 10:45:25.691563  4732 net.cpp:454] conv1 <- data
I1127 10:45:25.691572  4732 net.cpp:411] conv1 -> conv1
I1127 10:45:25.691814  4732 net.cpp:150] Setting up conv1
I1127 10:45:25.691828  4732 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:45:25.691833  4732 net.cpp:165] Memory required for data: 4922800
I1127 10:45:25.691848  4732 layer_factory.hpp:76] Creating layer pool1
I1127 10:45:25.691860  4732 net.cpp:106] Creating Layer pool1
I1127 10:45:25.691867  4732 net.cpp:454] pool1 <- conv1
I1127 10:45:25.691891  4732 net.cpp:411] pool1 -> pool1
I1127 10:45:25.691933  4732 net.cpp:150] Setting up pool1
I1127 10:45:25.691944  4732 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:45:25.691951  4732 net.cpp:165] Memory required for data: 6074800
I1127 10:45:25.691957  4732 layer_factory.hpp:76] Creating layer conv2
I1127 10:45:25.691977  4732 net.cpp:106] Creating Layer conv2
I1127 10:45:25.691983  4732 net.cpp:454] conv2 <- pool1
I1127 10:45:25.691994  4732 net.cpp:411] conv2 -> conv2
I1127 10:45:25.692606  4732 net.cpp:150] Setting up conv2
I1127 10:45:25.692620  4732 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:45:25.692631  4732 net.cpp:165] Memory required for data: 7354800
I1127 10:45:25.692646  4732 layer_factory.hpp:76] Creating layer pool2
I1127 10:45:25.692656  4732 net.cpp:106] Creating Layer pool2
I1127 10:45:25.692663  4732 net.cpp:454] pool2 <- conv2
I1127 10:45:25.692672  4732 net.cpp:411] pool2 -> pool2
I1127 10:45:25.692716  4732 net.cpp:150] Setting up pool2
I1127 10:45:25.692730  4732 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:45:25.692737  4732 net.cpp:165] Memory required for data: 7674800
I1127 10:45:25.692744  4732 layer_factory.hpp:76] Creating layer ip1
I1127 10:45:25.692755  4732 net.cpp:106] Creating Layer ip1
I1127 10:45:25.692762  4732 net.cpp:454] ip1 <- pool2
I1127 10:45:25.692775  4732 net.cpp:411] ip1 -> ip1
I1127 10:45:25.696583  4732 net.cpp:150] Setting up ip1
I1127 10:45:25.696599  4732 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:45:25.696606  4732 net.cpp:165] Memory required for data: 7874800
I1127 10:45:25.696620  4732 layer_factory.hpp:76] Creating layer relu1
I1127 10:45:25.696630  4732 net.cpp:106] Creating Layer relu1
I1127 10:45:25.696638  4732 net.cpp:454] relu1 <- ip1
I1127 10:45:25.696647  4732 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:45:25.696658  4732 net.cpp:150] Setting up relu1
I1127 10:45:25.696666  4732 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:45:25.696673  4732 net.cpp:165] Memory required for data: 8074800
I1127 10:45:25.696679  4732 layer_factory.hpp:76] Creating layer ip2
I1127 10:45:25.696693  4732 net.cpp:106] Creating Layer ip2
I1127 10:45:25.696702  4732 net.cpp:454] ip2 <- ip1
I1127 10:45:25.696712  4732 net.cpp:411] ip2 -> ip2
I1127 10:45:25.696861  4732 net.cpp:150] Setting up ip2
I1127 10:45:25.696871  4732 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:45:25.696877  4732 net.cpp:165] Memory required for data: 8078800
I1127 10:45:25.696888  4732 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:45:25.696898  4732 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:45:25.696905  4732 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:45:25.696913  4732 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:45:25.696924  4732 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:45:25.696965  4732 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:45:25.696975  4732 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:45:25.696984  4732 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:45:25.696990  4732 net.cpp:165] Memory required for data: 8086800
I1127 10:45:25.696996  4732 layer_factory.hpp:76] Creating layer accuracy
I1127 10:45:25.697007  4732 net.cpp:106] Creating Layer accuracy
I1127 10:45:25.697013  4732 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:45:25.697021  4732 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:45:25.697032  4732 net.cpp:411] accuracy -> accuracy
I1127 10:45:25.697046  4732 net.cpp:150] Setting up accuracy
I1127 10:45:25.697053  4732 net.cpp:157] Top shape: (1)
I1127 10:45:25.697060  4732 net.cpp:165] Memory required for data: 8086804
I1127 10:45:25.697067  4732 layer_factory.hpp:76] Creating layer loss
I1127 10:45:25.697077  4732 net.cpp:106] Creating Layer loss
I1127 10:45:25.697083  4732 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:45:25.697091  4732 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:45:25.697101  4732 net.cpp:411] loss -> loss
I1127 10:45:25.697113  4732 layer_factory.hpp:76] Creating layer loss
I1127 10:45:25.697216  4732 net.cpp:150] Setting up loss
I1127 10:45:25.697228  4732 net.cpp:157] Top shape: (1)
I1127 10:45:25.697235  4732 net.cpp:160]     with loss weight 1
I1127 10:45:25.697250  4732 net.cpp:165] Memory required for data: 8086808
I1127 10:45:25.697257  4732 net.cpp:226] loss needs backward computation.
I1127 10:45:25.697268  4732 net.cpp:228] accuracy does not need backward computation.
I1127 10:45:25.697279  4732 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:45:25.697286  4732 net.cpp:226] ip2 needs backward computation.
I1127 10:45:25.697293  4732 net.cpp:226] relu1 needs backward computation.
I1127 10:45:25.697299  4732 net.cpp:226] ip1 needs backward computation.
I1127 10:45:25.697306  4732 net.cpp:226] pool2 needs backward computation.
I1127 10:45:25.697314  4732 net.cpp:226] conv2 needs backward computation.
I1127 10:45:25.697320  4732 net.cpp:226] pool1 needs backward computation.
I1127 10:45:25.697326  4732 net.cpp:226] conv1 needs backward computation.
I1127 10:45:25.697335  4732 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:45:25.697341  4732 net.cpp:228] mnist does not need backward computation.
I1127 10:45:25.697347  4732 net.cpp:270] This network produces output accuracy
I1127 10:45:25.697355  4732 net.cpp:270] This network produces output loss
I1127 10:45:25.697368  4732 net.cpp:283] Network initialization done.
I1127 10:45:25.697429  4732 solver.cpp:59] Solver scaffolding done.
I1127 10:45:25.697727  4732 caffe.cpp:212] Starting Optimization
I1127 10:45:25.697736  4732 solver.cpp:287] Solving LeNet
I1127 10:45:25.697742  4732 solver.cpp:288] Learning Rate Policy: inv
I1127 10:45:25.698209  4732 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:45:28.041635  4732 solver.cpp:408]     Test net output #0: accuracy = 0.0887
I1127 10:45:28.041702  4732 solver.cpp:408]     Test net output #1: loss = 2.42341 (* 1 = 2.42341 loss)
I1127 10:45:28.053289  4732 solver.cpp:236] Iteration 0, loss = 2.37737
I1127 10:45:28.053345  4732 solver.cpp:252]     Train net output #0: loss = 2.37737 (* 1 = 2.37737 loss)
I1127 10:45:28.053362  4732 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:45:41.468556  4732 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:45:43.034183  4732 solver.cpp:408]     Test net output #0: accuracy = 0.9716
I1127 10:45:43.034222  4732 solver.cpp:408]     Test net output #1: loss = 0.0912983 (* 1 = 0.0912983 loss)
I1127 10:45:43.064229  4732 solver.cpp:236] Iteration 500, loss = 0.119023
I1127 10:45:43.064245  4732 solver.cpp:252]     Train net output #0: loss = 0.119023 (* 1 = 0.119023 loss)
I1127 10:45:43.064255  4732 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:45:56.181283  4732 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:45:56.277142  4732 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:45:56.304363  4732 solver.cpp:320] Iteration 1000, loss = 0.0857316
I1127 10:45:56.304384  4732 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:45:59.157909  4732 solver.cpp:408]     Test net output #0: accuracy = 0.9821
I1127 10:45:59.158066  4732 solver.cpp:408]     Test net output #1: loss = 0.0578257 (* 1 = 0.0578257 loss)
I1127 10:45:59.158116  4732 solver.cpp:325] Optimization Done.
I1127 10:45:59.158156  4732 caffe.cpp:215] Optimization Done.
I1127 10:45:59.277781  4761 caffe.cpp:184] Using GPUs 0
I1127 10:45:59.665066  4761 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:45:59.665328  4761 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:45:59.665855  4761 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:45:59.665896  4761 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:45:59.666055  4761 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:45:59.666229  4761 layer_factory.hpp:76] Creating layer mnist
I1127 10:45:59.666824  4761 net.cpp:106] Creating Layer mnist
I1127 10:45:59.666859  4761 net.cpp:411] mnist -> data
I1127 10:45:59.666904  4761 net.cpp:411] mnist -> label
I1127 10:45:59.668108  4765 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:45:59.681244  4761 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:45:59.683271  4761 net.cpp:150] Setting up mnist
I1127 10:45:59.683370  4761 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:45:59.683383  4761 net.cpp:157] Top shape: 64 (64)
I1127 10:45:59.683390  4761 net.cpp:165] Memory required for data: 200960
I1127 10:45:59.683418  4761 layer_factory.hpp:76] Creating layer conv1
I1127 10:45:59.683475  4761 net.cpp:106] Creating Layer conv1
I1127 10:45:59.683495  4761 net.cpp:454] conv1 <- data
I1127 10:45:59.683533  4761 net.cpp:411] conv1 -> conv1
I1127 10:45:59.685087  4761 net.cpp:150] Setting up conv1
I1127 10:45:59.685173  4761 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:45:59.685180  4761 net.cpp:165] Memory required for data: 3150080
I1127 10:45:59.685217  4761 layer_factory.hpp:76] Creating layer pool1
I1127 10:45:59.685257  4761 net.cpp:106] Creating Layer pool1
I1127 10:45:59.685268  4761 net.cpp:454] pool1 <- conv1
I1127 10:45:59.685283  4761 net.cpp:411] pool1 -> pool1
I1127 10:45:59.685423  4761 net.cpp:150] Setting up pool1
I1127 10:45:59.685439  4761 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:45:59.685446  4761 net.cpp:165] Memory required for data: 3887360
I1127 10:45:59.685452  4761 layer_factory.hpp:76] Creating layer conv2
I1127 10:45:59.685472  4761 net.cpp:106] Creating Layer conv2
I1127 10:45:59.685480  4761 net.cpp:454] conv2 <- pool1
I1127 10:45:59.685492  4761 net.cpp:411] conv2 -> conv2
I1127 10:45:59.686002  4761 net.cpp:150] Setting up conv2
I1127 10:45:59.686074  4761 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:45:59.686086  4761 net.cpp:165] Memory required for data: 4706560
I1127 10:45:59.686120  4761 layer_factory.hpp:76] Creating layer pool2
I1127 10:45:59.686172  4761 net.cpp:106] Creating Layer pool2
I1127 10:45:59.686192  4761 net.cpp:454] pool2 <- conv2
I1127 10:45:59.686209  4761 net.cpp:411] pool2 -> pool2
I1127 10:45:59.686314  4761 net.cpp:150] Setting up pool2
I1127 10:45:59.686354  4761 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:45:59.686367  4761 net.cpp:165] Memory required for data: 4911360
I1127 10:45:59.686377  4761 layer_factory.hpp:76] Creating layer ip1
I1127 10:45:59.686401  4761 net.cpp:106] Creating Layer ip1
I1127 10:45:59.686419  4761 net.cpp:454] ip1 <- pool2
I1127 10:45:59.686432  4761 net.cpp:411] ip1 -> ip1
I1127 10:45:59.691170  4761 net.cpp:150] Setting up ip1
I1127 10:45:59.691285  4761 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:45:59.691300  4761 net.cpp:165] Memory required for data: 5039360
I1127 10:45:59.691344  4761 layer_factory.hpp:76] Creating layer relu1
I1127 10:45:59.691375  4761 net.cpp:106] Creating Layer relu1
I1127 10:45:59.691390  4761 net.cpp:454] relu1 <- ip1
I1127 10:45:59.691409  4761 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:45:59.691452  4761 net.cpp:150] Setting up relu1
I1127 10:45:59.691462  4761 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:45:59.691472  4761 net.cpp:165] Memory required for data: 5167360
I1127 10:45:59.691480  4761 layer_factory.hpp:76] Creating layer ip2
I1127 10:45:59.691511  4761 net.cpp:106] Creating Layer ip2
I1127 10:45:59.691522  4761 net.cpp:454] ip2 <- ip1
I1127 10:45:59.691536  4761 net.cpp:411] ip2 -> ip2
I1127 10:45:59.692962  4761 net.cpp:150] Setting up ip2
I1127 10:45:59.693059  4761 net.cpp:157] Top shape: 64 10 (640)
I1127 10:45:59.693071  4761 net.cpp:165] Memory required for data: 5169920
I1127 10:45:59.693099  4761 layer_factory.hpp:76] Creating layer loss
I1127 10:45:59.693136  4761 net.cpp:106] Creating Layer loss
I1127 10:45:59.693151  4761 net.cpp:454] loss <- ip2
I1127 10:45:59.693169  4761 net.cpp:454] loss <- label
I1127 10:45:59.693192  4761 net.cpp:411] loss -> loss
I1127 10:45:59.693224  4761 layer_factory.hpp:76] Creating layer loss
I1127 10:45:59.693395  4761 net.cpp:150] Setting up loss
I1127 10:45:59.693414  4761 net.cpp:157] Top shape: (1)
I1127 10:45:59.693423  4761 net.cpp:160]     with loss weight 1
I1127 10:45:59.693483  4761 net.cpp:165] Memory required for data: 5169924
I1127 10:45:59.693496  4761 net.cpp:226] loss needs backward computation.
I1127 10:45:59.693505  4761 net.cpp:226] ip2 needs backward computation.
I1127 10:45:59.693513  4761 net.cpp:226] relu1 needs backward computation.
I1127 10:45:59.693521  4761 net.cpp:226] ip1 needs backward computation.
I1127 10:45:59.693529  4761 net.cpp:226] pool2 needs backward computation.
I1127 10:45:59.693537  4761 net.cpp:226] conv2 needs backward computation.
I1127 10:45:59.693547  4761 net.cpp:226] pool1 needs backward computation.
I1127 10:45:59.693555  4761 net.cpp:226] conv1 needs backward computation.
I1127 10:45:59.693563  4761 net.cpp:228] mnist does not need backward computation.
I1127 10:45:59.693572  4761 net.cpp:270] This network produces output loss
I1127 10:45:59.693589  4761 net.cpp:283] Network initialization done.
I1127 10:45:59.694123  4761 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:45:59.694231  4761 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:45:59.694483  4761 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:45:59.694625  4761 layer_factory.hpp:76] Creating layer mnist
I1127 10:45:59.694865  4761 net.cpp:106] Creating Layer mnist
I1127 10:45:59.694883  4761 net.cpp:411] mnist -> data
I1127 10:45:59.694910  4761 net.cpp:411] mnist -> label
I1127 10:45:59.699730  4767 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:45:59.702414  4761 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:45:59.704711  4761 net.cpp:150] Setting up mnist
I1127 10:45:59.704789  4761 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:45:59.704802  4761 net.cpp:157] Top shape: 100 (100)
I1127 10:45:59.704809  4761 net.cpp:165] Memory required for data: 314000
I1127 10:45:59.704823  4761 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:45:59.704852  4761 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:45:59.704860  4761 net.cpp:454] label_mnist_1_split <- label
I1127 10:45:59.704874  4761 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:45:59.704888  4761 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:45:59.704957  4761 net.cpp:150] Setting up label_mnist_1_split
I1127 10:45:59.704968  4761 net.cpp:157] Top shape: 100 (100)
I1127 10:45:59.704977  4761 net.cpp:157] Top shape: 100 (100)
I1127 10:45:59.704983  4761 net.cpp:165] Memory required for data: 314800
I1127 10:45:59.704989  4761 layer_factory.hpp:76] Creating layer conv1
I1127 10:45:59.705015  4761 net.cpp:106] Creating Layer conv1
I1127 10:45:59.705023  4761 net.cpp:454] conv1 <- data
I1127 10:45:59.705032  4761 net.cpp:411] conv1 -> conv1
I1127 10:45:59.705292  4761 net.cpp:150] Setting up conv1
I1127 10:45:59.705307  4761 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:45:59.705312  4761 net.cpp:165] Memory required for data: 4922800
I1127 10:45:59.705328  4761 layer_factory.hpp:76] Creating layer pool1
I1127 10:45:59.705343  4761 net.cpp:106] Creating Layer pool1
I1127 10:45:59.705350  4761 net.cpp:454] pool1 <- conv1
I1127 10:45:59.705391  4761 net.cpp:411] pool1 -> pool1
I1127 10:45:59.705437  4761 net.cpp:150] Setting up pool1
I1127 10:45:59.705449  4761 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:45:59.705456  4761 net.cpp:165] Memory required for data: 6074800
I1127 10:45:59.705462  4761 layer_factory.hpp:76] Creating layer conv2
I1127 10:45:59.705482  4761 net.cpp:106] Creating Layer conv2
I1127 10:45:59.705488  4761 net.cpp:454] conv2 <- pool1
I1127 10:45:59.705498  4761 net.cpp:411] conv2 -> conv2
I1127 10:45:59.706066  4761 net.cpp:150] Setting up conv2
I1127 10:45:59.706125  4761 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:45:59.706135  4761 net.cpp:165] Memory required for data: 7354800
I1127 10:45:59.706169  4761 layer_factory.hpp:76] Creating layer pool2
I1127 10:45:59.706190  4761 net.cpp:106] Creating Layer pool2
I1127 10:45:59.706198  4761 net.cpp:454] pool2 <- conv2
I1127 10:45:59.706224  4761 net.cpp:411] pool2 -> pool2
I1127 10:45:59.706784  4761 net.cpp:150] Setting up pool2
I1127 10:45:59.706818  4761 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:45:59.706828  4761 net.cpp:165] Memory required for data: 7674800
I1127 10:45:59.706840  4761 layer_factory.hpp:76] Creating layer ip1
I1127 10:45:59.706864  4761 net.cpp:106] Creating Layer ip1
I1127 10:45:59.706874  4761 net.cpp:454] ip1 <- pool2
I1127 10:45:59.706892  4761 net.cpp:411] ip1 -> ip1
I1127 10:45:59.712594  4761 net.cpp:150] Setting up ip1
I1127 10:45:59.712712  4761 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:45:59.712723  4761 net.cpp:165] Memory required for data: 7874800
I1127 10:45:59.712757  4761 layer_factory.hpp:76] Creating layer relu1
I1127 10:45:59.712790  4761 net.cpp:106] Creating Layer relu1
I1127 10:45:59.712805  4761 net.cpp:454] relu1 <- ip1
I1127 10:45:59.712821  4761 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:45:59.712846  4761 net.cpp:150] Setting up relu1
I1127 10:45:59.712857  4761 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:45:59.712864  4761 net.cpp:165] Memory required for data: 8074800
I1127 10:45:59.712872  4761 layer_factory.hpp:76] Creating layer ip2
I1127 10:45:59.712899  4761 net.cpp:106] Creating Layer ip2
I1127 10:45:59.712910  4761 net.cpp:454] ip2 <- ip1
I1127 10:45:59.712921  4761 net.cpp:411] ip2 -> ip2
I1127 10:45:59.713186  4761 net.cpp:150] Setting up ip2
I1127 10:45:59.713207  4761 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:45:59.713213  4761 net.cpp:165] Memory required for data: 8078800
I1127 10:45:59.713227  4761 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:45:59.713243  4761 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:45:59.713251  4761 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:45:59.713264  4761 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:45:59.713276  4761 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:45:59.713328  4761 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:45:59.713343  4761 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:45:59.713354  4761 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:45:59.713361  4761 net.cpp:165] Memory required for data: 8086800
I1127 10:45:59.713369  4761 layer_factory.hpp:76] Creating layer accuracy
I1127 10:45:59.713384  4761 net.cpp:106] Creating Layer accuracy
I1127 10:45:59.713393  4761 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:45:59.713402  4761 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:45:59.713417  4761 net.cpp:411] accuracy -> accuracy
I1127 10:45:59.713433  4761 net.cpp:150] Setting up accuracy
I1127 10:45:59.713443  4761 net.cpp:157] Top shape: (1)
I1127 10:45:59.713451  4761 net.cpp:165] Memory required for data: 8086804
I1127 10:45:59.713459  4761 layer_factory.hpp:76] Creating layer loss
I1127 10:45:59.713471  4761 net.cpp:106] Creating Layer loss
I1127 10:45:59.713479  4761 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:45:59.713487  4761 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:45:59.713498  4761 net.cpp:411] loss -> loss
I1127 10:45:59.713516  4761 layer_factory.hpp:76] Creating layer loss
I1127 10:45:59.713676  4761 net.cpp:150] Setting up loss
I1127 10:45:59.713690  4761 net.cpp:157] Top shape: (1)
I1127 10:45:59.713696  4761 net.cpp:160]     with loss weight 1
I1127 10:45:59.713727  4761 net.cpp:165] Memory required for data: 8086808
I1127 10:45:59.713734  4761 net.cpp:226] loss needs backward computation.
I1127 10:45:59.713749  4761 net.cpp:228] accuracy does not need backward computation.
I1127 10:45:59.713757  4761 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:45:59.713762  4761 net.cpp:226] ip2 needs backward computation.
I1127 10:45:59.713769  4761 net.cpp:226] relu1 needs backward computation.
I1127 10:45:59.713776  4761 net.cpp:226] ip1 needs backward computation.
I1127 10:45:59.713783  4761 net.cpp:226] pool2 needs backward computation.
I1127 10:45:59.713790  4761 net.cpp:226] conv2 needs backward computation.
I1127 10:45:59.713798  4761 net.cpp:226] pool1 needs backward computation.
I1127 10:45:59.713810  4761 net.cpp:226] conv1 needs backward computation.
I1127 10:45:59.713819  4761 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:45:59.713826  4761 net.cpp:228] mnist does not need backward computation.
I1127 10:45:59.713832  4761 net.cpp:270] This network produces output accuracy
I1127 10:45:59.713840  4761 net.cpp:270] This network produces output loss
I1127 10:45:59.713860  4761 net.cpp:283] Network initialization done.
I1127 10:45:59.713996  4761 solver.cpp:59] Solver scaffolding done.
I1127 10:45:59.714529  4761 caffe.cpp:212] Starting Optimization
I1127 10:45:59.714576  4761 solver.cpp:287] Solving LeNet
I1127 10:45:59.714586  4761 solver.cpp:288] Learning Rate Policy: inv
I1127 10:45:59.715849  4761 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:45:59.717002  4761 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:46:00.855195  4761 solver.cpp:408]     Test net output #0: accuracy = 0.0941
I1127 10:46:00.855365  4761 solver.cpp:408]     Test net output #1: loss = 2.32857 (* 1 = 2.32857 loss)
I1127 10:46:00.869941  4761 solver.cpp:236] Iteration 0, loss = 2.34371
I1127 10:46:00.870029  4761 solver.cpp:252]     Train net output #0: loss = 2.34371 (* 1 = 2.34371 loss)
I1127 10:46:00.870046  4761 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:46:13.922423  4761 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:46:15.567191  4761 solver.cpp:408]     Test net output #0: accuracy = 0.9733
I1127 10:46:15.567349  4761 solver.cpp:408]     Test net output #1: loss = 0.0849498 (* 1 = 0.0849498 loss)
I1127 10:46:15.579848  4761 solver.cpp:236] Iteration 500, loss = 0.126201
I1127 10:46:15.579948  4761 solver.cpp:252]     Train net output #0: loss = 0.126201 (* 1 = 0.126201 loss)
I1127 10:46:15.579965  4761 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:46:29.040051  4761 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:46:29.054675  4761 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:46:29.064393  4761 solver.cpp:320] Iteration 1000, loss = 0.0753584
I1127 10:46:29.064467  4761 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:46:31.632786  4761 solver.cpp:408]     Test net output #0: accuracy = 0.9824
I1127 10:46:31.632964  4761 solver.cpp:408]     Test net output #1: loss = 0.0548174 (* 1 = 0.0548174 loss)
I1127 10:46:31.632977  4761 solver.cpp:325] Optimization Done.
I1127 10:46:31.632983  4761 caffe.cpp:215] Optimization Done.
I1127 10:46:31.754721  4790 caffe.cpp:184] Using GPUs 0
I1127 10:46:32.174629  4790 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:46:32.174934  4790 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:46:32.175389  4790 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:46:32.175412  4790 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:46:32.175549  4790 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:46:32.175642  4790 layer_factory.hpp:76] Creating layer mnist
I1127 10:46:32.176198  4790 net.cpp:106] Creating Layer mnist
I1127 10:46:32.176239  4790 net.cpp:411] mnist -> data
I1127 10:46:32.176317  4790 net.cpp:411] mnist -> label
I1127 10:46:32.177620  4793 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:46:32.205178  4790 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:46:32.207306  4790 net.cpp:150] Setting up mnist
I1127 10:46:32.207451  4790 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:46:32.207469  4790 net.cpp:157] Top shape: 64 (64)
I1127 10:46:32.207478  4790 net.cpp:165] Memory required for data: 200960
I1127 10:46:32.207504  4790 layer_factory.hpp:76] Creating layer conv1
I1127 10:46:32.207567  4790 net.cpp:106] Creating Layer conv1
I1127 10:46:32.207588  4790 net.cpp:454] conv1 <- data
I1127 10:46:32.207620  4790 net.cpp:411] conv1 -> conv1
I1127 10:46:32.211695  4790 net.cpp:150] Setting up conv1
I1127 10:46:32.211724  4790 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:46:32.211730  4790 net.cpp:165] Memory required for data: 3150080
I1127 10:46:32.211743  4790 layer_factory.hpp:76] Creating layer pool1
I1127 10:46:32.211757  4790 net.cpp:106] Creating Layer pool1
I1127 10:46:32.211762  4790 net.cpp:454] pool1 <- conv1
I1127 10:46:32.211769  4790 net.cpp:411] pool1 -> pool1
I1127 10:46:32.211823  4790 net.cpp:150] Setting up pool1
I1127 10:46:32.211832  4790 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:46:32.211835  4790 net.cpp:165] Memory required for data: 3887360
I1127 10:46:32.211839  4790 layer_factory.hpp:76] Creating layer conv2
I1127 10:46:32.211854  4790 net.cpp:106] Creating Layer conv2
I1127 10:46:32.211859  4790 net.cpp:454] conv2 <- pool1
I1127 10:46:32.211866  4790 net.cpp:411] conv2 -> conv2
I1127 10:46:32.212133  4790 net.cpp:150] Setting up conv2
I1127 10:46:32.212146  4790 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:46:32.212152  4790 net.cpp:165] Memory required for data: 4706560
I1127 10:46:32.212160  4790 layer_factory.hpp:76] Creating layer pool2
I1127 10:46:32.212168  4790 net.cpp:106] Creating Layer pool2
I1127 10:46:32.212172  4790 net.cpp:454] pool2 <- conv2
I1127 10:46:32.212178  4790 net.cpp:411] pool2 -> pool2
I1127 10:46:32.212208  4790 net.cpp:150] Setting up pool2
I1127 10:46:32.212214  4790 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:46:32.212218  4790 net.cpp:165] Memory required for data: 4911360
I1127 10:46:32.212223  4790 layer_factory.hpp:76] Creating layer ip1
I1127 10:46:32.212232  4790 net.cpp:106] Creating Layer ip1
I1127 10:46:32.212236  4790 net.cpp:454] ip1 <- pool2
I1127 10:46:32.212244  4790 net.cpp:411] ip1 -> ip1
I1127 10:46:32.215476  4790 net.cpp:150] Setting up ip1
I1127 10:46:32.215605  4790 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:46:32.215641  4790 net.cpp:165] Memory required for data: 5039360
I1127 10:46:32.215683  4790 layer_factory.hpp:76] Creating layer relu1
I1127 10:46:32.215715  4790 net.cpp:106] Creating Layer relu1
I1127 10:46:32.215730  4790 net.cpp:454] relu1 <- ip1
I1127 10:46:32.215747  4790 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:46:32.215783  4790 net.cpp:150] Setting up relu1
I1127 10:46:32.215795  4790 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:46:32.215801  4790 net.cpp:165] Memory required for data: 5167360
I1127 10:46:32.215808  4790 layer_factory.hpp:76] Creating layer ip2
I1127 10:46:32.215832  4790 net.cpp:106] Creating Layer ip2
I1127 10:46:32.215842  4790 net.cpp:454] ip2 <- ip1
I1127 10:46:32.215852  4790 net.cpp:411] ip2 -> ip2
I1127 10:46:32.217339  4790 net.cpp:150] Setting up ip2
I1127 10:46:32.217448  4790 net.cpp:157] Top shape: 64 10 (640)
I1127 10:46:32.217463  4790 net.cpp:165] Memory required for data: 5169920
I1127 10:46:32.217491  4790 layer_factory.hpp:76] Creating layer loss
I1127 10:46:32.217526  4790 net.cpp:106] Creating Layer loss
I1127 10:46:32.217540  4790 net.cpp:454] loss <- ip2
I1127 10:46:32.217555  4790 net.cpp:454] loss <- label
I1127 10:46:32.217576  4790 net.cpp:411] loss -> loss
I1127 10:46:32.217617  4790 layer_factory.hpp:76] Creating layer loss
I1127 10:46:32.217809  4790 net.cpp:150] Setting up loss
I1127 10:46:32.217834  4790 net.cpp:157] Top shape: (1)
I1127 10:46:32.217842  4790 net.cpp:160]     with loss weight 1
I1127 10:46:32.217880  4790 net.cpp:165] Memory required for data: 5169924
I1127 10:46:32.217890  4790 net.cpp:226] loss needs backward computation.
I1127 10:46:32.217900  4790 net.cpp:226] ip2 needs backward computation.
I1127 10:46:32.217908  4790 net.cpp:226] relu1 needs backward computation.
I1127 10:46:32.217916  4790 net.cpp:226] ip1 needs backward computation.
I1127 10:46:32.217923  4790 net.cpp:226] pool2 needs backward computation.
I1127 10:46:32.217931  4790 net.cpp:226] conv2 needs backward computation.
I1127 10:46:32.217939  4790 net.cpp:226] pool1 needs backward computation.
I1127 10:46:32.217947  4790 net.cpp:226] conv1 needs backward computation.
I1127 10:46:32.217957  4790 net.cpp:228] mnist does not need backward computation.
I1127 10:46:32.217964  4790 net.cpp:270] This network produces output loss
I1127 10:46:32.217979  4790 net.cpp:283] Network initialization done.
I1127 10:46:32.218477  4790 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:46:32.218600  4790 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:46:32.218889  4790 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:46:32.219007  4790 layer_factory.hpp:76] Creating layer mnist
I1127 10:46:32.219198  4790 net.cpp:106] Creating Layer mnist
I1127 10:46:32.219219  4790 net.cpp:411] mnist -> data
I1127 10:46:32.219244  4790 net.cpp:411] mnist -> label
I1127 10:46:32.220329  4795 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:46:32.225172  4790 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:46:32.227452  4790 net.cpp:150] Setting up mnist
I1127 10:46:32.227551  4790 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:46:32.227574  4790 net.cpp:157] Top shape: 100 (100)
I1127 10:46:32.227586  4790 net.cpp:165] Memory required for data: 314000
I1127 10:46:32.227602  4790 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:46:32.227632  4790 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:46:32.227640  4790 net.cpp:454] label_mnist_1_split <- label
I1127 10:46:32.227653  4790 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:46:32.227669  4790 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:46:32.227733  4790 net.cpp:150] Setting up label_mnist_1_split
I1127 10:46:32.227748  4790 net.cpp:157] Top shape: 100 (100)
I1127 10:46:32.227758  4790 net.cpp:157] Top shape: 100 (100)
I1127 10:46:32.227766  4790 net.cpp:165] Memory required for data: 314800
I1127 10:46:32.227774  4790 layer_factory.hpp:76] Creating layer conv1
I1127 10:46:32.227797  4790 net.cpp:106] Creating Layer conv1
I1127 10:46:32.227808  4790 net.cpp:454] conv1 <- data
I1127 10:46:32.227825  4790 net.cpp:411] conv1 -> conv1
I1127 10:46:32.228124  4790 net.cpp:150] Setting up conv1
I1127 10:46:32.228138  4790 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:46:32.228145  4790 net.cpp:165] Memory required for data: 4922800
I1127 10:46:32.228159  4790 layer_factory.hpp:76] Creating layer pool1
I1127 10:46:32.228170  4790 net.cpp:106] Creating Layer pool1
I1127 10:46:32.228178  4790 net.cpp:454] pool1 <- conv1
I1127 10:46:32.228200  4790 net.cpp:411] pool1 -> pool1
I1127 10:46:32.228242  4790 net.cpp:150] Setting up pool1
I1127 10:46:32.228253  4790 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:46:32.228260  4790 net.cpp:165] Memory required for data: 6074800
I1127 10:46:32.228266  4790 layer_factory.hpp:76] Creating layer conv2
I1127 10:46:32.228281  4790 net.cpp:106] Creating Layer conv2
I1127 10:46:32.228287  4790 net.cpp:454] conv2 <- pool1
I1127 10:46:32.228298  4790 net.cpp:411] conv2 -> conv2
I1127 10:46:32.236174  4790 net.cpp:150] Setting up conv2
I1127 10:46:32.236264  4790 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:46:32.236274  4790 net.cpp:165] Memory required for data: 7354800
I1127 10:46:32.236305  4790 layer_factory.hpp:76] Creating layer pool2
I1127 10:46:32.236335  4790 net.cpp:106] Creating Layer pool2
I1127 10:46:32.236346  4790 net.cpp:454] pool2 <- conv2
I1127 10:46:32.236363  4790 net.cpp:411] pool2 -> pool2
I1127 10:46:32.236435  4790 net.cpp:150] Setting up pool2
I1127 10:46:32.236449  4790 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:46:32.236455  4790 net.cpp:165] Memory required for data: 7674800
I1127 10:46:32.236461  4790 layer_factory.hpp:76] Creating layer ip1
I1127 10:46:32.236482  4790 net.cpp:106] Creating Layer ip1
I1127 10:46:32.236490  4790 net.cpp:454] ip1 <- pool2
I1127 10:46:32.236500  4790 net.cpp:411] ip1 -> ip1
I1127 10:46:32.241689  4790 net.cpp:150] Setting up ip1
I1127 10:46:32.241796  4790 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:46:32.241809  4790 net.cpp:165] Memory required for data: 7874800
I1127 10:46:32.241844  4790 layer_factory.hpp:76] Creating layer relu1
I1127 10:46:32.241884  4790 net.cpp:106] Creating Layer relu1
I1127 10:46:32.241904  4790 net.cpp:454] relu1 <- ip1
I1127 10:46:32.241925  4790 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:46:32.241953  4790 net.cpp:150] Setting up relu1
I1127 10:46:32.241966  4790 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:46:32.241976  4790 net.cpp:165] Memory required for data: 8074800
I1127 10:46:32.241986  4790 layer_factory.hpp:76] Creating layer ip2
I1127 10:46:32.242013  4790 net.cpp:106] Creating Layer ip2
I1127 10:46:32.242024  4790 net.cpp:454] ip2 <- ip1
I1127 10:46:32.242064  4790 net.cpp:411] ip2 -> ip2
I1127 10:46:32.245262  4790 net.cpp:150] Setting up ip2
I1127 10:46:32.245347  4790 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:46:32.245357  4790 net.cpp:165] Memory required for data: 8078800
I1127 10:46:32.245378  4790 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:46:32.245400  4790 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:46:32.245411  4790 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:46:32.245427  4790 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:46:32.245452  4790 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:46:32.245525  4790 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:46:32.245542  4790 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:46:32.245550  4790 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:46:32.245556  4790 net.cpp:165] Memory required for data: 8086800
I1127 10:46:32.245563  4790 layer_factory.hpp:76] Creating layer accuracy
I1127 10:46:32.245578  4790 net.cpp:106] Creating Layer accuracy
I1127 10:46:32.245584  4790 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:46:32.245592  4790 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:46:32.245601  4790 net.cpp:411] accuracy -> accuracy
I1127 10:46:32.245616  4790 net.cpp:150] Setting up accuracy
I1127 10:46:32.245625  4790 net.cpp:157] Top shape: (1)
I1127 10:46:32.245631  4790 net.cpp:165] Memory required for data: 8086804
I1127 10:46:32.245637  4790 layer_factory.hpp:76] Creating layer loss
I1127 10:46:32.245651  4790 net.cpp:106] Creating Layer loss
I1127 10:46:32.245658  4790 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:46:32.245666  4790 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:46:32.245674  4790 net.cpp:411] loss -> loss
I1127 10:46:32.245695  4790 layer_factory.hpp:76] Creating layer loss
I1127 10:46:32.245905  4790 net.cpp:150] Setting up loss
I1127 10:46:32.245921  4790 net.cpp:157] Top shape: (1)
I1127 10:46:32.245928  4790 net.cpp:160]     with loss weight 1
I1127 10:46:32.245954  4790 net.cpp:165] Memory required for data: 8086808
I1127 10:46:32.245960  4790 net.cpp:226] loss needs backward computation.
I1127 10:46:32.245975  4790 net.cpp:228] accuracy does not need backward computation.
I1127 10:46:32.245982  4790 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:46:32.245990  4790 net.cpp:226] ip2 needs backward computation.
I1127 10:46:32.245995  4790 net.cpp:226] relu1 needs backward computation.
I1127 10:46:32.246002  4790 net.cpp:226] ip1 needs backward computation.
I1127 10:46:32.246009  4790 net.cpp:226] pool2 needs backward computation.
I1127 10:46:32.246016  4790 net.cpp:226] conv2 needs backward computation.
I1127 10:46:32.246022  4790 net.cpp:226] pool1 needs backward computation.
I1127 10:46:32.246028  4790 net.cpp:226] conv1 needs backward computation.
I1127 10:46:32.246036  4790 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:46:32.246044  4790 net.cpp:228] mnist does not need backward computation.
I1127 10:46:32.246050  4790 net.cpp:270] This network produces output accuracy
I1127 10:46:32.246057  4790 net.cpp:270] This network produces output loss
I1127 10:46:32.246076  4790 net.cpp:283] Network initialization done.
I1127 10:46:32.246247  4790 solver.cpp:59] Solver scaffolding done.
I1127 10:46:32.246671  4790 caffe.cpp:212] Starting Optimization
I1127 10:46:32.246691  4790 solver.cpp:287] Solving LeNet
I1127 10:46:32.246698  4790 solver.cpp:288] Learning Rate Policy: inv
I1127 10:46:32.247575  4790 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:46:33.379309  4790 solver.cpp:408]     Test net output #0: accuracy = 0.1401
I1127 10:46:33.379428  4790 solver.cpp:408]     Test net output #1: loss = 2.3782 (* 1 = 2.3782 loss)
I1127 10:46:33.391185  4790 solver.cpp:236] Iteration 0, loss = 2.38586
I1127 10:46:33.391294  4790 solver.cpp:252]     Train net output #0: loss = 2.38586 (* 1 = 2.38586 loss)
I1127 10:46:33.391336  4790 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:46:45.674650  4790 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:46:48.067770  4790 solver.cpp:408]     Test net output #0: accuracy = 0.9716
I1127 10:46:48.067826  4790 solver.cpp:408]     Test net output #1: loss = 0.0885299 (* 1 = 0.0885299 loss)
I1127 10:46:48.080679  4790 solver.cpp:236] Iteration 500, loss = 0.0959988
I1127 10:46:48.080790  4790 solver.cpp:252]     Train net output #0: loss = 0.0959989 (* 1 = 0.0959989 loss)
I1127 10:46:48.080816  4790 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:47:01.307778  4790 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:47:01.327473  4790 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:47:01.359084  4790 solver.cpp:320] Iteration 1000, loss = 0.117471
I1127 10:47:01.359104  4790 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:47:02.692621  4790 solver.cpp:408]     Test net output #0: accuracy = 0.9822
I1127 10:47:02.692759  4790 solver.cpp:408]     Test net output #1: loss = 0.0576605 (* 1 = 0.0576605 loss)
I1127 10:47:02.692775  4790 solver.cpp:325] Optimization Done.
I1127 10:47:02.692780  4790 caffe.cpp:215] Optimization Done.
I1127 10:47:02.834759  4816 caffe.cpp:184] Using GPUs 0
I1127 10:47:03.168892  4816 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:47:03.169006  4816 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:47:03.169265  4816 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:47:03.169281  4816 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:47:03.169366  4816 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:47:03.169427  4816 layer_factory.hpp:76] Creating layer mnist
I1127 10:47:03.169741  4816 net.cpp:106] Creating Layer mnist
I1127 10:47:03.169752  4816 net.cpp:411] mnist -> data
I1127 10:47:03.169773  4816 net.cpp:411] mnist -> label
I1127 10:47:03.170562  4820 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:47:03.202797  4816 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:47:03.209164  4816 net.cpp:150] Setting up mnist
I1127 10:47:03.209185  4816 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:47:03.209192  4816 net.cpp:157] Top shape: 64 (64)
I1127 10:47:03.209197  4816 net.cpp:165] Memory required for data: 200960
I1127 10:47:03.209205  4816 layer_factory.hpp:76] Creating layer conv1
I1127 10:47:03.209220  4816 net.cpp:106] Creating Layer conv1
I1127 10:47:03.209228  4816 net.cpp:454] conv1 <- data
I1127 10:47:03.209239  4816 net.cpp:411] conv1 -> conv1
I1127 10:47:03.209846  4816 net.cpp:150] Setting up conv1
I1127 10:47:03.209857  4816 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:47:03.209861  4816 net.cpp:165] Memory required for data: 3150080
I1127 10:47:03.209873  4816 layer_factory.hpp:76] Creating layer pool1
I1127 10:47:03.209882  4816 net.cpp:106] Creating Layer pool1
I1127 10:47:03.209887  4816 net.cpp:454] pool1 <- conv1
I1127 10:47:03.209892  4816 net.cpp:411] pool1 -> pool1
I1127 10:47:03.209940  4816 net.cpp:150] Setting up pool1
I1127 10:47:03.209947  4816 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:47:03.209951  4816 net.cpp:165] Memory required for data: 3887360
I1127 10:47:03.209956  4816 layer_factory.hpp:76] Creating layer conv2
I1127 10:47:03.209965  4816 net.cpp:106] Creating Layer conv2
I1127 10:47:03.209970  4816 net.cpp:454] conv2 <- pool1
I1127 10:47:03.209977  4816 net.cpp:411] conv2 -> conv2
I1127 10:47:03.210335  4816 net.cpp:150] Setting up conv2
I1127 10:47:03.210345  4816 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:47:03.210348  4816 net.cpp:165] Memory required for data: 4706560
I1127 10:47:03.210358  4816 layer_factory.hpp:76] Creating layer pool2
I1127 10:47:03.210366  4816 net.cpp:106] Creating Layer pool2
I1127 10:47:03.210371  4816 net.cpp:454] pool2 <- conv2
I1127 10:47:03.210376  4816 net.cpp:411] pool2 -> pool2
I1127 10:47:03.210403  4816 net.cpp:150] Setting up pool2
I1127 10:47:03.210410  4816 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:47:03.210414  4816 net.cpp:165] Memory required for data: 4911360
I1127 10:47:03.210418  4816 layer_factory.hpp:76] Creating layer ip1
I1127 10:47:03.210427  4816 net.cpp:106] Creating Layer ip1
I1127 10:47:03.210430  4816 net.cpp:454] ip1 <- pool2
I1127 10:47:03.210438  4816 net.cpp:411] ip1 -> ip1
I1127 10:47:03.212534  4816 net.cpp:150] Setting up ip1
I1127 10:47:03.212545  4816 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:47:03.212549  4816 net.cpp:165] Memory required for data: 5039360
I1127 10:47:03.212558  4816 layer_factory.hpp:76] Creating layer relu1
I1127 10:47:03.212565  4816 net.cpp:106] Creating Layer relu1
I1127 10:47:03.212570  4816 net.cpp:454] relu1 <- ip1
I1127 10:47:03.212577  4816 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:47:03.212585  4816 net.cpp:150] Setting up relu1
I1127 10:47:03.212591  4816 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:47:03.212595  4816 net.cpp:165] Memory required for data: 5167360
I1127 10:47:03.212604  4816 layer_factory.hpp:76] Creating layer ip2
I1127 10:47:03.212611  4816 net.cpp:106] Creating Layer ip2
I1127 10:47:03.212615  4816 net.cpp:454] ip2 <- ip1
I1127 10:47:03.212623  4816 net.cpp:411] ip2 -> ip2
I1127 10:47:03.213008  4816 net.cpp:150] Setting up ip2
I1127 10:47:03.213017  4816 net.cpp:157] Top shape: 64 10 (640)
I1127 10:47:03.213022  4816 net.cpp:165] Memory required for data: 5169920
I1127 10:47:03.213029  4816 layer_factory.hpp:76] Creating layer loss
I1127 10:47:03.213037  4816 net.cpp:106] Creating Layer loss
I1127 10:47:03.213042  4816 net.cpp:454] loss <- ip2
I1127 10:47:03.213047  4816 net.cpp:454] loss <- label
I1127 10:47:03.213053  4816 net.cpp:411] loss -> loss
I1127 10:47:03.213064  4816 layer_factory.hpp:76] Creating layer loss
I1127 10:47:03.213132  4816 net.cpp:150] Setting up loss
I1127 10:47:03.213140  4816 net.cpp:157] Top shape: (1)
I1127 10:47:03.213143  4816 net.cpp:160]     with loss weight 1
I1127 10:47:03.213160  4816 net.cpp:165] Memory required for data: 5169924
I1127 10:47:03.213163  4816 net.cpp:226] loss needs backward computation.
I1127 10:47:03.213168  4816 net.cpp:226] ip2 needs backward computation.
I1127 10:47:03.213172  4816 net.cpp:226] relu1 needs backward computation.
I1127 10:47:03.213176  4816 net.cpp:226] ip1 needs backward computation.
I1127 10:47:03.213181  4816 net.cpp:226] pool2 needs backward computation.
I1127 10:47:03.213184  4816 net.cpp:226] conv2 needs backward computation.
I1127 10:47:03.213189  4816 net.cpp:226] pool1 needs backward computation.
I1127 10:47:03.213193  4816 net.cpp:226] conv1 needs backward computation.
I1127 10:47:03.213197  4816 net.cpp:228] mnist does not need backward computation.
I1127 10:47:03.213201  4816 net.cpp:270] This network produces output loss
I1127 10:47:03.213212  4816 net.cpp:283] Network initialization done.
I1127 10:47:03.213445  4816 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:47:03.213467  4816 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:47:03.213575  4816 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:47:03.213637  4816 layer_factory.hpp:76] Creating layer mnist
I1127 10:47:03.213718  4816 net.cpp:106] Creating Layer mnist
I1127 10:47:03.213727  4816 net.cpp:411] mnist -> data
I1127 10:47:03.213735  4816 net.cpp:411] mnist -> label
I1127 10:47:03.214467  4822 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:47:03.214576  4816 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:47:03.218086  4816 net.cpp:150] Setting up mnist
I1127 10:47:03.218097  4816 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:47:03.218103  4816 net.cpp:157] Top shape: 100 (100)
I1127 10:47:03.218107  4816 net.cpp:165] Memory required for data: 314000
I1127 10:47:03.218112  4816 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:47:03.218121  4816 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:47:03.218124  4816 net.cpp:454] label_mnist_1_split <- label
I1127 10:47:03.218130  4816 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:47:03.218138  4816 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:47:03.218184  4816 net.cpp:150] Setting up label_mnist_1_split
I1127 10:47:03.218192  4816 net.cpp:157] Top shape: 100 (100)
I1127 10:47:03.218197  4816 net.cpp:157] Top shape: 100 (100)
I1127 10:47:03.218201  4816 net.cpp:165] Memory required for data: 314800
I1127 10:47:03.218206  4816 layer_factory.hpp:76] Creating layer conv1
I1127 10:47:03.218214  4816 net.cpp:106] Creating Layer conv1
I1127 10:47:03.218219  4816 net.cpp:454] conv1 <- data
I1127 10:47:03.218226  4816 net.cpp:411] conv1 -> conv1
I1127 10:47:03.218372  4816 net.cpp:150] Setting up conv1
I1127 10:47:03.218381  4816 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:47:03.218385  4816 net.cpp:165] Memory required for data: 4922800
I1127 10:47:03.218394  4816 layer_factory.hpp:76] Creating layer pool1
I1127 10:47:03.218402  4816 net.cpp:106] Creating Layer pool1
I1127 10:47:03.218407  4816 net.cpp:454] pool1 <- conv1
I1127 10:47:03.218420  4816 net.cpp:411] pool1 -> pool1
I1127 10:47:03.218448  4816 net.cpp:150] Setting up pool1
I1127 10:47:03.218456  4816 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:47:03.218459  4816 net.cpp:165] Memory required for data: 6074800
I1127 10:47:03.218463  4816 layer_factory.hpp:76] Creating layer conv2
I1127 10:47:03.218472  4816 net.cpp:106] Creating Layer conv2
I1127 10:47:03.218477  4816 net.cpp:454] conv2 <- pool1
I1127 10:47:03.218484  4816 net.cpp:411] conv2 -> conv2
I1127 10:47:03.218732  4816 net.cpp:150] Setting up conv2
I1127 10:47:03.218741  4816 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:47:03.218745  4816 net.cpp:165] Memory required for data: 7354800
I1127 10:47:03.218754  4816 layer_factory.hpp:76] Creating layer pool2
I1127 10:47:03.218772  4816 net.cpp:106] Creating Layer pool2
I1127 10:47:03.218780  4816 net.cpp:454] pool2 <- conv2
I1127 10:47:03.218786  4816 net.cpp:411] pool2 -> pool2
I1127 10:47:03.218823  4816 net.cpp:150] Setting up pool2
I1127 10:47:03.218832  4816 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:47:03.218837  4816 net.cpp:165] Memory required for data: 7674800
I1127 10:47:03.218840  4816 layer_factory.hpp:76] Creating layer ip1
I1127 10:47:03.218848  4816 net.cpp:106] Creating Layer ip1
I1127 10:47:03.218853  4816 net.cpp:454] ip1 <- pool2
I1127 10:47:03.218859  4816 net.cpp:411] ip1 -> ip1
I1127 10:47:03.221364  4816 net.cpp:150] Setting up ip1
I1127 10:47:03.221375  4816 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:47:03.221380  4816 net.cpp:165] Memory required for data: 7874800
I1127 10:47:03.221390  4816 layer_factory.hpp:76] Creating layer relu1
I1127 10:47:03.221398  4816 net.cpp:106] Creating Layer relu1
I1127 10:47:03.221403  4816 net.cpp:454] relu1 <- ip1
I1127 10:47:03.221410  4816 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:47:03.221416  4816 net.cpp:150] Setting up relu1
I1127 10:47:03.221421  4816 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:47:03.221429  4816 net.cpp:165] Memory required for data: 8074800
I1127 10:47:03.221434  4816 layer_factory.hpp:76] Creating layer ip2
I1127 10:47:03.221443  4816 net.cpp:106] Creating Layer ip2
I1127 10:47:03.221447  4816 net.cpp:454] ip2 <- ip1
I1127 10:47:03.221453  4816 net.cpp:411] ip2 -> ip2
I1127 10:47:03.221549  4816 net.cpp:150] Setting up ip2
I1127 10:47:03.221557  4816 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:47:03.221561  4816 net.cpp:165] Memory required for data: 8078800
I1127 10:47:03.221567  4816 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:47:03.221575  4816 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:47:03.221580  4816 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:47:03.221585  4816 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:47:03.221591  4816 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:47:03.221619  4816 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:47:03.221626  4816 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:47:03.221632  4816 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:47:03.221635  4816 net.cpp:165] Memory required for data: 8086800
I1127 10:47:03.221639  4816 layer_factory.hpp:76] Creating layer accuracy
I1127 10:47:03.221647  4816 net.cpp:106] Creating Layer accuracy
I1127 10:47:03.221650  4816 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:47:03.221655  4816 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:47:03.221663  4816 net.cpp:411] accuracy -> accuracy
I1127 10:47:03.221671  4816 net.cpp:150] Setting up accuracy
I1127 10:47:03.221678  4816 net.cpp:157] Top shape: (1)
I1127 10:47:03.221681  4816 net.cpp:165] Memory required for data: 8086804
I1127 10:47:03.221686  4816 layer_factory.hpp:76] Creating layer loss
I1127 10:47:03.221691  4816 net.cpp:106] Creating Layer loss
I1127 10:47:03.221696  4816 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:47:03.221701  4816 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:47:03.221709  4816 net.cpp:411] loss -> loss
I1127 10:47:03.221716  4816 layer_factory.hpp:76] Creating layer loss
I1127 10:47:03.221784  4816 net.cpp:150] Setting up loss
I1127 10:47:03.221792  4816 net.cpp:157] Top shape: (1)
I1127 10:47:03.221796  4816 net.cpp:160]     with loss weight 1
I1127 10:47:03.221806  4816 net.cpp:165] Memory required for data: 8086808
I1127 10:47:03.221809  4816 net.cpp:226] loss needs backward computation.
I1127 10:47:03.221817  4816 net.cpp:228] accuracy does not need backward computation.
I1127 10:47:03.221822  4816 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:47:03.221827  4816 net.cpp:226] ip2 needs backward computation.
I1127 10:47:03.221830  4816 net.cpp:226] relu1 needs backward computation.
I1127 10:47:03.221834  4816 net.cpp:226] ip1 needs backward computation.
I1127 10:47:03.221838  4816 net.cpp:226] pool2 needs backward computation.
I1127 10:47:03.221843  4816 net.cpp:226] conv2 needs backward computation.
I1127 10:47:03.221848  4816 net.cpp:226] pool1 needs backward computation.
I1127 10:47:03.221851  4816 net.cpp:226] conv1 needs backward computation.
I1127 10:47:03.221856  4816 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:47:03.221863  4816 net.cpp:228] mnist does not need backward computation.
I1127 10:47:03.221868  4816 net.cpp:270] This network produces output accuracy
I1127 10:47:03.221871  4816 net.cpp:270] This network produces output loss
I1127 10:47:03.221882  4816 net.cpp:283] Network initialization done.
I1127 10:47:03.221915  4816 solver.cpp:59] Solver scaffolding done.
I1127 10:47:03.222101  4816 caffe.cpp:212] Starting Optimization
I1127 10:47:03.222107  4816 solver.cpp:287] Solving LeNet
I1127 10:47:03.222111  4816 solver.cpp:288] Learning Rate Policy: inv
I1127 10:47:03.222434  4816 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:47:06.209440  4816 solver.cpp:408]     Test net output #0: accuracy = 0.0833
I1127 10:47:06.209513  4816 solver.cpp:408]     Test net output #1: loss = 2.37117 (* 1 = 2.37117 loss)
I1127 10:47:06.220463  4816 solver.cpp:236] Iteration 0, loss = 2.3894
I1127 10:47:06.220513  4816 solver.cpp:252]     Train net output #0: loss = 2.3894 (* 1 = 2.3894 loss)
I1127 10:47:06.220535  4816 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:47:19.334678  4816 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:47:20.802032  4816 solver.cpp:408]     Test net output #0: accuracy = 0.9722
I1127 10:47:20.802160  4816 solver.cpp:408]     Test net output #1: loss = 0.0874137 (* 1 = 0.0874137 loss)
I1127 10:47:20.814672  4816 solver.cpp:236] Iteration 500, loss = 0.110863
I1127 10:47:20.814771  4816 solver.cpp:252]     Train net output #0: loss = 0.110863 (* 1 = 0.110863 loss)
I1127 10:47:20.814795  4816 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:47:32.353631  4816 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:47:32.387922  4816 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:47:32.400849  4816 solver.cpp:320] Iteration 1000, loss = 0.109399
I1127 10:47:32.400903  4816 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:47:35.450567  4816 solver.cpp:408]     Test net output #0: accuracy = 0.9815
I1127 10:47:35.450676  4816 solver.cpp:408]     Test net output #1: loss = 0.0580621 (* 1 = 0.0580621 loss)
I1127 10:47:35.450685  4816 solver.cpp:325] Optimization Done.
I1127 10:47:35.450690  4816 caffe.cpp:215] Optimization Done.
I1127 10:47:35.516403  4844 caffe.cpp:184] Using GPUs 0
I1127 10:47:35.839247  4844 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:47:35.839601  4844 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:47:35.840138  4844 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:47:35.840181  4844 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:47:35.840359  4844 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:47:35.840490  4844 layer_factory.hpp:76] Creating layer mnist
I1127 10:47:35.841073  4844 net.cpp:106] Creating Layer mnist
I1127 10:47:35.841104  4844 net.cpp:411] mnist -> data
I1127 10:47:35.841143  4844 net.cpp:411] mnist -> label
I1127 10:47:35.841996  4848 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:47:35.854456  4844 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:47:35.894306  4844 net.cpp:150] Setting up mnist
I1127 10:47:35.894400  4844 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:47:35.894439  4844 net.cpp:157] Top shape: 64 (64)
I1127 10:47:35.894464  4844 net.cpp:165] Memory required for data: 200960
I1127 10:47:35.894490  4844 layer_factory.hpp:76] Creating layer conv1
I1127 10:47:35.894521  4844 net.cpp:106] Creating Layer conv1
I1127 10:47:35.894536  4844 net.cpp:454] conv1 <- data
I1127 10:47:35.894559  4844 net.cpp:411] conv1 -> conv1
I1127 10:47:35.896615  4844 net.cpp:150] Setting up conv1
I1127 10:47:35.896683  4844 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:47:35.896692  4844 net.cpp:165] Memory required for data: 3150080
I1127 10:47:35.896720  4844 layer_factory.hpp:76] Creating layer pool1
I1127 10:47:35.896741  4844 net.cpp:106] Creating Layer pool1
I1127 10:47:35.896750  4844 net.cpp:454] pool1 <- conv1
I1127 10:47:35.896761  4844 net.cpp:411] pool1 -> pool1
I1127 10:47:35.896847  4844 net.cpp:150] Setting up pool1
I1127 10:47:35.896858  4844 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:47:35.896865  4844 net.cpp:165] Memory required for data: 3887360
I1127 10:47:35.896872  4844 layer_factory.hpp:76] Creating layer conv2
I1127 10:47:35.896891  4844 net.cpp:106] Creating Layer conv2
I1127 10:47:35.896900  4844 net.cpp:454] conv2 <- pool1
I1127 10:47:35.896914  4844 net.cpp:411] conv2 -> conv2
I1127 10:47:35.897487  4844 net.cpp:150] Setting up conv2
I1127 10:47:35.897527  4844 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:47:35.897536  4844 net.cpp:165] Memory required for data: 4706560
I1127 10:47:35.897557  4844 layer_factory.hpp:76] Creating layer pool2
I1127 10:47:35.897578  4844 net.cpp:106] Creating Layer pool2
I1127 10:47:35.897588  4844 net.cpp:454] pool2 <- conv2
I1127 10:47:35.897598  4844 net.cpp:411] pool2 -> pool2
I1127 10:47:35.897650  4844 net.cpp:150] Setting up pool2
I1127 10:47:35.897663  4844 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:47:35.897670  4844 net.cpp:165] Memory required for data: 4911360
I1127 10:47:35.897677  4844 layer_factory.hpp:76] Creating layer ip1
I1127 10:47:35.897691  4844 net.cpp:106] Creating Layer ip1
I1127 10:47:35.897698  4844 net.cpp:454] ip1 <- pool2
I1127 10:47:35.897707  4844 net.cpp:411] ip1 -> ip1
I1127 10:47:35.900795  4844 net.cpp:150] Setting up ip1
I1127 10:47:35.900867  4844 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:47:35.900873  4844 net.cpp:165] Memory required for data: 5039360
I1127 10:47:35.900892  4844 layer_factory.hpp:76] Creating layer relu1
I1127 10:47:35.900910  4844 net.cpp:106] Creating Layer relu1
I1127 10:47:35.900918  4844 net.cpp:454] relu1 <- ip1
I1127 10:47:35.900926  4844 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:47:35.900945  4844 net.cpp:150] Setting up relu1
I1127 10:47:35.900952  4844 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:47:35.900956  4844 net.cpp:165] Memory required for data: 5167360
I1127 10:47:35.900962  4844 layer_factory.hpp:76] Creating layer ip2
I1127 10:47:35.900974  4844 net.cpp:106] Creating Layer ip2
I1127 10:47:35.900979  4844 net.cpp:454] ip2 <- ip1
I1127 10:47:35.900987  4844 net.cpp:411] ip2 -> ip2
I1127 10:47:35.901737  4844 net.cpp:150] Setting up ip2
I1127 10:47:35.901800  4844 net.cpp:157] Top shape: 64 10 (640)
I1127 10:47:35.901808  4844 net.cpp:165] Memory required for data: 5169920
I1127 10:47:35.901821  4844 layer_factory.hpp:76] Creating layer loss
I1127 10:47:35.901841  4844 net.cpp:106] Creating Layer loss
I1127 10:47:35.901850  4844 net.cpp:454] loss <- ip2
I1127 10:47:35.901873  4844 net.cpp:454] loss <- label
I1127 10:47:35.901887  4844 net.cpp:411] loss -> loss
I1127 10:47:35.901916  4844 layer_factory.hpp:76] Creating layer loss
I1127 10:47:35.902022  4844 net.cpp:150] Setting up loss
I1127 10:47:35.902034  4844 net.cpp:157] Top shape: (1)
I1127 10:47:35.902040  4844 net.cpp:160]     with loss weight 1
I1127 10:47:35.902072  4844 net.cpp:165] Memory required for data: 5169924
I1127 10:47:35.902078  4844 net.cpp:226] loss needs backward computation.
I1127 10:47:35.902083  4844 net.cpp:226] ip2 needs backward computation.
I1127 10:47:35.902087  4844 net.cpp:226] relu1 needs backward computation.
I1127 10:47:35.902092  4844 net.cpp:226] ip1 needs backward computation.
I1127 10:47:35.902096  4844 net.cpp:226] pool2 needs backward computation.
I1127 10:47:35.902101  4844 net.cpp:226] conv2 needs backward computation.
I1127 10:47:35.902107  4844 net.cpp:226] pool1 needs backward computation.
I1127 10:47:35.902111  4844 net.cpp:226] conv1 needs backward computation.
I1127 10:47:35.902115  4844 net.cpp:228] mnist does not need backward computation.
I1127 10:47:35.902120  4844 net.cpp:270] This network produces output loss
I1127 10:47:35.902133  4844 net.cpp:283] Network initialization done.
I1127 10:47:35.902459  4844 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:47:35.902501  4844 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:47:35.902636  4844 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:47:35.902719  4844 layer_factory.hpp:76] Creating layer mnist
I1127 10:47:35.902879  4844 net.cpp:106] Creating Layer mnist
I1127 10:47:35.902892  4844 net.cpp:411] mnist -> data
I1127 10:47:35.902918  4844 net.cpp:411] mnist -> label
I1127 10:47:35.904144  4850 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:47:35.904366  4844 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:47:35.905977  4844 net.cpp:150] Setting up mnist
I1127 10:47:35.906038  4844 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:47:35.906050  4844 net.cpp:157] Top shape: 100 (100)
I1127 10:47:35.906059  4844 net.cpp:165] Memory required for data: 314000
I1127 10:47:35.906075  4844 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:47:35.906103  4844 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:47:35.906114  4844 net.cpp:454] label_mnist_1_split <- label
I1127 10:47:35.906128  4844 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:47:35.906164  4844 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:47:35.906242  4844 net.cpp:150] Setting up label_mnist_1_split
I1127 10:47:35.906258  4844 net.cpp:157] Top shape: 100 (100)
I1127 10:47:35.906270  4844 net.cpp:157] Top shape: 100 (100)
I1127 10:47:35.906280  4844 net.cpp:165] Memory required for data: 314800
I1127 10:47:35.906287  4844 layer_factory.hpp:76] Creating layer conv1
I1127 10:47:35.906308  4844 net.cpp:106] Creating Layer conv1
I1127 10:47:35.906318  4844 net.cpp:454] conv1 <- data
I1127 10:47:35.906330  4844 net.cpp:411] conv1 -> conv1
I1127 10:47:35.906605  4844 net.cpp:150] Setting up conv1
I1127 10:47:35.906632  4844 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:47:35.906643  4844 net.cpp:165] Memory required for data: 4922800
I1127 10:47:35.906666  4844 layer_factory.hpp:76] Creating layer pool1
I1127 10:47:35.906682  4844 net.cpp:106] Creating Layer pool1
I1127 10:47:35.906690  4844 net.cpp:454] pool1 <- conv1
I1127 10:47:35.906738  4844 net.cpp:411] pool1 -> pool1
I1127 10:47:35.906805  4844 net.cpp:150] Setting up pool1
I1127 10:47:35.906819  4844 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:47:35.906826  4844 net.cpp:165] Memory required for data: 6074800
I1127 10:47:35.906833  4844 layer_factory.hpp:76] Creating layer conv2
I1127 10:47:35.906857  4844 net.cpp:106] Creating Layer conv2
I1127 10:47:35.906867  4844 net.cpp:454] conv2 <- pool1
I1127 10:47:35.906879  4844 net.cpp:411] conv2 -> conv2
I1127 10:47:35.907392  4844 net.cpp:150] Setting up conv2
I1127 10:47:35.907412  4844 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:47:35.907418  4844 net.cpp:165] Memory required for data: 7354800
I1127 10:47:35.907433  4844 layer_factory.hpp:76] Creating layer pool2
I1127 10:47:35.907448  4844 net.cpp:106] Creating Layer pool2
I1127 10:47:35.907454  4844 net.cpp:454] pool2 <- conv2
I1127 10:47:35.907464  4844 net.cpp:411] pool2 -> pool2
I1127 10:47:35.907505  4844 net.cpp:150] Setting up pool2
I1127 10:47:35.907515  4844 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:47:35.907521  4844 net.cpp:165] Memory required for data: 7674800
I1127 10:47:35.907528  4844 layer_factory.hpp:76] Creating layer ip1
I1127 10:47:35.907542  4844 net.cpp:106] Creating Layer ip1
I1127 10:47:35.907551  4844 net.cpp:454] ip1 <- pool2
I1127 10:47:35.907560  4844 net.cpp:411] ip1 -> ip1
I1127 10:47:35.911175  4844 net.cpp:150] Setting up ip1
I1127 10:47:35.911229  4844 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:47:35.911237  4844 net.cpp:165] Memory required for data: 7874800
I1127 10:47:35.911257  4844 layer_factory.hpp:76] Creating layer relu1
I1127 10:47:35.911278  4844 net.cpp:106] Creating Layer relu1
I1127 10:47:35.911289  4844 net.cpp:454] relu1 <- ip1
I1127 10:47:35.911301  4844 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:47:35.911319  4844 net.cpp:150] Setting up relu1
I1127 10:47:35.911326  4844 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:47:35.911334  4844 net.cpp:165] Memory required for data: 8074800
I1127 10:47:35.911340  4844 layer_factory.hpp:76] Creating layer ip2
I1127 10:47:35.911358  4844 net.cpp:106] Creating Layer ip2
I1127 10:47:35.911366  4844 net.cpp:454] ip2 <- ip1
I1127 10:47:35.911376  4844 net.cpp:411] ip2 -> ip2
I1127 10:47:35.911530  4844 net.cpp:150] Setting up ip2
I1127 10:47:35.911541  4844 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:47:35.911548  4844 net.cpp:165] Memory required for data: 8078800
I1127 10:47:35.911558  4844 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:47:35.911581  4844 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:47:35.911589  4844 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:47:35.911598  4844 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:47:35.911609  4844 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:47:35.911648  4844 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:47:35.911659  4844 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:47:35.911666  4844 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:47:35.911674  4844 net.cpp:165] Memory required for data: 8086800
I1127 10:47:35.911680  4844 layer_factory.hpp:76] Creating layer accuracy
I1127 10:47:35.911692  4844 net.cpp:106] Creating Layer accuracy
I1127 10:47:35.911700  4844 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:47:35.911707  4844 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:47:35.911716  4844 net.cpp:411] accuracy -> accuracy
I1127 10:47:35.911728  4844 net.cpp:150] Setting up accuracy
I1127 10:47:35.911736  4844 net.cpp:157] Top shape: (1)
I1127 10:47:35.911742  4844 net.cpp:165] Memory required for data: 8086804
I1127 10:47:35.911749  4844 layer_factory.hpp:76] Creating layer loss
I1127 10:47:35.911759  4844 net.cpp:106] Creating Layer loss
I1127 10:47:35.911767  4844 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:47:35.911774  4844 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:47:35.911785  4844 net.cpp:411] loss -> loss
I1127 10:47:35.911798  4844 layer_factory.hpp:76] Creating layer loss
I1127 10:47:35.911911  4844 net.cpp:150] Setting up loss
I1127 10:47:35.911923  4844 net.cpp:157] Top shape: (1)
I1127 10:47:35.911929  4844 net.cpp:160]     with loss weight 1
I1127 10:47:35.911952  4844 net.cpp:165] Memory required for data: 8086808
I1127 10:47:35.911959  4844 net.cpp:226] loss needs backward computation.
I1127 10:47:35.911973  4844 net.cpp:228] accuracy does not need backward computation.
I1127 10:47:35.911981  4844 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:47:35.911988  4844 net.cpp:226] ip2 needs backward computation.
I1127 10:47:35.911995  4844 net.cpp:226] relu1 needs backward computation.
I1127 10:47:35.912003  4844 net.cpp:226] ip1 needs backward computation.
I1127 10:47:35.912009  4844 net.cpp:226] pool2 needs backward computation.
I1127 10:47:35.912015  4844 net.cpp:226] conv2 needs backward computation.
I1127 10:47:35.912025  4844 net.cpp:226] pool1 needs backward computation.
I1127 10:47:35.912034  4844 net.cpp:226] conv1 needs backward computation.
I1127 10:47:35.912040  4844 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:47:35.912050  4844 net.cpp:228] mnist does not need backward computation.
I1127 10:47:35.912055  4844 net.cpp:270] This network produces output accuracy
I1127 10:47:35.912062  4844 net.cpp:270] This network produces output loss
I1127 10:47:35.912081  4844 net.cpp:283] Network initialization done.
I1127 10:47:35.912164  4844 solver.cpp:59] Solver scaffolding done.
I1127 10:47:35.912466  4844 caffe.cpp:212] Starting Optimization
I1127 10:47:35.912480  4844 solver.cpp:287] Solving LeNet
I1127 10:47:35.912487  4844 solver.cpp:288] Learning Rate Policy: inv
I1127 10:47:35.913350  4844 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:47:37.070513  4844 solver.cpp:408]     Test net output #0: accuracy = 0.1336
I1127 10:47:37.070554  4844 solver.cpp:408]     Test net output #1: loss = 2.32769 (* 1 = 2.32769 loss)
I1127 10:47:37.101627  4844 solver.cpp:236] Iteration 0, loss = 2.32975
I1127 10:47:37.101647  4844 solver.cpp:252]     Train net output #0: loss = 2.32975 (* 1 = 2.32975 loss)
I1127 10:47:37.101663  4844 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:47:50.452944  4844 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:47:53.593749  4844 solver.cpp:408]     Test net output #0: accuracy = 0.973
I1127 10:47:53.593879  4844 solver.cpp:408]     Test net output #1: loss = 0.0864592 (* 1 = 0.0864592 loss)
I1127 10:47:53.606329  4844 solver.cpp:236] Iteration 500, loss = 0.121615
I1127 10:47:53.606467  4844 solver.cpp:252]     Train net output #0: loss = 0.121615 (* 1 = 0.121615 loss)
I1127 10:47:53.606542  4844 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:48:06.062130  4844 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:48:06.081970  4844 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:48:06.108387  4844 solver.cpp:320] Iteration 1000, loss = 0.0872866
I1127 10:48:06.108409  4844 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:48:08.212199  4844 solver.cpp:408]     Test net output #0: accuracy = 0.9821
I1127 10:48:08.212316  4844 solver.cpp:408]     Test net output #1: loss = 0.0584564 (* 1 = 0.0584564 loss)
I1127 10:48:08.212334  4844 solver.cpp:325] Optimization Done.
I1127 10:48:08.212343  4844 caffe.cpp:215] Optimization Done.
I1127 10:48:08.350615  4872 caffe.cpp:184] Using GPUs 0
I1127 10:48:08.749428  4872 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:48:08.749563  4872 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:48:08.749907  4872 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:48:08.749929  4872 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:48:08.750042  4872 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:48:08.750118  4872 layer_factory.hpp:76] Creating layer mnist
I1127 10:48:08.750641  4872 net.cpp:106] Creating Layer mnist
I1127 10:48:08.750680  4872 net.cpp:411] mnist -> data
I1127 10:48:08.750731  4872 net.cpp:411] mnist -> label
I1127 10:48:08.752146  4875 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:48:08.771405  4872 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:48:08.773684  4872 net.cpp:150] Setting up mnist
I1127 10:48:08.773797  4872 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:48:08.773815  4872 net.cpp:157] Top shape: 64 (64)
I1127 10:48:08.773824  4872 net.cpp:165] Memory required for data: 200960
I1127 10:48:08.773840  4872 layer_factory.hpp:76] Creating layer conv1
I1127 10:48:08.773885  4872 net.cpp:106] Creating Layer conv1
I1127 10:48:08.773915  4872 net.cpp:454] conv1 <- data
I1127 10:48:08.773962  4872 net.cpp:411] conv1 -> conv1
I1127 10:48:08.775274  4872 net.cpp:150] Setting up conv1
I1127 10:48:08.775341  4872 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:48:08.775351  4872 net.cpp:165] Memory required for data: 3150080
I1127 10:48:08.775382  4872 layer_factory.hpp:76] Creating layer pool1
I1127 10:48:08.775403  4872 net.cpp:106] Creating Layer pool1
I1127 10:48:08.775411  4872 net.cpp:454] pool1 <- conv1
I1127 10:48:08.775423  4872 net.cpp:411] pool1 -> pool1
I1127 10:48:08.775498  4872 net.cpp:150] Setting up pool1
I1127 10:48:08.775516  4872 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:48:08.775527  4872 net.cpp:165] Memory required for data: 3887360
I1127 10:48:08.775537  4872 layer_factory.hpp:76] Creating layer conv2
I1127 10:48:08.775563  4872 net.cpp:106] Creating Layer conv2
I1127 10:48:08.775575  4872 net.cpp:454] conv2 <- pool1
I1127 10:48:08.775596  4872 net.cpp:411] conv2 -> conv2
I1127 10:48:08.776024  4872 net.cpp:150] Setting up conv2
I1127 10:48:08.776046  4872 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:48:08.776056  4872 net.cpp:165] Memory required for data: 4706560
I1127 10:48:08.776072  4872 layer_factory.hpp:76] Creating layer pool2
I1127 10:48:08.776089  4872 net.cpp:106] Creating Layer pool2
I1127 10:48:08.776098  4872 net.cpp:454] pool2 <- conv2
I1127 10:48:08.776109  4872 net.cpp:411] pool2 -> pool2
I1127 10:48:08.776156  4872 net.cpp:150] Setting up pool2
I1127 10:48:08.776170  4872 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:48:08.776178  4872 net.cpp:165] Memory required for data: 4911360
I1127 10:48:08.776187  4872 layer_factory.hpp:76] Creating layer ip1
I1127 10:48:08.776209  4872 net.cpp:106] Creating Layer ip1
I1127 10:48:08.776218  4872 net.cpp:454] ip1 <- pool2
I1127 10:48:08.776232  4872 net.cpp:411] ip1 -> ip1
I1127 10:48:08.781522  4872 net.cpp:150] Setting up ip1
I1127 10:48:08.781589  4872 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:48:08.781597  4872 net.cpp:165] Memory required for data: 5039360
I1127 10:48:08.781621  4872 layer_factory.hpp:76] Creating layer relu1
I1127 10:48:08.781641  4872 net.cpp:106] Creating Layer relu1
I1127 10:48:08.781651  4872 net.cpp:454] relu1 <- ip1
I1127 10:48:08.781667  4872 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:48:08.781685  4872 net.cpp:150] Setting up relu1
I1127 10:48:08.781697  4872 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:48:08.781703  4872 net.cpp:165] Memory required for data: 5167360
I1127 10:48:08.781710  4872 layer_factory.hpp:76] Creating layer ip2
I1127 10:48:08.781723  4872 net.cpp:106] Creating Layer ip2
I1127 10:48:08.781731  4872 net.cpp:454] ip2 <- ip1
I1127 10:48:08.781744  4872 net.cpp:411] ip2 -> ip2
I1127 10:48:08.790411  4872 net.cpp:150] Setting up ip2
I1127 10:48:08.790508  4872 net.cpp:157] Top shape: 64 10 (640)
I1127 10:48:08.790520  4872 net.cpp:165] Memory required for data: 5169920
I1127 10:48:08.790547  4872 layer_factory.hpp:76] Creating layer loss
I1127 10:48:08.790585  4872 net.cpp:106] Creating Layer loss
I1127 10:48:08.790601  4872 net.cpp:454] loss <- ip2
I1127 10:48:08.790617  4872 net.cpp:454] loss <- label
I1127 10:48:08.790637  4872 net.cpp:411] loss -> loss
I1127 10:48:08.790674  4872 layer_factory.hpp:76] Creating layer loss
I1127 10:48:08.790920  4872 net.cpp:150] Setting up loss
I1127 10:48:08.790940  4872 net.cpp:157] Top shape: (1)
I1127 10:48:08.790946  4872 net.cpp:160]     with loss weight 1
I1127 10:48:08.790992  4872 net.cpp:165] Memory required for data: 5169924
I1127 10:48:08.791000  4872 net.cpp:226] loss needs backward computation.
I1127 10:48:08.791010  4872 net.cpp:226] ip2 needs backward computation.
I1127 10:48:08.791039  4872 net.cpp:226] relu1 needs backward computation.
I1127 10:48:08.791051  4872 net.cpp:226] ip1 needs backward computation.
I1127 10:48:08.791061  4872 net.cpp:226] pool2 needs backward computation.
I1127 10:48:08.791070  4872 net.cpp:226] conv2 needs backward computation.
I1127 10:48:08.791079  4872 net.cpp:226] pool1 needs backward computation.
I1127 10:48:08.791087  4872 net.cpp:226] conv1 needs backward computation.
I1127 10:48:08.791097  4872 net.cpp:228] mnist does not need backward computation.
I1127 10:48:08.791105  4872 net.cpp:270] This network produces output loss
I1127 10:48:08.791124  4872 net.cpp:283] Network initialization done.
I1127 10:48:08.791712  4872 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:48:08.791844  4872 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:48:08.792124  4872 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:48:08.792264  4872 layer_factory.hpp:76] Creating layer mnist
I1127 10:48:08.792429  4872 net.cpp:106] Creating Layer mnist
I1127 10:48:08.792456  4872 net.cpp:411] mnist -> data
I1127 10:48:08.792489  4872 net.cpp:411] mnist -> label
I1127 10:48:08.795809  4877 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:48:08.798575  4872 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:48:08.801815  4872 net.cpp:150] Setting up mnist
I1127 10:48:08.801914  4872 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:48:08.801949  4872 net.cpp:157] Top shape: 100 (100)
I1127 10:48:08.801964  4872 net.cpp:165] Memory required for data: 314000
I1127 10:48:08.801983  4872 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:48:08.802013  4872 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:48:08.802031  4872 net.cpp:454] label_mnist_1_split <- label
I1127 10:48:08.802060  4872 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:48:08.802121  4872 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:48:08.802320  4872 net.cpp:150] Setting up label_mnist_1_split
I1127 10:48:08.802353  4872 net.cpp:157] Top shape: 100 (100)
I1127 10:48:08.802366  4872 net.cpp:157] Top shape: 100 (100)
I1127 10:48:08.802376  4872 net.cpp:165] Memory required for data: 314800
I1127 10:48:08.802386  4872 layer_factory.hpp:76] Creating layer conv1
I1127 10:48:08.802412  4872 net.cpp:106] Creating Layer conv1
I1127 10:48:08.802422  4872 net.cpp:454] conv1 <- data
I1127 10:48:08.802433  4872 net.cpp:411] conv1 -> conv1
I1127 10:48:08.802865  4872 net.cpp:150] Setting up conv1
I1127 10:48:08.802891  4872 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:48:08.802897  4872 net.cpp:165] Memory required for data: 4922800
I1127 10:48:08.802917  4872 layer_factory.hpp:76] Creating layer pool1
I1127 10:48:08.802932  4872 net.cpp:106] Creating Layer pool1
I1127 10:48:08.802939  4872 net.cpp:454] pool1 <- conv1
I1127 10:48:08.802975  4872 net.cpp:411] pool1 -> pool1
I1127 10:48:08.803062  4872 net.cpp:150] Setting up pool1
I1127 10:48:08.803081  4872 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:48:08.803088  4872 net.cpp:165] Memory required for data: 6074800
I1127 10:48:08.803097  4872 layer_factory.hpp:76] Creating layer conv2
I1127 10:48:08.803125  4872 net.cpp:106] Creating Layer conv2
I1127 10:48:08.803138  4872 net.cpp:454] conv2 <- pool1
I1127 10:48:08.803156  4872 net.cpp:411] conv2 -> conv2
I1127 10:48:08.803809  4872 net.cpp:150] Setting up conv2
I1127 10:48:08.803844  4872 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:48:08.803858  4872 net.cpp:165] Memory required for data: 7354800
I1127 10:48:08.803884  4872 layer_factory.hpp:76] Creating layer pool2
I1127 10:48:08.803915  4872 net.cpp:106] Creating Layer pool2
I1127 10:48:08.803936  4872 net.cpp:454] pool2 <- conv2
I1127 10:48:08.803962  4872 net.cpp:411] pool2 -> pool2
I1127 10:48:08.804087  4872 net.cpp:150] Setting up pool2
I1127 10:48:08.804116  4872 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:48:08.804131  4872 net.cpp:165] Memory required for data: 7674800
I1127 10:48:08.804147  4872 layer_factory.hpp:76] Creating layer ip1
I1127 10:48:08.804188  4872 net.cpp:106] Creating Layer ip1
I1127 10:48:08.804206  4872 net.cpp:454] ip1 <- pool2
I1127 10:48:08.804230  4872 net.cpp:411] ip1 -> ip1
I1127 10:48:08.809361  4872 net.cpp:150] Setting up ip1
I1127 10:48:08.809463  4872 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:48:08.809492  4872 net.cpp:165] Memory required for data: 7874800
I1127 10:48:08.809551  4872 layer_factory.hpp:76] Creating layer relu1
I1127 10:48:08.809598  4872 net.cpp:106] Creating Layer relu1
I1127 10:48:08.809628  4872 net.cpp:454] relu1 <- ip1
I1127 10:48:08.809660  4872 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:48:08.809695  4872 net.cpp:150] Setting up relu1
I1127 10:48:08.809718  4872 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:48:08.809731  4872 net.cpp:165] Memory required for data: 8074800
I1127 10:48:08.809743  4872 layer_factory.hpp:76] Creating layer ip2
I1127 10:48:08.809777  4872 net.cpp:106] Creating Layer ip2
I1127 10:48:08.809793  4872 net.cpp:454] ip2 <- ip1
I1127 10:48:08.809833  4872 net.cpp:411] ip2 -> ip2
I1127 10:48:08.814537  4872 net.cpp:150] Setting up ip2
I1127 10:48:08.814690  4872 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:48:08.814730  4872 net.cpp:165] Memory required for data: 8078800
I1127 10:48:08.814781  4872 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:48:08.814827  4872 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:48:08.814852  4872 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:48:08.814885  4872 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:48:08.814930  4872 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:48:08.815141  4872 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:48:08.815171  4872 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:48:08.815191  4872 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:48:08.815207  4872 net.cpp:165] Memory required for data: 8086800
I1127 10:48:08.815250  4872 layer_factory.hpp:76] Creating layer accuracy
I1127 10:48:08.815282  4872 net.cpp:106] Creating Layer accuracy
I1127 10:48:08.815302  4872 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:48:08.815323  4872 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:48:08.815354  4872 net.cpp:411] accuracy -> accuracy
I1127 10:48:08.815397  4872 net.cpp:150] Setting up accuracy
I1127 10:48:08.815419  4872 net.cpp:157] Top shape: (1)
I1127 10:48:08.815430  4872 net.cpp:165] Memory required for data: 8086804
I1127 10:48:08.815443  4872 layer_factory.hpp:76] Creating layer loss
I1127 10:48:08.815460  4872 net.cpp:106] Creating Layer loss
I1127 10:48:08.815475  4872 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:48:08.815490  4872 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:48:08.815512  4872 net.cpp:411] loss -> loss
I1127 10:48:08.815541  4872 layer_factory.hpp:76] Creating layer loss
I1127 10:48:08.816082  4872 net.cpp:150] Setting up loss
I1127 10:48:08.816134  4872 net.cpp:157] Top shape: (1)
I1127 10:48:08.816148  4872 net.cpp:160]     with loss weight 1
I1127 10:48:08.816180  4872 net.cpp:165] Memory required for data: 8086808
I1127 10:48:08.816195  4872 net.cpp:226] loss needs backward computation.
I1127 10:48:08.816216  4872 net.cpp:228] accuracy does not need backward computation.
I1127 10:48:08.816229  4872 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:48:08.816241  4872 net.cpp:226] ip2 needs backward computation.
I1127 10:48:08.816248  4872 net.cpp:226] relu1 needs backward computation.
I1127 10:48:08.816257  4872 net.cpp:226] ip1 needs backward computation.
I1127 10:48:08.816267  4872 net.cpp:226] pool2 needs backward computation.
I1127 10:48:08.816277  4872 net.cpp:226] conv2 needs backward computation.
I1127 10:48:08.816288  4872 net.cpp:226] pool1 needs backward computation.
I1127 10:48:08.816298  4872 net.cpp:226] conv1 needs backward computation.
I1127 10:48:08.816308  4872 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:48:08.816320  4872 net.cpp:228] mnist does not need backward computation.
I1127 10:48:08.816330  4872 net.cpp:270] This network produces output accuracy
I1127 10:48:08.816339  4872 net.cpp:270] This network produces output loss
I1127 10:48:08.816368  4872 net.cpp:283] Network initialization done.
I1127 10:48:08.816653  4872 solver.cpp:59] Solver scaffolding done.
I1127 10:48:08.817963  4872 caffe.cpp:212] Starting Optimization
I1127 10:48:08.818029  4872 solver.cpp:287] Solving LeNet
I1127 10:48:08.818053  4872 solver.cpp:288] Learning Rate Policy: inv
I1127 10:48:08.819969  4872 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:48:09.943500  4872 solver.cpp:408]     Test net output #0: accuracy = 0.0612
I1127 10:48:09.943590  4872 solver.cpp:408]     Test net output #1: loss = 2.47934 (* 1 = 2.47934 loss)
I1127 10:48:09.956419  4872 solver.cpp:236] Iteration 0, loss = 2.49576
I1127 10:48:09.956542  4872 solver.cpp:252]     Train net output #0: loss = 2.49576 (* 1 = 2.49576 loss)
I1127 10:48:09.956569  4872 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:48:20.793436  4872 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:48:22.209528  4872 solver.cpp:408]     Test net output #0: accuracy = 0.9736
I1127 10:48:22.209573  4872 solver.cpp:408]     Test net output #1: loss = 0.084914 (* 1 = 0.084914 loss)
I1127 10:48:22.223073  4872 solver.cpp:236] Iteration 500, loss = 0.107191
I1127 10:48:22.223109  4872 solver.cpp:252]     Train net output #0: loss = 0.107191 (* 1 = 0.107191 loss)
I1127 10:48:22.223119  4872 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:48:31.628088  4872 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:48:31.643479  4872 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:48:31.653811  4872 solver.cpp:320] Iteration 1000, loss = 0.115542
I1127 10:48:31.653856  4872 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:48:33.056790  4872 solver.cpp:408]     Test net output #0: accuracy = 0.9815
I1127 10:48:33.056840  4872 solver.cpp:408]     Test net output #1: loss = 0.0597625 (* 1 = 0.0597625 loss)
I1127 10:48:33.056849  4872 solver.cpp:325] Optimization Done.
I1127 10:48:33.056852  4872 caffe.cpp:215] Optimization Done.
I1127 10:48:33.139894  4930 caffe.cpp:184] Using GPUs 0
I1127 10:48:33.433711  4930 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:48:33.433832  4930 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:48:33.434154  4930 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:48:33.434176  4930 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:48:33.434289  4930 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:48:33.434383  4930 layer_factory.hpp:76] Creating layer mnist
I1127 10:48:33.434783  4930 net.cpp:106] Creating Layer mnist
I1127 10:48:33.434798  4930 net.cpp:411] mnist -> data
I1127 10:48:33.434834  4930 net.cpp:411] mnist -> label
I1127 10:48:33.435608  4934 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:48:33.442512  4930 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:48:33.444193  4930 net.cpp:150] Setting up mnist
I1127 10:48:33.444247  4930 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:48:33.444259  4930 net.cpp:157] Top shape: 64 (64)
I1127 10:48:33.444267  4930 net.cpp:165] Memory required for data: 200960
I1127 10:48:33.444283  4930 layer_factory.hpp:76] Creating layer conv1
I1127 10:48:33.444311  4930 net.cpp:106] Creating Layer conv1
I1127 10:48:33.444332  4930 net.cpp:454] conv1 <- data
I1127 10:48:33.444353  4930 net.cpp:411] conv1 -> conv1
I1127 10:48:33.445113  4930 net.cpp:150] Setting up conv1
I1127 10:48:33.445153  4930 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:48:33.445163  4930 net.cpp:165] Memory required for data: 3150080
I1127 10:48:33.445188  4930 layer_factory.hpp:76] Creating layer pool1
I1127 10:48:33.445209  4930 net.cpp:106] Creating Layer pool1
I1127 10:48:33.445219  4930 net.cpp:454] pool1 <- conv1
I1127 10:48:33.445232  4930 net.cpp:411] pool1 -> pool1
I1127 10:48:33.445313  4930 net.cpp:150] Setting up pool1
I1127 10:48:33.445329  4930 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:48:33.445338  4930 net.cpp:165] Memory required for data: 3887360
I1127 10:48:33.445346  4930 layer_factory.hpp:76] Creating layer conv2
I1127 10:48:33.445363  4930 net.cpp:106] Creating Layer conv2
I1127 10:48:33.445371  4930 net.cpp:454] conv2 <- pool1
I1127 10:48:33.445386  4930 net.cpp:411] conv2 -> conv2
I1127 10:48:33.445850  4930 net.cpp:150] Setting up conv2
I1127 10:48:33.445868  4930 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:48:33.445883  4930 net.cpp:165] Memory required for data: 4706560
I1127 10:48:33.445897  4930 layer_factory.hpp:76] Creating layer pool2
I1127 10:48:33.445917  4930 net.cpp:106] Creating Layer pool2
I1127 10:48:33.445929  4930 net.cpp:454] pool2 <- conv2
I1127 10:48:33.445938  4930 net.cpp:411] pool2 -> pool2
I1127 10:48:33.445988  4930 net.cpp:150] Setting up pool2
I1127 10:48:33.446002  4930 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:48:33.446010  4930 net.cpp:165] Memory required for data: 4911360
I1127 10:48:33.446020  4930 layer_factory.hpp:76] Creating layer ip1
I1127 10:48:33.446041  4930 net.cpp:106] Creating Layer ip1
I1127 10:48:33.446053  4930 net.cpp:454] ip1 <- pool2
I1127 10:48:33.446069  4930 net.cpp:411] ip1 -> ip1
I1127 10:48:33.449955  4930 net.cpp:150] Setting up ip1
I1127 10:48:33.450014  4930 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:48:33.450026  4930 net.cpp:165] Memory required for data: 5039360
I1127 10:48:33.450053  4930 layer_factory.hpp:76] Creating layer relu1
I1127 10:48:33.450081  4930 net.cpp:106] Creating Layer relu1
I1127 10:48:33.450096  4930 net.cpp:454] relu1 <- ip1
I1127 10:48:33.450117  4930 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:48:33.450160  4930 net.cpp:150] Setting up relu1
I1127 10:48:33.450173  4930 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:48:33.450181  4930 net.cpp:165] Memory required for data: 5167360
I1127 10:48:33.450188  4930 layer_factory.hpp:76] Creating layer ip2
I1127 10:48:33.450206  4930 net.cpp:106] Creating Layer ip2
I1127 10:48:33.450215  4930 net.cpp:454] ip2 <- ip1
I1127 10:48:33.450228  4930 net.cpp:411] ip2 -> ip2
I1127 10:48:33.451155  4930 net.cpp:150] Setting up ip2
I1127 10:48:33.451198  4930 net.cpp:157] Top shape: 64 10 (640)
I1127 10:48:33.451210  4930 net.cpp:165] Memory required for data: 5169920
I1127 10:48:33.451232  4930 layer_factory.hpp:76] Creating layer loss
I1127 10:48:33.451259  4930 net.cpp:106] Creating Layer loss
I1127 10:48:33.451272  4930 net.cpp:454] loss <- ip2
I1127 10:48:33.451284  4930 net.cpp:454] loss <- label
I1127 10:48:33.451303  4930 net.cpp:411] loss -> loss
I1127 10:48:33.451331  4930 layer_factory.hpp:76] Creating layer loss
I1127 10:48:33.451449  4930 net.cpp:150] Setting up loss
I1127 10:48:33.451462  4930 net.cpp:157] Top shape: (1)
I1127 10:48:33.451469  4930 net.cpp:160]     with loss weight 1
I1127 10:48:33.451495  4930 net.cpp:165] Memory required for data: 5169924
I1127 10:48:33.451503  4930 net.cpp:226] loss needs backward computation.
I1127 10:48:33.451511  4930 net.cpp:226] ip2 needs backward computation.
I1127 10:48:33.451519  4930 net.cpp:226] relu1 needs backward computation.
I1127 10:48:33.451526  4930 net.cpp:226] ip1 needs backward computation.
I1127 10:48:33.451534  4930 net.cpp:226] pool2 needs backward computation.
I1127 10:48:33.451540  4930 net.cpp:226] conv2 needs backward computation.
I1127 10:48:33.451547  4930 net.cpp:226] pool1 needs backward computation.
I1127 10:48:33.451555  4930 net.cpp:226] conv1 needs backward computation.
I1127 10:48:33.451563  4930 net.cpp:228] mnist does not need backward computation.
I1127 10:48:33.451584  4930 net.cpp:270] This network produces output loss
I1127 10:48:33.451601  4930 net.cpp:283] Network initialization done.
I1127 10:48:33.452016  4930 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:48:33.452049  4930 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:48:33.452209  4930 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:48:33.452299  4930 layer_factory.hpp:76] Creating layer mnist
I1127 10:48:33.452461  4930 net.cpp:106] Creating Layer mnist
I1127 10:48:33.452477  4930 net.cpp:411] mnist -> data
I1127 10:48:33.452492  4930 net.cpp:411] mnist -> label
I1127 10:48:33.453343  4936 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:48:33.453634  4930 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:48:33.455070  4930 net.cpp:150] Setting up mnist
I1127 10:48:33.455132  4930 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:48:33.455144  4930 net.cpp:157] Top shape: 100 (100)
I1127 10:48:33.455153  4930 net.cpp:165] Memory required for data: 314000
I1127 10:48:33.455168  4930 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:48:33.455198  4930 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:48:33.455209  4930 net.cpp:454] label_mnist_1_split <- label
I1127 10:48:33.455225  4930 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:48:33.455241  4930 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:48:33.455292  4930 net.cpp:150] Setting up label_mnist_1_split
I1127 10:48:33.455303  4930 net.cpp:157] Top shape: 100 (100)
I1127 10:48:33.455312  4930 net.cpp:157] Top shape: 100 (100)
I1127 10:48:33.455318  4930 net.cpp:165] Memory required for data: 314800
I1127 10:48:33.455325  4930 layer_factory.hpp:76] Creating layer conv1
I1127 10:48:33.455343  4930 net.cpp:106] Creating Layer conv1
I1127 10:48:33.455351  4930 net.cpp:454] conv1 <- data
I1127 10:48:33.455363  4930 net.cpp:411] conv1 -> conv1
I1127 10:48:33.455585  4930 net.cpp:150] Setting up conv1
I1127 10:48:33.455595  4930 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:48:33.455600  4930 net.cpp:165] Memory required for data: 4922800
I1127 10:48:33.455610  4930 layer_factory.hpp:76] Creating layer pool1
I1127 10:48:33.455618  4930 net.cpp:106] Creating Layer pool1
I1127 10:48:33.455623  4930 net.cpp:454] pool1 <- conv1
I1127 10:48:33.455641  4930 net.cpp:411] pool1 -> pool1
I1127 10:48:33.455669  4930 net.cpp:150] Setting up pool1
I1127 10:48:33.455677  4930 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:48:33.455680  4930 net.cpp:165] Memory required for data: 6074800
I1127 10:48:33.455685  4930 layer_factory.hpp:76] Creating layer conv2
I1127 10:48:33.455696  4930 net.cpp:106] Creating Layer conv2
I1127 10:48:33.455701  4930 net.cpp:454] conv2 <- pool1
I1127 10:48:33.455708  4930 net.cpp:411] conv2 -> conv2
I1127 10:48:33.456012  4930 net.cpp:150] Setting up conv2
I1127 10:48:33.456020  4930 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:48:33.456025  4930 net.cpp:165] Memory required for data: 7354800
I1127 10:48:33.456033  4930 layer_factory.hpp:76] Creating layer pool2
I1127 10:48:33.456040  4930 net.cpp:106] Creating Layer pool2
I1127 10:48:33.456045  4930 net.cpp:454] pool2 <- conv2
I1127 10:48:33.456053  4930 net.cpp:411] pool2 -> pool2
I1127 10:48:33.456079  4930 net.cpp:150] Setting up pool2
I1127 10:48:33.456086  4930 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:48:33.456090  4930 net.cpp:165] Memory required for data: 7674800
I1127 10:48:33.456095  4930 layer_factory.hpp:76] Creating layer ip1
I1127 10:48:33.456104  4930 net.cpp:106] Creating Layer ip1
I1127 10:48:33.456110  4930 net.cpp:454] ip1 <- pool2
I1127 10:48:33.456116  4930 net.cpp:411] ip1 -> ip1
I1127 10:48:33.458361  4930 net.cpp:150] Setting up ip1
I1127 10:48:33.458405  4930 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:48:33.458410  4930 net.cpp:165] Memory required for data: 7874800
I1127 10:48:33.458425  4930 layer_factory.hpp:76] Creating layer relu1
I1127 10:48:33.458438  4930 net.cpp:106] Creating Layer relu1
I1127 10:48:33.458444  4930 net.cpp:454] relu1 <- ip1
I1127 10:48:33.458453  4930 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:48:33.458464  4930 net.cpp:150] Setting up relu1
I1127 10:48:33.458470  4930 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:48:33.458473  4930 net.cpp:165] Memory required for data: 8074800
I1127 10:48:33.458477  4930 layer_factory.hpp:76] Creating layer ip2
I1127 10:48:33.458489  4930 net.cpp:106] Creating Layer ip2
I1127 10:48:33.458494  4930 net.cpp:454] ip2 <- ip1
I1127 10:48:33.458503  4930 net.cpp:411] ip2 -> ip2
I1127 10:48:33.458622  4930 net.cpp:150] Setting up ip2
I1127 10:48:33.458631  4930 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:48:33.458634  4930 net.cpp:165] Memory required for data: 8078800
I1127 10:48:33.458642  4930 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:48:33.458649  4930 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:48:33.458653  4930 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:48:33.458659  4930 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:48:33.458665  4930 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:48:33.458694  4930 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:48:33.458701  4930 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:48:33.458706  4930 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:48:33.458710  4930 net.cpp:165] Memory required for data: 8086800
I1127 10:48:33.458714  4930 layer_factory.hpp:76] Creating layer accuracy
I1127 10:48:33.458724  4930 net.cpp:106] Creating Layer accuracy
I1127 10:48:33.458729  4930 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:48:33.458734  4930 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:48:33.458740  4930 net.cpp:411] accuracy -> accuracy
I1127 10:48:33.458750  4930 net.cpp:150] Setting up accuracy
I1127 10:48:33.458755  4930 net.cpp:157] Top shape: (1)
I1127 10:48:33.458760  4930 net.cpp:165] Memory required for data: 8086804
I1127 10:48:33.458772  4930 layer_factory.hpp:76] Creating layer loss
I1127 10:48:33.458781  4930 net.cpp:106] Creating Layer loss
I1127 10:48:33.458786  4930 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:48:33.458791  4930 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:48:33.458797  4930 net.cpp:411] loss -> loss
I1127 10:48:33.458807  4930 layer_factory.hpp:76] Creating layer loss
I1127 10:48:33.458892  4930 net.cpp:150] Setting up loss
I1127 10:48:33.458899  4930 net.cpp:157] Top shape: (1)
I1127 10:48:33.458904  4930 net.cpp:160]     with loss weight 1
I1127 10:48:33.458920  4930 net.cpp:165] Memory required for data: 8086808
I1127 10:48:33.458925  4930 net.cpp:226] loss needs backward computation.
I1127 10:48:33.458932  4930 net.cpp:228] accuracy does not need backward computation.
I1127 10:48:33.458937  4930 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:48:33.458941  4930 net.cpp:226] ip2 needs backward computation.
I1127 10:48:33.458945  4930 net.cpp:226] relu1 needs backward computation.
I1127 10:48:33.458950  4930 net.cpp:226] ip1 needs backward computation.
I1127 10:48:33.458953  4930 net.cpp:226] pool2 needs backward computation.
I1127 10:48:33.458958  4930 net.cpp:226] conv2 needs backward computation.
I1127 10:48:33.458962  4930 net.cpp:226] pool1 needs backward computation.
I1127 10:48:33.458966  4930 net.cpp:226] conv1 needs backward computation.
I1127 10:48:33.458971  4930 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:48:33.458976  4930 net.cpp:228] mnist does not need backward computation.
I1127 10:48:33.458979  4930 net.cpp:270] This network produces output accuracy
I1127 10:48:33.458983  4930 net.cpp:270] This network produces output loss
I1127 10:48:33.458995  4930 net.cpp:283] Network initialization done.
I1127 10:48:33.459051  4930 solver.cpp:59] Solver scaffolding done.
I1127 10:48:33.459244  4930 caffe.cpp:212] Starting Optimization
I1127 10:48:33.459251  4930 solver.cpp:287] Solving LeNet
I1127 10:48:33.459255  4930 solver.cpp:288] Learning Rate Policy: inv
I1127 10:48:33.459707  4930 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:48:34.964205  4930 solver.cpp:408]     Test net output #0: accuracy = 0.1081
I1127 10:48:34.964251  4930 solver.cpp:408]     Test net output #1: loss = 2.34919 (* 1 = 2.34919 loss)
I1127 10:48:34.994985  4930 solver.cpp:236] Iteration 0, loss = 2.36852
I1127 10:48:34.995007  4930 solver.cpp:252]     Train net output #0: loss = 2.36852 (* 1 = 2.36852 loss)
I1127 10:48:34.995024  4930 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:48:44.345796  4930 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:48:45.794950  4930 solver.cpp:408]     Test net output #0: accuracy = 0.9765
I1127 10:48:45.794999  4930 solver.cpp:408]     Test net output #1: loss = 0.0795982 (* 1 = 0.0795982 loss)
I1127 10:48:45.822788  4930 solver.cpp:236] Iteration 500, loss = 0.12038
I1127 10:48:45.822810  4930 solver.cpp:252]     Train net output #0: loss = 0.12038 (* 1 = 0.12038 loss)
I1127 10:48:45.822819  4930 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:48:55.090996  4930 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:48:55.192278  4930 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:48:55.201341  4930 solver.cpp:320] Iteration 1000, loss = 0.118029
I1127 10:48:55.201380  4930 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:48:56.594307  4930 solver.cpp:408]     Test net output #0: accuracy = 0.9811
I1127 10:48:56.594353  4930 solver.cpp:408]     Test net output #1: loss = 0.0592181 (* 1 = 0.0592181 loss)
I1127 10:48:56.594362  4930 solver.cpp:325] Optimization Done.
I1127 10:48:56.594365  4930 caffe.cpp:215] Optimization Done.
I1127 10:48:56.660519  4998 caffe.cpp:184] Using GPUs 0
I1127 10:48:56.911144  4998 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:48:56.911381  4998 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:48:56.911855  4998 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:48:56.911893  4998 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:48:56.912044  4998 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:48:56.912114  4998 layer_factory.hpp:76] Creating layer mnist
I1127 10:48:56.954272  4998 net.cpp:106] Creating Layer mnist
I1127 10:48:56.954331  4998 net.cpp:411] mnist -> data
I1127 10:48:56.954372  4998 net.cpp:411] mnist -> label
I1127 10:48:56.955180  5001 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:48:56.968204  4998 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:48:57.018219  4998 net.cpp:150] Setting up mnist
I1127 10:48:57.018268  4998 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:48:57.018277  4998 net.cpp:157] Top shape: 64 (64)
I1127 10:48:57.018282  4998 net.cpp:165] Memory required for data: 200960
I1127 10:48:57.018295  4998 layer_factory.hpp:76] Creating layer conv1
I1127 10:48:57.018314  4998 net.cpp:106] Creating Layer conv1
I1127 10:48:57.018322  4998 net.cpp:454] conv1 <- data
I1127 10:48:57.018335  4998 net.cpp:411] conv1 -> conv1
I1127 10:48:57.019083  4998 net.cpp:150] Setting up conv1
I1127 10:48:57.019100  4998 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:48:57.019105  4998 net.cpp:165] Memory required for data: 3150080
I1127 10:48:57.019119  4998 layer_factory.hpp:76] Creating layer pool1
I1127 10:48:57.019129  4998 net.cpp:106] Creating Layer pool1
I1127 10:48:57.019134  4998 net.cpp:454] pool1 <- conv1
I1127 10:48:57.019140  4998 net.cpp:411] pool1 -> pool1
I1127 10:48:57.019198  4998 net.cpp:150] Setting up pool1
I1127 10:48:57.019207  4998 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:48:57.019210  4998 net.cpp:165] Memory required for data: 3887360
I1127 10:48:57.019218  4998 layer_factory.hpp:76] Creating layer conv2
I1127 10:48:57.019229  4998 net.cpp:106] Creating Layer conv2
I1127 10:48:57.019234  4998 net.cpp:454] conv2 <- pool1
I1127 10:48:57.019242  4998 net.cpp:411] conv2 -> conv2
I1127 10:48:57.019614  4998 net.cpp:150] Setting up conv2
I1127 10:48:57.019629  4998 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:48:57.019634  4998 net.cpp:165] Memory required for data: 4706560
I1127 10:48:57.019644  4998 layer_factory.hpp:76] Creating layer pool2
I1127 10:48:57.019654  4998 net.cpp:106] Creating Layer pool2
I1127 10:48:57.019657  4998 net.cpp:454] pool2 <- conv2
I1127 10:48:57.019664  4998 net.cpp:411] pool2 -> pool2
I1127 10:48:57.019693  4998 net.cpp:150] Setting up pool2
I1127 10:48:57.019701  4998 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:48:57.019706  4998 net.cpp:165] Memory required for data: 4911360
I1127 10:48:57.019709  4998 layer_factory.hpp:76] Creating layer ip1
I1127 10:48:57.019719  4998 net.cpp:106] Creating Layer ip1
I1127 10:48:57.019723  4998 net.cpp:454] ip1 <- pool2
I1127 10:48:57.019731  4998 net.cpp:411] ip1 -> ip1
I1127 10:48:57.021930  4998 net.cpp:150] Setting up ip1
I1127 10:48:57.021942  4998 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:48:57.021946  4998 net.cpp:165] Memory required for data: 5039360
I1127 10:48:57.021956  4998 layer_factory.hpp:76] Creating layer relu1
I1127 10:48:57.021965  4998 net.cpp:106] Creating Layer relu1
I1127 10:48:57.021970  4998 net.cpp:454] relu1 <- ip1
I1127 10:48:57.021975  4998 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:48:57.021983  4998 net.cpp:150] Setting up relu1
I1127 10:48:57.021989  4998 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:48:57.021993  4998 net.cpp:165] Memory required for data: 5167360
I1127 10:48:57.021997  4998 layer_factory.hpp:76] Creating layer ip2
I1127 10:48:57.022004  4998 net.cpp:106] Creating Layer ip2
I1127 10:48:57.022008  4998 net.cpp:454] ip2 <- ip1
I1127 10:48:57.022016  4998 net.cpp:411] ip2 -> ip2
I1127 10:48:57.022438  4998 net.cpp:150] Setting up ip2
I1127 10:48:57.022449  4998 net.cpp:157] Top shape: 64 10 (640)
I1127 10:48:57.022452  4998 net.cpp:165] Memory required for data: 5169920
I1127 10:48:57.022460  4998 layer_factory.hpp:76] Creating layer loss
I1127 10:48:57.022469  4998 net.cpp:106] Creating Layer loss
I1127 10:48:57.022474  4998 net.cpp:454] loss <- ip2
I1127 10:48:57.022478  4998 net.cpp:454] loss <- label
I1127 10:48:57.022485  4998 net.cpp:411] loss -> loss
I1127 10:48:57.022496  4998 layer_factory.hpp:76] Creating layer loss
I1127 10:48:57.022568  4998 net.cpp:150] Setting up loss
I1127 10:48:57.022577  4998 net.cpp:157] Top shape: (1)
I1127 10:48:57.022580  4998 net.cpp:160]     with loss weight 1
I1127 10:48:57.022598  4998 net.cpp:165] Memory required for data: 5169924
I1127 10:48:57.022603  4998 net.cpp:226] loss needs backward computation.
I1127 10:48:57.022606  4998 net.cpp:226] ip2 needs backward computation.
I1127 10:48:57.022610  4998 net.cpp:226] relu1 needs backward computation.
I1127 10:48:57.022614  4998 net.cpp:226] ip1 needs backward computation.
I1127 10:48:57.022619  4998 net.cpp:226] pool2 needs backward computation.
I1127 10:48:57.022624  4998 net.cpp:226] conv2 needs backward computation.
I1127 10:48:57.022627  4998 net.cpp:226] pool1 needs backward computation.
I1127 10:48:57.022631  4998 net.cpp:226] conv1 needs backward computation.
I1127 10:48:57.022636  4998 net.cpp:228] mnist does not need backward computation.
I1127 10:48:57.022640  4998 net.cpp:270] This network produces output loss
I1127 10:48:57.022650  4998 net.cpp:283] Network initialization done.
I1127 10:48:57.022886  4998 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:48:57.022907  4998 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:48:57.023020  4998 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:48:57.023082  4998 layer_factory.hpp:76] Creating layer mnist
I1127 10:48:57.023166  4998 net.cpp:106] Creating Layer mnist
I1127 10:48:57.023175  4998 net.cpp:411] mnist -> data
I1127 10:48:57.023183  4998 net.cpp:411] mnist -> label
I1127 10:48:57.023856  5004 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:48:57.023967  4998 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:48:57.028303  4998 net.cpp:150] Setting up mnist
I1127 10:48:57.028316  4998 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:48:57.028322  4998 net.cpp:157] Top shape: 100 (100)
I1127 10:48:57.028326  4998 net.cpp:165] Memory required for data: 314000
I1127 10:48:57.028331  4998 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:48:57.028338  4998 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:48:57.028343  4998 net.cpp:454] label_mnist_1_split <- label
I1127 10:48:57.028348  4998 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:48:57.028357  4998 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:48:57.028388  4998 net.cpp:150] Setting up label_mnist_1_split
I1127 10:48:57.028395  4998 net.cpp:157] Top shape: 100 (100)
I1127 10:48:57.028400  4998 net.cpp:157] Top shape: 100 (100)
I1127 10:48:57.028404  4998 net.cpp:165] Memory required for data: 314800
I1127 10:48:57.028409  4998 layer_factory.hpp:76] Creating layer conv1
I1127 10:48:57.028419  4998 net.cpp:106] Creating Layer conv1
I1127 10:48:57.028422  4998 net.cpp:454] conv1 <- data
I1127 10:48:57.028429  4998 net.cpp:411] conv1 -> conv1
I1127 10:48:57.028571  4998 net.cpp:150] Setting up conv1
I1127 10:48:57.028579  4998 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:48:57.028584  4998 net.cpp:165] Memory required for data: 4922800
I1127 10:48:57.028592  4998 layer_factory.hpp:76] Creating layer pool1
I1127 10:48:57.028599  4998 net.cpp:106] Creating Layer pool1
I1127 10:48:57.028604  4998 net.cpp:454] pool1 <- conv1
I1127 10:48:57.028616  4998 net.cpp:411] pool1 -> pool1
I1127 10:48:57.028646  4998 net.cpp:150] Setting up pool1
I1127 10:48:57.028656  4998 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:48:57.028662  4998 net.cpp:165] Memory required for data: 6074800
I1127 10:48:57.028667  4998 layer_factory.hpp:76] Creating layer conv2
I1127 10:48:57.028674  4998 net.cpp:106] Creating Layer conv2
I1127 10:48:57.028679  4998 net.cpp:454] conv2 <- pool1
I1127 10:48:57.028687  4998 net.cpp:411] conv2 -> conv2
I1127 10:48:57.028944  4998 net.cpp:150] Setting up conv2
I1127 10:48:57.028952  4998 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:48:57.028956  4998 net.cpp:165] Memory required for data: 7354800
I1127 10:48:57.028965  4998 layer_factory.hpp:76] Creating layer pool2
I1127 10:48:57.028971  4998 net.cpp:106] Creating Layer pool2
I1127 10:48:57.028975  4998 net.cpp:454] pool2 <- conv2
I1127 10:48:57.028981  4998 net.cpp:411] pool2 -> pool2
I1127 10:48:57.029661  4998 net.cpp:150] Setting up pool2
I1127 10:48:57.029677  4998 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:48:57.029685  4998 net.cpp:165] Memory required for data: 7674800
I1127 10:48:57.029691  4998 layer_factory.hpp:76] Creating layer ip1
I1127 10:48:57.029701  4998 net.cpp:106] Creating Layer ip1
I1127 10:48:57.029706  4998 net.cpp:454] ip1 <- pool2
I1127 10:48:57.029713  4998 net.cpp:411] ip1 -> ip1
I1127 10:48:57.031920  4998 net.cpp:150] Setting up ip1
I1127 10:48:57.031932  4998 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:48:57.031936  4998 net.cpp:165] Memory required for data: 7874800
I1127 10:48:57.031945  4998 layer_factory.hpp:76] Creating layer relu1
I1127 10:48:57.031952  4998 net.cpp:106] Creating Layer relu1
I1127 10:48:57.031957  4998 net.cpp:454] relu1 <- ip1
I1127 10:48:57.031963  4998 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:48:57.031971  4998 net.cpp:150] Setting up relu1
I1127 10:48:57.031977  4998 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:48:57.031981  4998 net.cpp:165] Memory required for data: 8074800
I1127 10:48:57.031985  4998 layer_factory.hpp:76] Creating layer ip2
I1127 10:48:57.031992  4998 net.cpp:106] Creating Layer ip2
I1127 10:48:57.031997  4998 net.cpp:454] ip2 <- ip1
I1127 10:48:57.032002  4998 net.cpp:411] ip2 -> ip2
I1127 10:48:57.032094  4998 net.cpp:150] Setting up ip2
I1127 10:48:57.032102  4998 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:48:57.032106  4998 net.cpp:165] Memory required for data: 8078800
I1127 10:48:57.032112  4998 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:48:57.032119  4998 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:48:57.032124  4998 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:48:57.032130  4998 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:48:57.032136  4998 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:48:57.032163  4998 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:48:57.032171  4998 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:48:57.032176  4998 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:48:57.032179  4998 net.cpp:165] Memory required for data: 8086800
I1127 10:48:57.032183  4998 layer_factory.hpp:76] Creating layer accuracy
I1127 10:48:57.032191  4998 net.cpp:106] Creating Layer accuracy
I1127 10:48:57.032196  4998 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:48:57.032201  4998 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:48:57.032205  4998 net.cpp:411] accuracy -> accuracy
I1127 10:48:57.032214  4998 net.cpp:150] Setting up accuracy
I1127 10:48:57.032219  4998 net.cpp:157] Top shape: (1)
I1127 10:48:57.032224  4998 net.cpp:165] Memory required for data: 8086804
I1127 10:48:57.032228  4998 layer_factory.hpp:76] Creating layer loss
I1127 10:48:57.032233  4998 net.cpp:106] Creating Layer loss
I1127 10:48:57.032238  4998 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:48:57.032243  4998 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:48:57.032249  4998 net.cpp:411] loss -> loss
I1127 10:48:57.032258  4998 layer_factory.hpp:76] Creating layer loss
I1127 10:48:57.032328  4998 net.cpp:150] Setting up loss
I1127 10:48:57.032335  4998 net.cpp:157] Top shape: (1)
I1127 10:48:57.032340  4998 net.cpp:160]     with loss weight 1
I1127 10:48:57.032351  4998 net.cpp:165] Memory required for data: 8086808
I1127 10:48:57.032356  4998 net.cpp:226] loss needs backward computation.
I1127 10:48:57.032361  4998 net.cpp:228] accuracy does not need backward computation.
I1127 10:48:57.032366  4998 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:48:57.032371  4998 net.cpp:226] ip2 needs backward computation.
I1127 10:48:57.032377  4998 net.cpp:226] relu1 needs backward computation.
I1127 10:48:57.032380  4998 net.cpp:226] ip1 needs backward computation.
I1127 10:48:57.032384  4998 net.cpp:226] pool2 needs backward computation.
I1127 10:48:57.032389  4998 net.cpp:226] conv2 needs backward computation.
I1127 10:48:57.032393  4998 net.cpp:226] pool1 needs backward computation.
I1127 10:48:57.032397  4998 net.cpp:226] conv1 needs backward computation.
I1127 10:48:57.032402  4998 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:48:57.032407  4998 net.cpp:228] mnist does not need backward computation.
I1127 10:48:57.032410  4998 net.cpp:270] This network produces output accuracy
I1127 10:48:57.032414  4998 net.cpp:270] This network produces output loss
I1127 10:48:57.032424  4998 net.cpp:283] Network initialization done.
I1127 10:48:57.032456  4998 solver.cpp:59] Solver scaffolding done.
I1127 10:48:57.032640  4998 caffe.cpp:212] Starting Optimization
I1127 10:48:57.032646  4998 solver.cpp:287] Solving LeNet
I1127 10:48:57.032650  4998 solver.cpp:288] Learning Rate Policy: inv
I1127 10:48:57.032959  4998 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:48:58.360622  4998 solver.cpp:408]     Test net output #0: accuracy = 0.11
I1127 10:48:58.360692  4998 solver.cpp:408]     Test net output #1: loss = 2.36229 (* 1 = 2.36229 loss)
I1127 10:48:58.372148  4998 solver.cpp:236] Iteration 0, loss = 2.36369
I1127 10:48:58.372252  4998 solver.cpp:252]     Train net output #0: loss = 2.36369 (* 1 = 2.36369 loss)
I1127 10:48:58.372280  4998 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:49:10.337807  4998 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:49:13.450179  4998 solver.cpp:408]     Test net output #0: accuracy = 0.9741
I1127 10:49:13.450269  4998 solver.cpp:408]     Test net output #1: loss = 0.0811356 (* 1 = 0.0811356 loss)
I1127 10:49:13.462170  4998 solver.cpp:236] Iteration 500, loss = 0.0706199
I1127 10:49:13.462277  4998 solver.cpp:252]     Train net output #0: loss = 0.0706199 (* 1 = 0.0706199 loss)
I1127 10:49:13.462378  4998 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:49:24.867105  4998 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:49:24.890326  4998 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:49:24.901340  4998 solver.cpp:320] Iteration 1000, loss = 0.0989266
I1127 10:49:24.901404  4998 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:49:25.248111  4998 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:49:26.005128  4998 solver.cpp:408]     Test net output #0: accuracy = 0.9811
I1127 10:49:26.005328  4998 solver.cpp:408]     Test net output #1: loss = 0.0577824 (* 1 = 0.0577824 loss)
I1127 10:49:26.005385  4998 solver.cpp:325] Optimization Done.
I1127 10:49:26.005421  4998 caffe.cpp:215] Optimization Done.
I1127 10:49:26.154985  5033 caffe.cpp:184] Using GPUs 0
I1127 10:49:26.724063  5033 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:49:26.724431  5033 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:49:26.725018  5033 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:49:26.725065  5033 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:49:26.725288  5033 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:49:26.725406  5033 layer_factory.hpp:76] Creating layer mnist
I1127 10:49:26.726096  5033 net.cpp:106] Creating Layer mnist
I1127 10:49:26.726164  5033 net.cpp:411] mnist -> data
I1127 10:49:26.726220  5033 net.cpp:411] mnist -> label
I1127 10:49:26.727401  5036 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:49:26.767838  5033 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:49:26.773283  5033 net.cpp:150] Setting up mnist
I1127 10:49:26.773442  5033 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:49:26.773460  5033 net.cpp:157] Top shape: 64 (64)
I1127 10:49:26.773468  5033 net.cpp:165] Memory required for data: 200960
I1127 10:49:26.773497  5033 layer_factory.hpp:76] Creating layer conv1
I1127 10:49:26.773555  5033 net.cpp:106] Creating Layer conv1
I1127 10:49:26.773572  5033 net.cpp:454] conv1 <- data
I1127 10:49:26.773597  5033 net.cpp:411] conv1 -> conv1
I1127 10:49:26.778496  5033 net.cpp:150] Setting up conv1
I1127 10:49:26.778573  5033 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:49:26.778580  5033 net.cpp:165] Memory required for data: 3150080
I1127 10:49:26.778614  5033 layer_factory.hpp:76] Creating layer pool1
I1127 10:49:26.778640  5033 net.cpp:106] Creating Layer pool1
I1127 10:49:26.778650  5033 net.cpp:454] pool1 <- conv1
I1127 10:49:26.778662  5033 net.cpp:411] pool1 -> pool1
I1127 10:49:26.778758  5033 net.cpp:150] Setting up pool1
I1127 10:49:26.778769  5033 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:49:26.778775  5033 net.cpp:165] Memory required for data: 3887360
I1127 10:49:26.778782  5033 layer_factory.hpp:76] Creating layer conv2
I1127 10:49:26.778800  5033 net.cpp:106] Creating Layer conv2
I1127 10:49:26.778807  5033 net.cpp:454] conv2 <- pool1
I1127 10:49:26.778820  5033 net.cpp:411] conv2 -> conv2
I1127 10:49:26.779268  5033 net.cpp:150] Setting up conv2
I1127 10:49:26.779290  5033 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:49:26.779297  5033 net.cpp:165] Memory required for data: 4706560
I1127 10:49:26.779322  5033 layer_factory.hpp:76] Creating layer pool2
I1127 10:49:26.779338  5033 net.cpp:106] Creating Layer pool2
I1127 10:49:26.779346  5033 net.cpp:454] pool2 <- conv2
I1127 10:49:26.779356  5033 net.cpp:411] pool2 -> pool2
I1127 10:49:26.779407  5033 net.cpp:150] Setting up pool2
I1127 10:49:26.779418  5033 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:49:26.779425  5033 net.cpp:165] Memory required for data: 4911360
I1127 10:49:26.779433  5033 layer_factory.hpp:76] Creating layer ip1
I1127 10:49:26.779443  5033 net.cpp:106] Creating Layer ip1
I1127 10:49:26.779450  5033 net.cpp:454] ip1 <- pool2
I1127 10:49:26.779460  5033 net.cpp:411] ip1 -> ip1
I1127 10:49:26.784396  5033 net.cpp:150] Setting up ip1
I1127 10:49:26.784488  5033 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:49:26.784499  5033 net.cpp:165] Memory required for data: 5039360
I1127 10:49:26.784533  5033 layer_factory.hpp:76] Creating layer relu1
I1127 10:49:26.784559  5033 net.cpp:106] Creating Layer relu1
I1127 10:49:26.784574  5033 net.cpp:454] relu1 <- ip1
I1127 10:49:26.784590  5033 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:49:26.784626  5033 net.cpp:150] Setting up relu1
I1127 10:49:26.784639  5033 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:49:26.784646  5033 net.cpp:165] Memory required for data: 5167360
I1127 10:49:26.784656  5033 layer_factory.hpp:76] Creating layer ip2
I1127 10:49:26.784680  5033 net.cpp:106] Creating Layer ip2
I1127 10:49:26.784693  5033 net.cpp:454] ip2 <- ip1
I1127 10:49:26.784711  5033 net.cpp:411] ip2 -> ip2
I1127 10:49:26.785923  5033 net.cpp:150] Setting up ip2
I1127 10:49:26.786000  5033 net.cpp:157] Top shape: 64 10 (640)
I1127 10:49:26.786011  5033 net.cpp:165] Memory required for data: 5169920
I1127 10:49:26.786033  5033 layer_factory.hpp:76] Creating layer loss
I1127 10:49:26.786068  5033 net.cpp:106] Creating Layer loss
I1127 10:49:26.786084  5033 net.cpp:454] loss <- ip2
I1127 10:49:26.786100  5033 net.cpp:454] loss <- label
I1127 10:49:26.786128  5033 net.cpp:411] loss -> loss
I1127 10:49:26.786165  5033 layer_factory.hpp:76] Creating layer loss
I1127 10:49:26.786322  5033 net.cpp:150] Setting up loss
I1127 10:49:26.786339  5033 net.cpp:157] Top shape: (1)
I1127 10:49:26.786347  5033 net.cpp:160]     with loss weight 1
I1127 10:49:26.786383  5033 net.cpp:165] Memory required for data: 5169924
I1127 10:49:26.786394  5033 net.cpp:226] loss needs backward computation.
I1127 10:49:26.786404  5033 net.cpp:226] ip2 needs backward computation.
I1127 10:49:26.786414  5033 net.cpp:226] relu1 needs backward computation.
I1127 10:49:26.786422  5033 net.cpp:226] ip1 needs backward computation.
I1127 10:49:26.786432  5033 net.cpp:226] pool2 needs backward computation.
I1127 10:49:26.786442  5033 net.cpp:226] conv2 needs backward computation.
I1127 10:49:26.786451  5033 net.cpp:226] pool1 needs backward computation.
I1127 10:49:26.786459  5033 net.cpp:226] conv1 needs backward computation.
I1127 10:49:26.786469  5033 net.cpp:228] mnist does not need backward computation.
I1127 10:49:26.786478  5033 net.cpp:270] This network produces output loss
I1127 10:49:26.786494  5033 net.cpp:283] Network initialization done.
I1127 10:49:26.786962  5033 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:49:26.787021  5033 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:49:26.787281  5033 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:49:26.787423  5033 layer_factory.hpp:76] Creating layer mnist
I1127 10:49:26.787580  5033 net.cpp:106] Creating Layer mnist
I1127 10:49:26.787602  5033 net.cpp:411] mnist -> data
I1127 10:49:26.787637  5033 net.cpp:411] mnist -> label
I1127 10:49:26.788806  5038 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:49:26.789070  5033 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:49:26.793500  5033 net.cpp:150] Setting up mnist
I1127 10:49:26.793548  5033 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:49:26.793555  5033 net.cpp:157] Top shape: 100 (100)
I1127 10:49:26.793560  5033 net.cpp:165] Memory required for data: 314000
I1127 10:49:26.793570  5033 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:49:26.793591  5033 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:49:26.793597  5033 net.cpp:454] label_mnist_1_split <- label
I1127 10:49:26.793608  5033 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:49:26.793627  5033 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:49:26.793736  5033 net.cpp:150] Setting up label_mnist_1_split
I1127 10:49:26.793750  5033 net.cpp:157] Top shape: 100 (100)
I1127 10:49:26.793754  5033 net.cpp:157] Top shape: 100 (100)
I1127 10:49:26.793759  5033 net.cpp:165] Memory required for data: 314800
I1127 10:49:26.793763  5033 layer_factory.hpp:76] Creating layer conv1
I1127 10:49:26.793790  5033 net.cpp:106] Creating Layer conv1
I1127 10:49:26.793797  5033 net.cpp:454] conv1 <- data
I1127 10:49:26.793807  5033 net.cpp:411] conv1 -> conv1
I1127 10:49:26.794044  5033 net.cpp:150] Setting up conv1
I1127 10:49:26.794062  5033 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:49:26.794066  5033 net.cpp:165] Memory required for data: 4922800
I1127 10:49:26.794077  5033 layer_factory.hpp:76] Creating layer pool1
I1127 10:49:26.794088  5033 net.cpp:106] Creating Layer pool1
I1127 10:49:26.794093  5033 net.cpp:454] pool1 <- conv1
I1127 10:49:26.794126  5033 net.cpp:411] pool1 -> pool1
I1127 10:49:26.794180  5033 net.cpp:150] Setting up pool1
I1127 10:49:26.794190  5033 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:49:26.794194  5033 net.cpp:165] Memory required for data: 6074800
I1127 10:49:26.794198  5033 layer_factory.hpp:76] Creating layer conv2
I1127 10:49:26.794209  5033 net.cpp:106] Creating Layer conv2
I1127 10:49:26.794214  5033 net.cpp:454] conv2 <- pool1
I1127 10:49:26.794221  5033 net.cpp:411] conv2 -> conv2
I1127 10:49:26.794483  5033 net.cpp:150] Setting up conv2
I1127 10:49:26.794494  5033 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:49:26.794505  5033 net.cpp:165] Memory required for data: 7354800
I1127 10:49:26.794515  5033 layer_factory.hpp:76] Creating layer pool2
I1127 10:49:26.794524  5033 net.cpp:106] Creating Layer pool2
I1127 10:49:26.794529  5033 net.cpp:454] pool2 <- conv2
I1127 10:49:26.794535  5033 net.cpp:411] pool2 -> pool2
I1127 10:49:26.794569  5033 net.cpp:150] Setting up pool2
I1127 10:49:26.794575  5033 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:49:26.794580  5033 net.cpp:165] Memory required for data: 7674800
I1127 10:49:26.794584  5033 layer_factory.hpp:76] Creating layer ip1
I1127 10:49:26.794594  5033 net.cpp:106] Creating Layer ip1
I1127 10:49:26.794597  5033 net.cpp:454] ip1 <- pool2
I1127 10:49:26.794605  5033 net.cpp:411] ip1 -> ip1
I1127 10:49:26.797638  5033 net.cpp:150] Setting up ip1
I1127 10:49:26.797734  5033 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:49:26.797740  5033 net.cpp:165] Memory required for data: 7874800
I1127 10:49:26.797763  5033 layer_factory.hpp:76] Creating layer relu1
I1127 10:49:26.797796  5033 net.cpp:106] Creating Layer relu1
I1127 10:49:26.797806  5033 net.cpp:454] relu1 <- ip1
I1127 10:49:26.797817  5033 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:49:26.797837  5033 net.cpp:150] Setting up relu1
I1127 10:49:26.797843  5033 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:49:26.797847  5033 net.cpp:165] Memory required for data: 8074800
I1127 10:49:26.797852  5033 layer_factory.hpp:76] Creating layer ip2
I1127 10:49:26.797868  5033 net.cpp:106] Creating Layer ip2
I1127 10:49:26.797873  5033 net.cpp:454] ip2 <- ip1
I1127 10:49:26.797883  5033 net.cpp:411] ip2 -> ip2
I1127 10:49:26.798034  5033 net.cpp:150] Setting up ip2
I1127 10:49:26.798050  5033 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:49:26.798055  5033 net.cpp:165] Memory required for data: 8078800
I1127 10:49:26.798063  5033 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:49:26.798073  5033 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:49:26.798076  5033 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:49:26.798082  5033 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:49:26.798090  5033 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:49:26.798126  5033 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:49:26.798135  5033 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:49:26.798151  5033 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:49:26.798159  5033 net.cpp:165] Memory required for data: 8086800
I1127 10:49:26.798167  5033 layer_factory.hpp:76] Creating layer accuracy
I1127 10:49:26.798178  5033 net.cpp:106] Creating Layer accuracy
I1127 10:49:26.798184  5033 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:49:26.798190  5033 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:49:26.798203  5033 net.cpp:411] accuracy -> accuracy
I1127 10:49:26.798214  5033 net.cpp:150] Setting up accuracy
I1127 10:49:26.798220  5033 net.cpp:157] Top shape: (1)
I1127 10:49:26.798224  5033 net.cpp:165] Memory required for data: 8086804
I1127 10:49:26.798228  5033 layer_factory.hpp:76] Creating layer loss
I1127 10:49:26.798235  5033 net.cpp:106] Creating Layer loss
I1127 10:49:26.798239  5033 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:49:26.798244  5033 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:49:26.798251  5033 net.cpp:411] loss -> loss
I1127 10:49:26.798264  5033 layer_factory.hpp:76] Creating layer loss
I1127 10:49:26.798362  5033 net.cpp:150] Setting up loss
I1127 10:49:26.798372  5033 net.cpp:157] Top shape: (1)
I1127 10:49:26.798375  5033 net.cpp:160]     with loss weight 1
I1127 10:49:26.798396  5033 net.cpp:165] Memory required for data: 8086808
I1127 10:49:26.798400  5033 net.cpp:226] loss needs backward computation.
I1127 10:49:26.798411  5033 net.cpp:228] accuracy does not need backward computation.
I1127 10:49:26.798416  5033 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:49:26.798421  5033 net.cpp:226] ip2 needs backward computation.
I1127 10:49:26.798425  5033 net.cpp:226] relu1 needs backward computation.
I1127 10:49:26.798434  5033 net.cpp:226] ip1 needs backward computation.
I1127 10:49:26.798439  5033 net.cpp:226] pool2 needs backward computation.
I1127 10:49:26.798444  5033 net.cpp:226] conv2 needs backward computation.
I1127 10:49:26.798449  5033 net.cpp:226] pool1 needs backward computation.
I1127 10:49:26.798452  5033 net.cpp:226] conv1 needs backward computation.
I1127 10:49:26.798457  5033 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:49:26.798462  5033 net.cpp:228] mnist does not need backward computation.
I1127 10:49:26.798468  5033 net.cpp:270] This network produces output accuracy
I1127 10:49:26.798472  5033 net.cpp:270] This network produces output loss
I1127 10:49:26.798482  5033 net.cpp:283] Network initialization done.
I1127 10:49:26.798549  5033 solver.cpp:59] Solver scaffolding done.
I1127 10:49:26.798751  5033 caffe.cpp:212] Starting Optimization
I1127 10:49:26.798759  5033 solver.cpp:287] Solving LeNet
I1127 10:49:26.798763  5033 solver.cpp:288] Learning Rate Policy: inv
I1127 10:49:26.799407  5033 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:49:29.215224  5033 solver.cpp:408]     Test net output #0: accuracy = 0.1082
I1127 10:49:29.215296  5033 solver.cpp:408]     Test net output #1: loss = 2.41227 (* 1 = 2.41227 loss)
I1127 10:49:29.227021  5033 solver.cpp:236] Iteration 0, loss = 2.42116
I1127 10:49:29.227105  5033 solver.cpp:252]     Train net output #0: loss = 2.42116 (* 1 = 2.42116 loss)
I1127 10:49:29.227139  5033 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:49:42.568076  5033 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:49:44.276345  5033 solver.cpp:408]     Test net output #0: accuracy = 0.9714
I1127 10:49:44.276382  5033 solver.cpp:408]     Test net output #1: loss = 0.0890211 (* 1 = 0.0890211 loss)
I1127 10:49:44.306195  5033 solver.cpp:236] Iteration 500, loss = 0.103543
I1127 10:49:44.306215  5033 solver.cpp:252]     Train net output #0: loss = 0.103543 (* 1 = 0.103543 loss)
I1127 10:49:44.306223  5033 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:49:57.081231  5033 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:49:57.092844  5033 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:49:57.102854  5033 solver.cpp:320] Iteration 1000, loss = 0.0885305
I1127 10:49:57.102931  5033 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:49:58.208403  5033 solver.cpp:408]     Test net output #0: accuracy = 0.9799
I1127 10:49:58.208464  5033 solver.cpp:408]     Test net output #1: loss = 0.0623369 (* 1 = 0.0623369 loss)
I1127 10:49:58.208472  5033 solver.cpp:325] Optimization Done.
I1127 10:49:58.208477  5033 caffe.cpp:215] Optimization Done.
I1127 10:49:58.349472  5059 caffe.cpp:184] Using GPUs 0
I1127 10:49:58.673359  5059 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:49:58.673554  5059 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:49:58.673938  5059 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:49:58.673964  5059 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:49:58.674069  5059 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:49:58.674157  5059 layer_factory.hpp:76] Creating layer mnist
I1127 10:49:58.695569  5059 net.cpp:106] Creating Layer mnist
I1127 10:49:58.695626  5059 net.cpp:411] mnist -> data
I1127 10:49:58.695682  5059 net.cpp:411] mnist -> label
I1127 10:49:58.696933  5062 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:49:58.712383  5059 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:49:58.714301  5059 net.cpp:150] Setting up mnist
I1127 10:49:58.714402  5059 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:49:58.714421  5059 net.cpp:157] Top shape: 64 (64)
I1127 10:49:58.714434  5059 net.cpp:165] Memory required for data: 200960
I1127 10:49:58.714459  5059 layer_factory.hpp:76] Creating layer conv1
I1127 10:49:58.714516  5059 net.cpp:106] Creating Layer conv1
I1127 10:49:58.714540  5059 net.cpp:454] conv1 <- data
I1127 10:49:58.714570  5059 net.cpp:411] conv1 -> conv1
I1127 10:49:58.716075  5059 net.cpp:150] Setting up conv1
I1127 10:49:58.716145  5059 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:49:58.716153  5059 net.cpp:165] Memory required for data: 3150080
I1127 10:49:58.716181  5059 layer_factory.hpp:76] Creating layer pool1
I1127 10:49:58.716204  5059 net.cpp:106] Creating Layer pool1
I1127 10:49:58.716213  5059 net.cpp:454] pool1 <- conv1
I1127 10:49:58.716228  5059 net.cpp:411] pool1 -> pool1
I1127 10:49:58.716322  5059 net.cpp:150] Setting up pool1
I1127 10:49:58.716338  5059 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:49:58.716346  5059 net.cpp:165] Memory required for data: 3887360
I1127 10:49:58.716356  5059 layer_factory.hpp:76] Creating layer conv2
I1127 10:49:58.716374  5059 net.cpp:106] Creating Layer conv2
I1127 10:49:58.716382  5059 net.cpp:454] conv2 <- pool1
I1127 10:49:58.716395  5059 net.cpp:411] conv2 -> conv2
I1127 10:49:58.716732  5059 net.cpp:150] Setting up conv2
I1127 10:49:58.716750  5059 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:49:58.716754  5059 net.cpp:165] Memory required for data: 4706560
I1127 10:49:58.716766  5059 layer_factory.hpp:76] Creating layer pool2
I1127 10:49:58.716780  5059 net.cpp:106] Creating Layer pool2
I1127 10:49:58.716785  5059 net.cpp:454] pool2 <- conv2
I1127 10:49:58.716791  5059 net.cpp:411] pool2 -> pool2
I1127 10:49:58.716828  5059 net.cpp:150] Setting up pool2
I1127 10:49:58.716837  5059 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:49:58.716841  5059 net.cpp:165] Memory required for data: 4911360
I1127 10:49:58.716846  5059 layer_factory.hpp:76] Creating layer ip1
I1127 10:49:58.716862  5059 net.cpp:106] Creating Layer ip1
I1127 10:49:58.716877  5059 net.cpp:454] ip1 <- pool2
I1127 10:49:58.716883  5059 net.cpp:411] ip1 -> ip1
I1127 10:49:58.720510  5059 net.cpp:150] Setting up ip1
I1127 10:49:58.720605  5059 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:49:58.720616  5059 net.cpp:165] Memory required for data: 5039360
I1127 10:49:58.720657  5059 layer_factory.hpp:76] Creating layer relu1
I1127 10:49:58.720690  5059 net.cpp:106] Creating Layer relu1
I1127 10:49:58.720705  5059 net.cpp:454] relu1 <- ip1
I1127 10:49:58.720726  5059 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:49:58.720749  5059 net.cpp:150] Setting up relu1
I1127 10:49:58.720763  5059 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:49:58.720772  5059 net.cpp:165] Memory required for data: 5167360
I1127 10:49:58.720782  5059 layer_factory.hpp:76] Creating layer ip2
I1127 10:49:58.720805  5059 net.cpp:106] Creating Layer ip2
I1127 10:49:58.720816  5059 net.cpp:454] ip2 <- ip1
I1127 10:49:58.720839  5059 net.cpp:411] ip2 -> ip2
I1127 10:49:58.722307  5059 net.cpp:150] Setting up ip2
I1127 10:49:58.722371  5059 net.cpp:157] Top shape: 64 10 (640)
I1127 10:49:58.722384  5059 net.cpp:165] Memory required for data: 5169920
I1127 10:49:58.722407  5059 layer_factory.hpp:76] Creating layer loss
I1127 10:49:58.722426  5059 net.cpp:106] Creating Layer loss
I1127 10:49:58.722435  5059 net.cpp:454] loss <- ip2
I1127 10:49:58.722446  5059 net.cpp:454] loss <- label
I1127 10:49:58.722467  5059 net.cpp:411] loss -> loss
I1127 10:49:58.722503  5059 layer_factory.hpp:76] Creating layer loss
I1127 10:49:58.722640  5059 net.cpp:150] Setting up loss
I1127 10:49:58.722654  5059 net.cpp:157] Top shape: (1)
I1127 10:49:58.722662  5059 net.cpp:160]     with loss weight 1
I1127 10:49:58.722717  5059 net.cpp:165] Memory required for data: 5169924
I1127 10:49:58.722726  5059 net.cpp:226] loss needs backward computation.
I1127 10:49:58.722735  5059 net.cpp:226] ip2 needs backward computation.
I1127 10:49:58.722744  5059 net.cpp:226] relu1 needs backward computation.
I1127 10:49:58.722754  5059 net.cpp:226] ip1 needs backward computation.
I1127 10:49:58.722762  5059 net.cpp:226] pool2 needs backward computation.
I1127 10:49:58.722771  5059 net.cpp:226] conv2 needs backward computation.
I1127 10:49:58.722779  5059 net.cpp:226] pool1 needs backward computation.
I1127 10:49:58.722789  5059 net.cpp:226] conv1 needs backward computation.
I1127 10:49:58.722797  5059 net.cpp:228] mnist does not need backward computation.
I1127 10:49:58.722805  5059 net.cpp:270] This network produces output loss
I1127 10:49:58.722818  5059 net.cpp:283] Network initialization done.
I1127 10:49:58.723234  5059 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:49:58.723284  5059 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:49:58.723484  5059 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:49:58.723579  5059 layer_factory.hpp:76] Creating layer mnist
I1127 10:49:58.723717  5059 net.cpp:106] Creating Layer mnist
I1127 10:49:58.723731  5059 net.cpp:411] mnist -> data
I1127 10:49:58.723750  5059 net.cpp:411] mnist -> label
I1127 10:49:58.724704  5064 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:49:58.724975  5059 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:49:58.726932  5059 net.cpp:150] Setting up mnist
I1127 10:49:58.726989  5059 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:49:58.727002  5059 net.cpp:157] Top shape: 100 (100)
I1127 10:49:58.727011  5059 net.cpp:165] Memory required for data: 314000
I1127 10:49:58.727025  5059 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:49:58.727051  5059 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:49:58.727061  5059 net.cpp:454] label_mnist_1_split <- label
I1127 10:49:58.727072  5059 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:49:58.727088  5059 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:49:58.727144  5059 net.cpp:150] Setting up label_mnist_1_split
I1127 10:49:58.727156  5059 net.cpp:157] Top shape: 100 (100)
I1127 10:49:58.727164  5059 net.cpp:157] Top shape: 100 (100)
I1127 10:49:58.727172  5059 net.cpp:165] Memory required for data: 314800
I1127 10:49:58.727180  5059 layer_factory.hpp:76] Creating layer conv1
I1127 10:49:58.727202  5059 net.cpp:106] Creating Layer conv1
I1127 10:49:58.727210  5059 net.cpp:454] conv1 <- data
I1127 10:49:58.727227  5059 net.cpp:411] conv1 -> conv1
I1127 10:49:58.727540  5059 net.cpp:150] Setting up conv1
I1127 10:49:58.727565  5059 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:49:58.727572  5059 net.cpp:165] Memory required for data: 4922800
I1127 10:49:58.727588  5059 layer_factory.hpp:76] Creating layer pool1
I1127 10:49:58.727602  5059 net.cpp:106] Creating Layer pool1
I1127 10:49:58.727610  5059 net.cpp:454] pool1 <- conv1
I1127 10:49:58.727638  5059 net.cpp:411] pool1 -> pool1
I1127 10:49:58.727682  5059 net.cpp:150] Setting up pool1
I1127 10:49:58.727694  5059 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:49:58.727701  5059 net.cpp:165] Memory required for data: 6074800
I1127 10:49:58.727708  5059 layer_factory.hpp:76] Creating layer conv2
I1127 10:49:58.727725  5059 net.cpp:106] Creating Layer conv2
I1127 10:49:58.727732  5059 net.cpp:454] conv2 <- pool1
I1127 10:49:58.727743  5059 net.cpp:411] conv2 -> conv2
I1127 10:49:58.728317  5059 net.cpp:150] Setting up conv2
I1127 10:49:58.728361  5059 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:49:58.728374  5059 net.cpp:165] Memory required for data: 7354800
I1127 10:49:58.728396  5059 layer_factory.hpp:76] Creating layer pool2
I1127 10:49:58.728417  5059 net.cpp:106] Creating Layer pool2
I1127 10:49:58.728430  5059 net.cpp:454] pool2 <- conv2
I1127 10:49:58.728445  5059 net.cpp:411] pool2 -> pool2
I1127 10:49:58.728516  5059 net.cpp:150] Setting up pool2
I1127 10:49:58.728531  5059 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:49:58.728540  5059 net.cpp:165] Memory required for data: 7674800
I1127 10:49:58.728549  5059 layer_factory.hpp:76] Creating layer ip1
I1127 10:49:58.728574  5059 net.cpp:106] Creating Layer ip1
I1127 10:49:58.728584  5059 net.cpp:454] ip1 <- pool2
I1127 10:49:58.728600  5059 net.cpp:411] ip1 -> ip1
I1127 10:49:58.733094  5059 net.cpp:150] Setting up ip1
I1127 10:49:58.733181  5059 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:49:58.733191  5059 net.cpp:165] Memory required for data: 7874800
I1127 10:49:58.733214  5059 layer_factory.hpp:76] Creating layer relu1
I1127 10:49:58.733235  5059 net.cpp:106] Creating Layer relu1
I1127 10:49:58.733248  5059 net.cpp:454] relu1 <- ip1
I1127 10:49:58.733265  5059 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:49:58.733289  5059 net.cpp:150] Setting up relu1
I1127 10:49:58.733300  5059 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:49:58.733309  5059 net.cpp:165] Memory required for data: 8074800
I1127 10:49:58.733317  5059 layer_factory.hpp:76] Creating layer ip2
I1127 10:49:58.733341  5059 net.cpp:106] Creating Layer ip2
I1127 10:49:58.733352  5059 net.cpp:454] ip2 <- ip1
I1127 10:49:58.733371  5059 net.cpp:411] ip2 -> ip2
I1127 10:49:58.733583  5059 net.cpp:150] Setting up ip2
I1127 10:49:58.733602  5059 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:49:58.733609  5059 net.cpp:165] Memory required for data: 8078800
I1127 10:49:58.733623  5059 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:49:58.733639  5059 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:49:58.733647  5059 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:49:58.733659  5059 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:49:58.733677  5059 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:49:58.733744  5059 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:49:58.733764  5059 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:49:58.733777  5059 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:49:58.733788  5059 net.cpp:165] Memory required for data: 8086800
I1127 10:49:58.733800  5059 layer_factory.hpp:76] Creating layer accuracy
I1127 10:49:58.733825  5059 net.cpp:106] Creating Layer accuracy
I1127 10:49:58.733837  5059 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:49:58.733849  5059 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:49:58.733858  5059 net.cpp:411] accuracy -> accuracy
I1127 10:49:58.733875  5059 net.cpp:150] Setting up accuracy
I1127 10:49:58.733885  5059 net.cpp:157] Top shape: (1)
I1127 10:49:58.733891  5059 net.cpp:165] Memory required for data: 8086804
I1127 10:49:58.733899  5059 layer_factory.hpp:76] Creating layer loss
I1127 10:49:58.733913  5059 net.cpp:106] Creating Layer loss
I1127 10:49:58.733922  5059 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:49:58.733930  5059 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:49:58.733940  5059 net.cpp:411] loss -> loss
I1127 10:49:58.733958  5059 layer_factory.hpp:76] Creating layer loss
I1127 10:49:58.734119  5059 net.cpp:150] Setting up loss
I1127 10:49:58.734134  5059 net.cpp:157] Top shape: (1)
I1127 10:49:58.741086  5059 net.cpp:160]     with loss weight 1
I1127 10:49:58.741171  5059 net.cpp:165] Memory required for data: 8086808
I1127 10:49:58.741188  5059 net.cpp:226] loss needs backward computation.
I1127 10:49:58.741210  5059 net.cpp:228] accuracy does not need backward computation.
I1127 10:49:58.741222  5059 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:49:58.741233  5059 net.cpp:226] ip2 needs backward computation.
I1127 10:49:58.741245  5059 net.cpp:226] relu1 needs backward computation.
I1127 10:49:58.741251  5059 net.cpp:226] ip1 needs backward computation.
I1127 10:49:58.741258  5059 net.cpp:226] pool2 needs backward computation.
I1127 10:49:58.741267  5059 net.cpp:226] conv2 needs backward computation.
I1127 10:49:58.741277  5059 net.cpp:226] pool1 needs backward computation.
I1127 10:49:58.741287  5059 net.cpp:226] conv1 needs backward computation.
I1127 10:49:58.741297  5059 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:49:58.741305  5059 net.cpp:228] mnist does not need backward computation.
I1127 10:49:58.741313  5059 net.cpp:270] This network produces output accuracy
I1127 10:49:58.741344  5059 net.cpp:270] This network produces output loss
I1127 10:49:58.741379  5059 net.cpp:283] Network initialization done.
I1127 10:49:58.741518  5059 solver.cpp:59] Solver scaffolding done.
I1127 10:49:58.741942  5059 caffe.cpp:212] Starting Optimization
I1127 10:49:58.741953  5059 solver.cpp:287] Solving LeNet
I1127 10:49:58.741960  5059 solver.cpp:288] Learning Rate Policy: inv
I1127 10:49:58.742805  5059 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:50:01.483870  5059 solver.cpp:408]     Test net output #0: accuracy = 0.0882
I1127 10:50:01.483911  5059 solver.cpp:408]     Test net output #1: loss = 2.39286 (* 1 = 2.39286 loss)
I1127 10:50:01.515597  5059 solver.cpp:236] Iteration 0, loss = 2.33474
I1127 10:50:01.515619  5059 solver.cpp:252]     Train net output #0: loss = 2.33474 (* 1 = 2.33474 loss)
I1127 10:50:01.515636  5059 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:50:14.987750  5059 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:50:16.282321  5059 solver.cpp:408]     Test net output #0: accuracy = 0.9727
I1127 10:50:16.282439  5059 solver.cpp:408]     Test net output #1: loss = 0.0867199 (* 1 = 0.0867199 loss)
I1127 10:50:16.294371  5059 solver.cpp:236] Iteration 500, loss = 0.110049
I1127 10:50:16.294528  5059 solver.cpp:252]     Train net output #0: loss = 0.110049 (* 1 = 0.110049 loss)
I1127 10:50:16.294569  5059 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:50:29.692728  5059 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:50:29.711283  5059 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:50:29.722492  5059 solver.cpp:320] Iteration 1000, loss = 0.0863457
I1127 10:50:29.722591  5059 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:50:30.828259  5059 solver.cpp:408]     Test net output #0: accuracy = 0.9807
I1127 10:50:30.828373  5059 solver.cpp:408]     Test net output #1: loss = 0.0585428 (* 1 = 0.0585428 loss)
I1127 10:50:30.828387  5059 solver.cpp:325] Optimization Done.
I1127 10:50:30.828395  5059 caffe.cpp:215] Optimization Done.
I1127 10:50:30.946082  5088 caffe.cpp:184] Using GPUs 0
I1127 10:50:31.149605  5088 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:50:31.149791  5088 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:50:31.150120  5088 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:50:31.150136  5088 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:50:31.150243  5088 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:50:31.150331  5088 layer_factory.hpp:76] Creating layer mnist
I1127 10:50:31.150708  5088 net.cpp:106] Creating Layer mnist
I1127 10:50:31.150722  5088 net.cpp:411] mnist -> data
I1127 10:50:31.150753  5088 net.cpp:411] mnist -> label
I1127 10:50:31.151705  5093 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:50:31.159440  5088 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:50:31.160593  5088 net.cpp:150] Setting up mnist
I1127 10:50:31.160643  5088 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:50:31.160651  5088 net.cpp:157] Top shape: 64 (64)
I1127 10:50:31.160655  5088 net.cpp:165] Memory required for data: 200960
I1127 10:50:31.160670  5088 layer_factory.hpp:76] Creating layer conv1
I1127 10:50:31.160704  5088 net.cpp:106] Creating Layer conv1
I1127 10:50:31.160712  5088 net.cpp:454] conv1 <- data
I1127 10:50:31.160727  5088 net.cpp:411] conv1 -> conv1
I1127 10:50:31.161499  5088 net.cpp:150] Setting up conv1
I1127 10:50:31.161522  5088 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:50:31.161527  5088 net.cpp:165] Memory required for data: 3150080
I1127 10:50:31.161541  5088 layer_factory.hpp:76] Creating layer pool1
I1127 10:50:31.161553  5088 net.cpp:106] Creating Layer pool1
I1127 10:50:31.161558  5088 net.cpp:454] pool1 <- conv1
I1127 10:50:31.161567  5088 net.cpp:411] pool1 -> pool1
I1127 10:50:31.161633  5088 net.cpp:150] Setting up pool1
I1127 10:50:31.161640  5088 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:50:31.161644  5088 net.cpp:165] Memory required for data: 3887360
I1127 10:50:31.161649  5088 layer_factory.hpp:76] Creating layer conv2
I1127 10:50:31.161661  5088 net.cpp:106] Creating Layer conv2
I1127 10:50:31.161666  5088 net.cpp:454] conv2 <- pool1
I1127 10:50:31.161674  5088 net.cpp:411] conv2 -> conv2
I1127 10:50:31.162014  5088 net.cpp:150] Setting up conv2
I1127 10:50:31.162024  5088 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:50:31.162027  5088 net.cpp:165] Memory required for data: 4706560
I1127 10:50:31.162036  5088 layer_factory.hpp:76] Creating layer pool2
I1127 10:50:31.162045  5088 net.cpp:106] Creating Layer pool2
I1127 10:50:31.162050  5088 net.cpp:454] pool2 <- conv2
I1127 10:50:31.162055  5088 net.cpp:411] pool2 -> pool2
I1127 10:50:31.162084  5088 net.cpp:150] Setting up pool2
I1127 10:50:31.162091  5088 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:50:31.162096  5088 net.cpp:165] Memory required for data: 4911360
I1127 10:50:31.162101  5088 layer_factory.hpp:76] Creating layer ip1
I1127 10:50:31.162112  5088 net.cpp:106] Creating Layer ip1
I1127 10:50:31.162117  5088 net.cpp:454] ip1 <- pool2
I1127 10:50:31.162125  5088 net.cpp:411] ip1 -> ip1
I1127 10:50:31.164590  5088 net.cpp:150] Setting up ip1
I1127 10:50:31.164659  5088 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:50:31.164664  5088 net.cpp:165] Memory required for data: 5039360
I1127 10:50:31.164685  5088 layer_factory.hpp:76] Creating layer relu1
I1127 10:50:31.164706  5088 net.cpp:106] Creating Layer relu1
I1127 10:50:31.164715  5088 net.cpp:454] relu1 <- ip1
I1127 10:50:31.164724  5088 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:50:31.164757  5088 net.cpp:150] Setting up relu1
I1127 10:50:31.164763  5088 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:50:31.164767  5088 net.cpp:165] Memory required for data: 5167360
I1127 10:50:31.164772  5088 layer_factory.hpp:76] Creating layer ip2
I1127 10:50:31.164783  5088 net.cpp:106] Creating Layer ip2
I1127 10:50:31.164786  5088 net.cpp:454] ip2 <- ip1
I1127 10:50:31.164794  5088 net.cpp:411] ip2 -> ip2
I1127 10:50:31.165482  5088 net.cpp:150] Setting up ip2
I1127 10:50:31.165505  5088 net.cpp:157] Top shape: 64 10 (640)
I1127 10:50:31.165510  5088 net.cpp:165] Memory required for data: 5169920
I1127 10:50:31.165519  5088 layer_factory.hpp:76] Creating layer loss
I1127 10:50:31.165532  5088 net.cpp:106] Creating Layer loss
I1127 10:50:31.165539  5088 net.cpp:454] loss <- ip2
I1127 10:50:31.165544  5088 net.cpp:454] loss <- label
I1127 10:50:31.165555  5088 net.cpp:411] loss -> loss
I1127 10:50:31.165571  5088 layer_factory.hpp:76] Creating layer loss
I1127 10:50:31.165649  5088 net.cpp:150] Setting up loss
I1127 10:50:31.165657  5088 net.cpp:157] Top shape: (1)
I1127 10:50:31.165662  5088 net.cpp:160]     with loss weight 1
I1127 10:50:31.165693  5088 net.cpp:165] Memory required for data: 5169924
I1127 10:50:31.165699  5088 net.cpp:226] loss needs backward computation.
I1127 10:50:31.165704  5088 net.cpp:226] ip2 needs backward computation.
I1127 10:50:31.165707  5088 net.cpp:226] relu1 needs backward computation.
I1127 10:50:31.165712  5088 net.cpp:226] ip1 needs backward computation.
I1127 10:50:31.165716  5088 net.cpp:226] pool2 needs backward computation.
I1127 10:50:31.165720  5088 net.cpp:226] conv2 needs backward computation.
I1127 10:50:31.165725  5088 net.cpp:226] pool1 needs backward computation.
I1127 10:50:31.165730  5088 net.cpp:226] conv1 needs backward computation.
I1127 10:50:31.165735  5088 net.cpp:228] mnist does not need backward computation.
I1127 10:50:31.165740  5088 net.cpp:270] This network produces output loss
I1127 10:50:31.165750  5088 net.cpp:283] Network initialization done.
I1127 10:50:31.166056  5088 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:50:31.166090  5088 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:50:31.166234  5088 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:50:31.166311  5088 layer_factory.hpp:76] Creating layer mnist
I1127 10:50:31.166446  5088 net.cpp:106] Creating Layer mnist
I1127 10:50:31.166456  5088 net.cpp:411] mnist -> data
I1127 10:50:31.166467  5088 net.cpp:411] mnist -> label
I1127 10:50:31.167363  5095 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:50:31.167484  5088 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:50:31.170847  5088 net.cpp:150] Setting up mnist
I1127 10:50:31.170884  5088 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:50:31.170891  5088 net.cpp:157] Top shape: 100 (100)
I1127 10:50:31.170894  5088 net.cpp:165] Memory required for data: 314000
I1127 10:50:31.170904  5088 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:50:31.170924  5088 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:50:31.170933  5088 net.cpp:454] label_mnist_1_split <- label
I1127 10:50:31.170943  5088 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:50:31.170958  5088 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:50:31.171017  5088 net.cpp:150] Setting up label_mnist_1_split
I1127 10:50:31.171025  5088 net.cpp:157] Top shape: 100 (100)
I1127 10:50:31.171030  5088 net.cpp:157] Top shape: 100 (100)
I1127 10:50:31.171036  5088 net.cpp:165] Memory required for data: 314800
I1127 10:50:31.171039  5088 layer_factory.hpp:76] Creating layer conv1
I1127 10:50:31.171052  5088 net.cpp:106] Creating Layer conv1
I1127 10:50:31.171057  5088 net.cpp:454] conv1 <- data
I1127 10:50:31.171064  5088 net.cpp:411] conv1 -> conv1
I1127 10:50:31.171252  5088 net.cpp:150] Setting up conv1
I1127 10:50:31.171260  5088 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:50:31.171264  5088 net.cpp:165] Memory required for data: 4922800
I1127 10:50:31.171277  5088 layer_factory.hpp:76] Creating layer pool1
I1127 10:50:31.171288  5088 net.cpp:106] Creating Layer pool1
I1127 10:50:31.171291  5088 net.cpp:454] pool1 <- conv1
I1127 10:50:31.171321  5088 net.cpp:411] pool1 -> pool1
I1127 10:50:31.171356  5088 net.cpp:150] Setting up pool1
I1127 10:50:31.171365  5088 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:50:31.171368  5088 net.cpp:165] Memory required for data: 6074800
I1127 10:50:31.171372  5088 layer_factory.hpp:76] Creating layer conv2
I1127 10:50:31.171381  5088 net.cpp:106] Creating Layer conv2
I1127 10:50:31.171386  5088 net.cpp:454] conv2 <- pool1
I1127 10:50:31.171393  5088 net.cpp:411] conv2 -> conv2
I1127 10:50:31.171654  5088 net.cpp:150] Setting up conv2
I1127 10:50:31.171668  5088 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:50:31.171674  5088 net.cpp:165] Memory required for data: 7354800
I1127 10:50:31.171682  5088 layer_factory.hpp:76] Creating layer pool2
I1127 10:50:31.171689  5088 net.cpp:106] Creating Layer pool2
I1127 10:50:31.171694  5088 net.cpp:454] pool2 <- conv2
I1127 10:50:31.171699  5088 net.cpp:411] pool2 -> pool2
I1127 10:50:31.171743  5088 net.cpp:150] Setting up pool2
I1127 10:50:31.171753  5088 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:50:31.171757  5088 net.cpp:165] Memory required for data: 7674800
I1127 10:50:31.171762  5088 layer_factory.hpp:76] Creating layer ip1
I1127 10:50:31.171773  5088 net.cpp:106] Creating Layer ip1
I1127 10:50:31.171777  5088 net.cpp:454] ip1 <- pool2
I1127 10:50:31.171784  5088 net.cpp:411] ip1 -> ip1
I1127 10:50:31.174335  5088 net.cpp:150] Setting up ip1
I1127 10:50:31.174399  5088 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:50:31.174406  5088 net.cpp:165] Memory required for data: 7874800
I1127 10:50:31.174427  5088 layer_factory.hpp:76] Creating layer relu1
I1127 10:50:31.174437  5088 net.cpp:106] Creating Layer relu1
I1127 10:50:31.174443  5088 net.cpp:454] relu1 <- ip1
I1127 10:50:31.174458  5088 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:50:31.174468  5088 net.cpp:150] Setting up relu1
I1127 10:50:31.174474  5088 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:50:31.174477  5088 net.cpp:165] Memory required for data: 8074800
I1127 10:50:31.174482  5088 layer_factory.hpp:76] Creating layer ip2
I1127 10:50:31.174495  5088 net.cpp:106] Creating Layer ip2
I1127 10:50:31.174499  5088 net.cpp:454] ip2 <- ip1
I1127 10:50:31.174517  5088 net.cpp:411] ip2 -> ip2
I1127 10:50:31.174640  5088 net.cpp:150] Setting up ip2
I1127 10:50:31.174649  5088 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:50:31.174654  5088 net.cpp:165] Memory required for data: 8078800
I1127 10:50:31.174660  5088 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:50:31.174669  5088 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:50:31.174674  5088 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:50:31.174679  5088 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:50:31.174685  5088 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:50:31.174716  5088 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:50:31.174723  5088 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:50:31.174728  5088 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:50:31.174732  5088 net.cpp:165] Memory required for data: 8086800
I1127 10:50:31.174736  5088 layer_factory.hpp:76] Creating layer accuracy
I1127 10:50:31.174746  5088 net.cpp:106] Creating Layer accuracy
I1127 10:50:31.174751  5088 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:50:31.174757  5088 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:50:31.174762  5088 net.cpp:411] accuracy -> accuracy
I1127 10:50:31.174773  5088 net.cpp:150] Setting up accuracy
I1127 10:50:31.174778  5088 net.cpp:157] Top shape: (1)
I1127 10:50:31.174782  5088 net.cpp:165] Memory required for data: 8086804
I1127 10:50:31.174787  5088 layer_factory.hpp:76] Creating layer loss
I1127 10:50:31.174793  5088 net.cpp:106] Creating Layer loss
I1127 10:50:31.174798  5088 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:50:31.174803  5088 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:50:31.174809  5088 net.cpp:411] loss -> loss
I1127 10:50:31.174818  5088 layer_factory.hpp:76] Creating layer loss
I1127 10:50:31.174906  5088 net.cpp:150] Setting up loss
I1127 10:50:31.174913  5088 net.cpp:157] Top shape: (1)
I1127 10:50:31.174917  5088 net.cpp:160]     with loss weight 1
I1127 10:50:31.174938  5088 net.cpp:165] Memory required for data: 8086808
I1127 10:50:31.174942  5088 net.cpp:226] loss needs backward computation.
I1127 10:50:31.174949  5088 net.cpp:228] accuracy does not need backward computation.
I1127 10:50:31.174954  5088 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:50:31.174958  5088 net.cpp:226] ip2 needs backward computation.
I1127 10:50:31.174962  5088 net.cpp:226] relu1 needs backward computation.
I1127 10:50:31.174967  5088 net.cpp:226] ip1 needs backward computation.
I1127 10:50:31.174971  5088 net.cpp:226] pool2 needs backward computation.
I1127 10:50:31.174975  5088 net.cpp:226] conv2 needs backward computation.
I1127 10:50:31.174979  5088 net.cpp:226] pool1 needs backward computation.
I1127 10:50:31.174983  5088 net.cpp:226] conv1 needs backward computation.
I1127 10:50:31.174988  5088 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:50:31.174993  5088 net.cpp:228] mnist does not need backward computation.
I1127 10:50:31.174998  5088 net.cpp:270] This network produces output accuracy
I1127 10:50:31.175001  5088 net.cpp:270] This network produces output loss
I1127 10:50:31.175014  5088 net.cpp:283] Network initialization done.
I1127 10:50:31.175081  5088 solver.cpp:59] Solver scaffolding done.
I1127 10:50:31.175272  5088 caffe.cpp:212] Starting Optimization
I1127 10:50:31.175278  5088 solver.cpp:287] Solving LeNet
I1127 10:50:31.175282  5088 solver.cpp:288] Learning Rate Policy: inv
I1127 10:50:31.175822  5088 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:50:34.275781  5088 solver.cpp:408]     Test net output #0: accuracy = 0.0927
I1127 10:50:34.275846  5088 solver.cpp:408]     Test net output #1: loss = 2.35077 (* 1 = 2.35077 loss)
I1127 10:50:34.289031  5088 solver.cpp:236] Iteration 0, loss = 2.31494
I1127 10:50:34.289162  5088 solver.cpp:252]     Train net output #0: loss = 2.31494 (* 1 = 2.31494 loss)
I1127 10:50:34.289218  5088 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:50:46.775984  5088 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:50:48.730806  5088 solver.cpp:408]     Test net output #0: accuracy = 0.9739
I1127 10:50:48.730857  5088 solver.cpp:408]     Test net output #1: loss = 0.0817743 (* 1 = 0.0817743 loss)
I1127 10:50:48.740360  5088 solver.cpp:236] Iteration 500, loss = 0.0905553
I1127 10:50:48.740473  5088 solver.cpp:252]     Train net output #0: loss = 0.0905553 (* 1 = 0.0905553 loss)
I1127 10:50:48.740489  5088 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:51:02.140671  5088 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:51:02.155047  5088 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:51:02.165956  5088 solver.cpp:320] Iteration 1000, loss = 0.131418
I1127 10:51:02.166074  5088 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:51:02.460631  5088 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:51:03.470060  5088 solver.cpp:408]     Test net output #0: accuracy = 0.9807
I1127 10:51:03.470101  5088 solver.cpp:408]     Test net output #1: loss = 0.06068 (* 1 = 0.06068 loss)
I1127 10:51:03.470108  5088 solver.cpp:325] Optimization Done.
I1127 10:51:03.470113  5088 caffe.cpp:215] Optimization Done.
I1127 10:51:03.535217  5118 caffe.cpp:184] Using GPUs 0
I1127 10:51:03.892346  5118 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:51:03.892459  5118 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:51:03.892707  5118 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:51:03.892724  5118 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:51:03.892807  5118 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:51:03.892868  5118 layer_factory.hpp:76] Creating layer mnist
I1127 10:51:03.893177  5118 net.cpp:106] Creating Layer mnist
I1127 10:51:03.893187  5118 net.cpp:411] mnist -> data
I1127 10:51:03.893208  5118 net.cpp:411] mnist -> label
I1127 10:51:03.893950  5121 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:51:03.929250  5118 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:51:03.935736  5118 net.cpp:150] Setting up mnist
I1127 10:51:03.935763  5118 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:51:03.935771  5118 net.cpp:157] Top shape: 64 (64)
I1127 10:51:03.935776  5118 net.cpp:165] Memory required for data: 200960
I1127 10:51:03.935786  5118 layer_factory.hpp:76] Creating layer conv1
I1127 10:51:03.935801  5118 net.cpp:106] Creating Layer conv1
I1127 10:51:03.935807  5118 net.cpp:454] conv1 <- data
I1127 10:51:03.935820  5118 net.cpp:411] conv1 -> conv1
I1127 10:51:03.936471  5118 net.cpp:150] Setting up conv1
I1127 10:51:03.936486  5118 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:51:03.936491  5118 net.cpp:165] Memory required for data: 3150080
I1127 10:51:03.936503  5118 layer_factory.hpp:76] Creating layer pool1
I1127 10:51:03.936513  5118 net.cpp:106] Creating Layer pool1
I1127 10:51:03.936517  5118 net.cpp:454] pool1 <- conv1
I1127 10:51:03.936523  5118 net.cpp:411] pool1 -> pool1
I1127 10:51:03.936571  5118 net.cpp:150] Setting up pool1
I1127 10:51:03.936579  5118 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:51:03.936583  5118 net.cpp:165] Memory required for data: 3887360
I1127 10:51:03.936589  5118 layer_factory.hpp:76] Creating layer conv2
I1127 10:51:03.936597  5118 net.cpp:106] Creating Layer conv2
I1127 10:51:03.936601  5118 net.cpp:454] conv2 <- pool1
I1127 10:51:03.936609  5118 net.cpp:411] conv2 -> conv2
I1127 10:51:03.937011  5118 net.cpp:150] Setting up conv2
I1127 10:51:03.937026  5118 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:51:03.937031  5118 net.cpp:165] Memory required for data: 4706560
I1127 10:51:03.937041  5118 layer_factory.hpp:76] Creating layer pool2
I1127 10:51:03.937048  5118 net.cpp:106] Creating Layer pool2
I1127 10:51:03.937053  5118 net.cpp:454] pool2 <- conv2
I1127 10:51:03.937059  5118 net.cpp:411] pool2 -> pool2
I1127 10:51:03.937088  5118 net.cpp:150] Setting up pool2
I1127 10:51:03.937099  5118 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:51:03.937104  5118 net.cpp:165] Memory required for data: 4911360
I1127 10:51:03.937108  5118 layer_factory.hpp:76] Creating layer ip1
I1127 10:51:03.937116  5118 net.cpp:106] Creating Layer ip1
I1127 10:51:03.937120  5118 net.cpp:454] ip1 <- pool2
I1127 10:51:03.937129  5118 net.cpp:411] ip1 -> ip1
I1127 10:51:03.939229  5118 net.cpp:150] Setting up ip1
I1127 10:51:03.939240  5118 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:51:03.939245  5118 net.cpp:165] Memory required for data: 5039360
I1127 10:51:03.939254  5118 layer_factory.hpp:76] Creating layer relu1
I1127 10:51:03.939261  5118 net.cpp:106] Creating Layer relu1
I1127 10:51:03.939266  5118 net.cpp:454] relu1 <- ip1
I1127 10:51:03.939275  5118 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:51:03.939282  5118 net.cpp:150] Setting up relu1
I1127 10:51:03.939288  5118 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:51:03.939292  5118 net.cpp:165] Memory required for data: 5167360
I1127 10:51:03.939296  5118 layer_factory.hpp:76] Creating layer ip2
I1127 10:51:03.939303  5118 net.cpp:106] Creating Layer ip2
I1127 10:51:03.939307  5118 net.cpp:454] ip2 <- ip1
I1127 10:51:03.939314  5118 net.cpp:411] ip2 -> ip2
I1127 10:51:03.939703  5118 net.cpp:150] Setting up ip2
I1127 10:51:03.939718  5118 net.cpp:157] Top shape: 64 10 (640)
I1127 10:51:03.939723  5118 net.cpp:165] Memory required for data: 5169920
I1127 10:51:03.939729  5118 layer_factory.hpp:76] Creating layer loss
I1127 10:51:03.939738  5118 net.cpp:106] Creating Layer loss
I1127 10:51:03.939741  5118 net.cpp:454] loss <- ip2
I1127 10:51:03.939746  5118 net.cpp:454] loss <- label
I1127 10:51:03.939754  5118 net.cpp:411] loss -> loss
I1127 10:51:03.939764  5118 layer_factory.hpp:76] Creating layer loss
I1127 10:51:03.939831  5118 net.cpp:150] Setting up loss
I1127 10:51:03.939838  5118 net.cpp:157] Top shape: (1)
I1127 10:51:03.939843  5118 net.cpp:160]     with loss weight 1
I1127 10:51:03.939857  5118 net.cpp:165] Memory required for data: 5169924
I1127 10:51:03.939862  5118 net.cpp:226] loss needs backward computation.
I1127 10:51:03.939867  5118 net.cpp:226] ip2 needs backward computation.
I1127 10:51:03.939872  5118 net.cpp:226] relu1 needs backward computation.
I1127 10:51:03.939875  5118 net.cpp:226] ip1 needs backward computation.
I1127 10:51:03.939879  5118 net.cpp:226] pool2 needs backward computation.
I1127 10:51:03.939883  5118 net.cpp:226] conv2 needs backward computation.
I1127 10:51:03.939888  5118 net.cpp:226] pool1 needs backward computation.
I1127 10:51:03.939893  5118 net.cpp:226] conv1 needs backward computation.
I1127 10:51:03.939898  5118 net.cpp:228] mnist does not need backward computation.
I1127 10:51:03.939900  5118 net.cpp:270] This network produces output loss
I1127 10:51:03.939910  5118 net.cpp:283] Network initialization done.
I1127 10:51:03.940141  5118 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:51:03.940163  5118 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:51:03.940271  5118 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:51:03.940330  5118 layer_factory.hpp:76] Creating layer mnist
I1127 10:51:03.940413  5118 net.cpp:106] Creating Layer mnist
I1127 10:51:03.940424  5118 net.cpp:411] mnist -> data
I1127 10:51:03.940433  5118 net.cpp:411] mnist -> label
I1127 10:51:03.941121  5123 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:51:03.941233  5118 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:51:03.944686  5118 net.cpp:150] Setting up mnist
I1127 10:51:03.944700  5118 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:51:03.944705  5118 net.cpp:157] Top shape: 100 (100)
I1127 10:51:03.944710  5118 net.cpp:165] Memory required for data: 314000
I1127 10:51:03.944715  5118 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:51:03.944722  5118 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:51:03.944727  5118 net.cpp:454] label_mnist_1_split <- label
I1127 10:51:03.944733  5118 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:51:03.944741  5118 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:51:03.944773  5118 net.cpp:150] Setting up label_mnist_1_split
I1127 10:51:03.944780  5118 net.cpp:157] Top shape: 100 (100)
I1127 10:51:03.944785  5118 net.cpp:157] Top shape: 100 (100)
I1127 10:51:03.944789  5118 net.cpp:165] Memory required for data: 314800
I1127 10:51:03.944794  5118 layer_factory.hpp:76] Creating layer conv1
I1127 10:51:03.944802  5118 net.cpp:106] Creating Layer conv1
I1127 10:51:03.944807  5118 net.cpp:454] conv1 <- data
I1127 10:51:03.944814  5118 net.cpp:411] conv1 -> conv1
I1127 10:51:03.944963  5118 net.cpp:150] Setting up conv1
I1127 10:51:03.944972  5118 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:51:03.944977  5118 net.cpp:165] Memory required for data: 4922800
I1127 10:51:03.944985  5118 layer_factory.hpp:76] Creating layer pool1
I1127 10:51:03.944993  5118 net.cpp:106] Creating Layer pool1
I1127 10:51:03.944998  5118 net.cpp:454] pool1 <- conv1
I1127 10:51:03.945009  5118 net.cpp:411] pool1 -> pool1
I1127 10:51:03.945039  5118 net.cpp:150] Setting up pool1
I1127 10:51:03.945045  5118 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:51:03.945050  5118 net.cpp:165] Memory required for data: 6074800
I1127 10:51:03.945053  5118 layer_factory.hpp:76] Creating layer conv2
I1127 10:51:03.945062  5118 net.cpp:106] Creating Layer conv2
I1127 10:51:03.945067  5118 net.cpp:454] conv2 <- pool1
I1127 10:51:03.945075  5118 net.cpp:411] conv2 -> conv2
I1127 10:51:03.945322  5118 net.cpp:150] Setting up conv2
I1127 10:51:03.945330  5118 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:51:03.945334  5118 net.cpp:165] Memory required for data: 7354800
I1127 10:51:03.945343  5118 layer_factory.hpp:76] Creating layer pool2
I1127 10:51:03.945351  5118 net.cpp:106] Creating Layer pool2
I1127 10:51:03.945356  5118 net.cpp:454] pool2 <- conv2
I1127 10:51:03.945361  5118 net.cpp:411] pool2 -> pool2
I1127 10:51:03.945392  5118 net.cpp:150] Setting up pool2
I1127 10:51:03.945400  5118 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:51:03.945405  5118 net.cpp:165] Memory required for data: 7674800
I1127 10:51:03.945408  5118 layer_factory.hpp:76] Creating layer ip1
I1127 10:51:03.945416  5118 net.cpp:106] Creating Layer ip1
I1127 10:51:03.945421  5118 net.cpp:454] ip1 <- pool2
I1127 10:51:03.945427  5118 net.cpp:411] ip1 -> ip1
I1127 10:51:03.948010  5118 net.cpp:150] Setting up ip1
I1127 10:51:03.948027  5118 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:51:03.948032  5118 net.cpp:165] Memory required for data: 7874800
I1127 10:51:03.948042  5118 layer_factory.hpp:76] Creating layer relu1
I1127 10:51:03.948051  5118 net.cpp:106] Creating Layer relu1
I1127 10:51:03.948056  5118 net.cpp:454] relu1 <- ip1
I1127 10:51:03.948062  5118 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:51:03.948070  5118 net.cpp:150] Setting up relu1
I1127 10:51:03.948076  5118 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:51:03.948079  5118 net.cpp:165] Memory required for data: 8074800
I1127 10:51:03.948083  5118 layer_factory.hpp:76] Creating layer ip2
I1127 10:51:03.948092  5118 net.cpp:106] Creating Layer ip2
I1127 10:51:03.948097  5118 net.cpp:454] ip2 <- ip1
I1127 10:51:03.948107  5118 net.cpp:411] ip2 -> ip2
I1127 10:51:03.948200  5118 net.cpp:150] Setting up ip2
I1127 10:51:03.948209  5118 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:51:03.948212  5118 net.cpp:165] Memory required for data: 8078800
I1127 10:51:03.948220  5118 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:51:03.948226  5118 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:51:03.948231  5118 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:51:03.948237  5118 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:51:03.948243  5118 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:51:03.948271  5118 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:51:03.948278  5118 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:51:03.948283  5118 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:51:03.948287  5118 net.cpp:165] Memory required for data: 8086800
I1127 10:51:03.948292  5118 layer_factory.hpp:76] Creating layer accuracy
I1127 10:51:03.948298  5118 net.cpp:106] Creating Layer accuracy
I1127 10:51:03.948302  5118 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:51:03.948307  5118 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:51:03.948315  5118 net.cpp:411] accuracy -> accuracy
I1127 10:51:03.948323  5118 net.cpp:150] Setting up accuracy
I1127 10:51:03.948329  5118 net.cpp:157] Top shape: (1)
I1127 10:51:03.948333  5118 net.cpp:165] Memory required for data: 8086804
I1127 10:51:03.948338  5118 layer_factory.hpp:76] Creating layer loss
I1127 10:51:03.948343  5118 net.cpp:106] Creating Layer loss
I1127 10:51:03.948348  5118 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:51:03.948353  5118 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:51:03.948359  5118 net.cpp:411] loss -> loss
I1127 10:51:03.948367  5118 layer_factory.hpp:76] Creating layer loss
I1127 10:51:03.948436  5118 net.cpp:150] Setting up loss
I1127 10:51:03.948442  5118 net.cpp:157] Top shape: (1)
I1127 10:51:03.948446  5118 net.cpp:160]     with loss weight 1
I1127 10:51:03.948456  5118 net.cpp:165] Memory required for data: 8086808
I1127 10:51:03.948459  5118 net.cpp:226] loss needs backward computation.
I1127 10:51:03.948467  5118 net.cpp:228] accuracy does not need backward computation.
I1127 10:51:03.948470  5118 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:51:03.948475  5118 net.cpp:226] ip2 needs backward computation.
I1127 10:51:03.948479  5118 net.cpp:226] relu1 needs backward computation.
I1127 10:51:03.948483  5118 net.cpp:226] ip1 needs backward computation.
I1127 10:51:03.948487  5118 net.cpp:226] pool2 needs backward computation.
I1127 10:51:03.948492  5118 net.cpp:226] conv2 needs backward computation.
I1127 10:51:03.948496  5118 net.cpp:226] pool1 needs backward computation.
I1127 10:51:03.948500  5118 net.cpp:226] conv1 needs backward computation.
I1127 10:51:03.948504  5118 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:51:03.948511  5118 net.cpp:228] mnist does not need backward computation.
I1127 10:51:03.948515  5118 net.cpp:270] This network produces output accuracy
I1127 10:51:03.948520  5118 net.cpp:270] This network produces output loss
I1127 10:51:03.948530  5118 net.cpp:283] Network initialization done.
I1127 10:51:03.948564  5118 solver.cpp:59] Solver scaffolding done.
I1127 10:51:03.948750  5118 caffe.cpp:212] Starting Optimization
I1127 10:51:03.948756  5118 solver.cpp:287] Solving LeNet
I1127 10:51:03.948760  5118 solver.cpp:288] Learning Rate Policy: inv
I1127 10:51:03.949064  5118 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:51:06.406379  5118 solver.cpp:408]     Test net output #0: accuracy = 0.0921
I1127 10:51:06.406457  5118 solver.cpp:408]     Test net output #1: loss = 2.41737 (* 1 = 2.41737 loss)
I1127 10:51:06.423161  5118 solver.cpp:236] Iteration 0, loss = 2.39279
I1127 10:51:06.423310  5118 solver.cpp:252]     Train net output #0: loss = 2.39279 (* 1 = 2.39279 loss)
I1127 10:51:06.423369  5118 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:51:17.939134  5118 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:51:19.614562  5118 solver.cpp:408]     Test net output #0: accuracy = 0.9729
I1127 10:51:19.614608  5118 solver.cpp:408]     Test net output #1: loss = 0.0859793 (* 1 = 0.0859793 loss)
I1127 10:51:19.644433  5118 solver.cpp:236] Iteration 500, loss = 0.0978282
I1127 10:51:19.644470  5118 solver.cpp:252]     Train net output #0: loss = 0.0978283 (* 1 = 0.0978283 loss)
I1127 10:51:19.644480  5118 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:51:32.904927  5118 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:51:32.925278  5118 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:51:32.953048  5118 solver.cpp:320] Iteration 1000, loss = 0.118629
I1127 10:51:32.953068  5118 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:51:35.597983  5118 solver.cpp:408]     Test net output #0: accuracy = 0.9832
I1127 10:51:35.598378  5118 solver.cpp:408]     Test net output #1: loss = 0.0526183 (* 1 = 0.0526183 loss)
I1127 10:51:35.598398  5118 solver.cpp:325] Optimization Done.
I1127 10:51:35.598407  5118 caffe.cpp:215] Optimization Done.
I1127 10:51:35.745359  5146 caffe.cpp:184] Using GPUs 0
I1127 10:51:36.013160  5146 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:51:36.013316  5146 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:51:36.013622  5146 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:51:36.013639  5146 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:51:36.013737  5146 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:51:36.013811  5146 layer_factory.hpp:76] Creating layer mnist
I1127 10:51:36.014297  5146 net.cpp:106] Creating Layer mnist
I1127 10:51:36.014359  5146 net.cpp:411] mnist -> data
I1127 10:51:36.014415  5146 net.cpp:411] mnist -> label
I1127 10:51:36.015714  5149 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:51:36.029150  5146 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:51:36.031515  5146 net.cpp:150] Setting up mnist
I1127 10:51:36.031638  5146 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:51:36.031656  5146 net.cpp:157] Top shape: 64 (64)
I1127 10:51:36.031663  5146 net.cpp:165] Memory required for data: 200960
I1127 10:51:36.031689  5146 layer_factory.hpp:76] Creating layer conv1
I1127 10:51:36.031733  5146 net.cpp:106] Creating Layer conv1
I1127 10:51:36.031749  5146 net.cpp:454] conv1 <- data
I1127 10:51:36.031781  5146 net.cpp:411] conv1 -> conv1
I1127 10:51:36.033280  5146 net.cpp:150] Setting up conv1
I1127 10:51:36.033360  5146 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:51:36.033370  5146 net.cpp:165] Memory required for data: 3150080
I1127 10:51:36.033402  5146 layer_factory.hpp:76] Creating layer pool1
I1127 10:51:36.033439  5146 net.cpp:106] Creating Layer pool1
I1127 10:51:36.033453  5146 net.cpp:454] pool1 <- conv1
I1127 10:51:36.033473  5146 net.cpp:411] pool1 -> pool1
I1127 10:51:36.033699  5146 net.cpp:150] Setting up pool1
I1127 10:51:36.033726  5146 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:51:36.033737  5146 net.cpp:165] Memory required for data: 3887360
I1127 10:51:36.033751  5146 layer_factory.hpp:76] Creating layer conv2
I1127 10:51:36.033784  5146 net.cpp:106] Creating Layer conv2
I1127 10:51:36.033800  5146 net.cpp:454] conv2 <- pool1
I1127 10:51:36.033820  5146 net.cpp:411] conv2 -> conv2
I1127 10:51:36.034556  5146 net.cpp:150] Setting up conv2
I1127 10:51:36.034631  5146 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:51:36.034641  5146 net.cpp:165] Memory required for data: 4706560
I1127 10:51:36.034667  5146 layer_factory.hpp:76] Creating layer pool2
I1127 10:51:36.034695  5146 net.cpp:106] Creating Layer pool2
I1127 10:51:36.034708  5146 net.cpp:454] pool2 <- conv2
I1127 10:51:36.034723  5146 net.cpp:411] pool2 -> pool2
I1127 10:51:36.034775  5146 net.cpp:150] Setting up pool2
I1127 10:51:36.034787  5146 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:51:36.034795  5146 net.cpp:165] Memory required for data: 4911360
I1127 10:51:36.034802  5146 layer_factory.hpp:76] Creating layer ip1
I1127 10:51:36.034821  5146 net.cpp:106] Creating Layer ip1
I1127 10:51:36.034829  5146 net.cpp:454] ip1 <- pool2
I1127 10:51:36.034842  5146 net.cpp:411] ip1 -> ip1
I1127 10:51:36.039420  5146 net.cpp:150] Setting up ip1
I1127 10:51:36.039496  5146 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:51:36.039506  5146 net.cpp:165] Memory required for data: 5039360
I1127 10:51:36.039532  5146 layer_factory.hpp:76] Creating layer relu1
I1127 10:51:36.039553  5146 net.cpp:106] Creating Layer relu1
I1127 10:51:36.039564  5146 net.cpp:454] relu1 <- ip1
I1127 10:51:36.039582  5146 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:51:36.039608  5146 net.cpp:150] Setting up relu1
I1127 10:51:36.039619  5146 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:51:36.039626  5146 net.cpp:165] Memory required for data: 5167360
I1127 10:51:36.039634  5146 layer_factory.hpp:76] Creating layer ip2
I1127 10:51:36.039652  5146 net.cpp:106] Creating Layer ip2
I1127 10:51:36.039660  5146 net.cpp:454] ip2 <- ip1
I1127 10:51:36.039674  5146 net.cpp:411] ip2 -> ip2
I1127 10:51:36.040468  5146 net.cpp:150] Setting up ip2
I1127 10:51:36.040503  5146 net.cpp:157] Top shape: 64 10 (640)
I1127 10:51:36.040509  5146 net.cpp:165] Memory required for data: 5169920
I1127 10:51:36.040523  5146 layer_factory.hpp:76] Creating layer loss
I1127 10:51:36.040539  5146 net.cpp:106] Creating Layer loss
I1127 10:51:36.040546  5146 net.cpp:454] loss <- ip2
I1127 10:51:36.040555  5146 net.cpp:454] loss <- label
I1127 10:51:36.040568  5146 net.cpp:411] loss -> loss
I1127 10:51:36.040590  5146 layer_factory.hpp:76] Creating layer loss
I1127 10:51:36.040717  5146 net.cpp:150] Setting up loss
I1127 10:51:36.040743  5146 net.cpp:157] Top shape: (1)
I1127 10:51:36.040748  5146 net.cpp:160]     with loss weight 1
I1127 10:51:36.040782  5146 net.cpp:165] Memory required for data: 5169924
I1127 10:51:36.040789  5146 net.cpp:226] loss needs backward computation.
I1127 10:51:36.040796  5146 net.cpp:226] ip2 needs backward computation.
I1127 10:51:36.040799  5146 net.cpp:226] relu1 needs backward computation.
I1127 10:51:36.040805  5146 net.cpp:226] ip1 needs backward computation.
I1127 10:51:36.040810  5146 net.cpp:226] pool2 needs backward computation.
I1127 10:51:36.040815  5146 net.cpp:226] conv2 needs backward computation.
I1127 10:51:36.040822  5146 net.cpp:226] pool1 needs backward computation.
I1127 10:51:36.040827  5146 net.cpp:226] conv1 needs backward computation.
I1127 10:51:36.040833  5146 net.cpp:228] mnist does not need backward computation.
I1127 10:51:36.040839  5146 net.cpp:270] This network produces output loss
I1127 10:51:36.040854  5146 net.cpp:283] Network initialization done.
I1127 10:51:36.041265  5146 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:51:36.041326  5146 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:51:36.041530  5146 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:51:36.041630  5146 layer_factory.hpp:76] Creating layer mnist
I1127 10:51:36.041771  5146 net.cpp:106] Creating Layer mnist
I1127 10:51:36.041785  5146 net.cpp:411] mnist -> data
I1127 10:51:36.041800  5146 net.cpp:411] mnist -> label
I1127 10:51:36.043499  5151 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:51:36.043751  5146 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:51:36.045258  5146 net.cpp:150] Setting up mnist
I1127 10:51:36.045338  5146 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:51:36.045352  5146 net.cpp:157] Top shape: 100 (100)
I1127 10:51:36.045361  5146 net.cpp:165] Memory required for data: 314000
I1127 10:51:36.045374  5146 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:51:36.045410  5146 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:51:36.045425  5146 net.cpp:454] label_mnist_1_split <- label
I1127 10:51:36.045444  5146 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:51:36.045465  5146 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:51:36.045536  5146 net.cpp:150] Setting up label_mnist_1_split
I1127 10:51:36.045553  5146 net.cpp:157] Top shape: 100 (100)
I1127 10:51:36.045563  5146 net.cpp:157] Top shape: 100 (100)
I1127 10:51:36.045569  5146 net.cpp:165] Memory required for data: 314800
I1127 10:51:36.045578  5146 layer_factory.hpp:76] Creating layer conv1
I1127 10:51:36.045603  5146 net.cpp:106] Creating Layer conv1
I1127 10:51:36.045613  5146 net.cpp:454] conv1 <- data
I1127 10:51:36.045624  5146 net.cpp:411] conv1 -> conv1
I1127 10:51:36.045863  5146 net.cpp:150] Setting up conv1
I1127 10:51:36.045892  5146 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:51:36.045898  5146 net.cpp:165] Memory required for data: 4922800
I1127 10:51:36.045913  5146 layer_factory.hpp:76] Creating layer pool1
I1127 10:51:36.045928  5146 net.cpp:106] Creating Layer pool1
I1127 10:51:36.045933  5146 net.cpp:454] pool1 <- conv1
I1127 10:51:36.045969  5146 net.cpp:411] pool1 -> pool1
I1127 10:51:36.046017  5146 net.cpp:150] Setting up pool1
I1127 10:51:36.046026  5146 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:51:36.046030  5146 net.cpp:165] Memory required for data: 6074800
I1127 10:51:36.046036  5146 layer_factory.hpp:76] Creating layer conv2
I1127 10:51:36.046049  5146 net.cpp:106] Creating Layer conv2
I1127 10:51:36.046053  5146 net.cpp:454] conv2 <- pool1
I1127 10:51:36.046062  5146 net.cpp:411] conv2 -> conv2
I1127 10:51:36.046413  5146 net.cpp:150] Setting up conv2
I1127 10:51:36.046447  5146 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:51:36.046452  5146 net.cpp:165] Memory required for data: 7354800
I1127 10:51:36.046465  5146 layer_factory.hpp:76] Creating layer pool2
I1127 10:51:36.046478  5146 net.cpp:106] Creating Layer pool2
I1127 10:51:36.046483  5146 net.cpp:454] pool2 <- conv2
I1127 10:51:36.046489  5146 net.cpp:411] pool2 -> pool2
I1127 10:51:36.046533  5146 net.cpp:150] Setting up pool2
I1127 10:51:36.046541  5146 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:51:36.046545  5146 net.cpp:165] Memory required for data: 7674800
I1127 10:51:36.046550  5146 layer_factory.hpp:76] Creating layer ip1
I1127 10:51:36.046567  5146 net.cpp:106] Creating Layer ip1
I1127 10:51:36.046572  5146 net.cpp:454] ip1 <- pool2
I1127 10:51:36.046578  5146 net.cpp:411] ip1 -> ip1
I1127 10:51:36.050398  5146 net.cpp:150] Setting up ip1
I1127 10:51:36.050469  5146 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:51:36.050482  5146 net.cpp:165] Memory required for data: 7874800
I1127 10:51:36.050504  5146 layer_factory.hpp:76] Creating layer relu1
I1127 10:51:36.050523  5146 net.cpp:106] Creating Layer relu1
I1127 10:51:36.050531  5146 net.cpp:454] relu1 <- ip1
I1127 10:51:36.050542  5146 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:51:36.050559  5146 net.cpp:150] Setting up relu1
I1127 10:51:36.050568  5146 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:51:36.050573  5146 net.cpp:165] Memory required for data: 8074800
I1127 10:51:36.050580  5146 layer_factory.hpp:76] Creating layer ip2
I1127 10:51:36.050596  5146 net.cpp:106] Creating Layer ip2
I1127 10:51:36.050603  5146 net.cpp:454] ip2 <- ip1
I1127 10:51:36.050611  5146 net.cpp:411] ip2 -> ip2
I1127 10:51:36.050812  5146 net.cpp:150] Setting up ip2
I1127 10:51:36.050830  5146 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:51:36.050837  5146 net.cpp:165] Memory required for data: 8078800
I1127 10:51:36.050848  5146 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:51:36.050861  5146 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:51:36.050868  5146 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:51:36.050876  5146 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:51:36.050887  5146 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:51:36.050984  5146 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:51:36.050997  5146 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:51:36.051005  5146 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:51:36.051009  5146 net.cpp:165] Memory required for data: 8086800
I1127 10:51:36.051017  5146 layer_factory.hpp:76] Creating layer accuracy
I1127 10:51:36.051029  5146 net.cpp:106] Creating Layer accuracy
I1127 10:51:36.051035  5146 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:51:36.051041  5146 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:51:36.051050  5146 net.cpp:411] accuracy -> accuracy
I1127 10:51:36.051064  5146 net.cpp:150] Setting up accuracy
I1127 10:51:36.051072  5146 net.cpp:157] Top shape: (1)
I1127 10:51:36.051077  5146 net.cpp:165] Memory required for data: 8086804
I1127 10:51:36.051082  5146 layer_factory.hpp:76] Creating layer loss
I1127 10:51:36.051091  5146 net.cpp:106] Creating Layer loss
I1127 10:51:36.051096  5146 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:51:36.051102  5146 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:51:36.051110  5146 net.cpp:411] loss -> loss
I1127 10:51:36.051122  5146 layer_factory.hpp:76] Creating layer loss
I1127 10:51:36.051281  5146 net.cpp:150] Setting up loss
I1127 10:51:36.051296  5146 net.cpp:157] Top shape: (1)
I1127 10:51:36.051301  5146 net.cpp:160]     with loss weight 1
I1127 10:51:36.051321  5146 net.cpp:165] Memory required for data: 8086808
I1127 10:51:36.051328  5146 net.cpp:226] loss needs backward computation.
I1127 10:51:36.051340  5146 net.cpp:228] accuracy does not need backward computation.
I1127 10:51:36.051347  5146 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:51:36.051352  5146 net.cpp:226] ip2 needs backward computation.
I1127 10:51:36.051357  5146 net.cpp:226] relu1 needs backward computation.
I1127 10:51:36.051360  5146 net.cpp:226] ip1 needs backward computation.
I1127 10:51:36.051367  5146 net.cpp:226] pool2 needs backward computation.
I1127 10:51:36.051373  5146 net.cpp:226] conv2 needs backward computation.
I1127 10:51:36.051378  5146 net.cpp:226] pool1 needs backward computation.
I1127 10:51:36.051383  5146 net.cpp:226] conv1 needs backward computation.
I1127 10:51:36.051388  5146 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:51:36.051395  5146 net.cpp:228] mnist does not need backward computation.
I1127 10:51:36.051399  5146 net.cpp:270] This network produces output accuracy
I1127 10:51:36.051406  5146 net.cpp:270] This network produces output loss
I1127 10:51:36.051421  5146 net.cpp:283] Network initialization done.
I1127 10:51:36.051525  5146 solver.cpp:59] Solver scaffolding done.
I1127 10:51:36.051893  5146 caffe.cpp:212] Starting Optimization
I1127 10:51:36.051914  5146 solver.cpp:287] Solving LeNet
I1127 10:51:36.051921  5146 solver.cpp:288] Learning Rate Policy: inv
I1127 10:51:36.052896  5146 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:51:38.181208  5146 solver.cpp:408]     Test net output #0: accuracy = 0.0938
I1127 10:51:38.181248  5146 solver.cpp:408]     Test net output #1: loss = 2.52348 (* 1 = 2.52348 loss)
I1127 10:51:38.212391  5146 solver.cpp:236] Iteration 0, loss = 2.53322
I1127 10:51:38.212409  5146 solver.cpp:252]     Train net output #0: loss = 2.53322 (* 1 = 2.53322 loss)
I1127 10:51:38.212419  5146 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:51:50.686586  5146 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:51:52.512414  5146 solver.cpp:408]     Test net output #0: accuracy = 0.9692
I1127 10:51:52.512456  5146 solver.cpp:408]     Test net output #1: loss = 0.094603 (* 1 = 0.094603 loss)
I1127 10:51:52.540845  5146 solver.cpp:236] Iteration 500, loss = 0.0954084
I1127 10:51:52.540865  5146 solver.cpp:252]     Train net output #0: loss = 0.0954084 (* 1 = 0.0954084 loss)
I1127 10:51:52.540875  5146 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:52:05.325569  5146 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:52:05.338364  5146 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:52:05.347936  5146 solver.cpp:320] Iteration 1000, loss = 0.106741
I1127 10:52:05.348014  5146 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:52:07.082820  5146 solver.cpp:408]     Test net output #0: accuracy = 0.9805
I1127 10:52:07.082985  5146 solver.cpp:408]     Test net output #1: loss = 0.0603966 (* 1 = 0.0603966 loss)
I1127 10:52:07.082995  5146 solver.cpp:325] Optimization Done.
I1127 10:52:07.083000  5146 caffe.cpp:215] Optimization Done.
I1127 10:52:07.186336  5172 caffe.cpp:184] Using GPUs 0
I1127 10:52:07.659384  5172 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:52:07.659574  5172 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:52:07.659903  5172 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:52:07.659919  5172 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:52:07.660020  5172 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:52:07.660094  5172 layer_factory.hpp:76] Creating layer mnist
I1127 10:52:07.660471  5172 net.cpp:106] Creating Layer mnist
I1127 10:52:07.660486  5172 net.cpp:411] mnist -> data
I1127 10:52:07.660514  5172 net.cpp:411] mnist -> label
I1127 10:52:07.661392  5175 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:52:07.695443  5172 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:52:07.702011  5172 net.cpp:150] Setting up mnist
I1127 10:52:07.702033  5172 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:52:07.702039  5172 net.cpp:157] Top shape: 64 (64)
I1127 10:52:07.702044  5172 net.cpp:165] Memory required for data: 200960
I1127 10:52:07.702069  5172 layer_factory.hpp:76] Creating layer conv1
I1127 10:52:07.702100  5172 net.cpp:106] Creating Layer conv1
I1127 10:52:07.702108  5172 net.cpp:454] conv1 <- data
I1127 10:52:07.702124  5172 net.cpp:411] conv1 -> conv1
I1127 10:52:07.702776  5172 net.cpp:150] Setting up conv1
I1127 10:52:07.702793  5172 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:52:07.702798  5172 net.cpp:165] Memory required for data: 3150080
I1127 10:52:07.702811  5172 layer_factory.hpp:76] Creating layer pool1
I1127 10:52:07.702821  5172 net.cpp:106] Creating Layer pool1
I1127 10:52:07.702826  5172 net.cpp:454] pool1 <- conv1
I1127 10:52:07.702831  5172 net.cpp:411] pool1 -> pool1
I1127 10:52:07.702875  5172 net.cpp:150] Setting up pool1
I1127 10:52:07.702883  5172 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:52:07.702886  5172 net.cpp:165] Memory required for data: 3887360
I1127 10:52:07.702891  5172 layer_factory.hpp:76] Creating layer conv2
I1127 10:52:07.702900  5172 net.cpp:106] Creating Layer conv2
I1127 10:52:07.702904  5172 net.cpp:454] conv2 <- pool1
I1127 10:52:07.702913  5172 net.cpp:411] conv2 -> conv2
I1127 10:52:07.703346  5172 net.cpp:150] Setting up conv2
I1127 10:52:07.703361  5172 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:52:07.703366  5172 net.cpp:165] Memory required for data: 4706560
I1127 10:52:07.703375  5172 layer_factory.hpp:76] Creating layer pool2
I1127 10:52:07.703384  5172 net.cpp:106] Creating Layer pool2
I1127 10:52:07.703388  5172 net.cpp:454] pool2 <- conv2
I1127 10:52:07.703394  5172 net.cpp:411] pool2 -> pool2
I1127 10:52:07.703424  5172 net.cpp:150] Setting up pool2
I1127 10:52:07.703433  5172 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:52:07.703436  5172 net.cpp:165] Memory required for data: 4911360
I1127 10:52:07.703441  5172 layer_factory.hpp:76] Creating layer ip1
I1127 10:52:07.703449  5172 net.cpp:106] Creating Layer ip1
I1127 10:52:07.703454  5172 net.cpp:454] ip1 <- pool2
I1127 10:52:07.703461  5172 net.cpp:411] ip1 -> ip1
I1127 10:52:07.705569  5172 net.cpp:150] Setting up ip1
I1127 10:52:07.705580  5172 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:52:07.705585  5172 net.cpp:165] Memory required for data: 5039360
I1127 10:52:07.705595  5172 layer_factory.hpp:76] Creating layer relu1
I1127 10:52:07.705601  5172 net.cpp:106] Creating Layer relu1
I1127 10:52:07.705606  5172 net.cpp:454] relu1 <- ip1
I1127 10:52:07.705613  5172 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:52:07.705622  5172 net.cpp:150] Setting up relu1
I1127 10:52:07.705627  5172 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:52:07.705631  5172 net.cpp:165] Memory required for data: 5167360
I1127 10:52:07.705636  5172 layer_factory.hpp:76] Creating layer ip2
I1127 10:52:07.705643  5172 net.cpp:106] Creating Layer ip2
I1127 10:52:07.705647  5172 net.cpp:454] ip2 <- ip1
I1127 10:52:07.705654  5172 net.cpp:411] ip2 -> ip2
I1127 10:52:07.706050  5172 net.cpp:150] Setting up ip2
I1127 10:52:07.706060  5172 net.cpp:157] Top shape: 64 10 (640)
I1127 10:52:07.706064  5172 net.cpp:165] Memory required for data: 5169920
I1127 10:52:07.706071  5172 layer_factory.hpp:76] Creating layer loss
I1127 10:52:07.706079  5172 net.cpp:106] Creating Layer loss
I1127 10:52:07.706084  5172 net.cpp:454] loss <- ip2
I1127 10:52:07.706089  5172 net.cpp:454] loss <- label
I1127 10:52:07.706096  5172 net.cpp:411] loss -> loss
I1127 10:52:07.706107  5172 layer_factory.hpp:76] Creating layer loss
I1127 10:52:07.706179  5172 net.cpp:150] Setting up loss
I1127 10:52:07.706189  5172 net.cpp:157] Top shape: (1)
I1127 10:52:07.706193  5172 net.cpp:160]     with loss weight 1
I1127 10:52:07.706209  5172 net.cpp:165] Memory required for data: 5169924
I1127 10:52:07.706213  5172 net.cpp:226] loss needs backward computation.
I1127 10:52:07.706218  5172 net.cpp:226] ip2 needs backward computation.
I1127 10:52:07.706223  5172 net.cpp:226] relu1 needs backward computation.
I1127 10:52:07.706226  5172 net.cpp:226] ip1 needs backward computation.
I1127 10:52:07.706230  5172 net.cpp:226] pool2 needs backward computation.
I1127 10:52:07.706239  5172 net.cpp:226] conv2 needs backward computation.
I1127 10:52:07.706243  5172 net.cpp:226] pool1 needs backward computation.
I1127 10:52:07.706248  5172 net.cpp:226] conv1 needs backward computation.
I1127 10:52:07.706253  5172 net.cpp:228] mnist does not need backward computation.
I1127 10:52:07.706256  5172 net.cpp:270] This network produces output loss
I1127 10:52:07.706266  5172 net.cpp:283] Network initialization done.
I1127 10:52:07.706504  5172 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:52:07.706528  5172 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:52:07.706637  5172 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:52:07.706697  5172 layer_factory.hpp:76] Creating layer mnist
I1127 10:52:07.706782  5172 net.cpp:106] Creating Layer mnist
I1127 10:52:07.706791  5172 net.cpp:411] mnist -> data
I1127 10:52:07.706799  5172 net.cpp:411] mnist -> label
I1127 10:52:07.707512  5178 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:52:07.707589  5172 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:52:07.711138  5172 net.cpp:150] Setting up mnist
I1127 10:52:07.711150  5172 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:52:07.711156  5172 net.cpp:157] Top shape: 100 (100)
I1127 10:52:07.711160  5172 net.cpp:165] Memory required for data: 314000
I1127 10:52:07.711165  5172 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:52:07.711172  5172 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:52:07.711177  5172 net.cpp:454] label_mnist_1_split <- label
I1127 10:52:07.711182  5172 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:52:07.711190  5172 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:52:07.711223  5172 net.cpp:150] Setting up label_mnist_1_split
I1127 10:52:07.711230  5172 net.cpp:157] Top shape: 100 (100)
I1127 10:52:07.711236  5172 net.cpp:157] Top shape: 100 (100)
I1127 10:52:07.711243  5172 net.cpp:165] Memory required for data: 314800
I1127 10:52:07.711248  5172 layer_factory.hpp:76] Creating layer conv1
I1127 10:52:07.711256  5172 net.cpp:106] Creating Layer conv1
I1127 10:52:07.711261  5172 net.cpp:454] conv1 <- data
I1127 10:52:07.711268  5172 net.cpp:411] conv1 -> conv1
I1127 10:52:07.711415  5172 net.cpp:150] Setting up conv1
I1127 10:52:07.711423  5172 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:52:07.711427  5172 net.cpp:165] Memory required for data: 4922800
I1127 10:52:07.711436  5172 layer_factory.hpp:76] Creating layer pool1
I1127 10:52:07.711444  5172 net.cpp:106] Creating Layer pool1
I1127 10:52:07.711449  5172 net.cpp:454] pool1 <- conv1
I1127 10:52:07.711462  5172 net.cpp:411] pool1 -> pool1
I1127 10:52:07.711489  5172 net.cpp:150] Setting up pool1
I1127 10:52:07.711496  5172 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:52:07.711500  5172 net.cpp:165] Memory required for data: 6074800
I1127 10:52:07.711504  5172 layer_factory.hpp:76] Creating layer conv2
I1127 10:52:07.711513  5172 net.cpp:106] Creating Layer conv2
I1127 10:52:07.711518  5172 net.cpp:454] conv2 <- pool1
I1127 10:52:07.711524  5172 net.cpp:411] conv2 -> conv2
I1127 10:52:07.711769  5172 net.cpp:150] Setting up conv2
I1127 10:52:07.711777  5172 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:52:07.711782  5172 net.cpp:165] Memory required for data: 7354800
I1127 10:52:07.711791  5172 layer_factory.hpp:76] Creating layer pool2
I1127 10:52:07.711797  5172 net.cpp:106] Creating Layer pool2
I1127 10:52:07.711802  5172 net.cpp:454] pool2 <- conv2
I1127 10:52:07.711808  5172 net.cpp:411] pool2 -> pool2
I1127 10:52:07.712381  5172 net.cpp:150] Setting up pool2
I1127 10:52:07.712398  5172 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:52:07.712405  5172 net.cpp:165] Memory required for data: 7674800
I1127 10:52:07.712411  5172 layer_factory.hpp:76] Creating layer ip1
I1127 10:52:07.712422  5172 net.cpp:106] Creating Layer ip1
I1127 10:52:07.712427  5172 net.cpp:454] ip1 <- pool2
I1127 10:52:07.712434  5172 net.cpp:411] ip1 -> ip1
I1127 10:52:07.714565  5172 net.cpp:150] Setting up ip1
I1127 10:52:07.714578  5172 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:52:07.714583  5172 net.cpp:165] Memory required for data: 7874800
I1127 10:52:07.714592  5172 layer_factory.hpp:76] Creating layer relu1
I1127 10:52:07.714601  5172 net.cpp:106] Creating Layer relu1
I1127 10:52:07.714607  5172 net.cpp:454] relu1 <- ip1
I1127 10:52:07.714612  5172 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:52:07.714620  5172 net.cpp:150] Setting up relu1
I1127 10:52:07.714625  5172 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:52:07.714629  5172 net.cpp:165] Memory required for data: 8074800
I1127 10:52:07.714633  5172 layer_factory.hpp:76] Creating layer ip2
I1127 10:52:07.714643  5172 net.cpp:106] Creating Layer ip2
I1127 10:52:07.714646  5172 net.cpp:454] ip2 <- ip1
I1127 10:52:07.714653  5172 net.cpp:411] ip2 -> ip2
I1127 10:52:07.714743  5172 net.cpp:150] Setting up ip2
I1127 10:52:07.714751  5172 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:52:07.714756  5172 net.cpp:165] Memory required for data: 8078800
I1127 10:52:07.714762  5172 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:52:07.714768  5172 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:52:07.714772  5172 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:52:07.714779  5172 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:52:07.714787  5172 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:52:07.714812  5172 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:52:07.714819  5172 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:52:07.714824  5172 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:52:07.714828  5172 net.cpp:165] Memory required for data: 8086800
I1127 10:52:07.714833  5172 layer_factory.hpp:76] Creating layer accuracy
I1127 10:52:07.714839  5172 net.cpp:106] Creating Layer accuracy
I1127 10:52:07.714843  5172 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:52:07.714848  5172 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:52:07.714859  5172 net.cpp:411] accuracy -> accuracy
I1127 10:52:07.714869  5172 net.cpp:150] Setting up accuracy
I1127 10:52:07.714874  5172 net.cpp:157] Top shape: (1)
I1127 10:52:07.714879  5172 net.cpp:165] Memory required for data: 8086804
I1127 10:52:07.714882  5172 layer_factory.hpp:76] Creating layer loss
I1127 10:52:07.714889  5172 net.cpp:106] Creating Layer loss
I1127 10:52:07.714892  5172 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:52:07.714897  5172 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:52:07.714905  5172 net.cpp:411] loss -> loss
I1127 10:52:07.714912  5172 layer_factory.hpp:76] Creating layer loss
I1127 10:52:07.714980  5172 net.cpp:150] Setting up loss
I1127 10:52:07.714987  5172 net.cpp:157] Top shape: (1)
I1127 10:52:07.714992  5172 net.cpp:160]     with loss weight 1
I1127 10:52:07.715003  5172 net.cpp:165] Memory required for data: 8086808
I1127 10:52:07.715006  5172 net.cpp:226] loss needs backward computation.
I1127 10:52:07.715013  5172 net.cpp:228] accuracy does not need backward computation.
I1127 10:52:07.715018  5172 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:52:07.715023  5172 net.cpp:226] ip2 needs backward computation.
I1127 10:52:07.715028  5172 net.cpp:226] relu1 needs backward computation.
I1127 10:52:07.715030  5172 net.cpp:226] ip1 needs backward computation.
I1127 10:52:07.715034  5172 net.cpp:226] pool2 needs backward computation.
I1127 10:52:07.715039  5172 net.cpp:226] conv2 needs backward computation.
I1127 10:52:07.715044  5172 net.cpp:226] pool1 needs backward computation.
I1127 10:52:07.715047  5172 net.cpp:226] conv1 needs backward computation.
I1127 10:52:07.715052  5172 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:52:07.715056  5172 net.cpp:228] mnist does not need backward computation.
I1127 10:52:07.715060  5172 net.cpp:270] This network produces output accuracy
I1127 10:52:07.715066  5172 net.cpp:270] This network produces output loss
I1127 10:52:07.715075  5172 net.cpp:283] Network initialization done.
I1127 10:52:07.715114  5172 solver.cpp:59] Solver scaffolding done.
I1127 10:52:07.715298  5172 caffe.cpp:212] Starting Optimization
I1127 10:52:07.715306  5172 solver.cpp:287] Solving LeNet
I1127 10:52:07.715309  5172 solver.cpp:288] Learning Rate Policy: inv
I1127 10:52:07.715631  5172 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:52:09.556296  5172 solver.cpp:408]     Test net output #0: accuracy = 0.1255
I1127 10:52:09.556356  5172 solver.cpp:408]     Test net output #1: loss = 2.36031 (* 1 = 2.36031 loss)
I1127 10:52:09.572335  5172 solver.cpp:236] Iteration 0, loss = 2.35169
I1127 10:52:09.572485  5172 solver.cpp:252]     Train net output #0: loss = 2.35169 (* 1 = 2.35169 loss)
I1127 10:52:09.572545  5172 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:52:23.024061  5172 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:52:24.166052  5172 solver.cpp:408]     Test net output #0: accuracy = 0.9716
I1127 10:52:24.166196  5172 solver.cpp:408]     Test net output #1: loss = 0.0895663 (* 1 = 0.0895663 loss)
I1127 10:52:24.176203  5172 solver.cpp:236] Iteration 500, loss = 0.123382
I1127 10:52:24.176300  5172 solver.cpp:252]     Train net output #0: loss = 0.123382 (* 1 = 0.123382 loss)
I1127 10:52:24.176318  5172 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:52:37.545742  5172 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:52:37.565959  5172 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:52:37.593854  5172 solver.cpp:320] Iteration 1000, loss = 0.110242
I1127 10:52:37.593875  5172 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:52:38.724158  5172 solver.cpp:408]     Test net output #0: accuracy = 0.981
I1127 10:52:38.724267  5172 solver.cpp:408]     Test net output #1: loss = 0.0594394 (* 1 = 0.0594394 loss)
I1127 10:52:38.724277  5172 solver.cpp:325] Optimization Done.
I1127 10:52:38.724304  5172 caffe.cpp:215] Optimization Done.
I1127 10:52:38.825912  5199 caffe.cpp:184] Using GPUs 0
I1127 10:52:39.200919  5199 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:52:39.201144  5199 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:52:39.201521  5199 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:52:39.201544  5199 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:52:39.201668  5199 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:52:39.201769  5199 layer_factory.hpp:76] Creating layer mnist
I1127 10:52:39.202217  5199 net.cpp:106] Creating Layer mnist
I1127 10:52:39.202244  5199 net.cpp:411] mnist -> data
I1127 10:52:39.202283  5199 net.cpp:411] mnist -> label
I1127 10:52:39.203557  5203 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:52:39.217885  5199 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:52:39.266279  5199 net.cpp:150] Setting up mnist
I1127 10:52:39.266389  5199 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:52:39.266407  5199 net.cpp:157] Top shape: 64 (64)
I1127 10:52:39.266415  5199 net.cpp:165] Memory required for data: 200960
I1127 10:52:39.266435  5199 layer_factory.hpp:76] Creating layer conv1
I1127 10:52:39.266472  5199 net.cpp:106] Creating Layer conv1
I1127 10:52:39.266484  5199 net.cpp:454] conv1 <- data
I1127 10:52:39.266497  5199 net.cpp:411] conv1 -> conv1
I1127 10:52:39.267366  5199 net.cpp:150] Setting up conv1
I1127 10:52:39.267423  5199 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:52:39.267429  5199 net.cpp:165] Memory required for data: 3150080
I1127 10:52:39.267459  5199 layer_factory.hpp:76] Creating layer pool1
I1127 10:52:39.267484  5199 net.cpp:106] Creating Layer pool1
I1127 10:52:39.267510  5199 net.cpp:454] pool1 <- conv1
I1127 10:52:39.267521  5199 net.cpp:411] pool1 -> pool1
I1127 10:52:39.267606  5199 net.cpp:150] Setting up pool1
I1127 10:52:39.267616  5199 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:52:39.267621  5199 net.cpp:165] Memory required for data: 3887360
I1127 10:52:39.267626  5199 layer_factory.hpp:76] Creating layer conv2
I1127 10:52:39.267643  5199 net.cpp:106] Creating Layer conv2
I1127 10:52:39.267649  5199 net.cpp:454] conv2 <- pool1
I1127 10:52:39.267657  5199 net.cpp:411] conv2 -> conv2
I1127 10:52:39.267937  5199 net.cpp:150] Setting up conv2
I1127 10:52:39.267951  5199 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:52:39.267956  5199 net.cpp:165] Memory required for data: 4706560
I1127 10:52:39.267966  5199 layer_factory.hpp:76] Creating layer pool2
I1127 10:52:39.267974  5199 net.cpp:106] Creating Layer pool2
I1127 10:52:39.267979  5199 net.cpp:454] pool2 <- conv2
I1127 10:52:39.267985  5199 net.cpp:411] pool2 -> pool2
I1127 10:52:39.268018  5199 net.cpp:150] Setting up pool2
I1127 10:52:39.268025  5199 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:52:39.268029  5199 net.cpp:165] Memory required for data: 4911360
I1127 10:52:39.268034  5199 layer_factory.hpp:76] Creating layer ip1
I1127 10:52:39.268048  5199 net.cpp:106] Creating Layer ip1
I1127 10:52:39.268051  5199 net.cpp:454] ip1 <- pool2
I1127 10:52:39.268057  5199 net.cpp:411] ip1 -> ip1
I1127 10:52:39.272063  5199 net.cpp:150] Setting up ip1
I1127 10:52:39.272214  5199 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:52:39.272236  5199 net.cpp:165] Memory required for data: 5039360
I1127 10:52:39.272292  5199 layer_factory.hpp:76] Creating layer relu1
I1127 10:52:39.272327  5199 net.cpp:106] Creating Layer relu1
I1127 10:52:39.272344  5199 net.cpp:454] relu1 <- ip1
I1127 10:52:39.272367  5199 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:52:39.272423  5199 net.cpp:150] Setting up relu1
I1127 10:52:39.272445  5199 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:52:39.272469  5199 net.cpp:165] Memory required for data: 5167360
I1127 10:52:39.272485  5199 layer_factory.hpp:76] Creating layer ip2
I1127 10:52:39.272521  5199 net.cpp:106] Creating Layer ip2
I1127 10:52:39.272546  5199 net.cpp:454] ip2 <- ip1
I1127 10:52:39.272578  5199 net.cpp:411] ip2 -> ip2
I1127 10:52:39.273644  5199 net.cpp:150] Setting up ip2
I1127 10:52:39.273718  5199 net.cpp:157] Top shape: 64 10 (640)
I1127 10:52:39.273730  5199 net.cpp:165] Memory required for data: 5169920
I1127 10:52:39.273748  5199 layer_factory.hpp:76] Creating layer loss
I1127 10:52:39.273780  5199 net.cpp:106] Creating Layer loss
I1127 10:52:39.273792  5199 net.cpp:454] loss <- ip2
I1127 10:52:39.273802  5199 net.cpp:454] loss <- label
I1127 10:52:39.273825  5199 net.cpp:411] loss -> loss
I1127 10:52:39.273864  5199 layer_factory.hpp:76] Creating layer loss
I1127 10:52:39.274003  5199 net.cpp:150] Setting up loss
I1127 10:52:39.274019  5199 net.cpp:157] Top shape: (1)
I1127 10:52:39.274027  5199 net.cpp:160]     with loss weight 1
I1127 10:52:39.274060  5199 net.cpp:165] Memory required for data: 5169924
I1127 10:52:39.274070  5199 net.cpp:226] loss needs backward computation.
I1127 10:52:39.274077  5199 net.cpp:226] ip2 needs backward computation.
I1127 10:52:39.274082  5199 net.cpp:226] relu1 needs backward computation.
I1127 10:52:39.274091  5199 net.cpp:226] ip1 needs backward computation.
I1127 10:52:39.274101  5199 net.cpp:226] pool2 needs backward computation.
I1127 10:52:39.274108  5199 net.cpp:226] conv2 needs backward computation.
I1127 10:52:39.274113  5199 net.cpp:226] pool1 needs backward computation.
I1127 10:52:39.274119  5199 net.cpp:226] conv1 needs backward computation.
I1127 10:52:39.274129  5199 net.cpp:228] mnist does not need backward computation.
I1127 10:52:39.274137  5199 net.cpp:270] This network produces output loss
I1127 10:52:39.274160  5199 net.cpp:283] Network initialization done.
I1127 10:52:39.274534  5199 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:52:39.274588  5199 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:52:39.274765  5199 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:52:39.274858  5199 layer_factory.hpp:76] Creating layer mnist
I1127 10:52:39.275002  5199 net.cpp:106] Creating Layer mnist
I1127 10:52:39.275017  5199 net.cpp:411] mnist -> data
I1127 10:52:39.275033  5199 net.cpp:411] mnist -> label
I1127 10:52:39.275842  5205 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:52:39.276063  5199 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:52:39.279930  5199 net.cpp:150] Setting up mnist
I1127 10:52:39.279986  5199 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:52:39.279994  5199 net.cpp:157] Top shape: 100 (100)
I1127 10:52:39.279999  5199 net.cpp:165] Memory required for data: 314000
I1127 10:52:39.280007  5199 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:52:39.280025  5199 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:52:39.280032  5199 net.cpp:454] label_mnist_1_split <- label
I1127 10:52:39.280041  5199 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:52:39.280056  5199 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:52:39.280110  5199 net.cpp:150] Setting up label_mnist_1_split
I1127 10:52:39.280118  5199 net.cpp:157] Top shape: 100 (100)
I1127 10:52:39.280123  5199 net.cpp:157] Top shape: 100 (100)
I1127 10:52:39.280128  5199 net.cpp:165] Memory required for data: 314800
I1127 10:52:39.280133  5199 layer_factory.hpp:76] Creating layer conv1
I1127 10:52:39.280148  5199 net.cpp:106] Creating Layer conv1
I1127 10:52:39.280153  5199 net.cpp:454] conv1 <- data
I1127 10:52:39.280161  5199 net.cpp:411] conv1 -> conv1
I1127 10:52:39.280333  5199 net.cpp:150] Setting up conv1
I1127 10:52:39.280342  5199 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:52:39.280346  5199 net.cpp:165] Memory required for data: 4922800
I1127 10:52:39.280357  5199 layer_factory.hpp:76] Creating layer pool1
I1127 10:52:39.280375  5199 net.cpp:106] Creating Layer pool1
I1127 10:52:39.280380  5199 net.cpp:454] pool1 <- conv1
I1127 10:52:39.280396  5199 net.cpp:411] pool1 -> pool1
I1127 10:52:39.280427  5199 net.cpp:150] Setting up pool1
I1127 10:52:39.280436  5199 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:52:39.280439  5199 net.cpp:165] Memory required for data: 6074800
I1127 10:52:39.280443  5199 layer_factory.hpp:76] Creating layer conv2
I1127 10:52:39.280453  5199 net.cpp:106] Creating Layer conv2
I1127 10:52:39.280458  5199 net.cpp:454] conv2 <- pool1
I1127 10:52:39.280465  5199 net.cpp:411] conv2 -> conv2
I1127 10:52:39.280716  5199 net.cpp:150] Setting up conv2
I1127 10:52:39.280725  5199 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:52:39.280732  5199 net.cpp:165] Memory required for data: 7354800
I1127 10:52:39.280741  5199 layer_factory.hpp:76] Creating layer pool2
I1127 10:52:39.280750  5199 net.cpp:106] Creating Layer pool2
I1127 10:52:39.280755  5199 net.cpp:454] pool2 <- conv2
I1127 10:52:39.280761  5199 net.cpp:411] pool2 -> pool2
I1127 10:52:39.280833  5199 net.cpp:150] Setting up pool2
I1127 10:52:39.280848  5199 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:52:39.280855  5199 net.cpp:165] Memory required for data: 7674800
I1127 10:52:39.280864  5199 layer_factory.hpp:76] Creating layer ip1
I1127 10:52:39.280876  5199 net.cpp:106] Creating Layer ip1
I1127 10:52:39.280884  5199 net.cpp:454] ip1 <- pool2
I1127 10:52:39.280895  5199 net.cpp:411] ip1 -> ip1
I1127 10:52:39.285011  5199 net.cpp:150] Setting up ip1
I1127 10:52:39.285114  5199 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:52:39.285126  5199 net.cpp:165] Memory required for data: 7874800
I1127 10:52:39.285169  5199 layer_factory.hpp:76] Creating layer relu1
I1127 10:52:39.285207  5199 net.cpp:106] Creating Layer relu1
I1127 10:52:39.285222  5199 net.cpp:454] relu1 <- ip1
I1127 10:52:39.285243  5199 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:52:39.285270  5199 net.cpp:150] Setting up relu1
I1127 10:52:39.285282  5199 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:52:39.285291  5199 net.cpp:165] Memory required for data: 8074800
I1127 10:52:39.285300  5199 layer_factory.hpp:76] Creating layer ip2
I1127 10:52:39.285329  5199 net.cpp:106] Creating Layer ip2
I1127 10:52:39.285377  5199 net.cpp:454] ip2 <- ip1
I1127 10:52:39.285395  5199 net.cpp:411] ip2 -> ip2
I1127 10:52:39.285626  5199 net.cpp:150] Setting up ip2
I1127 10:52:39.285645  5199 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:52:39.285655  5199 net.cpp:165] Memory required for data: 8078800
I1127 10:52:39.285670  5199 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:52:39.285684  5199 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:52:39.285693  5199 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:52:39.285706  5199 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:52:39.285718  5199 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:52:39.285773  5199 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:52:39.285789  5199 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:52:39.285801  5199 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:52:39.285809  5199 net.cpp:165] Memory required for data: 8086800
I1127 10:52:39.285817  5199 layer_factory.hpp:76] Creating layer accuracy
I1127 10:52:39.285831  5199 net.cpp:106] Creating Layer accuracy
I1127 10:52:39.285842  5199 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:52:39.285857  5199 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:52:39.285869  5199 net.cpp:411] accuracy -> accuracy
I1127 10:52:39.285889  5199 net.cpp:150] Setting up accuracy
I1127 10:52:39.285902  5199 net.cpp:157] Top shape: (1)
I1127 10:52:39.285910  5199 net.cpp:165] Memory required for data: 8086804
I1127 10:52:39.285919  5199 layer_factory.hpp:76] Creating layer loss
I1127 10:52:39.285938  5199 net.cpp:106] Creating Layer loss
I1127 10:52:39.285946  5199 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:52:39.285959  5199 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:52:39.285991  5199 net.cpp:411] loss -> loss
I1127 10:52:39.286012  5199 layer_factory.hpp:76] Creating layer loss
I1127 10:52:39.286198  5199 net.cpp:150] Setting up loss
I1127 10:52:39.286211  5199 net.cpp:157] Top shape: (1)
I1127 10:52:39.286216  5199 net.cpp:160]     with loss weight 1
I1127 10:52:39.286238  5199 net.cpp:165] Memory required for data: 8086808
I1127 10:52:39.286243  5199 net.cpp:226] loss needs backward computation.
I1127 10:52:39.286255  5199 net.cpp:228] accuracy does not need backward computation.
I1127 10:52:39.286260  5199 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:52:39.286264  5199 net.cpp:226] ip2 needs backward computation.
I1127 10:52:39.286269  5199 net.cpp:226] relu1 needs backward computation.
I1127 10:52:39.286273  5199 net.cpp:226] ip1 needs backward computation.
I1127 10:52:39.286278  5199 net.cpp:226] pool2 needs backward computation.
I1127 10:52:39.286283  5199 net.cpp:226] conv2 needs backward computation.
I1127 10:52:39.286288  5199 net.cpp:226] pool1 needs backward computation.
I1127 10:52:39.286293  5199 net.cpp:226] conv1 needs backward computation.
I1127 10:52:39.286298  5199 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:52:39.286303  5199 net.cpp:228] mnist does not need backward computation.
I1127 10:52:39.286309  5199 net.cpp:270] This network produces output accuracy
I1127 10:52:39.286314  5199 net.cpp:270] This network produces output loss
I1127 10:52:39.286326  5199 net.cpp:283] Network initialization done.
I1127 10:52:39.286437  5199 solver.cpp:59] Solver scaffolding done.
I1127 10:52:39.286712  5199 caffe.cpp:212] Starting Optimization
I1127 10:52:39.286733  5199 solver.cpp:287] Solving LeNet
I1127 10:52:39.286738  5199 solver.cpp:288] Learning Rate Policy: inv
I1127 10:52:39.287327  5199 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:52:42.351085  5199 solver.cpp:408]     Test net output #0: accuracy = 0.0947
I1127 10:52:42.351213  5199 solver.cpp:408]     Test net output #1: loss = 2.34156 (* 1 = 2.34156 loss)
I1127 10:52:42.366951  5199 solver.cpp:236] Iteration 0, loss = 2.36285
I1127 10:52:42.367018  5199 solver.cpp:252]     Train net output #0: loss = 2.36285 (* 1 = 2.36285 loss)
I1127 10:52:42.367049  5199 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:52:55.788658  5199 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:52:57.077648  5199 solver.cpp:408]     Test net output #0: accuracy = 0.9743
I1127 10:52:57.077685  5199 solver.cpp:408]     Test net output #1: loss = 0.0831667 (* 1 = 0.0831667 loss)
I1127 10:52:57.106418  5199 solver.cpp:236] Iteration 500, loss = 0.0776144
I1127 10:52:57.106436  5199 solver.cpp:252]     Train net output #0: loss = 0.0776143 (* 1 = 0.0776143 loss)
I1127 10:52:57.106446  5199 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:53:10.381713  5199 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:53:10.399883  5199 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:53:10.413514  5199 solver.cpp:320] Iteration 1000, loss = 0.0753851
I1127 10:53:10.413661  5199 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:53:11.961411  5199 solver.cpp:408]     Test net output #0: accuracy = 0.982
I1127 10:53:11.961451  5199 solver.cpp:408]     Test net output #1: loss = 0.0560463 (* 1 = 0.0560463 loss)
I1127 10:53:11.961457  5199 solver.cpp:325] Optimization Done.
I1127 10:53:11.961462  5199 caffe.cpp:215] Optimization Done.
I1127 10:53:12.026427  5229 caffe.cpp:184] Using GPUs 0
I1127 10:53:12.460700  5229 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:53:12.460810  5229 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:53:12.461071  5229 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:53:12.461086  5229 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:53:12.461171  5229 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:53:12.461226  5229 layer_factory.hpp:76] Creating layer mnist
I1127 10:53:12.461537  5229 net.cpp:106] Creating Layer mnist
I1127 10:53:12.461570  5229 net.cpp:411] mnist -> data
I1127 10:53:12.461596  5229 net.cpp:411] mnist -> label
I1127 10:53:12.462333  5232 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:53:12.495854  5229 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:53:12.502228  5229 net.cpp:150] Setting up mnist
I1127 10:53:12.502248  5229 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:53:12.502254  5229 net.cpp:157] Top shape: 64 (64)
I1127 10:53:12.502259  5229 net.cpp:165] Memory required for data: 200960
I1127 10:53:12.502267  5229 layer_factory.hpp:76] Creating layer conv1
I1127 10:53:12.502285  5229 net.cpp:106] Creating Layer conv1
I1127 10:53:12.502291  5229 net.cpp:454] conv1 <- data
I1127 10:53:12.502301  5229 net.cpp:411] conv1 -> conv1
I1127 10:53:12.502897  5229 net.cpp:150] Setting up conv1
I1127 10:53:12.502907  5229 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:53:12.502912  5229 net.cpp:165] Memory required for data: 3150080
I1127 10:53:12.502923  5229 layer_factory.hpp:76] Creating layer pool1
I1127 10:53:12.502933  5229 net.cpp:106] Creating Layer pool1
I1127 10:53:12.502938  5229 net.cpp:454] pool1 <- conv1
I1127 10:53:12.502944  5229 net.cpp:411] pool1 -> pool1
I1127 10:53:12.502991  5229 net.cpp:150] Setting up pool1
I1127 10:53:12.503000  5229 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:53:12.503003  5229 net.cpp:165] Memory required for data: 3887360
I1127 10:53:12.503007  5229 layer_factory.hpp:76] Creating layer conv2
I1127 10:53:12.503016  5229 net.cpp:106] Creating Layer conv2
I1127 10:53:12.503021  5229 net.cpp:454] conv2 <- pool1
I1127 10:53:12.503027  5229 net.cpp:411] conv2 -> conv2
I1127 10:53:12.503319  5229 net.cpp:150] Setting up conv2
I1127 10:53:12.503327  5229 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:53:12.503331  5229 net.cpp:165] Memory required for data: 4706560
I1127 10:53:12.503340  5229 layer_factory.hpp:76] Creating layer pool2
I1127 10:53:12.503347  5229 net.cpp:106] Creating Layer pool2
I1127 10:53:12.503352  5229 net.cpp:454] pool2 <- conv2
I1127 10:53:12.503357  5229 net.cpp:411] pool2 -> pool2
I1127 10:53:12.503443  5229 net.cpp:150] Setting up pool2
I1127 10:53:12.503451  5229 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:53:12.503455  5229 net.cpp:165] Memory required for data: 4911360
I1127 10:53:12.503459  5229 layer_factory.hpp:76] Creating layer ip1
I1127 10:53:12.503469  5229 net.cpp:106] Creating Layer ip1
I1127 10:53:12.503473  5229 net.cpp:454] ip1 <- pool2
I1127 10:53:12.503480  5229 net.cpp:411] ip1 -> ip1
I1127 10:53:12.505575  5229 net.cpp:150] Setting up ip1
I1127 10:53:12.505585  5229 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:53:12.505590  5229 net.cpp:165] Memory required for data: 5039360
I1127 10:53:12.505599  5229 layer_factory.hpp:76] Creating layer relu1
I1127 10:53:12.505606  5229 net.cpp:106] Creating Layer relu1
I1127 10:53:12.505610  5229 net.cpp:454] relu1 <- ip1
I1127 10:53:12.505616  5229 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:53:12.505625  5229 net.cpp:150] Setting up relu1
I1127 10:53:12.505630  5229 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:53:12.505635  5229 net.cpp:165] Memory required for data: 5167360
I1127 10:53:12.505638  5229 layer_factory.hpp:76] Creating layer ip2
I1127 10:53:12.505646  5229 net.cpp:106] Creating Layer ip2
I1127 10:53:12.505651  5229 net.cpp:454] ip2 <- ip1
I1127 10:53:12.505657  5229 net.cpp:411] ip2 -> ip2
I1127 10:53:12.506048  5229 net.cpp:150] Setting up ip2
I1127 10:53:12.506058  5229 net.cpp:157] Top shape: 64 10 (640)
I1127 10:53:12.506062  5229 net.cpp:165] Memory required for data: 5169920
I1127 10:53:12.506069  5229 layer_factory.hpp:76] Creating layer loss
I1127 10:53:12.506078  5229 net.cpp:106] Creating Layer loss
I1127 10:53:12.506083  5229 net.cpp:454] loss <- ip2
I1127 10:53:12.506088  5229 net.cpp:454] loss <- label
I1127 10:53:12.506095  5229 net.cpp:411] loss -> loss
I1127 10:53:12.506108  5229 layer_factory.hpp:76] Creating layer loss
I1127 10:53:12.506187  5229 net.cpp:150] Setting up loss
I1127 10:53:12.506197  5229 net.cpp:157] Top shape: (1)
I1127 10:53:12.506201  5229 net.cpp:160]     with loss weight 1
I1127 10:53:12.506217  5229 net.cpp:165] Memory required for data: 5169924
I1127 10:53:12.506222  5229 net.cpp:226] loss needs backward computation.
I1127 10:53:12.506227  5229 net.cpp:226] ip2 needs backward computation.
I1127 10:53:12.506230  5229 net.cpp:226] relu1 needs backward computation.
I1127 10:53:12.506234  5229 net.cpp:226] ip1 needs backward computation.
I1127 10:53:12.506238  5229 net.cpp:226] pool2 needs backward computation.
I1127 10:53:12.506242  5229 net.cpp:226] conv2 needs backward computation.
I1127 10:53:12.506247  5229 net.cpp:226] pool1 needs backward computation.
I1127 10:53:12.506250  5229 net.cpp:226] conv1 needs backward computation.
I1127 10:53:12.506255  5229 net.cpp:228] mnist does not need backward computation.
I1127 10:53:12.506259  5229 net.cpp:270] This network produces output loss
I1127 10:53:12.506268  5229 net.cpp:283] Network initialization done.
I1127 10:53:12.506503  5229 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:53:12.506525  5229 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:53:12.506634  5229 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:53:12.506695  5229 layer_factory.hpp:76] Creating layer mnist
I1127 10:53:12.506777  5229 net.cpp:106] Creating Layer mnist
I1127 10:53:12.506786  5229 net.cpp:411] mnist -> data
I1127 10:53:12.506795  5229 net.cpp:411] mnist -> label
I1127 10:53:12.507475  5234 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:53:12.507576  5229 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:53:12.511131  5229 net.cpp:150] Setting up mnist
I1127 10:53:12.511143  5229 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:53:12.511148  5229 net.cpp:157] Top shape: 100 (100)
I1127 10:53:12.511153  5229 net.cpp:165] Memory required for data: 314000
I1127 10:53:12.511157  5229 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:53:12.511164  5229 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:53:12.511169  5229 net.cpp:454] label_mnist_1_split <- label
I1127 10:53:12.511175  5229 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:53:12.511183  5229 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:53:12.511225  5229 net.cpp:150] Setting up label_mnist_1_split
I1127 10:53:12.511234  5229 net.cpp:157] Top shape: 100 (100)
I1127 10:53:12.511240  5229 net.cpp:157] Top shape: 100 (100)
I1127 10:53:12.511243  5229 net.cpp:165] Memory required for data: 314800
I1127 10:53:12.511247  5229 layer_factory.hpp:76] Creating layer conv1
I1127 10:53:12.511257  5229 net.cpp:106] Creating Layer conv1
I1127 10:53:12.511262  5229 net.cpp:454] conv1 <- data
I1127 10:53:12.511270  5229 net.cpp:411] conv1 -> conv1
I1127 10:53:12.511412  5229 net.cpp:150] Setting up conv1
I1127 10:53:12.511421  5229 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:53:12.511425  5229 net.cpp:165] Memory required for data: 4922800
I1127 10:53:12.511435  5229 layer_factory.hpp:76] Creating layer pool1
I1127 10:53:12.511441  5229 net.cpp:106] Creating Layer pool1
I1127 10:53:12.511445  5229 net.cpp:454] pool1 <- conv1
I1127 10:53:12.511457  5229 net.cpp:411] pool1 -> pool1
I1127 10:53:12.511487  5229 net.cpp:150] Setting up pool1
I1127 10:53:12.511494  5229 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:53:12.511498  5229 net.cpp:165] Memory required for data: 6074800
I1127 10:53:12.511502  5229 layer_factory.hpp:76] Creating layer conv2
I1127 10:53:12.511512  5229 net.cpp:106] Creating Layer conv2
I1127 10:53:12.511518  5229 net.cpp:454] conv2 <- pool1
I1127 10:53:12.511526  5229 net.cpp:411] conv2 -> conv2
I1127 10:53:12.511773  5229 net.cpp:150] Setting up conv2
I1127 10:53:12.511781  5229 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:53:12.511785  5229 net.cpp:165] Memory required for data: 7354800
I1127 10:53:12.511793  5229 layer_factory.hpp:76] Creating layer pool2
I1127 10:53:12.511801  5229 net.cpp:106] Creating Layer pool2
I1127 10:53:12.511804  5229 net.cpp:454] pool2 <- conv2
I1127 10:53:12.511809  5229 net.cpp:411] pool2 -> pool2
I1127 10:53:12.511837  5229 net.cpp:150] Setting up pool2
I1127 10:53:12.511844  5229 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:53:12.511848  5229 net.cpp:165] Memory required for data: 7674800
I1127 10:53:12.511853  5229 layer_factory.hpp:76] Creating layer ip1
I1127 10:53:12.511859  5229 net.cpp:106] Creating Layer ip1
I1127 10:53:12.511864  5229 net.cpp:454] ip1 <- pool2
I1127 10:53:12.511873  5229 net.cpp:411] ip1 -> ip1
I1127 10:53:12.514024  5229 net.cpp:150] Setting up ip1
I1127 10:53:12.514036  5229 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:53:12.514040  5229 net.cpp:165] Memory required for data: 7874800
I1127 10:53:12.514050  5229 layer_factory.hpp:76] Creating layer relu1
I1127 10:53:12.514057  5229 net.cpp:106] Creating Layer relu1
I1127 10:53:12.514062  5229 net.cpp:454] relu1 <- ip1
I1127 10:53:12.514068  5229 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:53:12.514075  5229 net.cpp:150] Setting up relu1
I1127 10:53:12.514081  5229 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:53:12.514084  5229 net.cpp:165] Memory required for data: 8074800
I1127 10:53:12.514088  5229 layer_factory.hpp:76] Creating layer ip2
I1127 10:53:12.514096  5229 net.cpp:106] Creating Layer ip2
I1127 10:53:12.514101  5229 net.cpp:454] ip2 <- ip1
I1127 10:53:12.514109  5229 net.cpp:411] ip2 -> ip2
I1127 10:53:12.514216  5229 net.cpp:150] Setting up ip2
I1127 10:53:12.514226  5229 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:53:12.514230  5229 net.cpp:165] Memory required for data: 8078800
I1127 10:53:12.514237  5229 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:53:12.514243  5229 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:53:12.514247  5229 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:53:12.514253  5229 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:53:12.514260  5229 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:53:12.514286  5229 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:53:12.514293  5229 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:53:12.514302  5229 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:53:12.514307  5229 net.cpp:165] Memory required for data: 8086800
I1127 10:53:12.514310  5229 layer_factory.hpp:76] Creating layer accuracy
I1127 10:53:12.514317  5229 net.cpp:106] Creating Layer accuracy
I1127 10:53:12.514322  5229 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:53:12.514327  5229 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:53:12.514333  5229 net.cpp:411] accuracy -> accuracy
I1127 10:53:12.514340  5229 net.cpp:150] Setting up accuracy
I1127 10:53:12.514346  5229 net.cpp:157] Top shape: (1)
I1127 10:53:12.514350  5229 net.cpp:165] Memory required for data: 8086804
I1127 10:53:12.514354  5229 layer_factory.hpp:76] Creating layer loss
I1127 10:53:12.514361  5229 net.cpp:106] Creating Layer loss
I1127 10:53:12.514366  5229 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:53:12.514371  5229 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:53:12.514376  5229 net.cpp:411] loss -> loss
I1127 10:53:12.514384  5229 layer_factory.hpp:76] Creating layer loss
I1127 10:53:12.514453  5229 net.cpp:150] Setting up loss
I1127 10:53:12.514461  5229 net.cpp:157] Top shape: (1)
I1127 10:53:12.514466  5229 net.cpp:160]     with loss weight 1
I1127 10:53:12.514472  5229 net.cpp:165] Memory required for data: 8086808
I1127 10:53:12.514477  5229 net.cpp:226] loss needs backward computation.
I1127 10:53:12.514483  5229 net.cpp:228] accuracy does not need backward computation.
I1127 10:53:12.514487  5229 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:53:12.514495  5229 net.cpp:226] ip2 needs backward computation.
I1127 10:53:12.514499  5229 net.cpp:226] relu1 needs backward computation.
I1127 10:53:12.514503  5229 net.cpp:226] ip1 needs backward computation.
I1127 10:53:12.514508  5229 net.cpp:226] pool2 needs backward computation.
I1127 10:53:12.514513  5229 net.cpp:226] conv2 needs backward computation.
I1127 10:53:12.514516  5229 net.cpp:226] pool1 needs backward computation.
I1127 10:53:12.514520  5229 net.cpp:226] conv1 needs backward computation.
I1127 10:53:12.514525  5229 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:53:12.514530  5229 net.cpp:228] mnist does not need backward computation.
I1127 10:53:12.514533  5229 net.cpp:270] This network produces output accuracy
I1127 10:53:12.514539  5229 net.cpp:270] This network produces output loss
I1127 10:53:12.514549  5229 net.cpp:283] Network initialization done.
I1127 10:53:12.514580  5229 solver.cpp:59] Solver scaffolding done.
I1127 10:53:12.514798  5229 caffe.cpp:212] Starting Optimization
I1127 10:53:12.514806  5229 solver.cpp:287] Solving LeNet
I1127 10:53:12.514809  5229 solver.cpp:288] Learning Rate Policy: inv
I1127 10:53:12.515223  5229 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:53:14.646459  5229 solver.cpp:408]     Test net output #0: accuracy = 0.1384
I1127 10:53:14.646512  5229 solver.cpp:408]     Test net output #1: loss = 2.28586 (* 1 = 2.28586 loss)
I1127 10:53:14.657074  5229 solver.cpp:236] Iteration 0, loss = 2.29842
I1127 10:53:14.657138  5229 solver.cpp:252]     Train net output #0: loss = 2.29842 (* 1 = 2.29842 loss)
I1127 10:53:14.657166  5229 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:53:28.102536  5229 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:53:29.599477  5229 solver.cpp:408]     Test net output #0: accuracy = 0.9728
I1127 10:53:29.599510  5229 solver.cpp:408]     Test net output #1: loss = 0.0886085 (* 1 = 0.0886085 loss)
I1127 10:53:29.629163  5229 solver.cpp:236] Iteration 500, loss = 0.115117
I1127 10:53:29.629179  5229 solver.cpp:252]     Train net output #0: loss = 0.115117 (* 1 = 0.115117 loss)
I1127 10:53:29.629189  5229 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:53:42.670440  5229 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:53:42.690201  5229 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:53:42.703575  5229 solver.cpp:320] Iteration 1000, loss = 0.0852806
I1127 10:53:42.703706  5229 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:53:45.482678  5229 solver.cpp:408]     Test net output #0: accuracy = 0.9833
I1127 10:53:45.482714  5229 solver.cpp:408]     Test net output #1: loss = 0.0550844 (* 1 = 0.0550844 loss)
I1127 10:53:45.482722  5229 solver.cpp:325] Optimization Done.
I1127 10:53:45.482728  5229 caffe.cpp:215] Optimization Done.
I1127 10:53:45.549674  5256 caffe.cpp:184] Using GPUs 0
I1127 10:53:45.943665  5256 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:53:45.943794  5256 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:53:45.944078  5256 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:53:45.944097  5256 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:53:45.944198  5256 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:53:45.944273  5256 layer_factory.hpp:76] Creating layer mnist
I1127 10:53:45.944648  5256 net.cpp:106] Creating Layer mnist
I1127 10:53:45.944675  5256 net.cpp:411] mnist -> data
I1127 10:53:45.944715  5256 net.cpp:411] mnist -> label
I1127 10:53:45.945924  5260 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:53:45.953655  5256 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:53:45.955044  5256 net.cpp:150] Setting up mnist
I1127 10:53:45.955108  5256 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:53:45.955116  5256 net.cpp:157] Top shape: 64 (64)
I1127 10:53:45.955119  5256 net.cpp:165] Memory required for data: 200960
I1127 10:53:45.955132  5256 layer_factory.hpp:76] Creating layer conv1
I1127 10:53:45.955153  5256 net.cpp:106] Creating Layer conv1
I1127 10:53:45.955160  5256 net.cpp:454] conv1 <- data
I1127 10:53:45.955173  5256 net.cpp:411] conv1 -> conv1
I1127 10:53:45.956027  5256 net.cpp:150] Setting up conv1
I1127 10:53:45.956061  5256 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:53:45.956065  5256 net.cpp:165] Memory required for data: 3150080
I1127 10:53:45.956082  5256 layer_factory.hpp:76] Creating layer pool1
I1127 10:53:45.956096  5256 net.cpp:106] Creating Layer pool1
I1127 10:53:45.956101  5256 net.cpp:454] pool1 <- conv1
I1127 10:53:45.956110  5256 net.cpp:411] pool1 -> pool1
I1127 10:53:45.956171  5256 net.cpp:150] Setting up pool1
I1127 10:53:45.956179  5256 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:53:45.956184  5256 net.cpp:165] Memory required for data: 3887360
I1127 10:53:45.956188  5256 layer_factory.hpp:76] Creating layer conv2
I1127 10:53:45.956202  5256 net.cpp:106] Creating Layer conv2
I1127 10:53:45.956205  5256 net.cpp:454] conv2 <- pool1
I1127 10:53:45.956213  5256 net.cpp:411] conv2 -> conv2
I1127 10:53:45.956465  5256 net.cpp:150] Setting up conv2
I1127 10:53:45.956475  5256 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:53:45.956478  5256 net.cpp:165] Memory required for data: 4706560
I1127 10:53:45.956487  5256 layer_factory.hpp:76] Creating layer pool2
I1127 10:53:45.956496  5256 net.cpp:106] Creating Layer pool2
I1127 10:53:45.956501  5256 net.cpp:454] pool2 <- conv2
I1127 10:53:45.956506  5256 net.cpp:411] pool2 -> pool2
I1127 10:53:45.956532  5256 net.cpp:150] Setting up pool2
I1127 10:53:45.956539  5256 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:53:45.956548  5256 net.cpp:165] Memory required for data: 4911360
I1127 10:53:45.956552  5256 layer_factory.hpp:76] Creating layer ip1
I1127 10:53:45.956562  5256 net.cpp:106] Creating Layer ip1
I1127 10:53:45.956567  5256 net.cpp:454] ip1 <- pool2
I1127 10:53:45.956573  5256 net.cpp:411] ip1 -> ip1
I1127 10:53:45.959007  5256 net.cpp:150] Setting up ip1
I1127 10:53:45.959043  5256 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:53:45.959046  5256 net.cpp:165] Memory required for data: 5039360
I1127 10:53:45.959060  5256 layer_factory.hpp:76] Creating layer relu1
I1127 10:53:45.959072  5256 net.cpp:106] Creating Layer relu1
I1127 10:53:45.959077  5256 net.cpp:454] relu1 <- ip1
I1127 10:53:45.959086  5256 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:53:45.959098  5256 net.cpp:150] Setting up relu1
I1127 10:53:45.959105  5256 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:53:45.959108  5256 net.cpp:165] Memory required for data: 5167360
I1127 10:53:45.959112  5256 layer_factory.hpp:76] Creating layer ip2
I1127 10:53:45.959121  5256 net.cpp:106] Creating Layer ip2
I1127 10:53:45.959125  5256 net.cpp:454] ip2 <- ip1
I1127 10:53:45.959132  5256 net.cpp:411] ip2 -> ip2
I1127 10:53:45.959779  5256 net.cpp:150] Setting up ip2
I1127 10:53:45.959803  5256 net.cpp:157] Top shape: 64 10 (640)
I1127 10:53:45.959818  5256 net.cpp:165] Memory required for data: 5169920
I1127 10:53:45.959838  5256 layer_factory.hpp:76] Creating layer loss
I1127 10:53:45.959862  5256 net.cpp:106] Creating Layer loss
I1127 10:53:45.959872  5256 net.cpp:454] loss <- ip2
I1127 10:53:45.959884  5256 net.cpp:454] loss <- label
I1127 10:53:45.959897  5256 net.cpp:411] loss -> loss
I1127 10:53:45.959918  5256 layer_factory.hpp:76] Creating layer loss
I1127 10:53:45.960034  5256 net.cpp:150] Setting up loss
I1127 10:53:45.960052  5256 net.cpp:157] Top shape: (1)
I1127 10:53:45.960059  5256 net.cpp:160]     with loss weight 1
I1127 10:53:45.960096  5256 net.cpp:165] Memory required for data: 5169924
I1127 10:53:45.960105  5256 net.cpp:226] loss needs backward computation.
I1127 10:53:45.960114  5256 net.cpp:226] ip2 needs backward computation.
I1127 10:53:45.960122  5256 net.cpp:226] relu1 needs backward computation.
I1127 10:53:45.960134  5256 net.cpp:226] ip1 needs backward computation.
I1127 10:53:45.960141  5256 net.cpp:226] pool2 needs backward computation.
I1127 10:53:45.960150  5256 net.cpp:226] conv2 needs backward computation.
I1127 10:53:45.960155  5256 net.cpp:226] pool1 needs backward computation.
I1127 10:53:45.960160  5256 net.cpp:226] conv1 needs backward computation.
I1127 10:53:45.960165  5256 net.cpp:228] mnist does not need backward computation.
I1127 10:53:45.960170  5256 net.cpp:270] This network produces output loss
I1127 10:53:45.960183  5256 net.cpp:283] Network initialization done.
I1127 10:53:45.960602  5256 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:53:45.960641  5256 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:53:45.960779  5256 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:53:45.960860  5256 layer_factory.hpp:76] Creating layer mnist
I1127 10:53:45.961040  5256 net.cpp:106] Creating Layer mnist
I1127 10:53:45.961086  5256 net.cpp:411] mnist -> data
I1127 10:53:45.961110  5256 net.cpp:411] mnist -> label
I1127 10:53:45.962329  5262 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:53:45.962545  5256 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:53:45.963718  5256 net.cpp:150] Setting up mnist
I1127 10:53:45.963747  5256 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:53:45.963753  5256 net.cpp:157] Top shape: 100 (100)
I1127 10:53:45.963757  5256 net.cpp:165] Memory required for data: 314000
I1127 10:53:45.963764  5256 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:53:45.963778  5256 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:53:45.963783  5256 net.cpp:454] label_mnist_1_split <- label
I1127 10:53:45.963790  5256 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:53:45.963801  5256 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:53:45.963835  5256 net.cpp:150] Setting up label_mnist_1_split
I1127 10:53:45.963842  5256 net.cpp:157] Top shape: 100 (100)
I1127 10:53:45.963847  5256 net.cpp:157] Top shape: 100 (100)
I1127 10:53:45.963851  5256 net.cpp:165] Memory required for data: 314800
I1127 10:53:45.963855  5256 layer_factory.hpp:76] Creating layer conv1
I1127 10:53:45.963868  5256 net.cpp:106] Creating Layer conv1
I1127 10:53:45.963873  5256 net.cpp:454] conv1 <- data
I1127 10:53:45.963881  5256 net.cpp:411] conv1 -> conv1
I1127 10:53:45.964040  5256 net.cpp:150] Setting up conv1
I1127 10:53:45.964051  5256 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:53:45.964056  5256 net.cpp:165] Memory required for data: 4922800
I1127 10:53:45.964066  5256 layer_factory.hpp:76] Creating layer pool1
I1127 10:53:45.964074  5256 net.cpp:106] Creating Layer pool1
I1127 10:53:45.964078  5256 net.cpp:454] pool1 <- conv1
I1127 10:53:45.964105  5256 net.cpp:411] pool1 -> pool1
I1127 10:53:45.964139  5256 net.cpp:150] Setting up pool1
I1127 10:53:45.964148  5256 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:53:45.964151  5256 net.cpp:165] Memory required for data: 6074800
I1127 10:53:45.964156  5256 layer_factory.hpp:76] Creating layer conv2
I1127 10:53:45.964169  5256 net.cpp:106] Creating Layer conv2
I1127 10:53:45.964174  5256 net.cpp:454] conv2 <- pool1
I1127 10:53:45.964179  5256 net.cpp:411] conv2 -> conv2
I1127 10:53:45.964442  5256 net.cpp:150] Setting up conv2
I1127 10:53:45.964454  5256 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:53:45.964458  5256 net.cpp:165] Memory required for data: 7354800
I1127 10:53:45.964468  5256 layer_factory.hpp:76] Creating layer pool2
I1127 10:53:45.964475  5256 net.cpp:106] Creating Layer pool2
I1127 10:53:45.964479  5256 net.cpp:454] pool2 <- conv2
I1127 10:53:45.964488  5256 net.cpp:411] pool2 -> pool2
I1127 10:53:45.964524  5256 net.cpp:150] Setting up pool2
I1127 10:53:45.964530  5256 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:53:45.964534  5256 net.cpp:165] Memory required for data: 7674800
I1127 10:53:45.964540  5256 layer_factory.hpp:76] Creating layer ip1
I1127 10:53:45.964550  5256 net.cpp:106] Creating Layer ip1
I1127 10:53:45.964555  5256 net.cpp:454] ip1 <- pool2
I1127 10:53:45.964563  5256 net.cpp:411] ip1 -> ip1
I1127 10:53:45.967208  5256 net.cpp:150] Setting up ip1
I1127 10:53:45.967245  5256 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:53:45.967250  5256 net.cpp:165] Memory required for data: 7874800
I1127 10:53:45.967265  5256 layer_factory.hpp:76] Creating layer relu1
I1127 10:53:45.967277  5256 net.cpp:106] Creating Layer relu1
I1127 10:53:45.967283  5256 net.cpp:454] relu1 <- ip1
I1127 10:53:45.967290  5256 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:53:45.967300  5256 net.cpp:150] Setting up relu1
I1127 10:53:45.967305  5256 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:53:45.967309  5256 net.cpp:165] Memory required for data: 8074800
I1127 10:53:45.967314  5256 layer_factory.hpp:76] Creating layer ip2
I1127 10:53:45.967326  5256 net.cpp:106] Creating Layer ip2
I1127 10:53:45.967330  5256 net.cpp:454] ip2 <- ip1
I1127 10:53:45.967339  5256 net.cpp:411] ip2 -> ip2
I1127 10:53:45.967453  5256 net.cpp:150] Setting up ip2
I1127 10:53:45.967461  5256 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:53:45.967465  5256 net.cpp:165] Memory required for data: 8078800
I1127 10:53:45.967473  5256 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:53:45.967479  5256 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:53:45.967483  5256 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:53:45.967489  5256 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:53:45.967499  5256 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:53:45.967526  5256 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:53:45.967533  5256 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:53:45.967538  5256 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:53:45.967543  5256 net.cpp:165] Memory required for data: 8086800
I1127 10:53:45.967548  5256 layer_factory.hpp:76] Creating layer accuracy
I1127 10:53:45.967561  5256 net.cpp:106] Creating Layer accuracy
I1127 10:53:45.967566  5256 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:53:45.967571  5256 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:53:45.967576  5256 net.cpp:411] accuracy -> accuracy
I1127 10:53:45.967586  5256 net.cpp:150] Setting up accuracy
I1127 10:53:45.967592  5256 net.cpp:157] Top shape: (1)
I1127 10:53:45.967597  5256 net.cpp:165] Memory required for data: 8086804
I1127 10:53:45.967600  5256 layer_factory.hpp:76] Creating layer loss
I1127 10:53:45.967608  5256 net.cpp:106] Creating Layer loss
I1127 10:53:45.967613  5256 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:53:45.967618  5256 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:53:45.967624  5256 net.cpp:411] loss -> loss
I1127 10:53:45.967633  5256 layer_factory.hpp:76] Creating layer loss
I1127 10:53:45.967722  5256 net.cpp:150] Setting up loss
I1127 10:53:45.967730  5256 net.cpp:157] Top shape: (1)
I1127 10:53:45.967736  5256 net.cpp:160]     with loss weight 1
I1127 10:53:45.967751  5256 net.cpp:165] Memory required for data: 8086808
I1127 10:53:45.967756  5256 net.cpp:226] loss needs backward computation.
I1127 10:53:45.967766  5256 net.cpp:228] accuracy does not need backward computation.
I1127 10:53:45.967772  5256 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:53:45.967777  5256 net.cpp:226] ip2 needs backward computation.
I1127 10:53:45.967780  5256 net.cpp:226] relu1 needs backward computation.
I1127 10:53:45.967784  5256 net.cpp:226] ip1 needs backward computation.
I1127 10:53:45.967789  5256 net.cpp:226] pool2 needs backward computation.
I1127 10:53:45.967794  5256 net.cpp:226] conv2 needs backward computation.
I1127 10:53:45.967799  5256 net.cpp:226] pool1 needs backward computation.
I1127 10:53:45.967803  5256 net.cpp:226] conv1 needs backward computation.
I1127 10:53:45.967814  5256 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:53:45.967819  5256 net.cpp:228] mnist does not need backward computation.
I1127 10:53:45.967824  5256 net.cpp:270] This network produces output accuracy
I1127 10:53:45.967829  5256 net.cpp:270] This network produces output loss
I1127 10:53:45.967841  5256 net.cpp:283] Network initialization done.
I1127 10:53:45.967895  5256 solver.cpp:59] Solver scaffolding done.
I1127 10:53:45.968093  5256 caffe.cpp:212] Starting Optimization
I1127 10:53:45.968106  5256 solver.cpp:287] Solving LeNet
I1127 10:53:45.968109  5256 solver.cpp:288] Learning Rate Policy: inv
I1127 10:53:45.968693  5256 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:53:47.091907  5256 solver.cpp:408]     Test net output #0: accuracy = 0.051
I1127 10:53:47.091989  5256 solver.cpp:408]     Test net output #1: loss = 2.4124 (* 1 = 2.4124 loss)
I1127 10:53:47.103658  5256 solver.cpp:236] Iteration 0, loss = 2.40617
I1127 10:53:47.103732  5256 solver.cpp:252]     Train net output #0: loss = 2.40617 (* 1 = 2.40617 loss)
I1127 10:53:47.103768  5256 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:53:57.018461  5256 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:54:00.593102  5256 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:54:01.708204  5256 solver.cpp:408]     Test net output #0: accuracy = 0.9703
I1127 10:54:01.708276  5256 solver.cpp:408]     Test net output #1: loss = 0.0909487 (* 1 = 0.0909487 loss)
I1127 10:54:01.718679  5256 solver.cpp:236] Iteration 500, loss = 0.126786
I1127 10:54:01.718734  5256 solver.cpp:252]     Train net output #0: loss = 0.126786 (* 1 = 0.126786 loss)
I1127 10:54:01.718751  5256 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:54:15.188489  5256 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:54:15.204462  5256 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:54:15.215117  5256 solver.cpp:320] Iteration 1000, loss = 0.1144
I1127 10:54:15.215194  5256 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:54:17.351899  5256 solver.cpp:408]     Test net output #0: accuracy = 0.9801
I1127 10:54:17.352052  5256 solver.cpp:408]     Test net output #1: loss = 0.0591453 (* 1 = 0.0591453 loss)
I1127 10:54:17.352072  5256 solver.cpp:325] Optimization Done.
I1127 10:54:17.352082  5256 caffe.cpp:215] Optimization Done.
I1127 10:54:17.437078  5279 caffe.cpp:184] Using GPUs 0
I1127 10:54:17.833320  5279 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:54:17.833585  5279 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:54:17.834161  5279 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:54:17.834224  5279 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:54:17.834424  5279 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:54:17.834569  5279 layer_factory.hpp:76] Creating layer mnist
I1127 10:54:17.835165  5279 net.cpp:106] Creating Layer mnist
I1127 10:54:17.835202  5279 net.cpp:411] mnist -> data
I1127 10:54:17.835258  5279 net.cpp:411] mnist -> label
I1127 10:54:17.836251  5282 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:54:17.872256  5279 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:54:17.880290  5279 net.cpp:150] Setting up mnist
I1127 10:54:17.880378  5279 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:54:17.880409  5279 net.cpp:157] Top shape: 64 (64)
I1127 10:54:17.880429  5279 net.cpp:165] Memory required for data: 200960
I1127 10:54:17.880451  5279 layer_factory.hpp:76] Creating layer conv1
I1127 10:54:17.880491  5279 net.cpp:106] Creating Layer conv1
I1127 10:54:17.880507  5279 net.cpp:454] conv1 <- data
I1127 10:54:17.880542  5279 net.cpp:411] conv1 -> conv1
I1127 10:54:17.881950  5279 net.cpp:150] Setting up conv1
I1127 10:54:17.882007  5279 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:54:17.882025  5279 net.cpp:165] Memory required for data: 3150080
I1127 10:54:17.882067  5279 layer_factory.hpp:76] Creating layer pool1
I1127 10:54:17.882097  5279 net.cpp:106] Creating Layer pool1
I1127 10:54:17.882112  5279 net.cpp:454] pool1 <- conv1
I1127 10:54:17.882129  5279 net.cpp:411] pool1 -> pool1
I1127 10:54:17.882259  5279 net.cpp:150] Setting up pool1
I1127 10:54:17.882277  5279 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:54:17.882285  5279 net.cpp:165] Memory required for data: 3887360
I1127 10:54:17.882295  5279 layer_factory.hpp:76] Creating layer conv2
I1127 10:54:17.882316  5279 net.cpp:106] Creating Layer conv2
I1127 10:54:17.882328  5279 net.cpp:454] conv2 <- pool1
I1127 10:54:17.882344  5279 net.cpp:411] conv2 -> conv2
I1127 10:54:17.882918  5279 net.cpp:150] Setting up conv2
I1127 10:54:17.882946  5279 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:54:17.882957  5279 net.cpp:165] Memory required for data: 4706560
I1127 10:54:17.882979  5279 layer_factory.hpp:76] Creating layer pool2
I1127 10:54:17.883005  5279 net.cpp:106] Creating Layer pool2
I1127 10:54:17.883018  5279 net.cpp:454] pool2 <- conv2
I1127 10:54:17.883031  5279 net.cpp:411] pool2 -> pool2
I1127 10:54:17.883090  5279 net.cpp:150] Setting up pool2
I1127 10:54:17.883105  5279 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:54:17.883112  5279 net.cpp:165] Memory required for data: 4911360
I1127 10:54:17.883121  5279 layer_factory.hpp:76] Creating layer ip1
I1127 10:54:17.883138  5279 net.cpp:106] Creating Layer ip1
I1127 10:54:17.883148  5279 net.cpp:454] ip1 <- pool2
I1127 10:54:17.883164  5279 net.cpp:411] ip1 -> ip1
I1127 10:54:17.887408  5279 net.cpp:150] Setting up ip1
I1127 10:54:17.887472  5279 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:54:17.887490  5279 net.cpp:165] Memory required for data: 5039360
I1127 10:54:17.887542  5279 layer_factory.hpp:76] Creating layer relu1
I1127 10:54:17.887578  5279 net.cpp:106] Creating Layer relu1
I1127 10:54:17.887594  5279 net.cpp:454] relu1 <- ip1
I1127 10:54:17.887611  5279 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:54:17.887634  5279 net.cpp:150] Setting up relu1
I1127 10:54:17.887647  5279 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:54:17.887656  5279 net.cpp:165] Memory required for data: 5167360
I1127 10:54:17.887666  5279 layer_factory.hpp:76] Creating layer ip2
I1127 10:54:17.887689  5279 net.cpp:106] Creating Layer ip2
I1127 10:54:17.887701  5279 net.cpp:454] ip2 <- ip1
I1127 10:54:17.887714  5279 net.cpp:411] ip2 -> ip2
I1127 10:54:17.888730  5279 net.cpp:150] Setting up ip2
I1127 10:54:17.888777  5279 net.cpp:157] Top shape: 64 10 (640)
I1127 10:54:17.888792  5279 net.cpp:165] Memory required for data: 5169920
I1127 10:54:17.888814  5279 layer_factory.hpp:76] Creating layer loss
I1127 10:54:17.888841  5279 net.cpp:106] Creating Layer loss
I1127 10:54:17.888856  5279 net.cpp:454] loss <- ip2
I1127 10:54:17.888871  5279 net.cpp:454] loss <- label
I1127 10:54:17.888890  5279 net.cpp:411] loss -> loss
I1127 10:54:17.888922  5279 layer_factory.hpp:76] Creating layer loss
I1127 10:54:17.889107  5279 net.cpp:150] Setting up loss
I1127 10:54:17.889125  5279 net.cpp:157] Top shape: (1)
I1127 10:54:17.889134  5279 net.cpp:160]     with loss weight 1
I1127 10:54:17.889171  5279 net.cpp:165] Memory required for data: 5169924
I1127 10:54:17.889180  5279 net.cpp:226] loss needs backward computation.
I1127 10:54:17.889189  5279 net.cpp:226] ip2 needs backward computation.
I1127 10:54:17.889199  5279 net.cpp:226] relu1 needs backward computation.
I1127 10:54:17.889206  5279 net.cpp:226] ip1 needs backward computation.
I1127 10:54:17.889215  5279 net.cpp:226] pool2 needs backward computation.
I1127 10:54:17.889225  5279 net.cpp:226] conv2 needs backward computation.
I1127 10:54:17.889235  5279 net.cpp:226] pool1 needs backward computation.
I1127 10:54:17.889243  5279 net.cpp:226] conv1 needs backward computation.
I1127 10:54:17.889251  5279 net.cpp:228] mnist does not need backward computation.
I1127 10:54:17.889260  5279 net.cpp:270] This network produces output loss
I1127 10:54:17.889277  5279 net.cpp:283] Network initialization done.
I1127 10:54:17.889792  5279 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:54:17.889854  5279 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:54:17.890172  5279 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:54:17.890355  5279 layer_factory.hpp:76] Creating layer mnist
I1127 10:54:17.890681  5279 net.cpp:106] Creating Layer mnist
I1127 10:54:17.890714  5279 net.cpp:411] mnist -> data
I1127 10:54:17.890758  5279 net.cpp:411] mnist -> label
I1127 10:54:17.891882  5284 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:54:17.892189  5279 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:54:17.897570  5279 net.cpp:150] Setting up mnist
I1127 10:54:17.897614  5279 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:54:17.897645  5279 net.cpp:157] Top shape: 100 (100)
I1127 10:54:17.897668  5279 net.cpp:165] Memory required for data: 314000
I1127 10:54:17.897689  5279 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:54:17.897708  5279 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:54:17.897721  5279 net.cpp:454] label_mnist_1_split <- label
I1127 10:54:17.897735  5279 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:54:17.897758  5279 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:54:17.897825  5279 net.cpp:150] Setting up label_mnist_1_split
I1127 10:54:17.897842  5279 net.cpp:157] Top shape: 100 (100)
I1127 10:54:17.897852  5279 net.cpp:157] Top shape: 100 (100)
I1127 10:54:17.897867  5279 net.cpp:165] Memory required for data: 314800
I1127 10:54:17.897876  5279 layer_factory.hpp:76] Creating layer conv1
I1127 10:54:17.897897  5279 net.cpp:106] Creating Layer conv1
I1127 10:54:17.897910  5279 net.cpp:454] conv1 <- data
I1127 10:54:17.897929  5279 net.cpp:411] conv1 -> conv1
I1127 10:54:17.898391  5279 net.cpp:150] Setting up conv1
I1127 10:54:17.898424  5279 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:54:17.898445  5279 net.cpp:165] Memory required for data: 4922800
I1127 10:54:17.898481  5279 layer_factory.hpp:76] Creating layer pool1
I1127 10:54:17.898517  5279 net.cpp:106] Creating Layer pool1
I1127 10:54:17.898535  5279 net.cpp:454] pool1 <- conv1
I1127 10:54:17.898588  5279 net.cpp:411] pool1 -> pool1
I1127 10:54:17.898810  5279 net.cpp:150] Setting up pool1
I1127 10:54:17.898839  5279 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:54:17.898862  5279 net.cpp:165] Memory required for data: 6074800
I1127 10:54:17.898885  5279 layer_factory.hpp:76] Creating layer conv2
I1127 10:54:17.898926  5279 net.cpp:106] Creating Layer conv2
I1127 10:54:17.898952  5279 net.cpp:454] conv2 <- pool1
I1127 10:54:17.898982  5279 net.cpp:411] conv2 -> conv2
I1127 10:54:17.899884  5279 net.cpp:150] Setting up conv2
I1127 10:54:17.899915  5279 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:54:17.899940  5279 net.cpp:165] Memory required for data: 7354800
I1127 10:54:17.899977  5279 layer_factory.hpp:76] Creating layer pool2
I1127 10:54:17.900010  5279 net.cpp:106] Creating Layer pool2
I1127 10:54:17.900029  5279 net.cpp:454] pool2 <- conv2
I1127 10:54:17.900056  5279 net.cpp:411] pool2 -> pool2
I1127 10:54:17.900151  5279 net.cpp:150] Setting up pool2
I1127 10:54:17.900171  5279 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:54:17.900188  5279 net.cpp:165] Memory required for data: 7674800
I1127 10:54:17.900208  5279 layer_factory.hpp:76] Creating layer ip1
I1127 10:54:17.900243  5279 net.cpp:106] Creating Layer ip1
I1127 10:54:17.900266  5279 net.cpp:454] ip1 <- pool2
I1127 10:54:17.900303  5279 net.cpp:411] ip1 -> ip1
I1127 10:54:17.904495  5279 net.cpp:150] Setting up ip1
I1127 10:54:17.904579  5279 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:54:17.904599  5279 net.cpp:165] Memory required for data: 7874800
I1127 10:54:17.904630  5279 layer_factory.hpp:76] Creating layer relu1
I1127 10:54:17.904660  5279 net.cpp:106] Creating Layer relu1
I1127 10:54:17.904676  5279 net.cpp:454] relu1 <- ip1
I1127 10:54:17.904693  5279 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:54:17.904714  5279 net.cpp:150] Setting up relu1
I1127 10:54:17.904728  5279 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:54:17.904737  5279 net.cpp:165] Memory required for data: 8074800
I1127 10:54:17.904745  5279 layer_factory.hpp:76] Creating layer ip2
I1127 10:54:17.904770  5279 net.cpp:106] Creating Layer ip2
I1127 10:54:17.904781  5279 net.cpp:454] ip2 <- ip1
I1127 10:54:17.904798  5279 net.cpp:411] ip2 -> ip2
I1127 10:54:17.905086  5279 net.cpp:150] Setting up ip2
I1127 10:54:17.905102  5279 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:54:17.905112  5279 net.cpp:165] Memory required for data: 8078800
I1127 10:54:17.905128  5279 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:54:17.905143  5279 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:54:17.905151  5279 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:54:17.905166  5279 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:54:17.905182  5279 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:54:17.905251  5279 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:54:17.905266  5279 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:54:17.905277  5279 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:54:17.905284  5279 net.cpp:165] Memory required for data: 8086800
I1127 10:54:17.905293  5279 layer_factory.hpp:76] Creating layer accuracy
I1127 10:54:17.905310  5279 net.cpp:106] Creating Layer accuracy
I1127 10:54:17.905320  5279 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:54:17.905330  5279 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:54:17.905345  5279 net.cpp:411] accuracy -> accuracy
I1127 10:54:17.905364  5279 net.cpp:150] Setting up accuracy
I1127 10:54:17.905376  5279 net.cpp:157] Top shape: (1)
I1127 10:54:17.905385  5279 net.cpp:165] Memory required for data: 8086804
I1127 10:54:17.905392  5279 layer_factory.hpp:76] Creating layer loss
I1127 10:54:17.905405  5279 net.cpp:106] Creating Layer loss
I1127 10:54:17.905414  5279 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:54:17.905424  5279 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:54:17.905436  5279 net.cpp:411] loss -> loss
I1127 10:54:17.905457  5279 layer_factory.hpp:76] Creating layer loss
I1127 10:54:17.905637  5279 net.cpp:150] Setting up loss
I1127 10:54:17.905652  5279 net.cpp:157] Top shape: (1)
I1127 10:54:17.905659  5279 net.cpp:160]     with loss weight 1
I1127 10:54:17.905683  5279 net.cpp:165] Memory required for data: 8086808
I1127 10:54:17.905691  5279 net.cpp:226] loss needs backward computation.
I1127 10:54:17.905706  5279 net.cpp:228] accuracy does not need backward computation.
I1127 10:54:17.905717  5279 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:54:17.905726  5279 net.cpp:226] ip2 needs backward computation.
I1127 10:54:17.905733  5279 net.cpp:226] relu1 needs backward computation.
I1127 10:54:17.905743  5279 net.cpp:226] ip1 needs backward computation.
I1127 10:54:17.905751  5279 net.cpp:226] pool2 needs backward computation.
I1127 10:54:17.905760  5279 net.cpp:226] conv2 needs backward computation.
I1127 10:54:17.905767  5279 net.cpp:226] pool1 needs backward computation.
I1127 10:54:17.905776  5279 net.cpp:226] conv1 needs backward computation.
I1127 10:54:17.905784  5279 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:54:17.905794  5279 net.cpp:228] mnist does not need backward computation.
I1127 10:54:17.905802  5279 net.cpp:270] This network produces output accuracy
I1127 10:54:17.905810  5279 net.cpp:270] This network produces output loss
I1127 10:54:17.905835  5279 net.cpp:283] Network initialization done.
I1127 10:54:17.905956  5279 solver.cpp:59] Solver scaffolding done.
I1127 10:54:17.906383  5279 caffe.cpp:212] Starting Optimization
I1127 10:54:17.906400  5279 solver.cpp:287] Solving LeNet
I1127 10:54:17.906409  5279 solver.cpp:288] Learning Rate Policy: inv
I1127 10:54:17.907258  5279 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:54:19.443912  5279 solver.cpp:408]     Test net output #0: accuracy = 0.0903
I1127 10:54:19.444041  5279 solver.cpp:408]     Test net output #1: loss = 2.37333 (* 1 = 2.37333 loss)
I1127 10:54:19.458734  5279 solver.cpp:236] Iteration 0, loss = 2.3081
I1127 10:54:19.458871  5279 solver.cpp:252]     Train net output #0: loss = 2.3081 (* 1 = 2.3081 loss)
I1127 10:54:19.458928  5279 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:54:32.611073  5279 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:54:34.129691  5279 solver.cpp:408]     Test net output #0: accuracy = 0.9717
I1127 10:54:34.129788  5279 solver.cpp:408]     Test net output #1: loss = 0.0941051 (* 1 = 0.0941051 loss)
I1127 10:54:34.142907  5279 solver.cpp:236] Iteration 500, loss = 0.117868
I1127 10:54:34.143004  5279 solver.cpp:252]     Train net output #0: loss = 0.117868 (* 1 = 0.117868 loss)
I1127 10:54:34.143035  5279 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:54:47.641881  5279 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:54:47.658686  5279 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:54:47.669062  5279 solver.cpp:320] Iteration 1000, loss = 0.0612103
I1127 10:54:47.669176  5279 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:54:48.769225  5279 solver.cpp:408]     Test net output #0: accuracy = 0.981
I1127 10:54:48.769290  5279 solver.cpp:408]     Test net output #1: loss = 0.0573533 (* 1 = 0.0573533 loss)
I1127 10:54:48.769300  5279 solver.cpp:325] Optimization Done.
I1127 10:54:48.769309  5279 caffe.cpp:215] Optimization Done.
I1127 10:54:48.867363  5346 caffe.cpp:184] Using GPUs 0
I1127 10:54:49.287896  5346 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:54:49.288008  5346 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:54:49.288260  5346 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:54:49.288275  5346 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:54:49.288359  5346 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:54:49.288415  5346 layer_factory.hpp:76] Creating layer mnist
I1127 10:54:49.334254  5346 net.cpp:106] Creating Layer mnist
I1127 10:54:49.334295  5346 net.cpp:411] mnist -> data
I1127 10:54:49.334327  5346 net.cpp:411] mnist -> label
I1127 10:54:49.335069  5354 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:54:49.370764  5346 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:54:49.377482  5346 net.cpp:150] Setting up mnist
I1127 10:54:49.377502  5346 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:54:49.377509  5346 net.cpp:157] Top shape: 64 (64)
I1127 10:54:49.377513  5346 net.cpp:165] Memory required for data: 200960
I1127 10:54:49.377522  5346 layer_factory.hpp:76] Creating layer conv1
I1127 10:54:49.377538  5346 net.cpp:106] Creating Layer conv1
I1127 10:54:49.377545  5346 net.cpp:454] conv1 <- data
I1127 10:54:49.377555  5346 net.cpp:411] conv1 -> conv1
I1127 10:54:49.378136  5346 net.cpp:150] Setting up conv1
I1127 10:54:49.378154  5346 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:54:49.378159  5346 net.cpp:165] Memory required for data: 3150080
I1127 10:54:49.378171  5346 layer_factory.hpp:76] Creating layer pool1
I1127 10:54:49.378182  5346 net.cpp:106] Creating Layer pool1
I1127 10:54:49.378187  5346 net.cpp:454] pool1 <- conv1
I1127 10:54:49.378193  5346 net.cpp:411] pool1 -> pool1
I1127 10:54:49.378238  5346 net.cpp:150] Setting up pool1
I1127 10:54:49.378247  5346 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:54:49.378250  5346 net.cpp:165] Memory required for data: 3887360
I1127 10:54:49.378254  5346 layer_factory.hpp:76] Creating layer conv2
I1127 10:54:49.378263  5346 net.cpp:106] Creating Layer conv2
I1127 10:54:49.378268  5346 net.cpp:454] conv2 <- pool1
I1127 10:54:49.378278  5346 net.cpp:411] conv2 -> conv2
I1127 10:54:49.379019  5346 net.cpp:150] Setting up conv2
I1127 10:54:49.379029  5346 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:54:49.379034  5346 net.cpp:165] Memory required for data: 4706560
I1127 10:54:49.379041  5346 layer_factory.hpp:76] Creating layer pool2
I1127 10:54:49.379050  5346 net.cpp:106] Creating Layer pool2
I1127 10:54:49.379055  5346 net.cpp:454] pool2 <- conv2
I1127 10:54:49.379062  5346 net.cpp:411] pool2 -> pool2
I1127 10:54:49.379089  5346 net.cpp:150] Setting up pool2
I1127 10:54:49.379096  5346 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:54:49.379103  5346 net.cpp:165] Memory required for data: 4911360
I1127 10:54:49.379107  5346 layer_factory.hpp:76] Creating layer ip1
I1127 10:54:49.379117  5346 net.cpp:106] Creating Layer ip1
I1127 10:54:49.379122  5346 net.cpp:454] ip1 <- pool2
I1127 10:54:49.379128  5346 net.cpp:411] ip1 -> ip1
I1127 10:54:49.381405  5346 net.cpp:150] Setting up ip1
I1127 10:54:49.381435  5346 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:54:49.381439  5346 net.cpp:165] Memory required for data: 5039360
I1127 10:54:49.381453  5346 layer_factory.hpp:76] Creating layer relu1
I1127 10:54:49.381463  5346 net.cpp:106] Creating Layer relu1
I1127 10:54:49.381467  5346 net.cpp:454] relu1 <- ip1
I1127 10:54:49.381475  5346 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:54:49.381487  5346 net.cpp:150] Setting up relu1
I1127 10:54:49.381494  5346 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:54:49.381497  5346 net.cpp:165] Memory required for data: 5167360
I1127 10:54:49.381501  5346 layer_factory.hpp:76] Creating layer ip2
I1127 10:54:49.381516  5346 net.cpp:106] Creating Layer ip2
I1127 10:54:49.381521  5346 net.cpp:454] ip2 <- ip1
I1127 10:54:49.381528  5346 net.cpp:411] ip2 -> ip2
I1127 10:54:49.381997  5346 net.cpp:150] Setting up ip2
I1127 10:54:49.382009  5346 net.cpp:157] Top shape: 64 10 (640)
I1127 10:54:49.382014  5346 net.cpp:165] Memory required for data: 5169920
I1127 10:54:49.382021  5346 layer_factory.hpp:76] Creating layer loss
I1127 10:54:49.382031  5346 net.cpp:106] Creating Layer loss
I1127 10:54:49.382036  5346 net.cpp:454] loss <- ip2
I1127 10:54:49.382041  5346 net.cpp:454] loss <- label
I1127 10:54:49.382050  5346 net.cpp:411] loss -> loss
I1127 10:54:49.382061  5346 layer_factory.hpp:76] Creating layer loss
I1127 10:54:49.382129  5346 net.cpp:150] Setting up loss
I1127 10:54:49.382138  5346 net.cpp:157] Top shape: (1)
I1127 10:54:49.382150  5346 net.cpp:160]     with loss weight 1
I1127 10:54:49.382169  5346 net.cpp:165] Memory required for data: 5169924
I1127 10:54:49.382174  5346 net.cpp:226] loss needs backward computation.
I1127 10:54:49.382179  5346 net.cpp:226] ip2 needs backward computation.
I1127 10:54:49.382182  5346 net.cpp:226] relu1 needs backward computation.
I1127 10:54:49.382186  5346 net.cpp:226] ip1 needs backward computation.
I1127 10:54:49.382190  5346 net.cpp:226] pool2 needs backward computation.
I1127 10:54:49.382194  5346 net.cpp:226] conv2 needs backward computation.
I1127 10:54:49.382200  5346 net.cpp:226] pool1 needs backward computation.
I1127 10:54:49.382203  5346 net.cpp:226] conv1 needs backward computation.
I1127 10:54:49.382208  5346 net.cpp:228] mnist does not need backward computation.
I1127 10:54:49.382212  5346 net.cpp:270] This network produces output loss
I1127 10:54:49.382220  5346 net.cpp:283] Network initialization done.
I1127 10:54:49.382465  5346 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:54:49.382489  5346 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:54:49.382602  5346 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:54:49.382665  5346 layer_factory.hpp:76] Creating layer mnist
I1127 10:54:49.382762  5346 net.cpp:106] Creating Layer mnist
I1127 10:54:49.382771  5346 net.cpp:411] mnist -> data
I1127 10:54:49.382781  5346 net.cpp:411] mnist -> label
I1127 10:54:49.383716  5356 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:54:49.383832  5346 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:54:49.388222  5346 net.cpp:150] Setting up mnist
I1127 10:54:49.388285  5346 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:54:49.388298  5346 net.cpp:157] Top shape: 100 (100)
I1127 10:54:49.388305  5346 net.cpp:165] Memory required for data: 314000
I1127 10:54:49.388316  5346 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:54:49.388335  5346 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:54:49.388345  5346 net.cpp:454] label_mnist_1_split <- label
I1127 10:54:49.388360  5346 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:54:49.388376  5346 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:54:49.388444  5346 net.cpp:150] Setting up label_mnist_1_split
I1127 10:54:49.388459  5346 net.cpp:157] Top shape: 100 (100)
I1127 10:54:49.388469  5346 net.cpp:157] Top shape: 100 (100)
I1127 10:54:49.388478  5346 net.cpp:165] Memory required for data: 314800
I1127 10:54:49.388485  5346 layer_factory.hpp:76] Creating layer conv1
I1127 10:54:49.388509  5346 net.cpp:106] Creating Layer conv1
I1127 10:54:49.388520  5346 net.cpp:454] conv1 <- data
I1127 10:54:49.388532  5346 net.cpp:411] conv1 -> conv1
I1127 10:54:49.388790  5346 net.cpp:150] Setting up conv1
I1127 10:54:49.388803  5346 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:54:49.388809  5346 net.cpp:165] Memory required for data: 4922800
I1127 10:54:49.388823  5346 layer_factory.hpp:76] Creating layer pool1
I1127 10:54:49.388833  5346 net.cpp:106] Creating Layer pool1
I1127 10:54:49.388839  5346 net.cpp:454] pool1 <- conv1
I1127 10:54:49.388865  5346 net.cpp:411] pool1 -> pool1
I1127 10:54:49.388907  5346 net.cpp:150] Setting up pool1
I1127 10:54:49.388917  5346 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:54:49.388922  5346 net.cpp:165] Memory required for data: 6074800
I1127 10:54:49.388928  5346 layer_factory.hpp:76] Creating layer conv2
I1127 10:54:49.388942  5346 net.cpp:106] Creating Layer conv2
I1127 10:54:49.388948  5346 net.cpp:454] conv2 <- pool1
I1127 10:54:49.388958  5346 net.cpp:411] conv2 -> conv2
I1127 10:54:49.389490  5346 net.cpp:150] Setting up conv2
I1127 10:54:49.389511  5346 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:54:49.389520  5346 net.cpp:165] Memory required for data: 7354800
I1127 10:54:49.389536  5346 layer_factory.hpp:76] Creating layer pool2
I1127 10:54:49.389550  5346 net.cpp:106] Creating Layer pool2
I1127 10:54:49.389559  5346 net.cpp:454] pool2 <- conv2
I1127 10:54:49.389571  5346 net.cpp:411] pool2 -> pool2
I1127 10:54:49.389618  5346 net.cpp:150] Setting up pool2
I1127 10:54:49.389631  5346 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:54:49.389639  5346 net.cpp:165] Memory required for data: 7674800
I1127 10:54:49.389647  5346 layer_factory.hpp:76] Creating layer ip1
I1127 10:54:49.389664  5346 net.cpp:106] Creating Layer ip1
I1127 10:54:49.389673  5346 net.cpp:454] ip1 <- pool2
I1127 10:54:49.389685  5346 net.cpp:411] ip1 -> ip1
I1127 10:54:49.392707  5346 net.cpp:150] Setting up ip1
I1127 10:54:49.392743  5346 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:54:49.392748  5346 net.cpp:165] Memory required for data: 7874800
I1127 10:54:49.392761  5346 layer_factory.hpp:76] Creating layer relu1
I1127 10:54:49.392771  5346 net.cpp:106] Creating Layer relu1
I1127 10:54:49.392777  5346 net.cpp:454] relu1 <- ip1
I1127 10:54:49.392783  5346 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:54:49.392793  5346 net.cpp:150] Setting up relu1
I1127 10:54:49.392798  5346 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:54:49.392803  5346 net.cpp:165] Memory required for data: 8074800
I1127 10:54:49.392815  5346 layer_factory.hpp:76] Creating layer ip2
I1127 10:54:49.392832  5346 net.cpp:106] Creating Layer ip2
I1127 10:54:49.392838  5346 net.cpp:454] ip2 <- ip1
I1127 10:54:49.392844  5346 net.cpp:411] ip2 -> ip2
I1127 10:54:49.392946  5346 net.cpp:150] Setting up ip2
I1127 10:54:49.392954  5346 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:54:49.392958  5346 net.cpp:165] Memory required for data: 8078800
I1127 10:54:49.392964  5346 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:54:49.392971  5346 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:54:49.392976  5346 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:54:49.392982  5346 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:54:49.392987  5346 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:54:49.393017  5346 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:54:49.393023  5346 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:54:49.393031  5346 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:54:49.393038  5346 net.cpp:165] Memory required for data: 8086800
I1127 10:54:49.393045  5346 layer_factory.hpp:76] Creating layer accuracy
I1127 10:54:49.393059  5346 net.cpp:106] Creating Layer accuracy
I1127 10:54:49.393067  5346 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:54:49.393076  5346 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:54:49.393087  5346 net.cpp:411] accuracy -> accuracy
I1127 10:54:49.393105  5346 net.cpp:150] Setting up accuracy
I1127 10:54:49.393113  5346 net.cpp:157] Top shape: (1)
I1127 10:54:49.393121  5346 net.cpp:165] Memory required for data: 8086804
I1127 10:54:49.393128  5346 layer_factory.hpp:76] Creating layer loss
I1127 10:54:49.393139  5346 net.cpp:106] Creating Layer loss
I1127 10:54:49.393146  5346 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:54:49.393156  5346 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:54:49.393168  5346 net.cpp:411] loss -> loss
I1127 10:54:49.393183  5346 layer_factory.hpp:76] Creating layer loss
I1127 10:54:49.393312  5346 net.cpp:150] Setting up loss
I1127 10:54:49.393328  5346 net.cpp:157] Top shape: (1)
I1127 10:54:49.393335  5346 net.cpp:160]     with loss weight 1
I1127 10:54:49.393362  5346 net.cpp:165] Memory required for data: 8086808
I1127 10:54:49.393367  5346 net.cpp:226] loss needs backward computation.
I1127 10:54:49.393378  5346 net.cpp:228] accuracy does not need backward computation.
I1127 10:54:49.393383  5346 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:54:49.393388  5346 net.cpp:226] ip2 needs backward computation.
I1127 10:54:49.393391  5346 net.cpp:226] relu1 needs backward computation.
I1127 10:54:49.393395  5346 net.cpp:226] ip1 needs backward computation.
I1127 10:54:49.393400  5346 net.cpp:226] pool2 needs backward computation.
I1127 10:54:49.393405  5346 net.cpp:226] conv2 needs backward computation.
I1127 10:54:49.393409  5346 net.cpp:226] pool1 needs backward computation.
I1127 10:54:49.393414  5346 net.cpp:226] conv1 needs backward computation.
I1127 10:54:49.393419  5346 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:54:49.393424  5346 net.cpp:228] mnist does not need backward computation.
I1127 10:54:49.393427  5346 net.cpp:270] This network produces output accuracy
I1127 10:54:49.393431  5346 net.cpp:270] This network produces output loss
I1127 10:54:49.393442  5346 net.cpp:283] Network initialization done.
I1127 10:54:49.393525  5346 solver.cpp:59] Solver scaffolding done.
I1127 10:54:49.393738  5346 caffe.cpp:212] Starting Optimization
I1127 10:54:49.393744  5346 solver.cpp:287] Solving LeNet
I1127 10:54:49.393748  5346 solver.cpp:288] Learning Rate Policy: inv
I1127 10:54:49.394208  5346 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:54:52.123013  5346 solver.cpp:408]     Test net output #0: accuracy = 0.0744
I1127 10:54:52.123064  5346 solver.cpp:408]     Test net output #1: loss = 2.37694 (* 1 = 2.37694 loss)
I1127 10:54:52.137188  5346 solver.cpp:236] Iteration 0, loss = 2.4194
I1127 10:54:52.137313  5346 solver.cpp:252]     Train net output #0: loss = 2.4194 (* 1 = 2.4194 loss)
I1127 10:54:52.137390  5346 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:54:57.351519  5346 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:55:04.108160  5346 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:55:06.772557  5346 solver.cpp:408]     Test net output #0: accuracy = 0.9735
I1127 10:55:06.772681  5346 solver.cpp:408]     Test net output #1: loss = 0.0834849 (* 1 = 0.0834849 loss)
I1127 10:55:06.786051  5346 solver.cpp:236] Iteration 500, loss = 0.113924
I1127 10:55:06.786274  5346 solver.cpp:252]     Train net output #0: loss = 0.113924 (* 1 = 0.113924 loss)
I1127 10:55:06.786353  5346 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:55:20.059939  5346 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:55:20.083689  5346 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:55:20.112778  5346 solver.cpp:320] Iteration 1000, loss = 0.103711
I1127 10:55:20.112850  5346 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:55:21.383669  5346 solver.cpp:408]     Test net output #0: accuracy = 0.982
I1127 10:55:21.383816  5346 solver.cpp:408]     Test net output #1: loss = 0.056918 (* 1 = 0.056918 loss)
I1127 10:55:21.383832  5346 solver.cpp:325] Optimization Done.
I1127 10:55:21.383841  5346 caffe.cpp:215] Optimization Done.
I1127 10:55:21.507421  5424 caffe.cpp:184] Using GPUs 0
I1127 10:55:21.862551  5424 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:55:21.862828  5424 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:55:21.863373  5424 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:55:21.863418  5424 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:55:21.863626  5424 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:55:21.863823  5424 layer_factory.hpp:76] Creating layer mnist
I1127 10:55:21.864691  5424 net.cpp:106] Creating Layer mnist
I1127 10:55:21.864753  5424 net.cpp:411] mnist -> data
I1127 10:55:21.864837  5424 net.cpp:411] mnist -> label
I1127 10:55:21.866267  5428 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:55:21.883316  5424 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:55:21.885162  5424 net.cpp:150] Setting up mnist
I1127 10:55:21.885268  5424 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:55:21.885282  5424 net.cpp:157] Top shape: 64 (64)
I1127 10:55:21.885288  5424 net.cpp:165] Memory required for data: 200960
I1127 10:55:21.885303  5424 layer_factory.hpp:76] Creating layer conv1
I1127 10:55:21.885330  5424 net.cpp:106] Creating Layer conv1
I1127 10:55:21.885349  5424 net.cpp:454] conv1 <- data
I1127 10:55:21.885377  5424 net.cpp:411] conv1 -> conv1
I1127 10:55:21.889487  5424 net.cpp:150] Setting up conv1
I1127 10:55:21.889571  5424 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:55:21.889585  5424 net.cpp:165] Memory required for data: 3150080
I1127 10:55:21.889621  5424 layer_factory.hpp:76] Creating layer pool1
I1127 10:55:21.889654  5424 net.cpp:106] Creating Layer pool1
I1127 10:55:21.889667  5424 net.cpp:454] pool1 <- conv1
I1127 10:55:21.889685  5424 net.cpp:411] pool1 -> pool1
I1127 10:55:21.889821  5424 net.cpp:150] Setting up pool1
I1127 10:55:21.889842  5424 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:55:21.889852  5424 net.cpp:165] Memory required for data: 3887360
I1127 10:55:21.889861  5424 layer_factory.hpp:76] Creating layer conv2
I1127 10:55:21.889883  5424 net.cpp:106] Creating Layer conv2
I1127 10:55:21.889895  5424 net.cpp:454] conv2 <- pool1
I1127 10:55:21.889914  5424 net.cpp:411] conv2 -> conv2
I1127 10:55:21.890424  5424 net.cpp:150] Setting up conv2
I1127 10:55:21.890457  5424 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:55:21.890465  5424 net.cpp:165] Memory required for data: 4706560
I1127 10:55:21.890481  5424 layer_factory.hpp:76] Creating layer pool2
I1127 10:55:21.890501  5424 net.cpp:106] Creating Layer pool2
I1127 10:55:21.890508  5424 net.cpp:454] pool2 <- conv2
I1127 10:55:21.890518  5424 net.cpp:411] pool2 -> pool2
I1127 10:55:21.890561  5424 net.cpp:150] Setting up pool2
I1127 10:55:21.890571  5424 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:55:21.890578  5424 net.cpp:165] Memory required for data: 4911360
I1127 10:55:21.890583  5424 layer_factory.hpp:76] Creating layer ip1
I1127 10:55:21.890597  5424 net.cpp:106] Creating Layer ip1
I1127 10:55:21.890604  5424 net.cpp:454] ip1 <- pool2
I1127 10:55:21.890614  5424 net.cpp:411] ip1 -> ip1
I1127 10:55:21.895107  5424 net.cpp:150] Setting up ip1
I1127 10:55:21.895198  5424 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:55:21.895206  5424 net.cpp:165] Memory required for data: 5039360
I1127 10:55:21.895238  5424 layer_factory.hpp:76] Creating layer relu1
I1127 10:55:21.895265  5424 net.cpp:106] Creating Layer relu1
I1127 10:55:21.895277  5424 net.cpp:454] relu1 <- ip1
I1127 10:55:21.895297  5424 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:55:21.895329  5424 net.cpp:150] Setting up relu1
I1127 10:55:21.895339  5424 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:55:21.895345  5424 net.cpp:165] Memory required for data: 5167360
I1127 10:55:21.895352  5424 layer_factory.hpp:76] Creating layer ip2
I1127 10:55:21.895370  5424 net.cpp:106] Creating Layer ip2
I1127 10:55:21.895376  5424 net.cpp:454] ip2 <- ip1
I1127 10:55:21.895390  5424 net.cpp:411] ip2 -> ip2
I1127 10:55:21.896600  5424 net.cpp:150] Setting up ip2
I1127 10:55:21.896683  5424 net.cpp:157] Top shape: 64 10 (640)
I1127 10:55:21.896692  5424 net.cpp:165] Memory required for data: 5169920
I1127 10:55:21.896715  5424 layer_factory.hpp:76] Creating layer loss
I1127 10:55:21.896756  5424 net.cpp:106] Creating Layer loss
I1127 10:55:21.896808  5424 net.cpp:454] loss <- ip2
I1127 10:55:21.896831  5424 net.cpp:454] loss <- label
I1127 10:55:21.896859  5424 net.cpp:411] loss -> loss
I1127 10:55:21.896919  5424 layer_factory.hpp:76] Creating layer loss
I1127 10:55:21.897164  5424 net.cpp:150] Setting up loss
I1127 10:55:21.897187  5424 net.cpp:157] Top shape: (1)
I1127 10:55:21.897198  5424 net.cpp:160]     with loss weight 1
I1127 10:55:21.897248  5424 net.cpp:165] Memory required for data: 5169924
I1127 10:55:21.897258  5424 net.cpp:226] loss needs backward computation.
I1127 10:55:21.897266  5424 net.cpp:226] ip2 needs backward computation.
I1127 10:55:21.897274  5424 net.cpp:226] relu1 needs backward computation.
I1127 10:55:21.897282  5424 net.cpp:226] ip1 needs backward computation.
I1127 10:55:21.897290  5424 net.cpp:226] pool2 needs backward computation.
I1127 10:55:21.897299  5424 net.cpp:226] conv2 needs backward computation.
I1127 10:55:21.897307  5424 net.cpp:226] pool1 needs backward computation.
I1127 10:55:21.897315  5424 net.cpp:226] conv1 needs backward computation.
I1127 10:55:21.897323  5424 net.cpp:228] mnist does not need backward computation.
I1127 10:55:21.897331  5424 net.cpp:270] This network produces output loss
I1127 10:55:21.897352  5424 net.cpp:283] Network initialization done.
I1127 10:55:21.897792  5424 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:55:21.897841  5424 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:55:21.898046  5424 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:55:21.898254  5424 layer_factory.hpp:76] Creating layer mnist
I1127 10:55:21.898509  5424 net.cpp:106] Creating Layer mnist
I1127 10:55:21.898531  5424 net.cpp:411] mnist -> data
I1127 10:55:21.898571  5424 net.cpp:411] mnist -> label
I1127 10:55:21.903872  5430 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:55:21.904352  5424 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:55:21.908187  5424 net.cpp:150] Setting up mnist
I1127 10:55:21.908290  5424 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:55:21.908318  5424 net.cpp:157] Top shape: 100 (100)
I1127 10:55:21.908339  5424 net.cpp:165] Memory required for data: 314000
I1127 10:55:21.908362  5424 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:55:21.908403  5424 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:55:21.908421  5424 net.cpp:454] label_mnist_1_split <- label
I1127 10:55:21.908442  5424 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:55:21.908471  5424 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:55:21.908627  5424 net.cpp:150] Setting up label_mnist_1_split
I1127 10:55:21.908668  5424 net.cpp:157] Top shape: 100 (100)
I1127 10:55:21.908691  5424 net.cpp:157] Top shape: 100 (100)
I1127 10:55:21.908705  5424 net.cpp:165] Memory required for data: 314800
I1127 10:55:21.908722  5424 layer_factory.hpp:76] Creating layer conv1
I1127 10:55:21.908769  5424 net.cpp:106] Creating Layer conv1
I1127 10:55:21.908788  5424 net.cpp:454] conv1 <- data
I1127 10:55:21.908818  5424 net.cpp:411] conv1 -> conv1
I1127 10:55:21.909420  5424 net.cpp:150] Setting up conv1
I1127 10:55:21.909442  5424 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:55:21.909451  5424 net.cpp:165] Memory required for data: 4922800
I1127 10:55:21.909473  5424 layer_factory.hpp:76] Creating layer pool1
I1127 10:55:21.909493  5424 net.cpp:106] Creating Layer pool1
I1127 10:55:21.909503  5424 net.cpp:454] pool1 <- conv1
I1127 10:55:21.909551  5424 net.cpp:411] pool1 -> pool1
I1127 10:55:21.909608  5424 net.cpp:150] Setting up pool1
I1127 10:55:21.909625  5424 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:55:21.909633  5424 net.cpp:165] Memory required for data: 6074800
I1127 10:55:21.909644  5424 layer_factory.hpp:76] Creating layer conv2
I1127 10:55:21.909672  5424 net.cpp:106] Creating Layer conv2
I1127 10:55:21.909684  5424 net.cpp:454] conv2 <- pool1
I1127 10:55:21.909703  5424 net.cpp:411] conv2 -> conv2
I1127 10:55:21.918256  5424 net.cpp:150] Setting up conv2
I1127 10:55:21.918429  5424 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:55:21.918483  5424 net.cpp:165] Memory required for data: 7354800
I1127 10:55:21.918576  5424 layer_factory.hpp:76] Creating layer pool2
I1127 10:55:21.918653  5424 net.cpp:106] Creating Layer pool2
I1127 10:55:21.918697  5424 net.cpp:454] pool2 <- conv2
I1127 10:55:21.922323  5424 net.cpp:411] pool2 -> pool2
I1127 10:55:21.922631  5424 net.cpp:150] Setting up pool2
I1127 10:55:21.922663  5424 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:55:21.922678  5424 net.cpp:165] Memory required for data: 7674800
I1127 10:55:21.922695  5424 layer_factory.hpp:76] Creating layer ip1
I1127 10:55:21.922740  5424 net.cpp:106] Creating Layer ip1
I1127 10:55:21.922760  5424 net.cpp:454] ip1 <- pool2
I1127 10:55:21.922783  5424 net.cpp:411] ip1 -> ip1
I1127 10:55:21.928007  5424 net.cpp:150] Setting up ip1
I1127 10:55:21.928148  5424 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:55:21.928189  5424 net.cpp:165] Memory required for data: 7874800
I1127 10:55:21.928261  5424 layer_factory.hpp:76] Creating layer relu1
I1127 10:55:21.928303  5424 net.cpp:106] Creating Layer relu1
I1127 10:55:21.928319  5424 net.cpp:454] relu1 <- ip1
I1127 10:55:21.928340  5424 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:55:21.928364  5424 net.cpp:150] Setting up relu1
I1127 10:55:21.928375  5424 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:55:21.928382  5424 net.cpp:165] Memory required for data: 8074800
I1127 10:55:21.928390  5424 layer_factory.hpp:76] Creating layer ip2
I1127 10:55:21.928414  5424 net.cpp:106] Creating Layer ip2
I1127 10:55:21.928423  5424 net.cpp:454] ip2 <- ip1
I1127 10:55:21.928439  5424 net.cpp:411] ip2 -> ip2
I1127 10:55:21.928731  5424 net.cpp:150] Setting up ip2
I1127 10:55:21.928761  5424 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:55:21.928778  5424 net.cpp:165] Memory required for data: 8078800
I1127 10:55:21.928794  5424 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:55:21.928830  5424 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:55:21.928843  5424 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:55:21.928856  5424 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:55:21.928882  5424 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:55:21.929286  5424 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:55:21.929323  5424 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:55:21.929338  5424 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:55:21.929347  5424 net.cpp:165] Memory required for data: 8086800
I1127 10:55:21.929358  5424 layer_factory.hpp:76] Creating layer accuracy
I1127 10:55:21.929385  5424 net.cpp:106] Creating Layer accuracy
I1127 10:55:21.929399  5424 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:55:21.929412  5424 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:55:21.929428  5424 net.cpp:411] accuracy -> accuracy
I1127 10:55:21.929450  5424 net.cpp:150] Setting up accuracy
I1127 10:55:21.929463  5424 net.cpp:157] Top shape: (1)
I1127 10:55:21.929471  5424 net.cpp:165] Memory required for data: 8086804
I1127 10:55:21.929479  5424 layer_factory.hpp:76] Creating layer loss
I1127 10:55:21.929502  5424 net.cpp:106] Creating Layer loss
I1127 10:55:21.929512  5424 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:55:21.929523  5424 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:55:21.929538  5424 net.cpp:411] loss -> loss
I1127 10:55:21.929558  5424 layer_factory.hpp:76] Creating layer loss
I1127 10:55:21.929776  5424 net.cpp:150] Setting up loss
I1127 10:55:21.929792  5424 net.cpp:157] Top shape: (1)
I1127 10:55:21.929800  5424 net.cpp:160]     with loss weight 1
I1127 10:55:21.929831  5424 net.cpp:165] Memory required for data: 8086808
I1127 10:55:21.929841  5424 net.cpp:226] loss needs backward computation.
I1127 10:55:21.929860  5424 net.cpp:228] accuracy does not need backward computation.
I1127 10:55:21.929872  5424 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:55:21.929880  5424 net.cpp:226] ip2 needs backward computation.
I1127 10:55:21.929888  5424 net.cpp:226] relu1 needs backward computation.
I1127 10:55:21.929896  5424 net.cpp:226] ip1 needs backward computation.
I1127 10:55:21.929905  5424 net.cpp:226] pool2 needs backward computation.
I1127 10:55:21.929915  5424 net.cpp:226] conv2 needs backward computation.
I1127 10:55:21.929925  5424 net.cpp:226] pool1 needs backward computation.
I1127 10:55:21.929936  5424 net.cpp:226] conv1 needs backward computation.
I1127 10:55:21.929946  5424 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:55:21.929957  5424 net.cpp:228] mnist does not need backward computation.
I1127 10:55:21.929965  5424 net.cpp:270] This network produces output accuracy
I1127 10:55:21.929975  5424 net.cpp:270] This network produces output loss
I1127 10:55:21.929997  5424 net.cpp:283] Network initialization done.
I1127 10:55:21.930176  5424 solver.cpp:59] Solver scaffolding done.
I1127 10:55:21.930774  5424 caffe.cpp:212] Starting Optimization
I1127 10:55:21.930790  5424 solver.cpp:287] Solving LeNet
I1127 10:55:21.930799  5424 solver.cpp:288] Learning Rate Policy: inv
I1127 10:55:21.932201  5424 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:55:24.407096  5424 solver.cpp:408]     Test net output #0: accuracy = 0.1728
I1127 10:55:24.407135  5424 solver.cpp:408]     Test net output #1: loss = 2.36101 (* 1 = 2.36101 loss)
I1127 10:55:24.439163  5424 solver.cpp:236] Iteration 0, loss = 2.35347
I1127 10:55:24.439187  5424 solver.cpp:252]     Train net output #0: loss = 2.35347 (* 1 = 2.35347 loss)
I1127 10:55:24.439203  5424 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:55:36.573740  5424 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:55:37.673830  5424 solver.cpp:408]     Test net output #0: accuracy = 0.9723
I1127 10:55:37.673938  5424 solver.cpp:408]     Test net output #1: loss = 0.0849995 (* 1 = 0.0849995 loss)
I1127 10:55:37.683913  5424 solver.cpp:236] Iteration 500, loss = 0.09859
I1127 10:55:37.683991  5424 solver.cpp:252]     Train net output #0: loss = 0.09859 (* 1 = 0.09859 loss)
I1127 10:55:37.684005  5424 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:55:51.114207  5424 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:55:51.131180  5424 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:55:51.143316  5424 solver.cpp:320] Iteration 1000, loss = 0.0754702
I1127 10:55:51.143400  5424 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:55:51.207406  5424 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:55:53.470077  5424 solver.cpp:408]     Test net output #0: accuracy = 0.9817
I1127 10:55:53.470191  5424 solver.cpp:408]     Test net output #1: loss = 0.0562022 (* 1 = 0.0562022 loss)
I1127 10:55:53.470199  5424 solver.cpp:325] Optimization Done.
I1127 10:55:53.470204  5424 caffe.cpp:215] Optimization Done.
I1127 10:55:53.537395  5454 caffe.cpp:184] Using GPUs 0
I1127 10:55:54.007329  5454 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:55:54.007457  5454 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:55:54.007843  5454 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:55:54.007863  5454 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:55:54.007992  5454 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:55:54.008060  5454 layer_factory.hpp:76] Creating layer mnist
I1127 10:55:54.008518  5454 net.cpp:106] Creating Layer mnist
I1127 10:55:54.008545  5454 net.cpp:411] mnist -> data
I1127 10:55:54.008575  5454 net.cpp:411] mnist -> label
I1127 10:55:54.009306  5457 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:55:54.045881  5454 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:55:54.053324  5454 net.cpp:150] Setting up mnist
I1127 10:55:54.053349  5454 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:55:54.053360  5454 net.cpp:157] Top shape: 64 (64)
I1127 10:55:54.053367  5454 net.cpp:165] Memory required for data: 200960
I1127 10:55:54.053380  5454 layer_factory.hpp:76] Creating layer conv1
I1127 10:55:54.053401  5454 net.cpp:106] Creating Layer conv1
I1127 10:55:54.053411  5454 net.cpp:454] conv1 <- data
I1127 10:55:54.053426  5454 net.cpp:411] conv1 -> conv1
I1127 10:55:54.054193  5454 net.cpp:150] Setting up conv1
I1127 10:55:54.054208  5454 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:55:54.054215  5454 net.cpp:165] Memory required for data: 3150080
I1127 10:55:54.054234  5454 layer_factory.hpp:76] Creating layer pool1
I1127 10:55:54.054246  5454 net.cpp:106] Creating Layer pool1
I1127 10:55:54.054253  5454 net.cpp:454] pool1 <- conv1
I1127 10:55:54.054265  5454 net.cpp:411] pool1 -> pool1
I1127 10:55:54.054581  5454 net.cpp:150] Setting up pool1
I1127 10:55:54.054600  5454 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:55:54.054606  5454 net.cpp:165] Memory required for data: 3887360
I1127 10:55:54.054615  5454 layer_factory.hpp:76] Creating layer conv2
I1127 10:55:54.054630  5454 net.cpp:106] Creating Layer conv2
I1127 10:55:54.054637  5454 net.cpp:454] conv2 <- pool1
I1127 10:55:54.054647  5454 net.cpp:411] conv2 -> conv2
I1127 10:55:54.055049  5454 net.cpp:150] Setting up conv2
I1127 10:55:54.055064  5454 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:55:54.055075  5454 net.cpp:165] Memory required for data: 4706560
I1127 10:55:54.055089  5454 layer_factory.hpp:76] Creating layer pool2
I1127 10:55:54.055101  5454 net.cpp:106] Creating Layer pool2
I1127 10:55:54.055110  5454 net.cpp:454] pool2 <- conv2
I1127 10:55:54.055120  5454 net.cpp:411] pool2 -> pool2
I1127 10:55:54.055169  5454 net.cpp:150] Setting up pool2
I1127 10:55:54.055181  5454 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:55:54.055187  5454 net.cpp:165] Memory required for data: 4911360
I1127 10:55:54.055196  5454 layer_factory.hpp:76] Creating layer ip1
I1127 10:55:54.055207  5454 net.cpp:106] Creating Layer ip1
I1127 10:55:54.055214  5454 net.cpp:454] ip1 <- pool2
I1127 10:55:54.055225  5454 net.cpp:411] ip1 -> ip1
I1127 10:55:54.058887  5454 net.cpp:150] Setting up ip1
I1127 10:55:54.058904  5454 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:55:54.058912  5454 net.cpp:165] Memory required for data: 5039360
I1127 10:55:54.058926  5454 layer_factory.hpp:76] Creating layer relu1
I1127 10:55:54.058938  5454 net.cpp:106] Creating Layer relu1
I1127 10:55:54.058944  5454 net.cpp:454] relu1 <- ip1
I1127 10:55:54.058954  5454 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:55:54.058966  5454 net.cpp:150] Setting up relu1
I1127 10:55:54.058975  5454 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:55:54.058984  5454 net.cpp:165] Memory required for data: 5167360
I1127 10:55:54.058990  5454 layer_factory.hpp:76] Creating layer ip2
I1127 10:55:54.059002  5454 net.cpp:106] Creating Layer ip2
I1127 10:55:54.059010  5454 net.cpp:454] ip2 <- ip1
I1127 10:55:54.059020  5454 net.cpp:411] ip2 -> ip2
I1127 10:55:54.059598  5454 net.cpp:150] Setting up ip2
I1127 10:55:54.059613  5454 net.cpp:157] Top shape: 64 10 (640)
I1127 10:55:54.059622  5454 net.cpp:165] Memory required for data: 5169920
I1127 10:55:54.059633  5454 layer_factory.hpp:76] Creating layer loss
I1127 10:55:54.059644  5454 net.cpp:106] Creating Layer loss
I1127 10:55:54.059653  5454 net.cpp:454] loss <- ip2
I1127 10:55:54.059660  5454 net.cpp:454] loss <- label
I1127 10:55:54.059675  5454 net.cpp:411] loss -> loss
I1127 10:55:54.059691  5454 layer_factory.hpp:76] Creating layer loss
I1127 10:55:54.059793  5454 net.cpp:150] Setting up loss
I1127 10:55:54.059806  5454 net.cpp:157] Top shape: (1)
I1127 10:55:54.059813  5454 net.cpp:160]     with loss weight 1
I1127 10:55:54.059834  5454 net.cpp:165] Memory required for data: 5169924
I1127 10:55:54.059846  5454 net.cpp:226] loss needs backward computation.
I1127 10:55:54.059855  5454 net.cpp:226] ip2 needs backward computation.
I1127 10:55:54.059862  5454 net.cpp:226] relu1 needs backward computation.
I1127 10:55:54.059870  5454 net.cpp:226] ip1 needs backward computation.
I1127 10:55:54.059876  5454 net.cpp:226] pool2 needs backward computation.
I1127 10:55:54.059885  5454 net.cpp:226] conv2 needs backward computation.
I1127 10:55:54.059893  5454 net.cpp:226] pool1 needs backward computation.
I1127 10:55:54.059900  5454 net.cpp:226] conv1 needs backward computation.
I1127 10:55:54.059908  5454 net.cpp:228] mnist does not need backward computation.
I1127 10:55:54.059916  5454 net.cpp:270] This network produces output loss
I1127 10:55:54.059928  5454 net.cpp:283] Network initialization done.
I1127 10:55:54.060293  5454 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:55:54.060325  5454 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:55:54.060493  5454 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:55:54.060583  5454 layer_factory.hpp:76] Creating layer mnist
I1127 10:55:54.060701  5454 net.cpp:106] Creating Layer mnist
I1127 10:55:54.060715  5454 net.cpp:411] mnist -> data
I1127 10:55:54.060730  5454 net.cpp:411] mnist -> label
I1127 10:55:54.061429  5459 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:55:54.061529  5454 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:55:54.065717  5454 net.cpp:150] Setting up mnist
I1127 10:55:54.065734  5454 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:55:54.065745  5454 net.cpp:157] Top shape: 100 (100)
I1127 10:55:54.065752  5454 net.cpp:165] Memory required for data: 314000
I1127 10:55:54.065759  5454 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:55:54.065770  5454 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:55:54.065778  5454 net.cpp:454] label_mnist_1_split <- label
I1127 10:55:54.065791  5454 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:55:54.065804  5454 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:55:54.065848  5454 net.cpp:150] Setting up label_mnist_1_split
I1127 10:55:54.065860  5454 net.cpp:157] Top shape: 100 (100)
I1127 10:55:54.065867  5454 net.cpp:157] Top shape: 100 (100)
I1127 10:55:54.065873  5454 net.cpp:165] Memory required for data: 314800
I1127 10:55:54.065881  5454 layer_factory.hpp:76] Creating layer conv1
I1127 10:55:54.065894  5454 net.cpp:106] Creating Layer conv1
I1127 10:55:54.065901  5454 net.cpp:454] conv1 <- data
I1127 10:55:54.065913  5454 net.cpp:411] conv1 -> conv1
I1127 10:55:54.066149  5454 net.cpp:150] Setting up conv1
I1127 10:55:54.066165  5454 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:55:54.066181  5454 net.cpp:165] Memory required for data: 4922800
I1127 10:55:54.066195  5454 layer_factory.hpp:76] Creating layer pool1
I1127 10:55:54.066206  5454 net.cpp:106] Creating Layer pool1
I1127 10:55:54.066213  5454 net.cpp:454] pool1 <- conv1
I1127 10:55:54.066231  5454 net.cpp:411] pool1 -> pool1
I1127 10:55:54.066273  5454 net.cpp:150] Setting up pool1
I1127 10:55:54.066284  5454 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:55:54.066293  5454 net.cpp:165] Memory required for data: 6074800
I1127 10:55:54.066299  5454 layer_factory.hpp:76] Creating layer conv2
I1127 10:55:54.066313  5454 net.cpp:106] Creating Layer conv2
I1127 10:55:54.066321  5454 net.cpp:454] conv2 <- pool1
I1127 10:55:54.066332  5454 net.cpp:411] conv2 -> conv2
I1127 10:55:54.067059  5454 net.cpp:150] Setting up conv2
I1127 10:55:54.067072  5454 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:55:54.067080  5454 net.cpp:165] Memory required for data: 7354800
I1127 10:55:54.067093  5454 layer_factory.hpp:76] Creating layer pool2
I1127 10:55:54.067103  5454 net.cpp:106] Creating Layer pool2
I1127 10:55:54.067114  5454 net.cpp:454] pool2 <- conv2
I1127 10:55:54.067127  5454 net.cpp:411] pool2 -> pool2
I1127 10:55:54.067169  5454 net.cpp:150] Setting up pool2
I1127 10:55:54.067181  5454 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:55:54.067188  5454 net.cpp:165] Memory required for data: 7674800
I1127 10:55:54.067195  5454 layer_factory.hpp:76] Creating layer ip1
I1127 10:55:54.067205  5454 net.cpp:106] Creating Layer ip1
I1127 10:55:54.067212  5454 net.cpp:454] ip1 <- pool2
I1127 10:55:54.067224  5454 net.cpp:411] ip1 -> ip1
I1127 10:55:54.070811  5454 net.cpp:150] Setting up ip1
I1127 10:55:54.070828  5454 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:55:54.070835  5454 net.cpp:165] Memory required for data: 7874800
I1127 10:55:54.070849  5454 layer_factory.hpp:76] Creating layer relu1
I1127 10:55:54.070861  5454 net.cpp:106] Creating Layer relu1
I1127 10:55:54.070869  5454 net.cpp:454] relu1 <- ip1
I1127 10:55:54.070878  5454 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:55:54.070889  5454 net.cpp:150] Setting up relu1
I1127 10:55:54.070899  5454 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:55:54.070906  5454 net.cpp:165] Memory required for data: 8074800
I1127 10:55:54.070914  5454 layer_factory.hpp:76] Creating layer ip2
I1127 10:55:54.070925  5454 net.cpp:106] Creating Layer ip2
I1127 10:55:54.070932  5454 net.cpp:454] ip2 <- ip1
I1127 10:55:54.070945  5454 net.cpp:411] ip2 -> ip2
I1127 10:55:54.071089  5454 net.cpp:150] Setting up ip2
I1127 10:55:54.071100  5454 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:55:54.071108  5454 net.cpp:165] Memory required for data: 8078800
I1127 10:55:54.071120  5454 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:55:54.071128  5454 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:55:54.071135  5454 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:55:54.071144  5454 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:55:54.071156  5454 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:55:54.071193  5454 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:55:54.071205  5454 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:55:54.071218  5454 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:55:54.071226  5454 net.cpp:165] Memory required for data: 8086800
I1127 10:55:54.071233  5454 layer_factory.hpp:76] Creating layer accuracy
I1127 10:55:54.071244  5454 net.cpp:106] Creating Layer accuracy
I1127 10:55:54.071250  5454 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:55:54.071259  5454 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:55:54.071269  5454 net.cpp:411] accuracy -> accuracy
I1127 10:55:54.071282  5454 net.cpp:150] Setting up accuracy
I1127 10:55:54.071291  5454 net.cpp:157] Top shape: (1)
I1127 10:55:54.071298  5454 net.cpp:165] Memory required for data: 8086804
I1127 10:55:54.071305  5454 layer_factory.hpp:76] Creating layer loss
I1127 10:55:54.071316  5454 net.cpp:106] Creating Layer loss
I1127 10:55:54.071323  5454 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:55:54.071331  5454 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:55:54.071341  5454 net.cpp:411] loss -> loss
I1127 10:55:54.071352  5454 layer_factory.hpp:76] Creating layer loss
I1127 10:55:54.071450  5454 net.cpp:150] Setting up loss
I1127 10:55:54.071463  5454 net.cpp:157] Top shape: (1)
I1127 10:55:54.071470  5454 net.cpp:160]     with loss weight 1
I1127 10:55:54.071481  5454 net.cpp:165] Memory required for data: 8086808
I1127 10:55:54.071488  5454 net.cpp:226] loss needs backward computation.
I1127 10:55:54.071497  5454 net.cpp:228] accuracy does not need backward computation.
I1127 10:55:54.071506  5454 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:55:54.071512  5454 net.cpp:226] ip2 needs backward computation.
I1127 10:55:54.071519  5454 net.cpp:226] relu1 needs backward computation.
I1127 10:55:54.071527  5454 net.cpp:226] ip1 needs backward computation.
I1127 10:55:54.071532  5454 net.cpp:226] pool2 needs backward computation.
I1127 10:55:54.071539  5454 net.cpp:226] conv2 needs backward computation.
I1127 10:55:54.071547  5454 net.cpp:226] pool1 needs backward computation.
I1127 10:55:54.071553  5454 net.cpp:226] conv1 needs backward computation.
I1127 10:55:54.071562  5454 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:55:54.071568  5454 net.cpp:228] mnist does not need backward computation.
I1127 10:55:54.071574  5454 net.cpp:270] This network produces output accuracy
I1127 10:55:54.071581  5454 net.cpp:270] This network produces output loss
I1127 10:55:54.071599  5454 net.cpp:283] Network initialization done.
I1127 10:55:54.071650  5454 solver.cpp:59] Solver scaffolding done.
I1127 10:55:54.071941  5454 caffe.cpp:212] Starting Optimization
I1127 10:55:54.071951  5454 solver.cpp:287] Solving LeNet
I1127 10:55:54.071957  5454 solver.cpp:288] Learning Rate Policy: inv
I1127 10:55:54.072365  5454 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:55:55.334449  5454 solver.cpp:408]     Test net output #0: accuracy = 0.0695
I1127 10:55:55.334573  5454 solver.cpp:408]     Test net output #1: loss = 2.38989 (* 1 = 2.38989 loss)
I1127 10:55:55.348645  5454 solver.cpp:236] Iteration 0, loss = 2.44007
I1127 10:55:55.348754  5454 solver.cpp:252]     Train net output #0: loss = 2.44007 (* 1 = 2.44007 loss)
I1127 10:55:55.348788  5454 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:55:59.323978  5454 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:56:08.798130  5454 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:56:10.178853  5454 solver.cpp:408]     Test net output #0: accuracy = 0.9715
I1127 10:56:10.178890  5454 solver.cpp:408]     Test net output #1: loss = 0.0901275 (* 1 = 0.0901275 loss)
I1127 10:56:10.207437  5454 solver.cpp:236] Iteration 500, loss = 0.0952911
I1127 10:56:10.207454  5454 solver.cpp:252]     Train net output #0: loss = 0.0952911 (* 1 = 0.0952911 loss)
I1127 10:56:10.207463  5454 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:56:23.422041  5454 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:56:23.437002  5454 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:56:23.447875  5454 solver.cpp:320] Iteration 1000, loss = 0.0753997
I1127 10:56:23.447923  5454 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:56:24.539206  5454 solver.cpp:408]     Test net output #0: accuracy = 0.9812
I1127 10:56:24.539371  5454 solver.cpp:408]     Test net output #1: loss = 0.0593292 (* 1 = 0.0593292 loss)
I1127 10:56:24.539388  5454 solver.cpp:325] Optimization Done.
I1127 10:56:24.539397  5454 caffe.cpp:215] Optimization Done.
I1127 10:56:24.683079  5479 caffe.cpp:184] Using GPUs 0
I1127 10:56:25.089284  5479 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:56:25.089535  5479 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:56:25.090051  5479 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:56:25.090095  5479 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:56:25.090350  5479 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:56:25.090512  5479 layer_factory.hpp:76] Creating layer mnist
I1127 10:56:25.091301  5479 net.cpp:106] Creating Layer mnist
I1127 10:56:25.091370  5479 net.cpp:411] mnist -> data
I1127 10:56:25.091444  5479 net.cpp:411] mnist -> label
I1127 10:56:25.092867  5483 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:56:25.130287  5479 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:56:25.138044  5479 net.cpp:150] Setting up mnist
I1127 10:56:25.138157  5479 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:56:25.138187  5479 net.cpp:157] Top shape: 64 (64)
I1127 10:56:25.138195  5479 net.cpp:165] Memory required for data: 200960
I1127 10:56:25.138217  5479 layer_factory.hpp:76] Creating layer conv1
I1127 10:56:25.138263  5479 net.cpp:106] Creating Layer conv1
I1127 10:56:25.138295  5479 net.cpp:454] conv1 <- data
I1127 10:56:25.138321  5479 net.cpp:411] conv1 -> conv1
I1127 10:56:25.139891  5479 net.cpp:150] Setting up conv1
I1127 10:56:25.139992  5479 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:56:25.140008  5479 net.cpp:165] Memory required for data: 3150080
I1127 10:56:25.140045  5479 layer_factory.hpp:76] Creating layer pool1
I1127 10:56:25.140080  5479 net.cpp:106] Creating Layer pool1
I1127 10:56:25.140094  5479 net.cpp:454] pool1 <- conv1
I1127 10:56:25.140111  5479 net.cpp:411] pool1 -> pool1
I1127 10:56:25.140261  5479 net.cpp:150] Setting up pool1
I1127 10:56:25.140276  5479 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:56:25.140285  5479 net.cpp:165] Memory required for data: 3887360
I1127 10:56:25.140292  5479 layer_factory.hpp:76] Creating layer conv2
I1127 10:56:25.140317  5479 net.cpp:106] Creating Layer conv2
I1127 10:56:25.140328  5479 net.cpp:454] conv2 <- pool1
I1127 10:56:25.140341  5479 net.cpp:411] conv2 -> conv2
I1127 10:56:25.140802  5479 net.cpp:150] Setting up conv2
I1127 10:56:25.140825  5479 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:56:25.140833  5479 net.cpp:165] Memory required for data: 4706560
I1127 10:56:25.140848  5479 layer_factory.hpp:76] Creating layer pool2
I1127 10:56:25.140862  5479 net.cpp:106] Creating Layer pool2
I1127 10:56:25.140871  5479 net.cpp:454] pool2 <- conv2
I1127 10:56:25.140879  5479 net.cpp:411] pool2 -> pool2
I1127 10:56:25.140930  5479 net.cpp:150] Setting up pool2
I1127 10:56:25.140944  5479 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:56:25.140949  5479 net.cpp:165] Memory required for data: 4911360
I1127 10:56:25.140957  5479 layer_factory.hpp:76] Creating layer ip1
I1127 10:56:25.140970  5479 net.cpp:106] Creating Layer ip1
I1127 10:56:25.140979  5479 net.cpp:454] ip1 <- pool2
I1127 10:56:25.140990  5479 net.cpp:411] ip1 -> ip1
I1127 10:56:25.145366  5479 net.cpp:150] Setting up ip1
I1127 10:56:25.145437  5479 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:56:25.145444  5479 net.cpp:165] Memory required for data: 5039360
I1127 10:56:25.145473  5479 layer_factory.hpp:76] Creating layer relu1
I1127 10:56:25.145493  5479 net.cpp:106] Creating Layer relu1
I1127 10:56:25.145505  5479 net.cpp:454] relu1 <- ip1
I1127 10:56:25.145519  5479 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:56:25.145537  5479 net.cpp:150] Setting up relu1
I1127 10:56:25.145547  5479 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:56:25.145555  5479 net.cpp:165] Memory required for data: 5167360
I1127 10:56:25.145561  5479 layer_factory.hpp:76] Creating layer ip2
I1127 10:56:25.145576  5479 net.cpp:106] Creating Layer ip2
I1127 10:56:25.145583  5479 net.cpp:454] ip2 <- ip1
I1127 10:56:25.145593  5479 net.cpp:411] ip2 -> ip2
I1127 10:56:25.146517  5479 net.cpp:150] Setting up ip2
I1127 10:56:25.146558  5479 net.cpp:157] Top shape: 64 10 (640)
I1127 10:56:25.146565  5479 net.cpp:165] Memory required for data: 5169920
I1127 10:56:25.146581  5479 layer_factory.hpp:76] Creating layer loss
I1127 10:56:25.146600  5479 net.cpp:106] Creating Layer loss
I1127 10:56:25.146610  5479 net.cpp:454] loss <- ip2
I1127 10:56:25.146620  5479 net.cpp:454] loss <- label
I1127 10:56:25.146636  5479 net.cpp:411] loss -> loss
I1127 10:56:25.146662  5479 layer_factory.hpp:76] Creating layer loss
I1127 10:56:25.146795  5479 net.cpp:150] Setting up loss
I1127 10:56:25.146811  5479 net.cpp:157] Top shape: (1)
I1127 10:56:25.146818  5479 net.cpp:160]     with loss weight 1
I1127 10:56:25.146874  5479 net.cpp:165] Memory required for data: 5169924
I1127 10:56:25.146883  5479 net.cpp:226] loss needs backward computation.
I1127 10:56:25.146893  5479 net.cpp:226] ip2 needs backward computation.
I1127 10:56:25.146900  5479 net.cpp:226] relu1 needs backward computation.
I1127 10:56:25.146908  5479 net.cpp:226] ip1 needs backward computation.
I1127 10:56:25.146917  5479 net.cpp:226] pool2 needs backward computation.
I1127 10:56:25.146926  5479 net.cpp:226] conv2 needs backward computation.
I1127 10:56:25.146935  5479 net.cpp:226] pool1 needs backward computation.
I1127 10:56:25.146967  5479 net.cpp:226] conv1 needs backward computation.
I1127 10:56:25.146976  5479 net.cpp:228] mnist does not need backward computation.
I1127 10:56:25.146986  5479 net.cpp:270] This network produces output loss
I1127 10:56:25.147002  5479 net.cpp:283] Network initialization done.
I1127 10:56:25.147548  5479 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:56:25.147624  5479 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:56:25.147862  5479 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:56:25.147965  5479 layer_factory.hpp:76] Creating layer mnist
I1127 10:56:25.148146  5479 net.cpp:106] Creating Layer mnist
I1127 10:56:25.148162  5479 net.cpp:411] mnist -> data
I1127 10:56:25.148181  5479 net.cpp:411] mnist -> label
I1127 10:56:25.149194  5485 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:56:25.149337  5479 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:56:25.154258  5479 net.cpp:150] Setting up mnist
I1127 10:56:25.154296  5479 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:56:25.154304  5479 net.cpp:157] Top shape: 100 (100)
I1127 10:56:25.154309  5479 net.cpp:165] Memory required for data: 314000
I1127 10:56:25.154316  5479 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:56:25.154330  5479 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:56:25.154335  5479 net.cpp:454] label_mnist_1_split <- label
I1127 10:56:25.154345  5479 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:56:25.154357  5479 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:56:25.154417  5479 net.cpp:150] Setting up label_mnist_1_split
I1127 10:56:25.154424  5479 net.cpp:157] Top shape: 100 (100)
I1127 10:56:25.154430  5479 net.cpp:157] Top shape: 100 (100)
I1127 10:56:25.154434  5479 net.cpp:165] Memory required for data: 314800
I1127 10:56:25.154439  5479 layer_factory.hpp:76] Creating layer conv1
I1127 10:56:25.154463  5479 net.cpp:106] Creating Layer conv1
I1127 10:56:25.154469  5479 net.cpp:454] conv1 <- data
I1127 10:56:25.154476  5479 net.cpp:411] conv1 -> conv1
I1127 10:56:25.154634  5479 net.cpp:150] Setting up conv1
I1127 10:56:25.154644  5479 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:56:25.154649  5479 net.cpp:165] Memory required for data: 4922800
I1127 10:56:25.154659  5479 layer_factory.hpp:76] Creating layer pool1
I1127 10:56:25.154666  5479 net.cpp:106] Creating Layer pool1
I1127 10:56:25.154671  5479 net.cpp:454] pool1 <- conv1
I1127 10:56:25.154690  5479 net.cpp:411] pool1 -> pool1
I1127 10:56:25.154722  5479 net.cpp:150] Setting up pool1
I1127 10:56:25.154729  5479 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:56:25.154734  5479 net.cpp:165] Memory required for data: 6074800
I1127 10:56:25.154738  5479 layer_factory.hpp:76] Creating layer conv2
I1127 10:56:25.154749  5479 net.cpp:106] Creating Layer conv2
I1127 10:56:25.154753  5479 net.cpp:454] conv2 <- pool1
I1127 10:56:25.154762  5479 net.cpp:411] conv2 -> conv2
I1127 10:56:25.155015  5479 net.cpp:150] Setting up conv2
I1127 10:56:25.155026  5479 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:56:25.155031  5479 net.cpp:165] Memory required for data: 7354800
I1127 10:56:25.155040  5479 layer_factory.hpp:76] Creating layer pool2
I1127 10:56:25.155047  5479 net.cpp:106] Creating Layer pool2
I1127 10:56:25.155052  5479 net.cpp:454] pool2 <- conv2
I1127 10:56:25.155058  5479 net.cpp:411] pool2 -> pool2
I1127 10:56:25.155087  5479 net.cpp:150] Setting up pool2
I1127 10:56:25.155094  5479 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:56:25.155098  5479 net.cpp:165] Memory required for data: 7674800
I1127 10:56:25.155103  5479 layer_factory.hpp:76] Creating layer ip1
I1127 10:56:25.155114  5479 net.cpp:106] Creating Layer ip1
I1127 10:56:25.155118  5479 net.cpp:454] ip1 <- pool2
I1127 10:56:25.155127  5479 net.cpp:411] ip1 -> ip1
I1127 10:56:25.157615  5479 net.cpp:150] Setting up ip1
I1127 10:56:25.157644  5479 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:56:25.157649  5479 net.cpp:165] Memory required for data: 7874800
I1127 10:56:25.157661  5479 layer_factory.hpp:76] Creating layer relu1
I1127 10:56:25.157673  5479 net.cpp:106] Creating Layer relu1
I1127 10:56:25.157680  5479 net.cpp:454] relu1 <- ip1
I1127 10:56:25.157687  5479 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:56:25.157696  5479 net.cpp:150] Setting up relu1
I1127 10:56:25.157701  5479 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:56:25.157706  5479 net.cpp:165] Memory required for data: 8074800
I1127 10:56:25.157711  5479 layer_factory.hpp:76] Creating layer ip2
I1127 10:56:25.157721  5479 net.cpp:106] Creating Layer ip2
I1127 10:56:25.157724  5479 net.cpp:454] ip2 <- ip1
I1127 10:56:25.157737  5479 net.cpp:411] ip2 -> ip2
I1127 10:56:25.157842  5479 net.cpp:150] Setting up ip2
I1127 10:56:25.157852  5479 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:56:25.157856  5479 net.cpp:165] Memory required for data: 8078800
I1127 10:56:25.157865  5479 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:56:25.157871  5479 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:56:25.157876  5479 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:56:25.157881  5479 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:56:25.157889  5479 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:56:25.157922  5479 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:56:25.157930  5479 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:56:25.157935  5479 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:56:25.157940  5479 net.cpp:165] Memory required for data: 8086800
I1127 10:56:25.157944  5479 layer_factory.hpp:76] Creating layer accuracy
I1127 10:56:25.157955  5479 net.cpp:106] Creating Layer accuracy
I1127 10:56:25.157960  5479 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:56:25.157966  5479 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:56:25.157973  5479 net.cpp:411] accuracy -> accuracy
I1127 10:56:25.157999  5479 net.cpp:150] Setting up accuracy
I1127 10:56:25.158005  5479 net.cpp:157] Top shape: (1)
I1127 10:56:25.158010  5479 net.cpp:165] Memory required for data: 8086804
I1127 10:56:25.158015  5479 layer_factory.hpp:76] Creating layer loss
I1127 10:56:25.158022  5479 net.cpp:106] Creating Layer loss
I1127 10:56:25.158027  5479 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:56:25.158032  5479 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:56:25.158038  5479 net.cpp:411] loss -> loss
I1127 10:56:25.158046  5479 layer_factory.hpp:76] Creating layer loss
I1127 10:56:25.158135  5479 net.cpp:150] Setting up loss
I1127 10:56:25.158155  5479 net.cpp:157] Top shape: (1)
I1127 10:56:25.158161  5479 net.cpp:160]     with loss weight 1
I1127 10:56:25.158190  5479 net.cpp:165] Memory required for data: 8086808
I1127 10:56:25.158200  5479 net.cpp:226] loss needs backward computation.
I1127 10:56:25.158215  5479 net.cpp:228] accuracy does not need backward computation.
I1127 10:56:25.158221  5479 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:56:25.158226  5479 net.cpp:226] ip2 needs backward computation.
I1127 10:56:25.158231  5479 net.cpp:226] relu1 needs backward computation.
I1127 10:56:25.158236  5479 net.cpp:226] ip1 needs backward computation.
I1127 10:56:25.158241  5479 net.cpp:226] pool2 needs backward computation.
I1127 10:56:25.158246  5479 net.cpp:226] conv2 needs backward computation.
I1127 10:56:25.158252  5479 net.cpp:226] pool1 needs backward computation.
I1127 10:56:25.158257  5479 net.cpp:226] conv1 needs backward computation.
I1127 10:56:25.158262  5479 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:56:25.158267  5479 net.cpp:228] mnist does not need backward computation.
I1127 10:56:25.158272  5479 net.cpp:270] This network produces output accuracy
I1127 10:56:25.158277  5479 net.cpp:270] This network produces output loss
I1127 10:56:25.158289  5479 net.cpp:283] Network initialization done.
I1127 10:56:25.158395  5479 solver.cpp:59] Solver scaffolding done.
I1127 10:56:25.158632  5479 caffe.cpp:212] Starting Optimization
I1127 10:56:25.158648  5479 solver.cpp:287] Solving LeNet
I1127 10:56:25.158653  5479 solver.cpp:288] Learning Rate Policy: inv
I1127 10:56:25.159358  5479 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:56:28.177222  5479 solver.cpp:408]     Test net output #0: accuracy = 0.0922
I1127 10:56:28.177297  5479 solver.cpp:408]     Test net output #1: loss = 2.33849 (* 1 = 2.33849 loss)
I1127 10:56:28.192186  5479 solver.cpp:236] Iteration 0, loss = 2.30106
I1127 10:56:28.192348  5479 solver.cpp:252]     Train net output #0: loss = 2.30106 (* 1 = 2.30106 loss)
I1127 10:56:28.192392  5479 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:56:41.645382  5479 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:56:42.734735  5479 solver.cpp:408]     Test net output #0: accuracy = 0.9738
I1127 10:56:42.734810  5479 solver.cpp:408]     Test net output #1: loss = 0.0861591 (* 1 = 0.0861591 loss)
I1127 10:56:42.744101  5479 solver.cpp:236] Iteration 500, loss = 0.0834449
I1127 10:56:42.744151  5479 solver.cpp:252]     Train net output #0: loss = 0.0834449 (* 1 = 0.0834449 loss)
I1127 10:56:42.744164  5479 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:56:54.888459  5479 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:56:54.909695  5479 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:56:54.938359  5479 solver.cpp:320] Iteration 1000, loss = 0.103829
I1127 10:56:54.938494  5479 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:56:56.335805  5479 solver.cpp:408]     Test net output #0: accuracy = 0.984
I1127 10:56:56.335867  5479 solver.cpp:408]     Test net output #1: loss = 0.0532527 (* 1 = 0.0532527 loss)
I1127 10:56:56.335875  5479 solver.cpp:325] Optimization Done.
I1127 10:56:56.335880  5479 caffe.cpp:215] Optimization Done.
I1127 10:56:56.416023  5517 caffe.cpp:184] Using GPUs 0
I1127 10:56:56.767616  5517 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:56:56.767748  5517 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:56:56.768043  5517 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:56:56.768060  5517 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:56:56.768157  5517 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:56:56.768229  5517 layer_factory.hpp:76] Creating layer mnist
I1127 10:56:56.768669  5517 net.cpp:106] Creating Layer mnist
I1127 10:56:56.768695  5517 net.cpp:411] mnist -> data
I1127 10:56:56.768723  5517 net.cpp:411] mnist -> label
I1127 10:56:56.769860  5521 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:56:56.783249  5517 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:56:56.784837  5517 net.cpp:150] Setting up mnist
I1127 10:56:56.784948  5517 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:56:56.784965  5517 net.cpp:157] Top shape: 64 (64)
I1127 10:56:56.784977  5517 net.cpp:165] Memory required for data: 200960
I1127 10:56:56.785002  5517 layer_factory.hpp:76] Creating layer conv1
I1127 10:56:56.785044  5517 net.cpp:106] Creating Layer conv1
I1127 10:56:56.785058  5517 net.cpp:454] conv1 <- data
I1127 10:56:56.785084  5517 net.cpp:411] conv1 -> conv1
I1127 10:56:56.786267  5517 net.cpp:150] Setting up conv1
I1127 10:56:56.786330  5517 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:56:56.786339  5517 net.cpp:165] Memory required for data: 3150080
I1127 10:56:56.786368  5517 layer_factory.hpp:76] Creating layer pool1
I1127 10:56:56.786387  5517 net.cpp:106] Creating Layer pool1
I1127 10:56:56.786396  5517 net.cpp:454] pool1 <- conv1
I1127 10:56:56.786407  5517 net.cpp:411] pool1 -> pool1
I1127 10:56:56.786475  5517 net.cpp:150] Setting up pool1
I1127 10:56:56.786486  5517 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:56:56.786492  5517 net.cpp:165] Memory required for data: 3887360
I1127 10:56:56.786499  5517 layer_factory.hpp:76] Creating layer conv2
I1127 10:56:56.786516  5517 net.cpp:106] Creating Layer conv2
I1127 10:56:56.786523  5517 net.cpp:454] conv2 <- pool1
I1127 10:56:56.786533  5517 net.cpp:411] conv2 -> conv2
I1127 10:56:56.787004  5517 net.cpp:150] Setting up conv2
I1127 10:56:56.787027  5517 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:56:56.787039  5517 net.cpp:165] Memory required for data: 4706560
I1127 10:56:56.787058  5517 layer_factory.hpp:76] Creating layer pool2
I1127 10:56:56.787080  5517 net.cpp:106] Creating Layer pool2
I1127 10:56:56.787091  5517 net.cpp:454] pool2 <- conv2
I1127 10:56:56.787107  5517 net.cpp:411] pool2 -> pool2
I1127 10:56:56.787191  5517 net.cpp:150] Setting up pool2
I1127 10:56:56.787207  5517 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:56:56.787214  5517 net.cpp:165] Memory required for data: 4911360
I1127 10:56:56.787223  5517 layer_factory.hpp:76] Creating layer ip1
I1127 10:56:56.787240  5517 net.cpp:106] Creating Layer ip1
I1127 10:56:56.787250  5517 net.cpp:454] ip1 <- pool2
I1127 10:56:56.787263  5517 net.cpp:411] ip1 -> ip1
I1127 10:56:56.790532  5517 net.cpp:150] Setting up ip1
I1127 10:56:56.790591  5517 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:56:56.790596  5517 net.cpp:165] Memory required for data: 5039360
I1127 10:56:56.790616  5517 layer_factory.hpp:76] Creating layer relu1
I1127 10:56:56.790629  5517 net.cpp:106] Creating Layer relu1
I1127 10:56:56.790635  5517 net.cpp:454] relu1 <- ip1
I1127 10:56:56.790645  5517 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:56:56.790664  5517 net.cpp:150] Setting up relu1
I1127 10:56:56.790670  5517 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:56:56.790674  5517 net.cpp:165] Memory required for data: 5167360
I1127 10:56:56.790678  5517 layer_factory.hpp:76] Creating layer ip2
I1127 10:56:56.790690  5517 net.cpp:106] Creating Layer ip2
I1127 10:56:56.790695  5517 net.cpp:454] ip2 <- ip1
I1127 10:56:56.790704  5517 net.cpp:411] ip2 -> ip2
I1127 10:56:56.791395  5517 net.cpp:150] Setting up ip2
I1127 10:56:56.791424  5517 net.cpp:157] Top shape: 64 10 (640)
I1127 10:56:56.791427  5517 net.cpp:165] Memory required for data: 5169920
I1127 10:56:56.791436  5517 layer_factory.hpp:76] Creating layer loss
I1127 10:56:56.791452  5517 net.cpp:106] Creating Layer loss
I1127 10:56:56.791458  5517 net.cpp:454] loss <- ip2
I1127 10:56:56.791465  5517 net.cpp:454] loss <- label
I1127 10:56:56.791476  5517 net.cpp:411] loss -> loss
I1127 10:56:56.791493  5517 layer_factory.hpp:76] Creating layer loss
I1127 10:56:56.791565  5517 net.cpp:150] Setting up loss
I1127 10:56:56.791574  5517 net.cpp:157] Top shape: (1)
I1127 10:56:56.791579  5517 net.cpp:160]     with loss weight 1
I1127 10:56:56.791604  5517 net.cpp:165] Memory required for data: 5169924
I1127 10:56:56.791607  5517 net.cpp:226] loss needs backward computation.
I1127 10:56:56.791612  5517 net.cpp:226] ip2 needs backward computation.
I1127 10:56:56.791617  5517 net.cpp:226] relu1 needs backward computation.
I1127 10:56:56.791621  5517 net.cpp:226] ip1 needs backward computation.
I1127 10:56:56.791625  5517 net.cpp:226] pool2 needs backward computation.
I1127 10:56:56.791630  5517 net.cpp:226] conv2 needs backward computation.
I1127 10:56:56.791635  5517 net.cpp:226] pool1 needs backward computation.
I1127 10:56:56.791640  5517 net.cpp:226] conv1 needs backward computation.
I1127 10:56:56.791643  5517 net.cpp:228] mnist does not need backward computation.
I1127 10:56:56.791647  5517 net.cpp:270] This network produces output loss
I1127 10:56:56.791656  5517 net.cpp:283] Network initialization done.
I1127 10:56:56.791934  5517 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:56:56.791960  5517 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:56:56.792083  5517 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:56:56.792141  5517 layer_factory.hpp:76] Creating layer mnist
I1127 10:56:56.792245  5517 net.cpp:106] Creating Layer mnist
I1127 10:56:56.792254  5517 net.cpp:411] mnist -> data
I1127 10:56:56.792279  5517 net.cpp:411] mnist -> label
I1127 10:56:56.793231  5523 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:56:56.793395  5517 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:56:56.794585  5517 net.cpp:150] Setting up mnist
I1127 10:56:56.794608  5517 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:56:56.794615  5517 net.cpp:157] Top shape: 100 (100)
I1127 10:56:56.794618  5517 net.cpp:165] Memory required for data: 314000
I1127 10:56:56.794625  5517 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:56:56.794637  5517 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:56:56.794642  5517 net.cpp:454] label_mnist_1_split <- label
I1127 10:56:56.794651  5517 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:56:56.794661  5517 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:56:56.794693  5517 net.cpp:150] Setting up label_mnist_1_split
I1127 10:56:56.794700  5517 net.cpp:157] Top shape: 100 (100)
I1127 10:56:56.794705  5517 net.cpp:157] Top shape: 100 (100)
I1127 10:56:56.794709  5517 net.cpp:165] Memory required for data: 314800
I1127 10:56:56.794714  5517 layer_factory.hpp:76] Creating layer conv1
I1127 10:56:56.794725  5517 net.cpp:106] Creating Layer conv1
I1127 10:56:56.794730  5517 net.cpp:454] conv1 <- data
I1127 10:56:56.794736  5517 net.cpp:411] conv1 -> conv1
I1127 10:56:56.794895  5517 net.cpp:150] Setting up conv1
I1127 10:56:56.794904  5517 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:56:56.794909  5517 net.cpp:165] Memory required for data: 4922800
I1127 10:56:56.794919  5517 layer_factory.hpp:76] Creating layer pool1
I1127 10:56:56.794927  5517 net.cpp:106] Creating Layer pool1
I1127 10:56:56.794939  5517 net.cpp:454] pool1 <- conv1
I1127 10:56:56.794953  5517 net.cpp:411] pool1 -> pool1
I1127 10:56:56.794983  5517 net.cpp:150] Setting up pool1
I1127 10:56:56.794991  5517 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:56:56.794994  5517 net.cpp:165] Memory required for data: 6074800
I1127 10:56:56.794999  5517 layer_factory.hpp:76] Creating layer conv2
I1127 10:56:56.795008  5517 net.cpp:106] Creating Layer conv2
I1127 10:56:56.795013  5517 net.cpp:454] conv2 <- pool1
I1127 10:56:56.795019  5517 net.cpp:411] conv2 -> conv2
I1127 10:56:56.795531  5517 net.cpp:150] Setting up conv2
I1127 10:56:56.795572  5517 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:56:56.795580  5517 net.cpp:165] Memory required for data: 7354800
I1127 10:56:56.795598  5517 layer_factory.hpp:76] Creating layer pool2
I1127 10:56:56.795614  5517 net.cpp:106] Creating Layer pool2
I1127 10:56:56.795622  5517 net.cpp:454] pool2 <- conv2
I1127 10:56:56.795634  5517 net.cpp:411] pool2 -> pool2
I1127 10:56:56.795683  5517 net.cpp:150] Setting up pool2
I1127 10:56:56.795696  5517 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:56:56.795701  5517 net.cpp:165] Memory required for data: 7674800
I1127 10:56:56.795708  5517 layer_factory.hpp:76] Creating layer ip1
I1127 10:56:56.795727  5517 net.cpp:106] Creating Layer ip1
I1127 10:56:56.795734  5517 net.cpp:454] ip1 <- pool2
I1127 10:56:56.795747  5517 net.cpp:411] ip1 -> ip1
I1127 10:56:56.799341  5517 net.cpp:150] Setting up ip1
I1127 10:56:56.799412  5517 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:56:56.799419  5517 net.cpp:165] Memory required for data: 7874800
I1127 10:56:56.799440  5517 layer_factory.hpp:76] Creating layer relu1
I1127 10:56:56.799458  5517 net.cpp:106] Creating Layer relu1
I1127 10:56:56.799466  5517 net.cpp:454] relu1 <- ip1
I1127 10:56:56.799479  5517 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:56:56.799494  5517 net.cpp:150] Setting up relu1
I1127 10:56:56.799502  5517 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:56:56.799509  5517 net.cpp:165] Memory required for data: 8074800
I1127 10:56:56.799516  5517 layer_factory.hpp:76] Creating layer ip2
I1127 10:56:56.799533  5517 net.cpp:106] Creating Layer ip2
I1127 10:56:56.799541  5517 net.cpp:454] ip2 <- ip1
I1127 10:56:56.799554  5517 net.cpp:411] ip2 -> ip2
I1127 10:56:56.799721  5517 net.cpp:150] Setting up ip2
I1127 10:56:56.799734  5517 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:56:56.799741  5517 net.cpp:165] Memory required for data: 8078800
I1127 10:56:56.799754  5517 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:56:56.799767  5517 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:56:56.799779  5517 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:56:56.799793  5517 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:56:56.799804  5517 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:56:56.799849  5517 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:56:56.799860  5517 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:56:56.799870  5517 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:56:56.799875  5517 net.cpp:165] Memory required for data: 8086800
I1127 10:56:56.799882  5517 layer_factory.hpp:76] Creating layer accuracy
I1127 10:56:56.799893  5517 net.cpp:106] Creating Layer accuracy
I1127 10:56:56.799901  5517 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:56:56.799911  5517 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:56:56.799921  5517 net.cpp:411] accuracy -> accuracy
I1127 10:56:56.799932  5517 net.cpp:150] Setting up accuracy
I1127 10:56:56.799940  5517 net.cpp:157] Top shape: (1)
I1127 10:56:56.799947  5517 net.cpp:165] Memory required for data: 8086804
I1127 10:56:56.799953  5517 layer_factory.hpp:76] Creating layer loss
I1127 10:56:56.799965  5517 net.cpp:106] Creating Layer loss
I1127 10:56:56.799973  5517 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:56:56.799980  5517 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:56:56.799989  5517 net.cpp:411] loss -> loss
I1127 10:56:56.800003  5517 layer_factory.hpp:76] Creating layer loss
I1127 10:56:56.800122  5517 net.cpp:150] Setting up loss
I1127 10:56:56.800133  5517 net.cpp:157] Top shape: (1)
I1127 10:56:56.800140  5517 net.cpp:160]     with loss weight 1
I1127 10:56:56.800158  5517 net.cpp:165] Memory required for data: 8086808
I1127 10:56:56.800164  5517 net.cpp:226] loss needs backward computation.
I1127 10:56:56.800175  5517 net.cpp:228] accuracy does not need backward computation.
I1127 10:56:56.800182  5517 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:56:56.800189  5517 net.cpp:226] ip2 needs backward computation.
I1127 10:56:56.800195  5517 net.cpp:226] relu1 needs backward computation.
I1127 10:56:56.800201  5517 net.cpp:226] ip1 needs backward computation.
I1127 10:56:56.800209  5517 net.cpp:226] pool2 needs backward computation.
I1127 10:56:56.800215  5517 net.cpp:226] conv2 needs backward computation.
I1127 10:56:56.800221  5517 net.cpp:226] pool1 needs backward computation.
I1127 10:56:56.800228  5517 net.cpp:226] conv1 needs backward computation.
I1127 10:56:56.800235  5517 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:56:56.800243  5517 net.cpp:228] mnist does not need backward computation.
I1127 10:56:56.800250  5517 net.cpp:270] This network produces output accuracy
I1127 10:56:56.800256  5517 net.cpp:270] This network produces output loss
I1127 10:56:56.800274  5517 net.cpp:283] Network initialization done.
I1127 10:56:56.800336  5517 solver.cpp:59] Solver scaffolding done.
I1127 10:56:56.800616  5517 caffe.cpp:212] Starting Optimization
I1127 10:56:56.800626  5517 solver.cpp:287] Solving LeNet
I1127 10:56:56.800631  5517 solver.cpp:288] Learning Rate Policy: inv
I1127 10:56:56.801185  5517 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:56:58.306963  5517 solver.cpp:408]     Test net output #0: accuracy = 0.1067
I1127 10:56:58.307046  5517 solver.cpp:408]     Test net output #1: loss = 2.32461 (* 1 = 2.32461 loss)
I1127 10:56:58.317299  5517 solver.cpp:236] Iteration 0, loss = 2.31359
I1127 10:56:58.317351  5517 solver.cpp:252]     Train net output #0: loss = 2.31359 (* 1 = 2.31359 loss)
I1127 10:56:58.317368  5517 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:57:07.665282  5517 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:57:09.162279  5517 solver.cpp:408]     Test net output #0: accuracy = 0.9702
I1127 10:57:09.162339  5517 solver.cpp:408]     Test net output #1: loss = 0.0925516 (* 1 = 0.0925516 loss)
I1127 10:57:09.173925  5517 solver.cpp:236] Iteration 500, loss = 0.0771537
I1127 10:57:09.174067  5517 solver.cpp:252]     Train net output #0: loss = 0.0771537 (* 1 = 0.0771537 loss)
I1127 10:57:09.174100  5517 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:57:18.559734  5517 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:57:18.571204  5517 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:57:18.594913  5517 solver.cpp:320] Iteration 1000, loss = 0.110833
I1127 10:57:18.594946  5517 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:57:20.072105  5517 solver.cpp:408]     Test net output #0: accuracy = 0.9804
I1127 10:57:20.072171  5517 solver.cpp:408]     Test net output #1: loss = 0.0589423 (* 1 = 0.0589423 loss)
I1127 10:57:20.072190  5517 solver.cpp:325] Optimization Done.
I1127 10:57:20.072201  5517 caffe.cpp:215] Optimization Done.
I1127 10:57:20.143896  5607 caffe.cpp:184] Using GPUs 0
I1127 10:57:20.368379  5607 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:57:20.368541  5607 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:57:20.368825  5607 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:57:20.368849  5607 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:57:20.368942  5607 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:57:20.368999  5607 layer_factory.hpp:76] Creating layer mnist
I1127 10:57:20.369338  5607 net.cpp:106] Creating Layer mnist
I1127 10:57:20.369349  5607 net.cpp:411] mnist -> data
I1127 10:57:20.369374  5607 net.cpp:411] mnist -> label
I1127 10:57:20.370297  5610 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:57:20.379715  5607 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:57:20.381028  5607 net.cpp:150] Setting up mnist
I1127 10:57:20.381070  5607 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:57:20.381081  5607 net.cpp:157] Top shape: 64 (64)
I1127 10:57:20.381088  5607 net.cpp:165] Memory required for data: 200960
I1127 10:57:20.381101  5607 layer_factory.hpp:76] Creating layer conv1
I1127 10:57:20.381126  5607 net.cpp:106] Creating Layer conv1
I1127 10:57:20.381135  5607 net.cpp:454] conv1 <- data
I1127 10:57:20.381150  5607 net.cpp:411] conv1 -> conv1
I1127 10:57:20.382212  5607 net.cpp:150] Setting up conv1
I1127 10:57:20.382262  5607 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:57:20.382271  5607 net.cpp:165] Memory required for data: 3150080
I1127 10:57:20.382297  5607 layer_factory.hpp:76] Creating layer pool1
I1127 10:57:20.382318  5607 net.cpp:106] Creating Layer pool1
I1127 10:57:20.382328  5607 net.cpp:454] pool1 <- conv1
I1127 10:57:20.382340  5607 net.cpp:411] pool1 -> pool1
I1127 10:57:20.382459  5607 net.cpp:150] Setting up pool1
I1127 10:57:20.382472  5607 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:57:20.382479  5607 net.cpp:165] Memory required for data: 3887360
I1127 10:57:20.382486  5607 layer_factory.hpp:76] Creating layer conv2
I1127 10:57:20.382505  5607 net.cpp:106] Creating Layer conv2
I1127 10:57:20.382514  5607 net.cpp:454] conv2 <- pool1
I1127 10:57:20.382529  5607 net.cpp:411] conv2 -> conv2
I1127 10:57:20.382998  5607 net.cpp:150] Setting up conv2
I1127 10:57:20.383013  5607 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:57:20.383025  5607 net.cpp:165] Memory required for data: 4706560
I1127 10:57:20.383038  5607 layer_factory.hpp:76] Creating layer pool2
I1127 10:57:20.383055  5607 net.cpp:106] Creating Layer pool2
I1127 10:57:20.383062  5607 net.cpp:454] pool2 <- conv2
I1127 10:57:20.383071  5607 net.cpp:411] pool2 -> pool2
I1127 10:57:20.383108  5607 net.cpp:150] Setting up pool2
I1127 10:57:20.383121  5607 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:57:20.383126  5607 net.cpp:165] Memory required for data: 4911360
I1127 10:57:20.383132  5607 layer_factory.hpp:76] Creating layer ip1
I1127 10:57:20.383143  5607 net.cpp:106] Creating Layer ip1
I1127 10:57:20.383149  5607 net.cpp:454] ip1 <- pool2
I1127 10:57:20.383158  5607 net.cpp:411] ip1 -> ip1
I1127 10:57:20.387127  5607 net.cpp:150] Setting up ip1
I1127 10:57:20.387166  5607 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:57:20.387174  5607 net.cpp:165] Memory required for data: 5039360
I1127 10:57:20.387192  5607 layer_factory.hpp:76] Creating layer relu1
I1127 10:57:20.387208  5607 net.cpp:106] Creating Layer relu1
I1127 10:57:20.387217  5607 net.cpp:454] relu1 <- ip1
I1127 10:57:20.387228  5607 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:57:20.387244  5607 net.cpp:150] Setting up relu1
I1127 10:57:20.387253  5607 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:57:20.387259  5607 net.cpp:165] Memory required for data: 5167360
I1127 10:57:20.387264  5607 layer_factory.hpp:76] Creating layer ip2
I1127 10:57:20.387275  5607 net.cpp:106] Creating Layer ip2
I1127 10:57:20.387282  5607 net.cpp:454] ip2 <- ip1
I1127 10:57:20.387292  5607 net.cpp:411] ip2 -> ip2
I1127 10:57:20.388098  5607 net.cpp:150] Setting up ip2
I1127 10:57:20.388159  5607 net.cpp:157] Top shape: 64 10 (640)
I1127 10:57:20.388167  5607 net.cpp:165] Memory required for data: 5169920
I1127 10:57:20.388182  5607 layer_factory.hpp:76] Creating layer loss
I1127 10:57:20.388201  5607 net.cpp:106] Creating Layer loss
I1127 10:57:20.388211  5607 net.cpp:454] loss <- ip2
I1127 10:57:20.388222  5607 net.cpp:454] loss <- label
I1127 10:57:20.388242  5607 net.cpp:411] loss -> loss
I1127 10:57:20.388273  5607 layer_factory.hpp:76] Creating layer loss
I1127 10:57:20.388406  5607 net.cpp:150] Setting up loss
I1127 10:57:20.388419  5607 net.cpp:157] Top shape: (1)
I1127 10:57:20.388427  5607 net.cpp:160]     with loss weight 1
I1127 10:57:20.388461  5607 net.cpp:165] Memory required for data: 5169924
I1127 10:57:20.388469  5607 net.cpp:226] loss needs backward computation.
I1127 10:57:20.388478  5607 net.cpp:226] ip2 needs backward computation.
I1127 10:57:20.388484  5607 net.cpp:226] relu1 needs backward computation.
I1127 10:57:20.388490  5607 net.cpp:226] ip1 needs backward computation.
I1127 10:57:20.388497  5607 net.cpp:226] pool2 needs backward computation.
I1127 10:57:20.388504  5607 net.cpp:226] conv2 needs backward computation.
I1127 10:57:20.388511  5607 net.cpp:226] pool1 needs backward computation.
I1127 10:57:20.388519  5607 net.cpp:226] conv1 needs backward computation.
I1127 10:57:20.388526  5607 net.cpp:228] mnist does not need backward computation.
I1127 10:57:20.388533  5607 net.cpp:270] This network produces output loss
I1127 10:57:20.388555  5607 net.cpp:283] Network initialization done.
I1127 10:57:20.388957  5607 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:57:20.389019  5607 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:57:20.389232  5607 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:57:20.389353  5607 layer_factory.hpp:76] Creating layer mnist
I1127 10:57:20.389603  5607 net.cpp:106] Creating Layer mnist
I1127 10:57:20.389622  5607 net.cpp:411] mnist -> data
I1127 10:57:20.389683  5607 net.cpp:411] mnist -> label
I1127 10:57:20.390878  5612 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:57:20.391062  5607 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:57:20.392930  5607 net.cpp:150] Setting up mnist
I1127 10:57:20.392982  5607 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:57:20.392993  5607 net.cpp:157] Top shape: 100 (100)
I1127 10:57:20.393003  5607 net.cpp:165] Memory required for data: 314000
I1127 10:57:20.393014  5607 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:57:20.393035  5607 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:57:20.393046  5607 net.cpp:454] label_mnist_1_split <- label
I1127 10:57:20.393060  5607 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:57:20.393085  5607 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:57:20.393167  5607 net.cpp:150] Setting up label_mnist_1_split
I1127 10:57:20.393180  5607 net.cpp:157] Top shape: 100 (100)
I1127 10:57:20.393189  5607 net.cpp:157] Top shape: 100 (100)
I1127 10:57:20.393196  5607 net.cpp:165] Memory required for data: 314800
I1127 10:57:20.393205  5607 layer_factory.hpp:76] Creating layer conv1
I1127 10:57:20.393225  5607 net.cpp:106] Creating Layer conv1
I1127 10:57:20.393235  5607 net.cpp:454] conv1 <- data
I1127 10:57:20.393250  5607 net.cpp:411] conv1 -> conv1
I1127 10:57:20.393507  5607 net.cpp:150] Setting up conv1
I1127 10:57:20.393522  5607 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:57:20.393530  5607 net.cpp:165] Memory required for data: 4922800
I1127 10:57:20.393546  5607 layer_factory.hpp:76] Creating layer pool1
I1127 10:57:20.393560  5607 net.cpp:106] Creating Layer pool1
I1127 10:57:20.393569  5607 net.cpp:454] pool1 <- conv1
I1127 10:57:20.393597  5607 net.cpp:411] pool1 -> pool1
I1127 10:57:20.393647  5607 net.cpp:150] Setting up pool1
I1127 10:57:20.393661  5607 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:57:20.393668  5607 net.cpp:165] Memory required for data: 6074800
I1127 10:57:20.393676  5607 layer_factory.hpp:76] Creating layer conv2
I1127 10:57:20.393690  5607 net.cpp:106] Creating Layer conv2
I1127 10:57:20.393699  5607 net.cpp:454] conv2 <- pool1
I1127 10:57:20.393712  5607 net.cpp:411] conv2 -> conv2
I1127 10:57:20.394158  5607 net.cpp:150] Setting up conv2
I1127 10:57:20.394189  5607 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:57:20.394198  5607 net.cpp:165] Memory required for data: 7354800
I1127 10:57:20.394217  5607 layer_factory.hpp:76] Creating layer pool2
I1127 10:57:20.394233  5607 net.cpp:106] Creating Layer pool2
I1127 10:57:20.394244  5607 net.cpp:454] pool2 <- conv2
I1127 10:57:20.394255  5607 net.cpp:411] pool2 -> pool2
I1127 10:57:20.394309  5607 net.cpp:150] Setting up pool2
I1127 10:57:20.394325  5607 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:57:20.394335  5607 net.cpp:165] Memory required for data: 7674800
I1127 10:57:20.394343  5607 layer_factory.hpp:76] Creating layer ip1
I1127 10:57:20.394363  5607 net.cpp:106] Creating Layer ip1
I1127 10:57:20.394379  5607 net.cpp:454] ip1 <- pool2
I1127 10:57:20.394408  5607 net.cpp:411] ip1 -> ip1
I1127 10:57:20.399070  5607 net.cpp:150] Setting up ip1
I1127 10:57:20.399143  5607 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:57:20.399152  5607 net.cpp:165] Memory required for data: 7874800
I1127 10:57:20.399173  5607 layer_factory.hpp:76] Creating layer relu1
I1127 10:57:20.399190  5607 net.cpp:106] Creating Layer relu1
I1127 10:57:20.399199  5607 net.cpp:454] relu1 <- ip1
I1127 10:57:20.399212  5607 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:57:20.399227  5607 net.cpp:150] Setting up relu1
I1127 10:57:20.399235  5607 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:57:20.399241  5607 net.cpp:165] Memory required for data: 8074800
I1127 10:57:20.399248  5607 layer_factory.hpp:76] Creating layer ip2
I1127 10:57:20.399266  5607 net.cpp:106] Creating Layer ip2
I1127 10:57:20.399272  5607 net.cpp:454] ip2 <- ip1
I1127 10:57:20.399292  5607 net.cpp:411] ip2 -> ip2
I1127 10:57:20.399502  5607 net.cpp:150] Setting up ip2
I1127 10:57:20.399543  5607 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:57:20.399554  5607 net.cpp:165] Memory required for data: 8078800
I1127 10:57:20.399574  5607 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:57:20.399598  5607 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:57:20.399612  5607 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:57:20.399628  5607 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:57:20.399648  5607 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:57:20.399734  5607 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:57:20.399755  5607 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:57:20.399770  5607 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:57:20.399780  5607 net.cpp:165] Memory required for data: 8086800
I1127 10:57:20.399788  5607 layer_factory.hpp:76] Creating layer accuracy
I1127 10:57:20.399811  5607 net.cpp:106] Creating Layer accuracy
I1127 10:57:20.399823  5607 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:57:20.399835  5607 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:57:20.399852  5607 net.cpp:411] accuracy -> accuracy
I1127 10:57:20.399875  5607 net.cpp:150] Setting up accuracy
I1127 10:57:20.399889  5607 net.cpp:157] Top shape: (1)
I1127 10:57:20.399899  5607 net.cpp:165] Memory required for data: 8086804
I1127 10:57:20.399907  5607 layer_factory.hpp:76] Creating layer loss
I1127 10:57:20.399930  5607 net.cpp:106] Creating Layer loss
I1127 10:57:20.399941  5607 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:57:20.399953  5607 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:57:20.399963  5607 net.cpp:411] loss -> loss
I1127 10:57:20.399981  5607 layer_factory.hpp:76] Creating layer loss
I1127 10:57:20.400240  5607 net.cpp:150] Setting up loss
I1127 10:57:20.400257  5607 net.cpp:157] Top shape: (1)
I1127 10:57:20.400264  5607 net.cpp:160]     with loss weight 1
I1127 10:57:20.400290  5607 net.cpp:165] Memory required for data: 8086808
I1127 10:57:20.400297  5607 net.cpp:226] loss needs backward computation.
I1127 10:57:20.400311  5607 net.cpp:228] accuracy does not need backward computation.
I1127 10:57:20.400318  5607 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:57:20.400326  5607 net.cpp:226] ip2 needs backward computation.
I1127 10:57:20.400341  5607 net.cpp:226] relu1 needs backward computation.
I1127 10:57:20.400348  5607 net.cpp:226] ip1 needs backward computation.
I1127 10:57:20.400355  5607 net.cpp:226] pool2 needs backward computation.
I1127 10:57:20.400363  5607 net.cpp:226] conv2 needs backward computation.
I1127 10:57:20.400370  5607 net.cpp:226] pool1 needs backward computation.
I1127 10:57:20.400377  5607 net.cpp:226] conv1 needs backward computation.
I1127 10:57:20.400384  5607 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:57:20.400391  5607 net.cpp:228] mnist does not need backward computation.
I1127 10:57:20.400398  5607 net.cpp:270] This network produces output accuracy
I1127 10:57:20.400405  5607 net.cpp:270] This network produces output loss
I1127 10:57:20.400424  5607 net.cpp:283] Network initialization done.
I1127 10:57:20.400548  5607 solver.cpp:59] Solver scaffolding done.
I1127 10:57:20.400851  5607 caffe.cpp:212] Starting Optimization
I1127 10:57:20.400866  5607 solver.cpp:287] Solving LeNet
I1127 10:57:20.400872  5607 solver.cpp:288] Learning Rate Policy: inv
I1127 10:57:20.401672  5607 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:57:21.721694  5607 solver.cpp:408]     Test net output #0: accuracy = 0.1056
I1127 10:57:21.721855  5607 solver.cpp:408]     Test net output #1: loss = 2.4771 (* 1 = 2.4771 loss)
I1127 10:57:21.738651  5607 solver.cpp:236] Iteration 0, loss = 2.46946
I1127 10:57:21.738761  5607 solver.cpp:252]     Train net output #0: loss = 2.46946 (* 1 = 2.46946 loss)
I1127 10:57:21.738796  5607 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:57:31.205612  5607 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:57:32.518229  5607 solver.cpp:408]     Test net output #0: accuracy = 0.9716
I1127 10:57:32.518362  5607 solver.cpp:408]     Test net output #1: loss = 0.0886699 (* 1 = 0.0886699 loss)
I1127 10:57:32.528576  5607 solver.cpp:236] Iteration 500, loss = 0.0845099
I1127 10:57:32.528708  5607 solver.cpp:252]     Train net output #0: loss = 0.0845099 (* 1 = 0.0845099 loss)
I1127 10:57:32.528733  5607 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:57:43.141566  5607 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:57:43.157501  5607 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:57:43.168334  5607 solver.cpp:320] Iteration 1000, loss = 0.115776
I1127 10:57:43.168392  5607 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:57:46.048190  5607 solver.cpp:408]     Test net output #0: accuracy = 0.9815
I1127 10:57:46.048231  5607 solver.cpp:408]     Test net output #1: loss = 0.0587074 (* 1 = 0.0587074 loss)
I1127 10:57:46.048238  5607 solver.cpp:325] Optimization Done.
I1127 10:57:46.048243  5607 caffe.cpp:215] Optimization Done.
I1127 10:57:46.113379  5668 caffe.cpp:184] Using GPUs 0
I1127 10:57:46.383708  5668 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:57:46.383955  5668 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:57:46.384258  5668 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:57:46.384274  5668 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:57:46.384362  5668 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:57:46.384429  5668 layer_factory.hpp:76] Creating layer mnist
I1127 10:57:46.384757  5668 net.cpp:106] Creating Layer mnist
I1127 10:57:46.384768  5668 net.cpp:411] mnist -> data
I1127 10:57:46.384791  5668 net.cpp:411] mnist -> label
I1127 10:57:46.386097  5671 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:57:46.400178  5668 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:57:46.402186  5668 net.cpp:150] Setting up mnist
I1127 10:57:46.402271  5668 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:57:46.402281  5668 net.cpp:157] Top shape: 64 (64)
I1127 10:57:46.402286  5668 net.cpp:165] Memory required for data: 200960
I1127 10:57:46.402305  5668 layer_factory.hpp:76] Creating layer conv1
I1127 10:57:46.402346  5668 net.cpp:106] Creating Layer conv1
I1127 10:57:46.402359  5668 net.cpp:454] conv1 <- data
I1127 10:57:46.402376  5668 net.cpp:411] conv1 -> conv1
I1127 10:57:46.403553  5668 net.cpp:150] Setting up conv1
I1127 10:57:46.403652  5668 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:57:46.403658  5668 net.cpp:165] Memory required for data: 3150080
I1127 10:57:46.403692  5668 layer_factory.hpp:76] Creating layer pool1
I1127 10:57:46.403723  5668 net.cpp:106] Creating Layer pool1
I1127 10:57:46.403731  5668 net.cpp:454] pool1 <- conv1
I1127 10:57:46.403743  5668 net.cpp:411] pool1 -> pool1
I1127 10:57:46.403898  5668 net.cpp:150] Setting up pool1
I1127 10:57:46.403914  5668 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:57:46.403920  5668 net.cpp:165] Memory required for data: 3887360
I1127 10:57:46.403925  5668 layer_factory.hpp:76] Creating layer conv2
I1127 10:57:46.403941  5668 net.cpp:106] Creating Layer conv2
I1127 10:57:46.403947  5668 net.cpp:454] conv2 <- pool1
I1127 10:57:46.403956  5668 net.cpp:411] conv2 -> conv2
I1127 10:57:46.404254  5668 net.cpp:150] Setting up conv2
I1127 10:57:46.404269  5668 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:57:46.404273  5668 net.cpp:165] Memory required for data: 4706560
I1127 10:57:46.404284  5668 layer_factory.hpp:76] Creating layer pool2
I1127 10:57:46.404294  5668 net.cpp:106] Creating Layer pool2
I1127 10:57:46.404299  5668 net.cpp:454] pool2 <- conv2
I1127 10:57:46.404304  5668 net.cpp:411] pool2 -> pool2
I1127 10:57:46.404330  5668 net.cpp:150] Setting up pool2
I1127 10:57:46.404337  5668 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:57:46.404342  5668 net.cpp:165] Memory required for data: 4911360
I1127 10:57:46.404351  5668 layer_factory.hpp:76] Creating layer ip1
I1127 10:57:46.404361  5668 net.cpp:106] Creating Layer ip1
I1127 10:57:46.404366  5668 net.cpp:454] ip1 <- pool2
I1127 10:57:46.404371  5668 net.cpp:411] ip1 -> ip1
I1127 10:57:46.407593  5668 net.cpp:150] Setting up ip1
I1127 10:57:46.407632  5668 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:57:46.407637  5668 net.cpp:165] Memory required for data: 5039360
I1127 10:57:46.407650  5668 layer_factory.hpp:76] Creating layer relu1
I1127 10:57:46.407661  5668 net.cpp:106] Creating Layer relu1
I1127 10:57:46.407666  5668 net.cpp:454] relu1 <- ip1
I1127 10:57:46.407675  5668 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:57:46.407687  5668 net.cpp:150] Setting up relu1
I1127 10:57:46.407693  5668 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:57:46.407697  5668 net.cpp:165] Memory required for data: 5167360
I1127 10:57:46.407701  5668 layer_factory.hpp:76] Creating layer ip2
I1127 10:57:46.407709  5668 net.cpp:106] Creating Layer ip2
I1127 10:57:46.407714  5668 net.cpp:454] ip2 <- ip1
I1127 10:57:46.407721  5668 net.cpp:411] ip2 -> ip2
I1127 10:57:46.408341  5668 net.cpp:150] Setting up ip2
I1127 10:57:46.408385  5668 net.cpp:157] Top shape: 64 10 (640)
I1127 10:57:46.408390  5668 net.cpp:165] Memory required for data: 5169920
I1127 10:57:46.408403  5668 layer_factory.hpp:76] Creating layer loss
I1127 10:57:46.408424  5668 net.cpp:106] Creating Layer loss
I1127 10:57:46.408435  5668 net.cpp:454] loss <- ip2
I1127 10:57:46.408442  5668 net.cpp:454] loss <- label
I1127 10:57:46.408452  5668 net.cpp:411] loss -> loss
I1127 10:57:46.408470  5668 layer_factory.hpp:76] Creating layer loss
I1127 10:57:46.408649  5668 net.cpp:150] Setting up loss
I1127 10:57:46.408674  5668 net.cpp:157] Top shape: (1)
I1127 10:57:46.408679  5668 net.cpp:160]     with loss weight 1
I1127 10:57:46.408720  5668 net.cpp:165] Memory required for data: 5169924
I1127 10:57:46.408725  5668 net.cpp:226] loss needs backward computation.
I1127 10:57:46.408732  5668 net.cpp:226] ip2 needs backward computation.
I1127 10:57:46.408737  5668 net.cpp:226] relu1 needs backward computation.
I1127 10:57:46.408741  5668 net.cpp:226] ip1 needs backward computation.
I1127 10:57:46.408746  5668 net.cpp:226] pool2 needs backward computation.
I1127 10:57:46.408751  5668 net.cpp:226] conv2 needs backward computation.
I1127 10:57:46.408756  5668 net.cpp:226] pool1 needs backward computation.
I1127 10:57:46.408761  5668 net.cpp:226] conv1 needs backward computation.
I1127 10:57:46.408766  5668 net.cpp:228] mnist does not need backward computation.
I1127 10:57:46.408771  5668 net.cpp:270] This network produces output loss
I1127 10:57:46.408784  5668 net.cpp:283] Network initialization done.
I1127 10:57:46.409168  5668 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:57:46.409201  5668 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:57:46.409325  5668 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:57:46.409404  5668 layer_factory.hpp:76] Creating layer mnist
I1127 10:57:46.409512  5668 net.cpp:106] Creating Layer mnist
I1127 10:57:46.409523  5668 net.cpp:411] mnist -> data
I1127 10:57:46.409534  5668 net.cpp:411] mnist -> label
I1127 10:57:46.411206  5673 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:57:46.411474  5668 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:57:46.418297  5668 net.cpp:150] Setting up mnist
I1127 10:57:46.418407  5668 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:57:46.418422  5668 net.cpp:157] Top shape: 100 (100)
I1127 10:57:46.418429  5668 net.cpp:165] Memory required for data: 314000
I1127 10:57:46.418447  5668 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:57:46.418473  5668 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:57:46.418486  5668 net.cpp:454] label_mnist_1_split <- label
I1127 10:57:46.418504  5668 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:57:46.418529  5668 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:57:46.419217  5668 net.cpp:150] Setting up label_mnist_1_split
I1127 10:57:46.419270  5668 net.cpp:157] Top shape: 100 (100)
I1127 10:57:46.419281  5668 net.cpp:157] Top shape: 100 (100)
I1127 10:57:46.419287  5668 net.cpp:165] Memory required for data: 314800
I1127 10:57:46.419297  5668 layer_factory.hpp:76] Creating layer conv1
I1127 10:57:46.419340  5668 net.cpp:106] Creating Layer conv1
I1127 10:57:46.419363  5668 net.cpp:454] conv1 <- data
I1127 10:57:46.419380  5668 net.cpp:411] conv1 -> conv1
I1127 10:57:46.419630  5668 net.cpp:150] Setting up conv1
I1127 10:57:46.419641  5668 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:57:46.419648  5668 net.cpp:165] Memory required for data: 4922800
I1127 10:57:46.419663  5668 layer_factory.hpp:76] Creating layer pool1
I1127 10:57:46.419675  5668 net.cpp:106] Creating Layer pool1
I1127 10:57:46.419682  5668 net.cpp:454] pool1 <- conv1
I1127 10:57:46.419715  5668 net.cpp:411] pool1 -> pool1
I1127 10:57:46.422888  5668 net.cpp:150] Setting up pool1
I1127 10:57:46.422986  5668 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:57:46.422994  5668 net.cpp:165] Memory required for data: 6074800
I1127 10:57:46.423008  5668 layer_factory.hpp:76] Creating layer conv2
I1127 10:57:46.423048  5668 net.cpp:106] Creating Layer conv2
I1127 10:57:46.423058  5668 net.cpp:454] conv2 <- pool1
I1127 10:57:46.423073  5668 net.cpp:411] conv2 -> conv2
I1127 10:57:46.423624  5668 net.cpp:150] Setting up conv2
I1127 10:57:46.423667  5668 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:57:46.423674  5668 net.cpp:165] Memory required for data: 7354800
I1127 10:57:46.423696  5668 layer_factory.hpp:76] Creating layer pool2
I1127 10:57:46.423715  5668 net.cpp:106] Creating Layer pool2
I1127 10:57:46.423724  5668 net.cpp:454] pool2 <- conv2
I1127 10:57:46.423738  5668 net.cpp:411] pool2 -> pool2
I1127 10:57:46.423820  5668 net.cpp:150] Setting up pool2
I1127 10:57:46.423841  5668 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:57:46.423878  5668 net.cpp:165] Memory required for data: 7674800
I1127 10:57:46.423890  5668 layer_factory.hpp:76] Creating layer ip1
I1127 10:57:46.423910  5668 net.cpp:106] Creating Layer ip1
I1127 10:57:46.423923  5668 net.cpp:454] ip1 <- pool2
I1127 10:57:46.423940  5668 net.cpp:411] ip1 -> ip1
I1127 10:57:46.429930  5668 net.cpp:150] Setting up ip1
I1127 10:57:46.429996  5668 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:57:46.430011  5668 net.cpp:165] Memory required for data: 7874800
I1127 10:57:46.430037  5668 layer_factory.hpp:76] Creating layer relu1
I1127 10:57:46.430057  5668 net.cpp:106] Creating Layer relu1
I1127 10:57:46.430068  5668 net.cpp:454] relu1 <- ip1
I1127 10:57:46.430086  5668 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:57:46.430105  5668 net.cpp:150] Setting up relu1
I1127 10:57:46.430117  5668 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:57:46.430124  5668 net.cpp:165] Memory required for data: 8074800
I1127 10:57:46.430132  5668 layer_factory.hpp:76] Creating layer ip2
I1127 10:57:46.430177  5668 net.cpp:106] Creating Layer ip2
I1127 10:57:46.430191  5668 net.cpp:454] ip2 <- ip1
I1127 10:57:46.430209  5668 net.cpp:411] ip2 -> ip2
I1127 10:57:46.430415  5668 net.cpp:150] Setting up ip2
I1127 10:57:46.430431  5668 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:57:46.430440  5668 net.cpp:165] Memory required for data: 8078800
I1127 10:57:46.430455  5668 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:57:46.430470  5668 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:57:46.430480  5668 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:57:46.430491  5668 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:57:46.430505  5668 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:57:46.430563  5668 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:57:46.430577  5668 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:57:46.430585  5668 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:57:46.430591  5668 net.cpp:165] Memory required for data: 8086800
I1127 10:57:46.430598  5668 layer_factory.hpp:76] Creating layer accuracy
I1127 10:57:46.430613  5668 net.cpp:106] Creating Layer accuracy
I1127 10:57:46.430620  5668 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:57:46.430630  5668 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:57:46.430644  5668 net.cpp:411] accuracy -> accuracy
I1127 10:57:46.430670  5668 net.cpp:150] Setting up accuracy
I1127 10:57:46.430686  5668 net.cpp:157] Top shape: (1)
I1127 10:57:46.430697  5668 net.cpp:165] Memory required for data: 8086804
I1127 10:57:46.430711  5668 layer_factory.hpp:76] Creating layer loss
I1127 10:57:46.430737  5668 net.cpp:106] Creating Layer loss
I1127 10:57:46.430752  5668 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:57:46.430766  5668 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:57:46.430783  5668 net.cpp:411] loss -> loss
I1127 10:57:46.430801  5668 layer_factory.hpp:76] Creating layer loss
I1127 10:57:46.431038  5668 net.cpp:150] Setting up loss
I1127 10:57:46.431059  5668 net.cpp:157] Top shape: (1)
I1127 10:57:46.431072  5668 net.cpp:160]     with loss weight 1
I1127 10:57:46.431108  5668 net.cpp:165] Memory required for data: 8086808
I1127 10:57:46.431126  5668 net.cpp:226] loss needs backward computation.
I1127 10:57:46.431154  5668 net.cpp:228] accuracy does not need backward computation.
I1127 10:57:46.431171  5668 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:57:46.431182  5668 net.cpp:226] ip2 needs backward computation.
I1127 10:57:46.431191  5668 net.cpp:226] relu1 needs backward computation.
I1127 10:57:46.431198  5668 net.cpp:226] ip1 needs backward computation.
I1127 10:57:46.431206  5668 net.cpp:226] pool2 needs backward computation.
I1127 10:57:46.431216  5668 net.cpp:226] conv2 needs backward computation.
I1127 10:57:46.431224  5668 net.cpp:226] pool1 needs backward computation.
I1127 10:57:46.431232  5668 net.cpp:226] conv1 needs backward computation.
I1127 10:57:46.431242  5668 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:57:46.431267  5668 net.cpp:228] mnist does not need backward computation.
I1127 10:57:46.431282  5668 net.cpp:270] This network produces output accuracy
I1127 10:57:46.431299  5668 net.cpp:270] This network produces output loss
I1127 10:57:46.431334  5668 net.cpp:283] Network initialization done.
I1127 10:57:46.431588  5668 solver.cpp:59] Solver scaffolding done.
I1127 10:57:46.432260  5668 caffe.cpp:212] Starting Optimization
I1127 10:57:46.432284  5668 solver.cpp:287] Solving LeNet
I1127 10:57:46.432293  5668 solver.cpp:288] Learning Rate Policy: inv
I1127 10:57:46.433605  5668 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:57:47.538944  5668 solver.cpp:408]     Test net output #0: accuracy = 0.0405
I1127 10:57:47.539032  5668 solver.cpp:408]     Test net output #1: loss = 2.3825 (* 1 = 2.3825 loss)
I1127 10:57:47.556154  5668 solver.cpp:236] Iteration 0, loss = 2.35052
I1127 10:57:47.556248  5668 solver.cpp:252]     Train net output #0: loss = 2.35052 (* 1 = 2.35052 loss)
I1127 10:57:47.556277  5668 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:58:00.913391  5668 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:58:01.368453  5668 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:58:03.018460  5668 solver.cpp:408]     Test net output #0: accuracy = 0.9749
I1127 10:58:03.018610  5668 solver.cpp:408]     Test net output #1: loss = 0.0780431 (* 1 = 0.0780431 loss)
I1127 10:58:03.049929  5668 solver.cpp:236] Iteration 500, loss = 0.0841362
I1127 10:58:03.050052  5668 solver.cpp:252]     Train net output #0: loss = 0.0841362 (* 1 = 0.0841362 loss)
I1127 10:58:03.050082  5668 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:58:15.399164  5668 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:58:15.410656  5668 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:58:15.419841  5668 solver.cpp:320] Iteration 1000, loss = 0.0845395
I1127 10:58:15.419881  5668 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:58:18.530697  5668 solver.cpp:408]     Test net output #0: accuracy = 0.9812
I1127 10:58:18.530969  5668 solver.cpp:408]     Test net output #1: loss = 0.056932 (* 1 = 0.056932 loss)
I1127 10:58:18.530980  5668 solver.cpp:325] Optimization Done.
I1127 10:58:18.530987  5668 caffe.cpp:215] Optimization Done.
I1127 10:58:18.674531  5722 caffe.cpp:184] Using GPUs 0
I1127 10:58:19.059916  5722 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:58:19.060142  5722 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:58:19.060674  5722 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:58:19.060727  5722 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:58:19.060950  5722 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:58:19.061206  5722 layer_factory.hpp:76] Creating layer mnist
I1127 10:58:19.062000  5722 net.cpp:106] Creating Layer mnist
I1127 10:58:19.062041  5722 net.cpp:411] mnist -> data
I1127 10:58:19.062078  5722 net.cpp:411] mnist -> label
I1127 10:58:19.063627  5725 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:58:19.081176  5722 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:58:19.083688  5722 net.cpp:150] Setting up mnist
I1127 10:58:19.083847  5722 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:58:19.083899  5722 net.cpp:157] Top shape: 64 (64)
I1127 10:58:19.083930  5722 net.cpp:165] Memory required for data: 200960
I1127 10:58:19.083981  5722 layer_factory.hpp:76] Creating layer conv1
I1127 10:58:19.084079  5722 net.cpp:106] Creating Layer conv1
I1127 10:58:19.084122  5722 net.cpp:454] conv1 <- data
I1127 10:58:19.084183  5722 net.cpp:411] conv1 -> conv1
I1127 10:58:19.086172  5722 net.cpp:150] Setting up conv1
I1127 10:58:19.086308  5722 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:58:19.086347  5722 net.cpp:165] Memory required for data: 3150080
I1127 10:58:19.086419  5722 layer_factory.hpp:76] Creating layer pool1
I1127 10:58:19.086478  5722 net.cpp:106] Creating Layer pool1
I1127 10:58:19.086513  5722 net.cpp:454] pool1 <- conv1
I1127 10:58:19.086552  5722 net.cpp:411] pool1 -> pool1
I1127 10:58:19.086827  5722 net.cpp:150] Setting up pool1
I1127 10:58:19.086864  5722 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:58:19.086879  5722 net.cpp:165] Memory required for data: 3887360
I1127 10:58:19.086895  5722 layer_factory.hpp:76] Creating layer conv2
I1127 10:58:19.086931  5722 net.cpp:106] Creating Layer conv2
I1127 10:58:19.086949  5722 net.cpp:454] conv2 <- pool1
I1127 10:58:19.086972  5722 net.cpp:411] conv2 -> conv2
I1127 10:58:19.087834  5722 net.cpp:150] Setting up conv2
I1127 10:58:19.087870  5722 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:58:19.087880  5722 net.cpp:165] Memory required for data: 4706560
I1127 10:58:19.087903  5722 layer_factory.hpp:76] Creating layer pool2
I1127 10:58:19.087930  5722 net.cpp:106] Creating Layer pool2
I1127 10:58:19.087942  5722 net.cpp:454] pool2 <- conv2
I1127 10:58:19.087960  5722 net.cpp:411] pool2 -> pool2
I1127 10:58:19.088058  5722 net.cpp:150] Setting up pool2
I1127 10:58:19.088078  5722 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:58:19.088084  5722 net.cpp:165] Memory required for data: 4911360
I1127 10:58:19.088093  5722 layer_factory.hpp:76] Creating layer ip1
I1127 10:58:19.088114  5722 net.cpp:106] Creating Layer ip1
I1127 10:58:19.088124  5722 net.cpp:454] ip1 <- pool2
I1127 10:58:19.088136  5722 net.cpp:411] ip1 -> ip1
I1127 10:58:19.093662  5722 net.cpp:150] Setting up ip1
I1127 10:58:19.093741  5722 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:58:19.093760  5722 net.cpp:165] Memory required for data: 5039360
I1127 10:58:19.093801  5722 layer_factory.hpp:76] Creating layer relu1
I1127 10:58:19.093850  5722 net.cpp:106] Creating Layer relu1
I1127 10:58:19.093868  5722 net.cpp:454] relu1 <- ip1
I1127 10:58:19.093888  5722 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:58:19.093924  5722 net.cpp:150] Setting up relu1
I1127 10:58:19.093941  5722 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:58:19.093952  5722 net.cpp:165] Memory required for data: 5167360
I1127 10:58:19.093966  5722 layer_factory.hpp:76] Creating layer ip2
I1127 10:58:19.093996  5722 net.cpp:106] Creating Layer ip2
I1127 10:58:19.094010  5722 net.cpp:454] ip2 <- ip1
I1127 10:58:19.094034  5722 net.cpp:411] ip2 -> ip2
I1127 10:58:19.096078  5722 net.cpp:150] Setting up ip2
I1127 10:58:19.096223  5722 net.cpp:157] Top shape: 64 10 (640)
I1127 10:58:19.096261  5722 net.cpp:165] Memory required for data: 5169920
I1127 10:58:19.096330  5722 layer_factory.hpp:76] Creating layer loss
I1127 10:58:19.096417  5722 net.cpp:106] Creating Layer loss
I1127 10:58:19.096457  5722 net.cpp:454] loss <- ip2
I1127 10:58:19.096490  5722 net.cpp:454] loss <- label
I1127 10:58:19.096547  5722 net.cpp:411] loss -> loss
I1127 10:58:19.096645  5722 layer_factory.hpp:76] Creating layer loss
I1127 10:58:19.097338  5722 net.cpp:150] Setting up loss
I1127 10:58:19.097394  5722 net.cpp:157] Top shape: (1)
I1127 10:58:19.097419  5722 net.cpp:160]     with loss weight 1
I1127 10:58:19.097494  5722 net.cpp:165] Memory required for data: 5169924
I1127 10:58:19.097512  5722 net.cpp:226] loss needs backward computation.
I1127 10:58:19.097522  5722 net.cpp:226] ip2 needs backward computation.
I1127 10:58:19.097532  5722 net.cpp:226] relu1 needs backward computation.
I1127 10:58:19.097539  5722 net.cpp:226] ip1 needs backward computation.
I1127 10:58:19.097546  5722 net.cpp:226] pool2 needs backward computation.
I1127 10:58:19.097556  5722 net.cpp:226] conv2 needs backward computation.
I1127 10:58:19.097563  5722 net.cpp:226] pool1 needs backward computation.
I1127 10:58:19.097571  5722 net.cpp:226] conv1 needs backward computation.
I1127 10:58:19.097582  5722 net.cpp:228] mnist does not need backward computation.
I1127 10:58:19.097589  5722 net.cpp:270] This network produces output loss
I1127 10:58:19.097612  5722 net.cpp:283] Network initialization done.
I1127 10:58:19.098110  5722 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:58:19.098217  5722 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:58:19.098815  5722 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:58:19.099443  5722 layer_factory.hpp:76] Creating layer mnist
I1127 10:58:19.099694  5722 net.cpp:106] Creating Layer mnist
I1127 10:58:19.099719  5722 net.cpp:411] mnist -> data
I1127 10:58:19.099748  5722 net.cpp:411] mnist -> label
I1127 10:58:19.104022  5747 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:58:19.105187  5722 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:58:19.107661  5722 net.cpp:150] Setting up mnist
I1127 10:58:19.107707  5722 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:58:19.107719  5722 net.cpp:157] Top shape: 100 (100)
I1127 10:58:19.107728  5722 net.cpp:165] Memory required for data: 314000
I1127 10:58:19.107738  5722 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:58:19.107754  5722 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:58:19.107764  5722 net.cpp:454] label_mnist_1_split <- label
I1127 10:58:19.107810  5722 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:58:19.107841  5722 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:58:19.108006  5722 net.cpp:150] Setting up label_mnist_1_split
I1127 10:58:19.108054  5722 net.cpp:157] Top shape: 100 (100)
I1127 10:58:19.108068  5722 net.cpp:157] Top shape: 100 (100)
I1127 10:58:19.108077  5722 net.cpp:165] Memory required for data: 314800
I1127 10:58:19.108088  5722 layer_factory.hpp:76] Creating layer conv1
I1127 10:58:19.108165  5722 net.cpp:106] Creating Layer conv1
I1127 10:58:19.108182  5722 net.cpp:454] conv1 <- data
I1127 10:58:19.108204  5722 net.cpp:411] conv1 -> conv1
I1127 10:58:19.108819  5722 net.cpp:150] Setting up conv1
I1127 10:58:19.108847  5722 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:58:19.108855  5722 net.cpp:165] Memory required for data: 4922800
I1127 10:58:19.108871  5722 layer_factory.hpp:76] Creating layer pool1
I1127 10:58:19.108888  5722 net.cpp:106] Creating Layer pool1
I1127 10:58:19.108896  5722 net.cpp:454] pool1 <- conv1
I1127 10:58:19.108933  5722 net.cpp:411] pool1 -> pool1
I1127 10:58:19.108984  5722 net.cpp:150] Setting up pool1
I1127 10:58:19.108995  5722 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:58:19.109001  5722 net.cpp:165] Memory required for data: 6074800
I1127 10:58:19.109007  5722 layer_factory.hpp:76] Creating layer conv2
I1127 10:58:19.109024  5722 net.cpp:106] Creating Layer conv2
I1127 10:58:19.109030  5722 net.cpp:454] conv2 <- pool1
I1127 10:58:19.109041  5722 net.cpp:411] conv2 -> conv2
I1127 10:58:19.109444  5722 net.cpp:150] Setting up conv2
I1127 10:58:19.109469  5722 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:58:19.109477  5722 net.cpp:165] Memory required for data: 7354800
I1127 10:58:19.109496  5722 layer_factory.hpp:76] Creating layer pool2
I1127 10:58:19.109515  5722 net.cpp:106] Creating Layer pool2
I1127 10:58:19.109524  5722 net.cpp:454] pool2 <- conv2
I1127 10:58:19.109535  5722 net.cpp:411] pool2 -> pool2
I1127 10:58:19.109591  5722 net.cpp:150] Setting up pool2
I1127 10:58:19.109607  5722 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:58:19.109619  5722 net.cpp:165] Memory required for data: 7674800
I1127 10:58:19.109632  5722 layer_factory.hpp:76] Creating layer ip1
I1127 10:58:19.109663  5722 net.cpp:106] Creating Layer ip1
I1127 10:58:19.109679  5722 net.cpp:454] ip1 <- pool2
I1127 10:58:19.109701  5722 net.cpp:411] ip1 -> ip1
I1127 10:58:19.115094  5722 net.cpp:150] Setting up ip1
I1127 10:58:19.115195  5722 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:58:19.115219  5722 net.cpp:165] Memory required for data: 7874800
I1127 10:58:19.115308  5722 layer_factory.hpp:76] Creating layer relu1
I1127 10:58:19.115346  5722 net.cpp:106] Creating Layer relu1
I1127 10:58:19.115365  5722 net.cpp:454] relu1 <- ip1
I1127 10:58:19.115389  5722 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:58:19.115419  5722 net.cpp:150] Setting up relu1
I1127 10:58:19.115437  5722 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:58:19.115447  5722 net.cpp:165] Memory required for data: 8074800
I1127 10:58:19.115461  5722 layer_factory.hpp:76] Creating layer ip2
I1127 10:58:19.115494  5722 net.cpp:106] Creating Layer ip2
I1127 10:58:19.115511  5722 net.cpp:454] ip2 <- ip1
I1127 10:58:19.115540  5722 net.cpp:411] ip2 -> ip2
I1127 10:58:19.116044  5722 net.cpp:150] Setting up ip2
I1127 10:58:19.116081  5722 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:58:19.116094  5722 net.cpp:165] Memory required for data: 8078800
I1127 10:58:19.116117  5722 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:58:19.116143  5722 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:58:19.116158  5722 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:58:19.116176  5722 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:58:19.116201  5722 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:58:19.116365  5722 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:58:19.116389  5722 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:58:19.116402  5722 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:58:19.116412  5722 net.cpp:165] Memory required for data: 8086800
I1127 10:58:19.116425  5722 layer_factory.hpp:76] Creating layer accuracy
I1127 10:58:19.116448  5722 net.cpp:106] Creating Layer accuracy
I1127 10:58:19.116461  5722 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:58:19.116474  5722 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:58:19.116489  5722 net.cpp:411] accuracy -> accuracy
I1127 10:58:19.116518  5722 net.cpp:150] Setting up accuracy
I1127 10:58:19.116533  5722 net.cpp:157] Top shape: (1)
I1127 10:58:19.116541  5722 net.cpp:165] Memory required for data: 8086804
I1127 10:58:19.116552  5722 layer_factory.hpp:76] Creating layer loss
I1127 10:58:19.116580  5722 net.cpp:106] Creating Layer loss
I1127 10:58:19.116590  5722 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:58:19.116605  5722 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:58:19.116619  5722 net.cpp:411] loss -> loss
I1127 10:58:19.116647  5722 layer_factory.hpp:76] Creating layer loss
I1127 10:58:19.117027  5722 net.cpp:150] Setting up loss
I1127 10:58:19.117049  5722 net.cpp:157] Top shape: (1)
I1127 10:58:19.117058  5722 net.cpp:160]     with loss weight 1
I1127 10:58:19.117089  5722 net.cpp:165] Memory required for data: 8086808
I1127 10:58:19.117099  5722 net.cpp:226] loss needs backward computation.
I1127 10:58:19.117116  5722 net.cpp:228] accuracy does not need backward computation.
I1127 10:58:19.117125  5722 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:58:19.117133  5722 net.cpp:226] ip2 needs backward computation.
I1127 10:58:19.117141  5722 net.cpp:226] relu1 needs backward computation.
I1127 10:58:19.117149  5722 net.cpp:226] ip1 needs backward computation.
I1127 10:58:19.117157  5722 net.cpp:226] pool2 needs backward computation.
I1127 10:58:19.117166  5722 net.cpp:226] conv2 needs backward computation.
I1127 10:58:19.117173  5722 net.cpp:226] pool1 needs backward computation.
I1127 10:58:19.117182  5722 net.cpp:226] conv1 needs backward computation.
I1127 10:58:19.117192  5722 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:58:19.117200  5722 net.cpp:228] mnist does not need backward computation.
I1127 10:58:19.117207  5722 net.cpp:270] This network produces output accuracy
I1127 10:58:19.117215  5722 net.cpp:270] This network produces output loss
I1127 10:58:19.117238  5722 net.cpp:283] Network initialization done.
I1127 10:58:19.117466  5722 solver.cpp:59] Solver scaffolding done.
I1127 10:58:19.118113  5722 caffe.cpp:212] Starting Optimization
I1127 10:58:19.118170  5722 solver.cpp:287] Solving LeNet
I1127 10:58:19.118204  5722 solver.cpp:288] Learning Rate Policy: inv
I1127 10:58:19.119359  5722 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:58:19.120404  5722 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:58:20.282361  5722 solver.cpp:408]     Test net output #0: accuracy = 0.0943
I1127 10:58:20.282410  5722 solver.cpp:408]     Test net output #1: loss = 2.35341 (* 1 = 2.35341 loss)
I1127 10:58:20.294692  5722 solver.cpp:236] Iteration 0, loss = 2.37284
I1127 10:58:20.294771  5722 solver.cpp:252]     Train net output #0: loss = 2.37284 (* 1 = 2.37284 loss)
I1127 10:58:20.294806  5722 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:58:33.660643  5722 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:58:34.742697  5722 solver.cpp:408]     Test net output #0: accuracy = 0.9709
I1127 10:58:34.742759  5722 solver.cpp:408]     Test net output #1: loss = 0.0900646 (* 1 = 0.0900646 loss)
I1127 10:58:34.751641  5722 solver.cpp:236] Iteration 500, loss = 0.124822
I1127 10:58:34.751688  5722 solver.cpp:252]     Train net output #0: loss = 0.124822 (* 1 = 0.124822 loss)
I1127 10:58:34.751698  5722 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:58:48.101436  5722 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:58:48.116467  5722 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:58:48.143735  5722 solver.cpp:320] Iteration 1000, loss = 0.105652
I1127 10:58:48.143756  5722 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:58:51.215142  5722 solver.cpp:408]     Test net output #0: accuracy = 0.9821
I1127 10:58:51.215348  5722 solver.cpp:408]     Test net output #1: loss = 0.0576798 (* 1 = 0.0576798 loss)
I1127 10:58:51.215361  5722 solver.cpp:325] Optimization Done.
I1127 10:58:51.215371  5722 caffe.cpp:215] Optimization Done.
I1127 10:58:51.348791  5776 caffe.cpp:184] Using GPUs 0
I1127 10:58:51.719956  5776 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:58:51.720260  5776 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:58:51.720942  5776 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:58:51.721027  5776 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:58:51.721326  5776 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:58:51.721505  5776 layer_factory.hpp:76] Creating layer mnist
I1127 10:58:51.722271  5776 net.cpp:106] Creating Layer mnist
I1127 10:58:51.722357  5776 net.cpp:411] mnist -> data
I1127 10:58:51.722437  5776 net.cpp:411] mnist -> label
I1127 10:58:51.728282  5779 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:58:51.743299  5776 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:58:51.745434  5776 net.cpp:150] Setting up mnist
I1127 10:58:51.745554  5776 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:58:51.745578  5776 net.cpp:157] Top shape: 64 (64)
I1127 10:58:51.745586  5776 net.cpp:165] Memory required for data: 200960
I1127 10:58:51.745611  5776 layer_factory.hpp:76] Creating layer conv1
I1127 10:58:51.745663  5776 net.cpp:106] Creating Layer conv1
I1127 10:58:51.745679  5776 net.cpp:454] conv1 <- data
I1127 10:58:51.745710  5776 net.cpp:411] conv1 -> conv1
I1127 10:58:51.748955  5776 net.cpp:150] Setting up conv1
I1127 10:58:51.749109  5776 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:58:51.749131  5776 net.cpp:165] Memory required for data: 3150080
I1127 10:58:51.749194  5776 layer_factory.hpp:76] Creating layer pool1
I1127 10:58:51.749245  5776 net.cpp:106] Creating Layer pool1
I1127 10:58:51.749271  5776 net.cpp:454] pool1 <- conv1
I1127 10:58:51.749300  5776 net.cpp:411] pool1 -> pool1
I1127 10:58:51.749565  5776 net.cpp:150] Setting up pool1
I1127 10:58:51.749582  5776 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:58:51.749588  5776 net.cpp:165] Memory required for data: 3887360
I1127 10:58:51.749595  5776 layer_factory.hpp:76] Creating layer conv2
I1127 10:58:51.749624  5776 net.cpp:106] Creating Layer conv2
I1127 10:58:51.749632  5776 net.cpp:454] conv2 <- pool1
I1127 10:58:51.749644  5776 net.cpp:411] conv2 -> conv2
I1127 10:58:51.750124  5776 net.cpp:150] Setting up conv2
I1127 10:58:51.750161  5776 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:58:51.750169  5776 net.cpp:165] Memory required for data: 4706560
I1127 10:58:51.750187  5776 layer_factory.hpp:76] Creating layer pool2
I1127 10:58:51.750211  5776 net.cpp:106] Creating Layer pool2
I1127 10:58:51.750218  5776 net.cpp:454] pool2 <- conv2
I1127 10:58:51.750228  5776 net.cpp:411] pool2 -> pool2
I1127 10:58:51.750289  5776 net.cpp:150] Setting up pool2
I1127 10:58:51.750303  5776 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:58:51.750310  5776 net.cpp:165] Memory required for data: 4911360
I1127 10:58:51.750319  5776 layer_factory.hpp:76] Creating layer ip1
I1127 10:58:51.750339  5776 net.cpp:106] Creating Layer ip1
I1127 10:58:51.750349  5776 net.cpp:454] ip1 <- pool2
I1127 10:58:51.750367  5776 net.cpp:411] ip1 -> ip1
I1127 10:58:51.756100  5776 net.cpp:150] Setting up ip1
I1127 10:58:51.756249  5776 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:58:51.756266  5776 net.cpp:165] Memory required for data: 5039360
I1127 10:58:51.756310  5776 layer_factory.hpp:76] Creating layer relu1
I1127 10:58:51.756347  5776 net.cpp:106] Creating Layer relu1
I1127 10:58:51.756368  5776 net.cpp:454] relu1 <- ip1
I1127 10:58:51.756389  5776 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:58:51.756438  5776 net.cpp:150] Setting up relu1
I1127 10:58:51.756454  5776 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:58:51.756464  5776 net.cpp:165] Memory required for data: 5167360
I1127 10:58:51.756475  5776 layer_factory.hpp:76] Creating layer ip2
I1127 10:58:51.756510  5776 net.cpp:106] Creating Layer ip2
I1127 10:58:51.756551  5776 net.cpp:454] ip2 <- ip1
I1127 10:58:51.756573  5776 net.cpp:411] ip2 -> ip2
I1127 10:58:51.757683  5776 net.cpp:150] Setting up ip2
I1127 10:58:51.757745  5776 net.cpp:157] Top shape: 64 10 (640)
I1127 10:58:51.757752  5776 net.cpp:165] Memory required for data: 5169920
I1127 10:58:51.757766  5776 layer_factory.hpp:76] Creating layer loss
I1127 10:58:51.757784  5776 net.cpp:106] Creating Layer loss
I1127 10:58:51.757792  5776 net.cpp:454] loss <- ip2
I1127 10:58:51.757802  5776 net.cpp:454] loss <- label
I1127 10:58:51.757817  5776 net.cpp:411] loss -> loss
I1127 10:58:51.757839  5776 layer_factory.hpp:76] Creating layer loss
I1127 10:58:51.757938  5776 net.cpp:150] Setting up loss
I1127 10:58:51.757949  5776 net.cpp:157] Top shape: (1)
I1127 10:58:51.757956  5776 net.cpp:160]     with loss weight 1
I1127 10:58:51.757977  5776 net.cpp:165] Memory required for data: 5169924
I1127 10:58:51.757984  5776 net.cpp:226] loss needs backward computation.
I1127 10:58:51.757992  5776 net.cpp:226] ip2 needs backward computation.
I1127 10:58:51.757997  5776 net.cpp:226] relu1 needs backward computation.
I1127 10:58:51.758003  5776 net.cpp:226] ip1 needs backward computation.
I1127 10:58:51.758010  5776 net.cpp:226] pool2 needs backward computation.
I1127 10:58:51.758016  5776 net.cpp:226] conv2 needs backward computation.
I1127 10:58:51.758023  5776 net.cpp:226] pool1 needs backward computation.
I1127 10:58:51.758029  5776 net.cpp:226] conv1 needs backward computation.
I1127 10:58:51.758036  5776 net.cpp:228] mnist does not need backward computation.
I1127 10:58:51.758043  5776 net.cpp:270] This network produces output loss
I1127 10:58:51.758055  5776 net.cpp:283] Network initialization done.
I1127 10:58:51.767067  5776 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:58:51.767144  5776 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:58:51.767380  5776 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:58:51.767572  5776 layer_factory.hpp:76] Creating layer mnist
I1127 10:58:51.767827  5776 net.cpp:106] Creating Layer mnist
I1127 10:58:51.767861  5776 net.cpp:411] mnist -> data
I1127 10:58:51.767902  5776 net.cpp:411] mnist -> label
I1127 10:58:51.771865  5781 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:58:51.772281  5776 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:58:51.774663  5776 net.cpp:150] Setting up mnist
I1127 10:58:51.774777  5776 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:58:51.774793  5776 net.cpp:157] Top shape: 100 (100)
I1127 10:58:51.774801  5776 net.cpp:165] Memory required for data: 314000
I1127 10:58:51.774813  5776 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:58:51.774842  5776 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:58:51.774855  5776 net.cpp:454] label_mnist_1_split <- label
I1127 10:58:51.774870  5776 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:58:51.774891  5776 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:58:51.774996  5776 net.cpp:150] Setting up label_mnist_1_split
I1127 10:58:51.775019  5776 net.cpp:157] Top shape: 100 (100)
I1127 10:58:51.775034  5776 net.cpp:157] Top shape: 100 (100)
I1127 10:58:51.775046  5776 net.cpp:165] Memory required for data: 314800
I1127 10:58:51.775058  5776 layer_factory.hpp:76] Creating layer conv1
I1127 10:58:51.775091  5776 net.cpp:106] Creating Layer conv1
I1127 10:58:51.775104  5776 net.cpp:454] conv1 <- data
I1127 10:58:51.775120  5776 net.cpp:411] conv1 -> conv1
I1127 10:58:51.775434  5776 net.cpp:150] Setting up conv1
I1127 10:58:51.775456  5776 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:58:51.775466  5776 net.cpp:165] Memory required for data: 4922800
I1127 10:58:51.775487  5776 layer_factory.hpp:76] Creating layer pool1
I1127 10:58:51.775513  5776 net.cpp:106] Creating Layer pool1
I1127 10:58:51.775526  5776 net.cpp:454] pool1 <- conv1
I1127 10:58:51.775576  5776 net.cpp:411] pool1 -> pool1
I1127 10:58:51.775645  5776 net.cpp:150] Setting up pool1
I1127 10:58:51.775660  5776 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:58:51.775667  5776 net.cpp:165] Memory required for data: 6074800
I1127 10:58:51.775676  5776 layer_factory.hpp:76] Creating layer conv2
I1127 10:58:51.775699  5776 net.cpp:106] Creating Layer conv2
I1127 10:58:51.775710  5776 net.cpp:454] conv2 <- pool1
I1127 10:58:51.775724  5776 net.cpp:411] conv2 -> conv2
I1127 10:58:51.776188  5776 net.cpp:150] Setting up conv2
I1127 10:58:51.776212  5776 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:58:51.776221  5776 net.cpp:165] Memory required for data: 7354800
I1127 10:58:51.776242  5776 layer_factory.hpp:76] Creating layer pool2
I1127 10:58:51.776262  5776 net.cpp:106] Creating Layer pool2
I1127 10:58:51.776270  5776 net.cpp:454] pool2 <- conv2
I1127 10:58:51.776285  5776 net.cpp:411] pool2 -> pool2
I1127 10:58:51.776372  5776 net.cpp:150] Setting up pool2
I1127 10:58:51.776386  5776 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:58:51.776394  5776 net.cpp:165] Memory required for data: 7674800
I1127 10:58:51.776402  5776 layer_factory.hpp:76] Creating layer ip1
I1127 10:58:51.776419  5776 net.cpp:106] Creating Layer ip1
I1127 10:58:51.776427  5776 net.cpp:454] ip1 <- pool2
I1127 10:58:51.776440  5776 net.cpp:411] ip1 -> ip1
I1127 10:58:51.779912  5782 blocking_queue.cpp:50] Waiting for data
I1127 10:58:51.781138  5776 net.cpp:150] Setting up ip1
I1127 10:58:51.781237  5776 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:58:51.781251  5776 net.cpp:165] Memory required for data: 7874800
I1127 10:58:51.781287  5776 layer_factory.hpp:76] Creating layer relu1
I1127 10:58:51.781319  5776 net.cpp:106] Creating Layer relu1
I1127 10:58:51.781337  5776 net.cpp:454] relu1 <- ip1
I1127 10:58:51.781358  5776 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:58:51.781384  5776 net.cpp:150] Setting up relu1
I1127 10:58:51.781395  5776 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:58:51.781419  5776 net.cpp:165] Memory required for data: 8074800
I1127 10:58:51.781429  5776 layer_factory.hpp:76] Creating layer ip2
I1127 10:58:51.781448  5776 net.cpp:106] Creating Layer ip2
I1127 10:58:51.781458  5776 net.cpp:454] ip2 <- ip1
I1127 10:58:51.781473  5776 net.cpp:411] ip2 -> ip2
I1127 10:58:51.781694  5776 net.cpp:150] Setting up ip2
I1127 10:58:51.781711  5776 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:58:51.781721  5776 net.cpp:165] Memory required for data: 8078800
I1127 10:58:51.781735  5776 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:58:51.781756  5776 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:58:51.781776  5776 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:58:51.781798  5776 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:58:51.781828  5776 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:58:51.781934  5776 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:58:51.781956  5776 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:58:51.781972  5776 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:58:51.781983  5776 net.cpp:165] Memory required for data: 8086800
I1127 10:58:51.781998  5776 layer_factory.hpp:76] Creating layer accuracy
I1127 10:58:51.782017  5776 net.cpp:106] Creating Layer accuracy
I1127 10:58:51.782027  5776 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:58:51.782039  5776 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:58:51.782053  5776 net.cpp:411] accuracy -> accuracy
I1127 10:58:51.782083  5776 net.cpp:150] Setting up accuracy
I1127 10:58:51.782099  5776 net.cpp:157] Top shape: (1)
I1127 10:58:51.782107  5776 net.cpp:165] Memory required for data: 8086804
I1127 10:58:51.782119  5776 layer_factory.hpp:76] Creating layer loss
I1127 10:58:51.782135  5776 net.cpp:106] Creating Layer loss
I1127 10:58:51.786216  5776 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:58:51.786279  5776 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:58:51.786311  5776 net.cpp:411] loss -> loss
I1127 10:58:51.786391  5776 layer_factory.hpp:76] Creating layer loss
I1127 10:58:51.786638  5776 net.cpp:150] Setting up loss
I1127 10:58:51.786664  5776 net.cpp:157] Top shape: (1)
I1127 10:58:51.786670  5776 net.cpp:160]     with loss weight 1
I1127 10:58:51.786700  5776 net.cpp:165] Memory required for data: 8086808
I1127 10:58:51.786711  5776 net.cpp:226] loss needs backward computation.
I1127 10:58:51.786731  5776 net.cpp:228] accuracy does not need backward computation.
I1127 10:58:51.786742  5776 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:58:51.786749  5776 net.cpp:226] ip2 needs backward computation.
I1127 10:58:51.786757  5776 net.cpp:226] relu1 needs backward computation.
I1127 10:58:51.786766  5776 net.cpp:226] ip1 needs backward computation.
I1127 10:58:51.786772  5776 net.cpp:226] pool2 needs backward computation.
I1127 10:58:51.786779  5776 net.cpp:226] conv2 needs backward computation.
I1127 10:58:51.786787  5776 net.cpp:226] pool1 needs backward computation.
I1127 10:58:51.786794  5776 net.cpp:226] conv1 needs backward computation.
I1127 10:58:51.786803  5776 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:58:51.786809  5776 net.cpp:228] mnist does not need backward computation.
I1127 10:58:51.786816  5776 net.cpp:270] This network produces output accuracy
I1127 10:58:51.786824  5776 net.cpp:270] This network produces output loss
I1127 10:58:51.786841  5776 net.cpp:283] Network initialization done.
I1127 10:58:51.786977  5776 solver.cpp:59] Solver scaffolding done.
I1127 10:58:51.787380  5776 caffe.cpp:212] Starting Optimization
I1127 10:58:51.787395  5776 solver.cpp:287] Solving LeNet
I1127 10:58:51.787400  5776 solver.cpp:288] Learning Rate Policy: inv
I1127 10:58:51.788163  5776 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:58:51.789217  5776 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:58:52.917664  5776 solver.cpp:408]     Test net output #0: accuracy = 0.0935
I1127 10:58:52.917734  5776 solver.cpp:408]     Test net output #1: loss = 2.34814 (* 1 = 2.34814 loss)
I1127 10:58:52.928081  5776 solver.cpp:236] Iteration 0, loss = 2.2867
I1127 10:58:52.928154  5776 solver.cpp:252]     Train net output #0: loss = 2.2867 (* 1 = 2.2867 loss)
I1127 10:58:52.928181  5776 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:59:06.377483  5776 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:59:07.457360  5776 solver.cpp:408]     Test net output #0: accuracy = 0.9762
I1127 10:59:07.457473  5776 solver.cpp:408]     Test net output #1: loss = 0.0765871 (* 1 = 0.0765871 loss)
I1127 10:59:07.475052  5776 solver.cpp:236] Iteration 500, loss = 0.137748
I1127 10:59:07.475239  5776 solver.cpp:252]     Train net output #0: loss = 0.137748 (* 1 = 0.137748 loss)
I1127 10:59:07.475298  5776 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:59:20.804182  5776 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:59:20.822001  5776 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:59:20.835093  5776 solver.cpp:320] Iteration 1000, loss = 0.0690042
I1127 10:59:20.835222  5776 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:59:23.376056  5776 solver.cpp:408]     Test net output #0: accuracy = 0.984
I1127 10:59:23.376301  5776 solver.cpp:408]     Test net output #1: loss = 0.0521378 (* 1 = 0.0521378 loss)
I1127 10:59:23.376664  5776 solver.cpp:325] Optimization Done.
I1127 10:59:23.376675  5776 caffe.cpp:215] Optimization Done.
I1127 10:59:23.533905  5804 caffe.cpp:184] Using GPUs 0
I1127 10:59:23.930655  5804 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:59:23.930814  5804 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:59:23.931112  5804 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:59:23.931128  5804 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:59:23.931219  5804 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:59:23.931294  5804 layer_factory.hpp:76] Creating layer mnist
I1127 10:59:23.931651  5804 net.cpp:106] Creating Layer mnist
I1127 10:59:23.931663  5804 net.cpp:411] mnist -> data
I1127 10:59:23.931691  5804 net.cpp:411] mnist -> label
I1127 10:59:23.932515  5807 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:59:23.953833  5804 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:59:23.954686  5804 net.cpp:150] Setting up mnist
I1127 10:59:23.954725  5804 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:59:23.954733  5804 net.cpp:157] Top shape: 64 (64)
I1127 10:59:23.954737  5804 net.cpp:165] Memory required for data: 200960
I1127 10:59:23.954753  5804 layer_factory.hpp:76] Creating layer conv1
I1127 10:59:23.954787  5804 net.cpp:106] Creating Layer conv1
I1127 10:59:23.954795  5804 net.cpp:454] conv1 <- data
I1127 10:59:23.954812  5804 net.cpp:411] conv1 -> conv1
I1127 10:59:23.955593  5804 net.cpp:150] Setting up conv1
I1127 10:59:23.955641  5804 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:59:23.955647  5804 net.cpp:165] Memory required for data: 3150080
I1127 10:59:23.955674  5804 layer_factory.hpp:76] Creating layer pool1
I1127 10:59:23.955701  5804 net.cpp:106] Creating Layer pool1
I1127 10:59:23.955711  5804 net.cpp:454] pool1 <- conv1
I1127 10:59:23.955725  5804 net.cpp:411] pool1 -> pool1
I1127 10:59:23.955873  5804 net.cpp:150] Setting up pool1
I1127 10:59:23.955889  5804 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:59:23.955896  5804 net.cpp:165] Memory required for data: 3887360
I1127 10:59:23.955905  5804 layer_factory.hpp:76] Creating layer conv2
I1127 10:59:23.955927  5804 net.cpp:106] Creating Layer conv2
I1127 10:59:23.955936  5804 net.cpp:454] conv2 <- pool1
I1127 10:59:23.955948  5804 net.cpp:411] conv2 -> conv2
I1127 10:59:23.956449  5804 net.cpp:150] Setting up conv2
I1127 10:59:23.956467  5804 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:59:23.956475  5804 net.cpp:165] Memory required for data: 4706560
I1127 10:59:23.956495  5804 layer_factory.hpp:76] Creating layer pool2
I1127 10:59:23.956511  5804 net.cpp:106] Creating Layer pool2
I1127 10:59:23.956519  5804 net.cpp:454] pool2 <- conv2
I1127 10:59:23.956531  5804 net.cpp:411] pool2 -> pool2
I1127 10:59:23.956574  5804 net.cpp:150] Setting up pool2
I1127 10:59:23.956586  5804 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:59:23.956594  5804 net.cpp:165] Memory required for data: 4911360
I1127 10:59:23.956603  5804 layer_factory.hpp:76] Creating layer ip1
I1127 10:59:23.956616  5804 net.cpp:106] Creating Layer ip1
I1127 10:59:23.956624  5804 net.cpp:454] ip1 <- pool2
I1127 10:59:23.956637  5804 net.cpp:411] ip1 -> ip1
I1127 10:59:23.960227  5804 net.cpp:150] Setting up ip1
I1127 10:59:23.960314  5804 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:59:23.960319  5804 net.cpp:165] Memory required for data: 5039360
I1127 10:59:23.960342  5804 layer_factory.hpp:76] Creating layer relu1
I1127 10:59:23.960361  5804 net.cpp:106] Creating Layer relu1
I1127 10:59:23.960368  5804 net.cpp:454] relu1 <- ip1
I1127 10:59:23.960381  5804 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:59:23.960407  5804 net.cpp:150] Setting up relu1
I1127 10:59:23.960412  5804 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:59:23.960417  5804 net.cpp:165] Memory required for data: 5167360
I1127 10:59:23.960422  5804 layer_factory.hpp:76] Creating layer ip2
I1127 10:59:23.960435  5804 net.cpp:106] Creating Layer ip2
I1127 10:59:23.960439  5804 net.cpp:454] ip2 <- ip1
I1127 10:59:23.960448  5804 net.cpp:411] ip2 -> ip2
I1127 10:59:23.961084  5804 net.cpp:150] Setting up ip2
I1127 10:59:23.961127  5804 net.cpp:157] Top shape: 64 10 (640)
I1127 10:59:23.961133  5804 net.cpp:165] Memory required for data: 5169920
I1127 10:59:23.961143  5804 layer_factory.hpp:76] Creating layer loss
I1127 10:59:23.961158  5804 net.cpp:106] Creating Layer loss
I1127 10:59:23.961169  5804 net.cpp:454] loss <- ip2
I1127 10:59:23.961175  5804 net.cpp:454] loss <- label
I1127 10:59:23.961186  5804 net.cpp:411] loss -> loss
I1127 10:59:23.961210  5804 layer_factory.hpp:76] Creating layer loss
I1127 10:59:23.961287  5804 net.cpp:150] Setting up loss
I1127 10:59:23.961294  5804 net.cpp:157] Top shape: (1)
I1127 10:59:23.961299  5804 net.cpp:160]     with loss weight 1
I1127 10:59:23.961318  5804 net.cpp:165] Memory required for data: 5169924
I1127 10:59:23.961323  5804 net.cpp:226] loss needs backward computation.
I1127 10:59:23.961326  5804 net.cpp:226] ip2 needs backward computation.
I1127 10:59:23.961331  5804 net.cpp:226] relu1 needs backward computation.
I1127 10:59:23.961335  5804 net.cpp:226] ip1 needs backward computation.
I1127 10:59:23.961339  5804 net.cpp:226] pool2 needs backward computation.
I1127 10:59:23.961344  5804 net.cpp:226] conv2 needs backward computation.
I1127 10:59:23.961349  5804 net.cpp:226] pool1 needs backward computation.
I1127 10:59:23.961352  5804 net.cpp:226] conv1 needs backward computation.
I1127 10:59:23.961356  5804 net.cpp:228] mnist does not need backward computation.
I1127 10:59:23.961360  5804 net.cpp:270] This network produces output loss
I1127 10:59:23.961369  5804 net.cpp:283] Network initialization done.
I1127 10:59:23.961618  5804 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:59:23.961644  5804 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:59:23.961762  5804 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:59:23.961838  5804 layer_factory.hpp:76] Creating layer mnist
I1127 10:59:24.002315  5804 net.cpp:106] Creating Layer mnist
I1127 10:59:24.002389  5804 net.cpp:411] mnist -> data
I1127 10:59:24.002434  5804 net.cpp:411] mnist -> label
I1127 10:59:24.003795  5810 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:59:24.004067  5804 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:59:24.010241  5804 net.cpp:150] Setting up mnist
I1127 10:59:24.010287  5804 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:59:24.010294  5804 net.cpp:157] Top shape: 100 (100)
I1127 10:59:24.010300  5804 net.cpp:165] Memory required for data: 314000
I1127 10:59:24.010308  5804 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:59:24.010321  5804 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:59:24.010329  5804 net.cpp:454] label_mnist_1_split <- label
I1127 10:59:24.010336  5804 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:59:24.010350  5804 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:59:24.010433  5804 net.cpp:150] Setting up label_mnist_1_split
I1127 10:59:24.010442  5804 net.cpp:157] Top shape: 100 (100)
I1127 10:59:24.010447  5804 net.cpp:157] Top shape: 100 (100)
I1127 10:59:24.010452  5804 net.cpp:165] Memory required for data: 314800
I1127 10:59:24.010457  5804 layer_factory.hpp:76] Creating layer conv1
I1127 10:59:24.010471  5804 net.cpp:106] Creating Layer conv1
I1127 10:59:24.010475  5804 net.cpp:454] conv1 <- data
I1127 10:59:24.010483  5804 net.cpp:411] conv1 -> conv1
I1127 10:59:24.010686  5804 net.cpp:150] Setting up conv1
I1127 10:59:24.010706  5804 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:59:24.010710  5804 net.cpp:165] Memory required for data: 4922800
I1127 10:59:24.010723  5804 layer_factory.hpp:76] Creating layer pool1
I1127 10:59:24.010737  5804 net.cpp:106] Creating Layer pool1
I1127 10:59:24.010743  5804 net.cpp:454] pool1 <- conv1
I1127 10:59:24.010782  5804 net.cpp:411] pool1 -> pool1
I1127 10:59:24.010833  5804 net.cpp:150] Setting up pool1
I1127 10:59:24.010844  5804 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:59:24.010849  5804 net.cpp:165] Memory required for data: 6074800
I1127 10:59:24.010854  5804 layer_factory.hpp:76] Creating layer conv2
I1127 10:59:24.010870  5804 net.cpp:106] Creating Layer conv2
I1127 10:59:24.010875  5804 net.cpp:454] conv2 <- pool1
I1127 10:59:24.010885  5804 net.cpp:411] conv2 -> conv2
I1127 10:59:24.011189  5804 net.cpp:150] Setting up conv2
I1127 10:59:24.011205  5804 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:59:24.011209  5804 net.cpp:165] Memory required for data: 7354800
I1127 10:59:24.011220  5804 layer_factory.hpp:76] Creating layer pool2
I1127 10:59:24.011235  5804 net.cpp:106] Creating Layer pool2
I1127 10:59:24.011240  5804 net.cpp:454] pool2 <- conv2
I1127 10:59:24.011247  5804 net.cpp:411] pool2 -> pool2
I1127 10:59:24.011286  5804 net.cpp:150] Setting up pool2
I1127 10:59:24.011293  5804 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:59:24.011298  5804 net.cpp:165] Memory required for data: 7674800
I1127 10:59:24.011302  5804 layer_factory.hpp:76] Creating layer ip1
I1127 10:59:24.011317  5804 net.cpp:106] Creating Layer ip1
I1127 10:59:24.011322  5804 net.cpp:454] ip1 <- pool2
I1127 10:59:24.011327  5804 net.cpp:411] ip1 -> ip1
I1127 10:59:24.014636  5804 net.cpp:150] Setting up ip1
I1127 10:59:24.014694  5804 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:59:24.014703  5804 net.cpp:165] Memory required for data: 7874800
I1127 10:59:24.014731  5804 layer_factory.hpp:76] Creating layer relu1
I1127 10:59:24.014750  5804 net.cpp:106] Creating Layer relu1
I1127 10:59:24.014761  5804 net.cpp:454] relu1 <- ip1
I1127 10:59:24.014778  5804 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:59:24.014797  5804 net.cpp:150] Setting up relu1
I1127 10:59:24.014809  5804 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:59:24.014819  5804 net.cpp:165] Memory required for data: 8074800
I1127 10:59:24.014828  5804 layer_factory.hpp:76] Creating layer ip2
I1127 10:59:24.014844  5804 net.cpp:106] Creating Layer ip2
I1127 10:59:24.014858  5804 net.cpp:454] ip2 <- ip1
I1127 10:59:24.014871  5804 net.cpp:411] ip2 -> ip2
I1127 10:59:24.015044  5804 net.cpp:150] Setting up ip2
I1127 10:59:24.015061  5804 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:59:24.015069  5804 net.cpp:165] Memory required for data: 8078800
I1127 10:59:24.015094  5804 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:59:24.015106  5804 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:59:24.015113  5804 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:59:24.015123  5804 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:59:24.015136  5804 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:59:24.015182  5804 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:59:24.015192  5804 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:59:24.015200  5804 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:59:24.015207  5804 net.cpp:165] Memory required for data: 8086800
I1127 10:59:24.015213  5804 layer_factory.hpp:76] Creating layer accuracy
I1127 10:59:24.015225  5804 net.cpp:106] Creating Layer accuracy
I1127 10:59:24.015233  5804 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:59:24.015240  5804 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:59:24.015250  5804 net.cpp:411] accuracy -> accuracy
I1127 10:59:24.015264  5804 net.cpp:150] Setting up accuracy
I1127 10:59:24.015272  5804 net.cpp:157] Top shape: (1)
I1127 10:59:24.015278  5804 net.cpp:165] Memory required for data: 8086804
I1127 10:59:24.015285  5804 layer_factory.hpp:76] Creating layer loss
I1127 10:59:24.015295  5804 net.cpp:106] Creating Layer loss
I1127 10:59:24.015301  5804 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:59:24.015308  5804 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:59:24.015318  5804 net.cpp:411] loss -> loss
I1127 10:59:24.015334  5804 layer_factory.hpp:76] Creating layer loss
I1127 10:59:24.015593  5804 net.cpp:150] Setting up loss
I1127 10:59:24.015605  5804 net.cpp:157] Top shape: (1)
I1127 10:59:24.015612  5804 net.cpp:160]     with loss weight 1
I1127 10:59:24.015626  5804 net.cpp:165] Memory required for data: 8086808
I1127 10:59:24.015635  5804 net.cpp:226] loss needs backward computation.
I1127 10:59:24.015645  5804 net.cpp:228] accuracy does not need backward computation.
I1127 10:59:24.015653  5804 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:59:24.015661  5804 net.cpp:226] ip2 needs backward computation.
I1127 10:59:24.015667  5804 net.cpp:226] relu1 needs backward computation.
I1127 10:59:24.015676  5804 net.cpp:226] ip1 needs backward computation.
I1127 10:59:24.015689  5804 net.cpp:226] pool2 needs backward computation.
I1127 10:59:24.015697  5804 net.cpp:226] conv2 needs backward computation.
I1127 10:59:24.015704  5804 net.cpp:226] pool1 needs backward computation.
I1127 10:59:24.015712  5804 net.cpp:226] conv1 needs backward computation.
I1127 10:59:24.015727  5804 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:59:24.015734  5804 net.cpp:228] mnist does not need backward computation.
I1127 10:59:24.015741  5804 net.cpp:270] This network produces output accuracy
I1127 10:59:24.015753  5804 net.cpp:270] This network produces output loss
I1127 10:59:24.015774  5804 net.cpp:283] Network initialization done.
I1127 10:59:24.015866  5804 solver.cpp:59] Solver scaffolding done.
I1127 10:59:24.016194  5804 caffe.cpp:212] Starting Optimization
I1127 10:59:24.016211  5804 solver.cpp:287] Solving LeNet
I1127 10:59:24.016219  5804 solver.cpp:288] Learning Rate Policy: inv
I1127 10:59:24.016965  5804 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:59:24.917582  5804 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 10:59:25.089042  5804 solver.cpp:408]     Test net output #0: accuracy = 0.0711
I1127 10:59:25.089155  5804 solver.cpp:408]     Test net output #1: loss = 2.33223 (* 1 = 2.33223 loss)
I1127 10:59:25.101707  5804 solver.cpp:236] Iteration 0, loss = 2.3141
I1127 10:59:25.101847  5804 solver.cpp:252]     Train net output #0: loss = 2.3141 (* 1 = 2.3141 loss)
I1127 10:59:25.101878  5804 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 10:59:38.510679  5804 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 10:59:39.612588  5804 solver.cpp:408]     Test net output #0: accuracy = 0.9725
I1127 10:59:39.612723  5804 solver.cpp:408]     Test net output #1: loss = 0.0889395 (* 1 = 0.0889395 loss)
I1127 10:59:39.625221  5804 solver.cpp:236] Iteration 500, loss = 0.143816
I1127 10:59:39.625331  5804 solver.cpp:252]     Train net output #0: loss = 0.143816 (* 1 = 0.143816 loss)
I1127 10:59:39.625349  5804 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 10:59:52.126024  5804 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 10:59:52.144793  5804 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 10:59:52.171071  5804 solver.cpp:320] Iteration 1000, loss = 0.104687
I1127 10:59:52.171092  5804 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 10:59:54.249861  5804 solver.cpp:408]     Test net output #0: accuracy = 0.9797
I1127 10:59:54.250028  5804 solver.cpp:408]     Test net output #1: loss = 0.063766 (* 1 = 0.063766 loss)
I1127 10:59:54.250043  5804 solver.cpp:325] Optimization Done.
I1127 10:59:54.250053  5804 caffe.cpp:215] Optimization Done.
I1127 10:59:54.354154  5830 caffe.cpp:184] Using GPUs 0
I1127 10:59:54.665020  5830 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 10:59:54.665241  5830 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 10:59:54.665674  5830 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 10:59:54.665699  5830 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 10:59:54.665843  5830 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:59:54.665925  5830 layer_factory.hpp:76] Creating layer mnist
I1127 10:59:54.666474  5830 net.cpp:106] Creating Layer mnist
I1127 10:59:54.666501  5830 net.cpp:411] mnist -> data
I1127 10:59:54.666540  5830 net.cpp:411] mnist -> label
I1127 10:59:54.667454  5833 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 10:59:54.676131  5830 data_layer.cpp:41] output data size: 64,1,28,28
I1127 10:59:54.676873  5830 net.cpp:150] Setting up mnist
I1127 10:59:54.676897  5830 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 10:59:54.676903  5830 net.cpp:157] Top shape: 64 (64)
I1127 10:59:54.676908  5830 net.cpp:165] Memory required for data: 200960
I1127 10:59:54.676916  5830 layer_factory.hpp:76] Creating layer conv1
I1127 10:59:54.676935  5830 net.cpp:106] Creating Layer conv1
I1127 10:59:54.676942  5830 net.cpp:454] conv1 <- data
I1127 10:59:54.676952  5830 net.cpp:411] conv1 -> conv1
I1127 10:59:54.677541  5830 net.cpp:150] Setting up conv1
I1127 10:59:54.677563  5830 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 10:59:54.677569  5830 net.cpp:165] Memory required for data: 3150080
I1127 10:59:54.677582  5830 layer_factory.hpp:76] Creating layer pool1
I1127 10:59:54.677592  5830 net.cpp:106] Creating Layer pool1
I1127 10:59:54.677597  5830 net.cpp:454] pool1 <- conv1
I1127 10:59:54.677605  5830 net.cpp:411] pool1 -> pool1
I1127 10:59:54.677654  5830 net.cpp:150] Setting up pool1
I1127 10:59:54.677662  5830 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 10:59:54.677666  5830 net.cpp:165] Memory required for data: 3887360
I1127 10:59:54.677670  5830 layer_factory.hpp:76] Creating layer conv2
I1127 10:59:54.677680  5830 net.cpp:106] Creating Layer conv2
I1127 10:59:54.677686  5830 net.cpp:454] conv2 <- pool1
I1127 10:59:54.677692  5830 net.cpp:411] conv2 -> conv2
I1127 10:59:54.677990  5830 net.cpp:150] Setting up conv2
I1127 10:59:54.678001  5830 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 10:59:54.678005  5830 net.cpp:165] Memory required for data: 4706560
I1127 10:59:54.678014  5830 layer_factory.hpp:76] Creating layer pool2
I1127 10:59:54.678024  5830 net.cpp:106] Creating Layer pool2
I1127 10:59:54.678027  5830 net.cpp:454] pool2 <- conv2
I1127 10:59:54.678033  5830 net.cpp:411] pool2 -> pool2
I1127 10:59:54.678097  5830 net.cpp:150] Setting up pool2
I1127 10:59:54.678107  5830 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 10:59:54.678110  5830 net.cpp:165] Memory required for data: 4911360
I1127 10:59:54.678115  5830 layer_factory.hpp:76] Creating layer ip1
I1127 10:59:54.678128  5830 net.cpp:106] Creating Layer ip1
I1127 10:59:54.678133  5830 net.cpp:454] ip1 <- pool2
I1127 10:59:54.678138  5830 net.cpp:411] ip1 -> ip1
I1127 10:59:54.680605  5830 net.cpp:150] Setting up ip1
I1127 10:59:54.680636  5830 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:59:54.680641  5830 net.cpp:165] Memory required for data: 5039360
I1127 10:59:54.680655  5830 layer_factory.hpp:76] Creating layer relu1
I1127 10:59:54.680665  5830 net.cpp:106] Creating Layer relu1
I1127 10:59:54.680670  5830 net.cpp:454] relu1 <- ip1
I1127 10:59:54.680677  5830 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:59:54.680690  5830 net.cpp:150] Setting up relu1
I1127 10:59:54.680694  5830 net.cpp:157] Top shape: 64 500 (32000)
I1127 10:59:54.680698  5830 net.cpp:165] Memory required for data: 5167360
I1127 10:59:54.680702  5830 layer_factory.hpp:76] Creating layer ip2
I1127 10:59:54.680711  5830 net.cpp:106] Creating Layer ip2
I1127 10:59:54.680716  5830 net.cpp:454] ip2 <- ip1
I1127 10:59:54.680722  5830 net.cpp:411] ip2 -> ip2
I1127 10:59:54.681196  5830 net.cpp:150] Setting up ip2
I1127 10:59:54.681210  5830 net.cpp:157] Top shape: 64 10 (640)
I1127 10:59:54.681213  5830 net.cpp:165] Memory required for data: 5169920
I1127 10:59:54.681221  5830 layer_factory.hpp:76] Creating layer loss
I1127 10:59:54.681229  5830 net.cpp:106] Creating Layer loss
I1127 10:59:54.681234  5830 net.cpp:454] loss <- ip2
I1127 10:59:54.681239  5830 net.cpp:454] loss <- label
I1127 10:59:54.681252  5830 net.cpp:411] loss -> loss
I1127 10:59:54.681264  5830 layer_factory.hpp:76] Creating layer loss
I1127 10:59:54.681334  5830 net.cpp:150] Setting up loss
I1127 10:59:54.681341  5830 net.cpp:157] Top shape: (1)
I1127 10:59:54.681345  5830 net.cpp:160]     with loss weight 1
I1127 10:59:54.681365  5830 net.cpp:165] Memory required for data: 5169924
I1127 10:59:54.681375  5830 net.cpp:226] loss needs backward computation.
I1127 10:59:54.681381  5830 net.cpp:226] ip2 needs backward computation.
I1127 10:59:54.681385  5830 net.cpp:226] relu1 needs backward computation.
I1127 10:59:54.681390  5830 net.cpp:226] ip1 needs backward computation.
I1127 10:59:54.681393  5830 net.cpp:226] pool2 needs backward computation.
I1127 10:59:54.681398  5830 net.cpp:226] conv2 needs backward computation.
I1127 10:59:54.681402  5830 net.cpp:226] pool1 needs backward computation.
I1127 10:59:54.681406  5830 net.cpp:226] conv1 needs backward computation.
I1127 10:59:54.681412  5830 net.cpp:228] mnist does not need backward computation.
I1127 10:59:54.681416  5830 net.cpp:270] This network produces output loss
I1127 10:59:54.681426  5830 net.cpp:283] Network initialization done.
I1127 10:59:54.681674  5830 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 10:59:54.681704  5830 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 10:59:54.681825  5830 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 10:59:54.681897  5830 layer_factory.hpp:76] Creating layer mnist
I1127 10:59:54.682015  5830 net.cpp:106] Creating Layer mnist
I1127 10:59:54.682040  5830 net.cpp:411] mnist -> data
I1127 10:59:54.682051  5830 net.cpp:411] mnist -> label
I1127 10:59:54.683123  5835 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 10:59:54.683243  5830 data_layer.cpp:41] output data size: 100,1,28,28
I1127 10:59:54.684113  5830 net.cpp:150] Setting up mnist
I1127 10:59:54.684134  5830 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 10:59:54.684140  5830 net.cpp:157] Top shape: 100 (100)
I1127 10:59:54.684144  5830 net.cpp:165] Memory required for data: 314000
I1127 10:59:54.684150  5830 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 10:59:54.684162  5830 net.cpp:106] Creating Layer label_mnist_1_split
I1127 10:59:54.684175  5830 net.cpp:454] label_mnist_1_split <- label
I1127 10:59:54.684182  5830 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 10:59:54.684192  5830 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 10:59:54.684224  5830 net.cpp:150] Setting up label_mnist_1_split
I1127 10:59:54.684232  5830 net.cpp:157] Top shape: 100 (100)
I1127 10:59:54.684237  5830 net.cpp:157] Top shape: 100 (100)
I1127 10:59:54.684242  5830 net.cpp:165] Memory required for data: 314800
I1127 10:59:54.684245  5830 layer_factory.hpp:76] Creating layer conv1
I1127 10:59:54.684257  5830 net.cpp:106] Creating Layer conv1
I1127 10:59:54.684262  5830 net.cpp:454] conv1 <- data
I1127 10:59:54.684269  5830 net.cpp:411] conv1 -> conv1
I1127 10:59:54.684427  5830 net.cpp:150] Setting up conv1
I1127 10:59:54.684435  5830 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 10:59:54.684439  5830 net.cpp:165] Memory required for data: 4922800
I1127 10:59:54.684449  5830 layer_factory.hpp:76] Creating layer pool1
I1127 10:59:54.684458  5830 net.cpp:106] Creating Layer pool1
I1127 10:59:54.684461  5830 net.cpp:454] pool1 <- conv1
I1127 10:59:54.684478  5830 net.cpp:411] pool1 -> pool1
I1127 10:59:54.684509  5830 net.cpp:150] Setting up pool1
I1127 10:59:54.684517  5830 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 10:59:54.684521  5830 net.cpp:165] Memory required for data: 6074800
I1127 10:59:54.684526  5830 layer_factory.hpp:76] Creating layer conv2
I1127 10:59:54.684535  5830 net.cpp:106] Creating Layer conv2
I1127 10:59:54.684540  5830 net.cpp:454] conv2 <- pool1
I1127 10:59:54.684548  5830 net.cpp:411] conv2 -> conv2
I1127 10:59:54.684799  5830 net.cpp:150] Setting up conv2
I1127 10:59:54.684808  5830 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 10:59:54.684813  5830 net.cpp:165] Memory required for data: 7354800
I1127 10:59:54.684821  5830 layer_factory.hpp:76] Creating layer pool2
I1127 10:59:54.684828  5830 net.cpp:106] Creating Layer pool2
I1127 10:59:54.684833  5830 net.cpp:454] pool2 <- conv2
I1127 10:59:54.684839  5830 net.cpp:411] pool2 -> pool2
I1127 10:59:54.684865  5830 net.cpp:150] Setting up pool2
I1127 10:59:54.684873  5830 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 10:59:54.684877  5830 net.cpp:165] Memory required for data: 7674800
I1127 10:59:54.684882  5830 layer_factory.hpp:76] Creating layer ip1
I1127 10:59:54.684892  5830 net.cpp:106] Creating Layer ip1
I1127 10:59:54.684896  5830 net.cpp:454] ip1 <- pool2
I1127 10:59:54.684905  5830 net.cpp:411] ip1 -> ip1
I1127 10:59:54.687577  5830 net.cpp:150] Setting up ip1
I1127 10:59:54.687630  5830 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:59:54.687639  5830 net.cpp:165] Memory required for data: 7874800
I1127 10:59:54.687662  5830 layer_factory.hpp:76] Creating layer relu1
I1127 10:59:54.687680  5830 net.cpp:106] Creating Layer relu1
I1127 10:59:54.687695  5830 net.cpp:454] relu1 <- ip1
I1127 10:59:54.687708  5830 net.cpp:397] relu1 -> ip1 (in-place)
I1127 10:59:54.687724  5830 net.cpp:150] Setting up relu1
I1127 10:59:54.687734  5830 net.cpp:157] Top shape: 100 500 (50000)
I1127 10:59:54.687741  5830 net.cpp:165] Memory required for data: 8074800
I1127 10:59:54.687747  5830 layer_factory.hpp:76] Creating layer ip2
I1127 10:59:54.687763  5830 net.cpp:106] Creating Layer ip2
I1127 10:59:54.687772  5830 net.cpp:454] ip2 <- ip1
I1127 10:59:54.687785  5830 net.cpp:411] ip2 -> ip2
I1127 10:59:54.687961  5830 net.cpp:150] Setting up ip2
I1127 10:59:54.687973  5830 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:59:54.687980  5830 net.cpp:165] Memory required for data: 8078800
I1127 10:59:54.687991  5830 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 10:59:54.688002  5830 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 10:59:54.688009  5830 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 10:59:54.688019  5830 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 10:59:54.688030  5830 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 10:59:54.688072  5830 net.cpp:150] Setting up ip2_ip2_0_split
I1127 10:59:54.688086  5830 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:59:54.688105  5830 net.cpp:157] Top shape: 100 10 (1000)
I1127 10:59:54.688112  5830 net.cpp:165] Memory required for data: 8086800
I1127 10:59:54.688118  5830 layer_factory.hpp:76] Creating layer accuracy
I1127 10:59:54.688130  5830 net.cpp:106] Creating Layer accuracy
I1127 10:59:54.688138  5830 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 10:59:54.688148  5830 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 10:59:54.688154  5830 net.cpp:411] accuracy -> accuracy
I1127 10:59:54.688165  5830 net.cpp:150] Setting up accuracy
I1127 10:59:54.688171  5830 net.cpp:157] Top shape: (1)
I1127 10:59:54.688175  5830 net.cpp:165] Memory required for data: 8086804
I1127 10:59:54.688179  5830 layer_factory.hpp:76] Creating layer loss
I1127 10:59:54.688187  5830 net.cpp:106] Creating Layer loss
I1127 10:59:54.688192  5830 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 10:59:54.688197  5830 net.cpp:454] loss <- label_mnist_1_split_1
I1127 10:59:54.688204  5830 net.cpp:411] loss -> loss
I1127 10:59:54.688212  5830 layer_factory.hpp:76] Creating layer loss
I1127 10:59:54.688288  5830 net.cpp:150] Setting up loss
I1127 10:59:54.688297  5830 net.cpp:157] Top shape: (1)
I1127 10:59:54.688300  5830 net.cpp:160]     with loss weight 1
I1127 10:59:54.688313  5830 net.cpp:165] Memory required for data: 8086808
I1127 10:59:54.688318  5830 net.cpp:226] loss needs backward computation.
I1127 10:59:54.688326  5830 net.cpp:228] accuracy does not need backward computation.
I1127 10:59:54.688331  5830 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 10:59:54.688335  5830 net.cpp:226] ip2 needs backward computation.
I1127 10:59:54.688340  5830 net.cpp:226] relu1 needs backward computation.
I1127 10:59:54.688344  5830 net.cpp:226] ip1 needs backward computation.
I1127 10:59:54.688349  5830 net.cpp:226] pool2 needs backward computation.
I1127 10:59:54.688354  5830 net.cpp:226] conv2 needs backward computation.
I1127 10:59:54.688357  5830 net.cpp:226] pool1 needs backward computation.
I1127 10:59:54.688361  5830 net.cpp:226] conv1 needs backward computation.
I1127 10:59:54.688366  5830 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 10:59:54.688371  5830 net.cpp:228] mnist does not need backward computation.
I1127 10:59:54.688375  5830 net.cpp:270] This network produces output accuracy
I1127 10:59:54.688380  5830 net.cpp:270] This network produces output loss
I1127 10:59:54.688390  5830 net.cpp:283] Network initialization done.
I1127 10:59:54.688436  5830 solver.cpp:59] Solver scaffolding done.
I1127 10:59:54.688626  5830 caffe.cpp:212] Starting Optimization
I1127 10:59:54.688632  5830 solver.cpp:287] Solving LeNet
I1127 10:59:54.688637  5830 solver.cpp:288] Learning Rate Policy: inv
I1127 10:59:54.689059  5830 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 10:59:57.803017  5830 solver.cpp:408]     Test net output #0: accuracy = 0.0575
I1127 10:59:57.803115  5830 solver.cpp:408]     Test net output #1: loss = 2.47038 (* 1 = 2.47038 loss)
I1127 10:59:57.816534  5830 solver.cpp:236] Iteration 0, loss = 2.40197
I1127 10:59:57.816622  5830 solver.cpp:252]     Train net output #0: loss = 2.40197 (* 1 = 2.40197 loss)
I1127 10:59:57.816644  5830 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:00:11.213860  5830 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:00:12.339570  5830 solver.cpp:408]     Test net output #0: accuracy = 0.9709
I1127 11:00:12.339653  5830 solver.cpp:408]     Test net output #1: loss = 0.0900428 (* 1 = 0.0900428 loss)
I1127 11:00:12.352452  5830 solver.cpp:236] Iteration 500, loss = 0.0856353
I1127 11:00:12.352563  5830 solver.cpp:252]     Train net output #0: loss = 0.0856352 (* 1 = 0.0856352 loss)
I1127 11:00:12.352588  5830 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:00:15.902820  5830 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:00:25.366261  5830 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:00:25.386410  5830 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:00:25.413336  5830 solver.cpp:320] Iteration 1000, loss = 0.0780665
I1127 11:00:25.413357  5830 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:00:26.986032  5830 solver.cpp:408]     Test net output #0: accuracy = 0.98
I1127 11:00:26.986086  5830 solver.cpp:408]     Test net output #1: loss = 0.0595471 (* 1 = 0.0595471 loss)
I1127 11:00:26.986094  5830 solver.cpp:325] Optimization Done.
I1127 11:00:26.986099  5830 caffe.cpp:215] Optimization Done.
I1127 11:00:27.091826  5859 caffe.cpp:184] Using GPUs 0
I1127 11:00:27.419345  5859 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:00:27.419517  5859 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:00:27.419863  5859 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:00:27.419883  5859 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:00:27.419987  5859 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:00:27.420048  5859 layer_factory.hpp:76] Creating layer mnist
I1127 11:00:27.420439  5859 net.cpp:106] Creating Layer mnist
I1127 11:00:27.420457  5859 net.cpp:411] mnist -> data
I1127 11:00:27.420492  5859 net.cpp:411] mnist -> label
I1127 11:00:27.421569  5862 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:00:27.430467  5859 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:00:27.432014  5859 net.cpp:150] Setting up mnist
I1127 11:00:27.432080  5859 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:00:27.432087  5859 net.cpp:157] Top shape: 64 (64)
I1127 11:00:27.432091  5859 net.cpp:165] Memory required for data: 200960
I1127 11:00:27.432106  5859 layer_factory.hpp:76] Creating layer conv1
I1127 11:00:27.432145  5859 net.cpp:106] Creating Layer conv1
I1127 11:00:27.432154  5859 net.cpp:454] conv1 <- data
I1127 11:00:27.432168  5859 net.cpp:411] conv1 -> conv1
I1127 11:00:27.433364  5859 net.cpp:150] Setting up conv1
I1127 11:00:27.433470  5859 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:00:27.433477  5859 net.cpp:165] Memory required for data: 3150080
I1127 11:00:27.433508  5859 layer_factory.hpp:76] Creating layer pool1
I1127 11:00:27.433540  5859 net.cpp:106] Creating Layer pool1
I1127 11:00:27.433548  5859 net.cpp:454] pool1 <- conv1
I1127 11:00:27.433558  5859 net.cpp:411] pool1 -> pool1
I1127 11:00:27.433676  5859 net.cpp:150] Setting up pool1
I1127 11:00:27.433686  5859 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:00:27.433691  5859 net.cpp:165] Memory required for data: 3887360
I1127 11:00:27.433696  5859 layer_factory.hpp:76] Creating layer conv2
I1127 11:00:27.433707  5859 net.cpp:106] Creating Layer conv2
I1127 11:00:27.433712  5859 net.cpp:454] conv2 <- pool1
I1127 11:00:27.433718  5859 net.cpp:411] conv2 -> conv2
I1127 11:00:27.433974  5859 net.cpp:150] Setting up conv2
I1127 11:00:27.433984  5859 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:00:27.433989  5859 net.cpp:165] Memory required for data: 4706560
I1127 11:00:27.433996  5859 layer_factory.hpp:76] Creating layer pool2
I1127 11:00:27.434005  5859 net.cpp:106] Creating Layer pool2
I1127 11:00:27.434010  5859 net.cpp:454] pool2 <- conv2
I1127 11:00:27.434015  5859 net.cpp:411] pool2 -> pool2
I1127 11:00:27.434044  5859 net.cpp:150] Setting up pool2
I1127 11:00:27.434052  5859 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:00:27.434056  5859 net.cpp:165] Memory required for data: 4911360
I1127 11:00:27.434061  5859 layer_factory.hpp:76] Creating layer ip1
I1127 11:00:27.434072  5859 net.cpp:106] Creating Layer ip1
I1127 11:00:27.434077  5859 net.cpp:454] ip1 <- pool2
I1127 11:00:27.434083  5859 net.cpp:411] ip1 -> ip1
I1127 11:00:27.438323  5859 net.cpp:150] Setting up ip1
I1127 11:00:27.438405  5859 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:00:27.438416  5859 net.cpp:165] Memory required for data: 5039360
I1127 11:00:27.438454  5859 layer_factory.hpp:76] Creating layer relu1
I1127 11:00:27.438499  5859 net.cpp:106] Creating Layer relu1
I1127 11:00:27.438518  5859 net.cpp:454] relu1 <- ip1
I1127 11:00:27.438535  5859 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:00:27.438572  5859 net.cpp:150] Setting up relu1
I1127 11:00:27.438583  5859 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:00:27.438591  5859 net.cpp:165] Memory required for data: 5167360
I1127 11:00:27.438598  5859 layer_factory.hpp:76] Creating layer ip2
I1127 11:00:27.438622  5859 net.cpp:106] Creating Layer ip2
I1127 11:00:27.438630  5859 net.cpp:454] ip2 <- ip1
I1127 11:00:27.438644  5859 net.cpp:411] ip2 -> ip2
I1127 11:00:27.439970  5859 net.cpp:150] Setting up ip2
I1127 11:00:27.440064  5859 net.cpp:157] Top shape: 64 10 (640)
I1127 11:00:27.440073  5859 net.cpp:165] Memory required for data: 5169920
I1127 11:00:27.440099  5859 layer_factory.hpp:76] Creating layer loss
I1127 11:00:27.440142  5859 net.cpp:106] Creating Layer loss
I1127 11:00:27.440156  5859 net.cpp:454] loss <- ip2
I1127 11:00:27.440173  5859 net.cpp:454] loss <- label
I1127 11:00:27.440196  5859 net.cpp:411] loss -> loss
I1127 11:00:27.440240  5859 layer_factory.hpp:76] Creating layer loss
I1127 11:00:27.440419  5859 net.cpp:150] Setting up loss
I1127 11:00:27.440436  5859 net.cpp:157] Top shape: (1)
I1127 11:00:27.440443  5859 net.cpp:160]     with loss weight 1
I1127 11:00:27.440487  5859 net.cpp:165] Memory required for data: 5169924
I1127 11:00:27.440495  5859 net.cpp:226] loss needs backward computation.
I1127 11:00:27.440502  5859 net.cpp:226] ip2 needs backward computation.
I1127 11:00:27.440510  5859 net.cpp:226] relu1 needs backward computation.
I1127 11:00:27.440517  5859 net.cpp:226] ip1 needs backward computation.
I1127 11:00:27.440526  5859 net.cpp:226] pool2 needs backward computation.
I1127 11:00:27.440532  5859 net.cpp:226] conv2 needs backward computation.
I1127 11:00:27.440560  5859 net.cpp:226] pool1 needs backward computation.
I1127 11:00:27.440578  5859 net.cpp:226] conv1 needs backward computation.
I1127 11:00:27.440594  5859 net.cpp:228] mnist does not need backward computation.
I1127 11:00:27.440603  5859 net.cpp:270] This network produces output loss
I1127 11:00:27.440623  5859 net.cpp:283] Network initialization done.
I1127 11:00:27.441195  5859 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:00:27.441272  5859 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:00:27.441524  5859 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:00:27.441681  5859 layer_factory.hpp:76] Creating layer mnist
I1127 11:00:27.441942  5859 net.cpp:106] Creating Layer mnist
I1127 11:00:27.441998  5859 net.cpp:411] mnist -> data
I1127 11:00:27.442033  5859 net.cpp:411] mnist -> label
I1127 11:00:27.443799  5864 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:00:27.445204  5859 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:00:27.447917  5859 net.cpp:150] Setting up mnist
I1127 11:00:27.448045  5859 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:00:27.448063  5859 net.cpp:157] Top shape: 100 (100)
I1127 11:00:27.448072  5859 net.cpp:165] Memory required for data: 314000
I1127 11:00:27.448093  5859 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:00:27.448134  5859 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:00:27.448150  5859 net.cpp:454] label_mnist_1_split <- label
I1127 11:00:27.448179  5859 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:00:27.448204  5859 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:00:27.448367  5859 net.cpp:150] Setting up label_mnist_1_split
I1127 11:00:27.448385  5859 net.cpp:157] Top shape: 100 (100)
I1127 11:00:27.448392  5859 net.cpp:157] Top shape: 100 (100)
I1127 11:00:27.448398  5859 net.cpp:165] Memory required for data: 314800
I1127 11:00:27.448426  5859 layer_factory.hpp:76] Creating layer conv1
I1127 11:00:27.448457  5859 net.cpp:106] Creating Layer conv1
I1127 11:00:27.448464  5859 net.cpp:454] conv1 <- data
I1127 11:00:27.448474  5859 net.cpp:411] conv1 -> conv1
I1127 11:00:27.448770  5859 net.cpp:150] Setting up conv1
I1127 11:00:27.448786  5859 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:00:27.448791  5859 net.cpp:165] Memory required for data: 4922800
I1127 11:00:27.448809  5859 layer_factory.hpp:76] Creating layer pool1
I1127 11:00:27.448825  5859 net.cpp:106] Creating Layer pool1
I1127 11:00:27.448832  5859 net.cpp:454] pool1 <- conv1
I1127 11:00:27.448870  5859 net.cpp:411] pool1 -> pool1
I1127 11:00:27.448926  5859 net.cpp:150] Setting up pool1
I1127 11:00:27.448938  5859 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:00:27.448945  5859 net.cpp:165] Memory required for data: 6074800
I1127 11:00:27.448951  5859 layer_factory.hpp:76] Creating layer conv2
I1127 11:00:27.448969  5859 net.cpp:106] Creating Layer conv2
I1127 11:00:27.448976  5859 net.cpp:454] conv2 <- pool1
I1127 11:00:27.448987  5859 net.cpp:411] conv2 -> conv2
I1127 11:00:27.449471  5859 net.cpp:150] Setting up conv2
I1127 11:00:27.449491  5859 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:00:27.449498  5859 net.cpp:165] Memory required for data: 7354800
I1127 11:00:27.449512  5859 layer_factory.hpp:76] Creating layer pool2
I1127 11:00:27.449524  5859 net.cpp:106] Creating Layer pool2
I1127 11:00:27.449532  5859 net.cpp:454] pool2 <- conv2
I1127 11:00:27.449540  5859 net.cpp:411] pool2 -> pool2
I1127 11:00:27.449587  5859 net.cpp:150] Setting up pool2
I1127 11:00:27.449599  5859 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:00:27.449604  5859 net.cpp:165] Memory required for data: 7674800
I1127 11:00:27.449611  5859 layer_factory.hpp:76] Creating layer ip1
I1127 11:00:27.449625  5859 net.cpp:106] Creating Layer ip1
I1127 11:00:27.449632  5859 net.cpp:454] ip1 <- pool2
I1127 11:00:27.449645  5859 net.cpp:411] ip1 -> ip1
I1127 11:00:27.454522  5859 net.cpp:150] Setting up ip1
I1127 11:00:27.454602  5859 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:00:27.454612  5859 net.cpp:165] Memory required for data: 7874800
I1127 11:00:27.454641  5859 layer_factory.hpp:76] Creating layer relu1
I1127 11:00:27.454664  5859 net.cpp:106] Creating Layer relu1
I1127 11:00:27.454674  5859 net.cpp:454] relu1 <- ip1
I1127 11:00:27.454689  5859 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:00:27.454711  5859 net.cpp:150] Setting up relu1
I1127 11:00:27.454720  5859 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:00:27.454726  5859 net.cpp:165] Memory required for data: 8074800
I1127 11:00:27.454733  5859 layer_factory.hpp:76] Creating layer ip2
I1127 11:00:27.454756  5859 net.cpp:106] Creating Layer ip2
I1127 11:00:27.454762  5859 net.cpp:454] ip2 <- ip1
I1127 11:00:27.454772  5859 net.cpp:411] ip2 -> ip2
I1127 11:00:27.455047  5859 net.cpp:150] Setting up ip2
I1127 11:00:27.455096  5859 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:00:27.455111  5859 net.cpp:165] Memory required for data: 8078800
I1127 11:00:27.455132  5859 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:00:27.455155  5859 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:00:27.455169  5859 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:00:27.455189  5859 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:00:27.455212  5859 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:00:27.455296  5859 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:00:27.455312  5859 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:00:27.455327  5859 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:00:27.455337  5859 net.cpp:165] Memory required for data: 8086800
I1127 11:00:27.455348  5859 layer_factory.hpp:76] Creating layer accuracy
I1127 11:00:27.455364  5859 net.cpp:106] Creating Layer accuracy
I1127 11:00:27.455376  5859 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:00:27.455389  5859 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:00:27.455405  5859 net.cpp:411] accuracy -> accuracy
I1127 11:00:27.455441  5859 net.cpp:150] Setting up accuracy
I1127 11:00:27.455456  5859 net.cpp:157] Top shape: (1)
I1127 11:00:27.455467  5859 net.cpp:165] Memory required for data: 8086804
I1127 11:00:27.455477  5859 layer_factory.hpp:76] Creating layer loss
I1127 11:00:27.455492  5859 net.cpp:106] Creating Layer loss
I1127 11:00:27.455502  5859 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:00:27.455512  5859 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:00:27.455530  5859 net.cpp:411] loss -> loss
I1127 11:00:27.455550  5859 layer_factory.hpp:76] Creating layer loss
I1127 11:00:27.455770  5859 net.cpp:150] Setting up loss
I1127 11:00:27.455788  5859 net.cpp:157] Top shape: (1)
I1127 11:00:27.455797  5859 net.cpp:160]     with loss weight 1
I1127 11:00:27.455827  5859 net.cpp:165] Memory required for data: 8086808
I1127 11:00:27.455839  5859 net.cpp:226] loss needs backward computation.
I1127 11:00:27.455858  5859 net.cpp:228] accuracy does not need backward computation.
I1127 11:00:27.455867  5859 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:00:27.455876  5859 net.cpp:226] ip2 needs backward computation.
I1127 11:00:27.455886  5859 net.cpp:226] relu1 needs backward computation.
I1127 11:00:27.455893  5859 net.cpp:226] ip1 needs backward computation.
I1127 11:00:27.455904  5859 net.cpp:226] pool2 needs backward computation.
I1127 11:00:27.455919  5859 net.cpp:226] conv2 needs backward computation.
I1127 11:00:27.455926  5859 net.cpp:226] pool1 needs backward computation.
I1127 11:00:27.455936  5859 net.cpp:226] conv1 needs backward computation.
I1127 11:00:27.455946  5859 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:00:27.455956  5859 net.cpp:228] mnist does not need backward computation.
I1127 11:00:27.455965  5859 net.cpp:270] This network produces output accuracy
I1127 11:00:27.455976  5859 net.cpp:270] This network produces output loss
I1127 11:00:27.455996  5859 net.cpp:283] Network initialization done.
I1127 11:00:27.456174  5859 solver.cpp:59] Solver scaffolding done.
I1127 11:00:27.456616  5859 caffe.cpp:212] Starting Optimization
I1127 11:00:27.456636  5859 solver.cpp:287] Solving LeNet
I1127 11:00:27.456645  5859 solver.cpp:288] Learning Rate Policy: inv
I1127 11:00:27.457739  5859 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:00:29.721063  5859 solver.cpp:408]     Test net output #0: accuracy = 0.077
I1127 11:00:29.721107  5859 solver.cpp:408]     Test net output #1: loss = 2.35257 (* 1 = 2.35257 loss)
I1127 11:00:29.752640  5859 solver.cpp:236] Iteration 0, loss = 2.30685
I1127 11:00:29.752678  5859 solver.cpp:252]     Train net output #0: loss = 2.30685 (* 1 = 2.30685 loss)
I1127 11:00:29.752691  5859 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:00:43.314910  5859 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:00:45.170176  5859 solver.cpp:408]     Test net output #0: accuracy = 0.9748
I1127 11:00:45.170279  5859 solver.cpp:408]     Test net output #1: loss = 0.0843041 (* 1 = 0.0843041 loss)
I1127 11:00:45.182099  5859 solver.cpp:236] Iteration 500, loss = 0.0935495
I1127 11:00:45.182178  5859 solver.cpp:252]     Train net output #0: loss = 0.0935495 (* 1 = 0.0935495 loss)
I1127 11:00:45.182195  5859 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:00:49.457845  5859 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:00:58.633007  5859 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:00:58.654321  5859 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:00:58.667300  5859 solver.cpp:320] Iteration 1000, loss = 0.0805644
I1127 11:00:58.667423  5859 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:00:59.770123  5859 solver.cpp:408]     Test net output #0: accuracy = 0.9809
I1127 11:00:59.770246  5859 solver.cpp:408]     Test net output #1: loss = 0.0586957 (* 1 = 0.0586957 loss)
I1127 11:00:59.770261  5859 solver.cpp:325] Optimization Done.
I1127 11:00:59.770288  5859 caffe.cpp:215] Optimization Done.
I1127 11:00:59.869465  5887 caffe.cpp:184] Using GPUs 0
I1127 11:01:00.164880  5887 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:01:00.165071  5887 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:01:00.165421  5887 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:01:00.165457  5887 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:01:00.165581  5887 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:01:00.165671  5887 layer_factory.hpp:76] Creating layer mnist
I1127 11:01:00.166138  5887 net.cpp:106] Creating Layer mnist
I1127 11:01:00.166179  5887 net.cpp:411] mnist -> data
I1127 11:01:00.166223  5887 net.cpp:411] mnist -> label
I1127 11:01:00.167448  5890 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:01:00.175976  5887 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:01:00.177538  5887 net.cpp:150] Setting up mnist
I1127 11:01:00.177595  5887 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:01:00.177605  5887 net.cpp:157] Top shape: 64 (64)
I1127 11:01:00.177611  5887 net.cpp:165] Memory required for data: 200960
I1127 11:01:00.177625  5887 layer_factory.hpp:76] Creating layer conv1
I1127 11:01:00.177651  5887 net.cpp:106] Creating Layer conv1
I1127 11:01:00.177659  5887 net.cpp:454] conv1 <- data
I1127 11:01:00.177672  5887 net.cpp:411] conv1 -> conv1
I1127 11:01:00.178464  5887 net.cpp:150] Setting up conv1
I1127 11:01:00.178514  5887 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:01:00.178519  5887 net.cpp:165] Memory required for data: 3150080
I1127 11:01:00.178539  5887 layer_factory.hpp:76] Creating layer pool1
I1127 11:01:00.178557  5887 net.cpp:106] Creating Layer pool1
I1127 11:01:00.178575  5887 net.cpp:454] pool1 <- conv1
I1127 11:01:00.178583  5887 net.cpp:411] pool1 -> pool1
I1127 11:01:00.178649  5887 net.cpp:150] Setting up pool1
I1127 11:01:00.178658  5887 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:01:00.178663  5887 net.cpp:165] Memory required for data: 3887360
I1127 11:01:00.178666  5887 layer_factory.hpp:76] Creating layer conv2
I1127 11:01:00.178679  5887 net.cpp:106] Creating Layer conv2
I1127 11:01:00.178684  5887 net.cpp:454] conv2 <- pool1
I1127 11:01:00.178690  5887 net.cpp:411] conv2 -> conv2
I1127 11:01:00.178957  5887 net.cpp:150] Setting up conv2
I1127 11:01:00.178972  5887 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:01:00.178977  5887 net.cpp:165] Memory required for data: 4706560
I1127 11:01:00.178987  5887 layer_factory.hpp:76] Creating layer pool2
I1127 11:01:00.178998  5887 net.cpp:106] Creating Layer pool2
I1127 11:01:00.179003  5887 net.cpp:454] pool2 <- conv2
I1127 11:01:00.179013  5887 net.cpp:411] pool2 -> pool2
I1127 11:01:00.179044  5887 net.cpp:150] Setting up pool2
I1127 11:01:00.179052  5887 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:01:00.179057  5887 net.cpp:165] Memory required for data: 4911360
I1127 11:01:00.179061  5887 layer_factory.hpp:76] Creating layer ip1
I1127 11:01:00.179076  5887 net.cpp:106] Creating Layer ip1
I1127 11:01:00.179082  5887 net.cpp:454] ip1 <- pool2
I1127 11:01:00.179090  5887 net.cpp:411] ip1 -> ip1
I1127 11:01:00.182420  5887 net.cpp:150] Setting up ip1
I1127 11:01:00.182541  5887 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:01:00.182554  5887 net.cpp:165] Memory required for data: 5039360
I1127 11:01:00.182595  5887 layer_factory.hpp:76] Creating layer relu1
I1127 11:01:00.182634  5887 net.cpp:106] Creating Layer relu1
I1127 11:01:00.182651  5887 net.cpp:454] relu1 <- ip1
I1127 11:01:00.182667  5887 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:01:00.182700  5887 net.cpp:150] Setting up relu1
I1127 11:01:00.182713  5887 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:01:00.182721  5887 net.cpp:165] Memory required for data: 5167360
I1127 11:01:00.182731  5887 layer_factory.hpp:76] Creating layer ip2
I1127 11:01:00.182754  5887 net.cpp:106] Creating Layer ip2
I1127 11:01:00.182764  5887 net.cpp:454] ip2 <- ip1
I1127 11:01:00.182775  5887 net.cpp:411] ip2 -> ip2
I1127 11:01:00.184092  5887 net.cpp:150] Setting up ip2
I1127 11:01:00.184170  5887 net.cpp:157] Top shape: 64 10 (640)
I1127 11:01:00.184177  5887 net.cpp:165] Memory required for data: 5169920
I1127 11:01:00.184201  5887 layer_factory.hpp:76] Creating layer loss
I1127 11:01:00.184229  5887 net.cpp:106] Creating Layer loss
I1127 11:01:00.184240  5887 net.cpp:454] loss <- ip2
I1127 11:01:00.184254  5887 net.cpp:454] loss <- label
I1127 11:01:00.184273  5887 net.cpp:411] loss -> loss
I1127 11:01:00.184314  5887 layer_factory.hpp:76] Creating layer loss
I1127 11:01:00.184489  5887 net.cpp:150] Setting up loss
I1127 11:01:00.184515  5887 net.cpp:157] Top shape: (1)
I1127 11:01:00.184525  5887 net.cpp:160]     with loss weight 1
I1127 11:01:00.184569  5887 net.cpp:165] Memory required for data: 5169924
I1127 11:01:00.184581  5887 net.cpp:226] loss needs backward computation.
I1127 11:01:00.184592  5887 net.cpp:226] ip2 needs backward computation.
I1127 11:01:00.184600  5887 net.cpp:226] relu1 needs backward computation.
I1127 11:01:00.184607  5887 net.cpp:226] ip1 needs backward computation.
I1127 11:01:00.184617  5887 net.cpp:226] pool2 needs backward computation.
I1127 11:01:00.184624  5887 net.cpp:226] conv2 needs backward computation.
I1127 11:01:00.184633  5887 net.cpp:226] pool1 needs backward computation.
I1127 11:01:00.184640  5887 net.cpp:226] conv1 needs backward computation.
I1127 11:01:00.184650  5887 net.cpp:228] mnist does not need backward computation.
I1127 11:01:00.184659  5887 net.cpp:270] This network produces output loss
I1127 11:01:00.184676  5887 net.cpp:283] Network initialization done.
I1127 11:01:00.185187  5887 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:01:00.185299  5887 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:01:00.185559  5887 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:01:00.185695  5887 layer_factory.hpp:76] Creating layer mnist
I1127 11:01:00.185861  5887 net.cpp:106] Creating Layer mnist
I1127 11:01:00.185878  5887 net.cpp:411] mnist -> data
I1127 11:01:00.185896  5887 net.cpp:411] mnist -> label
I1127 11:01:00.186894  5892 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:01:00.187158  5887 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:01:00.188930  5887 net.cpp:150] Setting up mnist
I1127 11:01:00.188989  5887 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:01:00.189004  5887 net.cpp:157] Top shape: 100 (100)
I1127 11:01:00.189013  5887 net.cpp:165] Memory required for data: 314000
I1127 11:01:00.189028  5887 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:01:00.189056  5887 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:01:00.189066  5887 net.cpp:454] label_mnist_1_split <- label
I1127 11:01:00.189085  5887 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:01:00.189108  5887 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:01:00.189177  5887 net.cpp:150] Setting up label_mnist_1_split
I1127 11:01:00.189193  5887 net.cpp:157] Top shape: 100 (100)
I1127 11:01:00.189205  5887 net.cpp:157] Top shape: 100 (100)
I1127 11:01:00.189216  5887 net.cpp:165] Memory required for data: 314800
I1127 11:01:00.189225  5887 layer_factory.hpp:76] Creating layer conv1
I1127 11:01:00.189252  5887 net.cpp:106] Creating Layer conv1
I1127 11:01:00.189263  5887 net.cpp:454] conv1 <- data
I1127 11:01:00.189273  5887 net.cpp:411] conv1 -> conv1
I1127 11:01:00.189527  5887 net.cpp:150] Setting up conv1
I1127 11:01:00.189543  5887 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:01:00.189550  5887 net.cpp:165] Memory required for data: 4922800
I1127 11:01:00.189582  5887 layer_factory.hpp:76] Creating layer pool1
I1127 11:01:00.189595  5887 net.cpp:106] Creating Layer pool1
I1127 11:01:00.189604  5887 net.cpp:454] pool1 <- conv1
I1127 11:01:00.189632  5887 net.cpp:411] pool1 -> pool1
I1127 11:01:00.189679  5887 net.cpp:150] Setting up pool1
I1127 11:01:00.189690  5887 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:01:00.189697  5887 net.cpp:165] Memory required for data: 6074800
I1127 11:01:00.189703  5887 layer_factory.hpp:76] Creating layer conv2
I1127 11:01:00.189718  5887 net.cpp:106] Creating Layer conv2
I1127 11:01:00.189725  5887 net.cpp:454] conv2 <- pool1
I1127 11:01:00.189736  5887 net.cpp:411] conv2 -> conv2
I1127 11:01:00.190399  5887 net.cpp:150] Setting up conv2
I1127 11:01:00.190457  5887 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:01:00.190466  5887 net.cpp:165] Memory required for data: 7354800
I1127 11:01:00.190490  5887 layer_factory.hpp:76] Creating layer pool2
I1127 11:01:00.190511  5887 net.cpp:106] Creating Layer pool2
I1127 11:01:00.190521  5887 net.cpp:454] pool2 <- conv2
I1127 11:01:00.190532  5887 net.cpp:411] pool2 -> pool2
I1127 11:01:00.190584  5887 net.cpp:150] Setting up pool2
I1127 11:01:00.190596  5887 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:01:00.190603  5887 net.cpp:165] Memory required for data: 7674800
I1127 11:01:00.190611  5887 layer_factory.hpp:76] Creating layer ip1
I1127 11:01:00.190624  5887 net.cpp:106] Creating Layer ip1
I1127 11:01:00.190632  5887 net.cpp:454] ip1 <- pool2
I1127 11:01:00.190645  5887 net.cpp:411] ip1 -> ip1
I1127 11:01:00.194615  5887 net.cpp:150] Setting up ip1
I1127 11:01:00.194708  5887 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:01:00.194720  5887 net.cpp:165] Memory required for data: 7874800
I1127 11:01:00.194756  5887 layer_factory.hpp:76] Creating layer relu1
I1127 11:01:00.194784  5887 net.cpp:106] Creating Layer relu1
I1127 11:01:00.194797  5887 net.cpp:454] relu1 <- ip1
I1127 11:01:00.194814  5887 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:01:00.194836  5887 net.cpp:150] Setting up relu1
I1127 11:01:00.194847  5887 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:01:00.194854  5887 net.cpp:165] Memory required for data: 8074800
I1127 11:01:00.194864  5887 layer_factory.hpp:76] Creating layer ip2
I1127 11:01:00.194890  5887 net.cpp:106] Creating Layer ip2
I1127 11:01:00.194901  5887 net.cpp:454] ip2 <- ip1
I1127 11:01:00.194913  5887 net.cpp:411] ip2 -> ip2
I1127 11:01:00.195147  5887 net.cpp:150] Setting up ip2
I1127 11:01:00.195173  5887 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:01:00.195183  5887 net.cpp:165] Memory required for data: 8078800
I1127 11:01:00.195197  5887 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:01:00.195214  5887 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:01:00.195224  5887 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:01:00.195235  5887 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:01:00.195250  5887 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:01:00.195374  5887 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:01:00.195389  5887 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:01:00.195395  5887 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:01:00.195399  5887 net.cpp:165] Memory required for data: 8086800
I1127 11:01:00.195405  5887 layer_factory.hpp:76] Creating layer accuracy
I1127 11:01:00.195417  5887 net.cpp:106] Creating Layer accuracy
I1127 11:01:00.195422  5887 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:01:00.195430  5887 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:01:00.195439  5887 net.cpp:411] accuracy -> accuracy
I1127 11:01:00.195453  5887 net.cpp:150] Setting up accuracy
I1127 11:01:00.195459  5887 net.cpp:157] Top shape: (1)
I1127 11:01:00.195464  5887 net.cpp:165] Memory required for data: 8086804
I1127 11:01:00.195468  5887 layer_factory.hpp:76] Creating layer loss
I1127 11:01:00.195478  5887 net.cpp:106] Creating Layer loss
I1127 11:01:00.195483  5887 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:01:00.195488  5887 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:01:00.195513  5887 net.cpp:411] loss -> loss
I1127 11:01:00.195528  5887 layer_factory.hpp:76] Creating layer loss
I1127 11:01:00.195674  5887 net.cpp:150] Setting up loss
I1127 11:01:00.195688  5887 net.cpp:157] Top shape: (1)
I1127 11:01:00.195691  5887 net.cpp:160]     with loss weight 1
I1127 11:01:00.195722  5887 net.cpp:165] Memory required for data: 8086808
I1127 11:01:00.195729  5887 net.cpp:226] loss needs backward computation.
I1127 11:01:00.195744  5887 net.cpp:228] accuracy does not need backward computation.
I1127 11:01:00.195749  5887 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:01:00.195755  5887 net.cpp:226] ip2 needs backward computation.
I1127 11:01:00.195760  5887 net.cpp:226] relu1 needs backward computation.
I1127 11:01:00.195765  5887 net.cpp:226] ip1 needs backward computation.
I1127 11:01:00.195770  5887 net.cpp:226] pool2 needs backward computation.
I1127 11:01:00.195775  5887 net.cpp:226] conv2 needs backward computation.
I1127 11:01:00.195785  5887 net.cpp:226] pool1 needs backward computation.
I1127 11:01:00.195790  5887 net.cpp:226] conv1 needs backward computation.
I1127 11:01:00.195794  5887 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:01:00.195801  5887 net.cpp:228] mnist does not need backward computation.
I1127 11:01:00.195804  5887 net.cpp:270] This network produces output accuracy
I1127 11:01:00.195811  5887 net.cpp:270] This network produces output loss
I1127 11:01:00.195822  5887 net.cpp:283] Network initialization done.
I1127 11:01:00.195936  5887 solver.cpp:59] Solver scaffolding done.
I1127 11:01:00.196204  5887 caffe.cpp:212] Starting Optimization
I1127 11:01:00.196233  5887 solver.cpp:287] Solving LeNet
I1127 11:01:00.196238  5887 solver.cpp:288] Learning Rate Policy: inv
I1127 11:01:00.196888  5887 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:01:03.324921  5887 solver.cpp:408]     Test net output #0: accuracy = 0.1311
I1127 11:01:03.324977  5887 solver.cpp:408]     Test net output #1: loss = 2.32223 (* 1 = 2.32223 loss)
I1127 11:01:03.340131  5887 solver.cpp:236] Iteration 0, loss = 2.32297
I1127 11:01:03.340255  5887 solver.cpp:252]     Train net output #0: loss = 2.32297 (* 1 = 2.32297 loss)
I1127 11:01:03.340291  5887 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:01:14.790259  5887 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:01:16.810050  5887 solver.cpp:408]     Test net output #0: accuracy = 0.9744
I1127 11:01:16.810093  5887 solver.cpp:408]     Test net output #1: loss = 0.0847223 (* 1 = 0.0847223 loss)
I1127 11:01:16.838507  5887 solver.cpp:236] Iteration 500, loss = 0.0932229
I1127 11:01:16.838549  5887 solver.cpp:252]     Train net output #0: loss = 0.0932229 (* 1 = 0.0932229 loss)
I1127 11:01:16.838559  5887 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:01:30.443323  5887 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:01:30.469681  5887 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:01:30.498832  5887 solver.cpp:320] Iteration 1000, loss = 0.0707109
I1127 11:01:30.498973  5887 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:01:32.431933  5887 solver.cpp:408]     Test net output #0: accuracy = 0.9803
I1127 11:01:32.432020  5887 solver.cpp:408]     Test net output #1: loss = 0.0583379 (* 1 = 0.0583379 loss)
I1127 11:01:32.432030  5887 solver.cpp:325] Optimization Done.
I1127 11:01:32.432036  5887 caffe.cpp:215] Optimization Done.
I1127 11:01:32.526185  5915 caffe.cpp:184] Using GPUs 0
I1127 11:01:32.803585  5915 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:01:32.803814  5915 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:01:32.804296  5915 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:01:32.804332  5915 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:01:32.804512  5915 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:01:32.804622  5915 layer_factory.hpp:76] Creating layer mnist
I1127 11:01:32.842339  5915 net.cpp:106] Creating Layer mnist
I1127 11:01:32.842454  5915 net.cpp:411] mnist -> data
I1127 11:01:32.842533  5915 net.cpp:411] mnist -> label
I1127 11:01:32.843864  5918 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:01:32.858916  5915 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:01:32.861150  5915 net.cpp:150] Setting up mnist
I1127 11:01:32.861249  5915 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:01:32.861265  5915 net.cpp:157] Top shape: 64 (64)
I1127 11:01:32.861274  5915 net.cpp:165] Memory required for data: 200960
I1127 11:01:32.861297  5915 layer_factory.hpp:76] Creating layer conv1
I1127 11:01:32.861341  5915 net.cpp:106] Creating Layer conv1
I1127 11:01:32.861356  5915 net.cpp:454] conv1 <- data
I1127 11:01:32.861382  5915 net.cpp:411] conv1 -> conv1
I1127 11:01:32.863467  5915 net.cpp:150] Setting up conv1
I1127 11:01:32.863574  5915 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:01:32.863587  5915 net.cpp:165] Memory required for data: 3150080
I1127 11:01:32.863620  5915 layer_factory.hpp:76] Creating layer pool1
I1127 11:01:32.863648  5915 net.cpp:106] Creating Layer pool1
I1127 11:01:32.863662  5915 net.cpp:454] pool1 <- conv1
I1127 11:01:32.863677  5915 net.cpp:411] pool1 -> pool1
I1127 11:01:32.864176  5915 net.cpp:150] Setting up pool1
I1127 11:01:32.864199  5915 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:01:32.864208  5915 net.cpp:165] Memory required for data: 3887360
I1127 11:01:32.864218  5915 layer_factory.hpp:76] Creating layer conv2
I1127 11:01:32.864239  5915 net.cpp:106] Creating Layer conv2
I1127 11:01:32.864249  5915 net.cpp:454] conv2 <- pool1
I1127 11:01:32.864264  5915 net.cpp:411] conv2 -> conv2
I1127 11:01:32.865041  5915 net.cpp:150] Setting up conv2
I1127 11:01:32.865080  5915 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:01:32.865088  5915 net.cpp:165] Memory required for data: 4706560
I1127 11:01:32.865108  5915 layer_factory.hpp:76] Creating layer pool2
I1127 11:01:32.865126  5915 net.cpp:106] Creating Layer pool2
I1127 11:01:32.865134  5915 net.cpp:454] pool2 <- conv2
I1127 11:01:32.865145  5915 net.cpp:411] pool2 -> pool2
I1127 11:01:32.865196  5915 net.cpp:150] Setting up pool2
I1127 11:01:32.865208  5915 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:01:32.865214  5915 net.cpp:165] Memory required for data: 4911360
I1127 11:01:32.865221  5915 layer_factory.hpp:76] Creating layer ip1
I1127 11:01:32.865236  5915 net.cpp:106] Creating Layer ip1
I1127 11:01:32.865243  5915 net.cpp:454] ip1 <- pool2
I1127 11:01:32.865254  5915 net.cpp:411] ip1 -> ip1
I1127 11:01:32.869719  5915 net.cpp:150] Setting up ip1
I1127 11:01:32.869798  5915 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:01:32.869810  5915 net.cpp:165] Memory required for data: 5039360
I1127 11:01:32.869837  5915 layer_factory.hpp:76] Creating layer relu1
I1127 11:01:32.869856  5915 net.cpp:106] Creating Layer relu1
I1127 11:01:32.869868  5915 net.cpp:454] relu1 <- ip1
I1127 11:01:32.869881  5915 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:01:32.869901  5915 net.cpp:150] Setting up relu1
I1127 11:01:32.869915  5915 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:01:32.869922  5915 net.cpp:165] Memory required for data: 5167360
I1127 11:01:32.869930  5915 layer_factory.hpp:76] Creating layer ip2
I1127 11:01:32.869945  5915 net.cpp:106] Creating Layer ip2
I1127 11:01:32.869953  5915 net.cpp:454] ip2 <- ip1
I1127 11:01:32.869967  5915 net.cpp:411] ip2 -> ip2
I1127 11:01:32.871172  5915 net.cpp:150] Setting up ip2
I1127 11:01:32.871244  5915 net.cpp:157] Top shape: 64 10 (640)
I1127 11:01:32.871254  5915 net.cpp:165] Memory required for data: 5169920
I1127 11:01:32.871279  5915 layer_factory.hpp:76] Creating layer loss
I1127 11:01:32.871314  5915 net.cpp:106] Creating Layer loss
I1127 11:01:32.871326  5915 net.cpp:454] loss <- ip2
I1127 11:01:32.871340  5915 net.cpp:454] loss <- label
I1127 11:01:32.871358  5915 net.cpp:411] loss -> loss
I1127 11:01:32.871386  5915 layer_factory.hpp:76] Creating layer loss
I1127 11:01:32.871511  5915 net.cpp:150] Setting up loss
I1127 11:01:32.871532  5915 net.cpp:157] Top shape: (1)
I1127 11:01:32.871543  5915 net.cpp:160]     with loss weight 1
I1127 11:01:32.871588  5915 net.cpp:165] Memory required for data: 5169924
I1127 11:01:32.871600  5915 net.cpp:226] loss needs backward computation.
I1127 11:01:32.871610  5915 net.cpp:226] ip2 needs backward computation.
I1127 11:01:32.871620  5915 net.cpp:226] relu1 needs backward computation.
I1127 11:01:32.871629  5915 net.cpp:226] ip1 needs backward computation.
I1127 11:01:32.871639  5915 net.cpp:226] pool2 needs backward computation.
I1127 11:01:32.871649  5915 net.cpp:226] conv2 needs backward computation.
I1127 11:01:32.871662  5915 net.cpp:226] pool1 needs backward computation.
I1127 11:01:32.871675  5915 net.cpp:226] conv1 needs backward computation.
I1127 11:01:32.871685  5915 net.cpp:228] mnist does not need backward computation.
I1127 11:01:32.871695  5915 net.cpp:270] This network produces output loss
I1127 11:01:32.871714  5915 net.cpp:283] Network initialization done.
I1127 11:01:32.872202  5915 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:01:32.872241  5915 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:01:32.872418  5915 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:01:32.872529  5915 layer_factory.hpp:76] Creating layer mnist
I1127 11:01:32.872685  5915 net.cpp:106] Creating Layer mnist
I1127 11:01:32.872700  5915 net.cpp:411] mnist -> data
I1127 11:01:32.872719  5915 net.cpp:411] mnist -> label
I1127 11:01:32.875278  5920 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:01:32.875612  5915 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:01:32.877128  5915 net.cpp:150] Setting up mnist
I1127 11:01:32.877195  5915 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:01:32.877210  5915 net.cpp:157] Top shape: 100 (100)
I1127 11:01:32.877219  5915 net.cpp:165] Memory required for data: 314000
I1127 11:01:32.877234  5915 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:01:32.877274  5915 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:01:32.877293  5915 net.cpp:454] label_mnist_1_split <- label
I1127 11:01:32.877323  5915 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:01:32.877362  5915 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:01:32.877526  5915 net.cpp:150] Setting up label_mnist_1_split
I1127 11:01:32.877555  5915 net.cpp:157] Top shape: 100 (100)
I1127 11:01:32.877573  5915 net.cpp:157] Top shape: 100 (100)
I1127 11:01:32.877586  5915 net.cpp:165] Memory required for data: 314800
I1127 11:01:32.877600  5915 layer_factory.hpp:76] Creating layer conv1
I1127 11:01:32.877637  5915 net.cpp:106] Creating Layer conv1
I1127 11:01:32.877652  5915 net.cpp:454] conv1 <- data
I1127 11:01:32.877671  5915 net.cpp:411] conv1 -> conv1
I1127 11:01:32.878082  5915 net.cpp:150] Setting up conv1
I1127 11:01:32.878106  5915 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:01:32.878114  5915 net.cpp:165] Memory required for data: 4922800
I1127 11:01:32.878135  5915 layer_factory.hpp:76] Creating layer pool1
I1127 11:01:32.878168  5915 net.cpp:106] Creating Layer pool1
I1127 11:01:32.878178  5915 net.cpp:454] pool1 <- conv1
I1127 11:01:32.878219  5915 net.cpp:411] pool1 -> pool1
I1127 11:01:32.879009  5915 net.cpp:150] Setting up pool1
I1127 11:01:32.879058  5915 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:01:32.879068  5915 net.cpp:165] Memory required for data: 6074800
I1127 11:01:32.879081  5915 layer_factory.hpp:76] Creating layer conv2
I1127 11:01:32.882308  5915 net.cpp:106] Creating Layer conv2
I1127 11:01:32.882392  5915 net.cpp:454] conv2 <- pool1
I1127 11:01:32.882458  5915 net.cpp:411] conv2 -> conv2
I1127 11:01:32.883381  5915 net.cpp:150] Setting up conv2
I1127 11:01:32.883422  5915 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:01:32.883430  5915 net.cpp:165] Memory required for data: 7354800
I1127 11:01:32.883447  5915 layer_factory.hpp:76] Creating layer pool2
I1127 11:01:32.883463  5915 net.cpp:106] Creating Layer pool2
I1127 11:01:32.883471  5915 net.cpp:454] pool2 <- conv2
I1127 11:01:32.883482  5915 net.cpp:411] pool2 -> pool2
I1127 11:01:32.883558  5915 net.cpp:150] Setting up pool2
I1127 11:01:32.883569  5915 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:01:32.883574  5915 net.cpp:165] Memory required for data: 7674800
I1127 11:01:32.883580  5915 layer_factory.hpp:76] Creating layer ip1
I1127 11:01:32.883592  5915 net.cpp:106] Creating Layer ip1
I1127 11:01:32.883599  5915 net.cpp:454] ip1 <- pool2
I1127 11:01:32.883625  5915 net.cpp:411] ip1 -> ip1
I1127 11:01:32.889971  5915 net.cpp:150] Setting up ip1
I1127 11:01:32.890017  5915 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:01:32.890027  5915 net.cpp:165] Memory required for data: 7874800
I1127 11:01:32.890050  5915 layer_factory.hpp:76] Creating layer relu1
I1127 11:01:32.890071  5915 net.cpp:106] Creating Layer relu1
I1127 11:01:32.890084  5915 net.cpp:454] relu1 <- ip1
I1127 11:01:32.890097  5915 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:01:32.890115  5915 net.cpp:150] Setting up relu1
I1127 11:01:32.890125  5915 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:01:32.890132  5915 net.cpp:165] Memory required for data: 8074800
I1127 11:01:32.890153  5915 layer_factory.hpp:76] Creating layer ip2
I1127 11:01:32.890172  5915 net.cpp:106] Creating Layer ip2
I1127 11:01:32.890182  5915 net.cpp:454] ip2 <- ip1
I1127 11:01:32.890200  5915 net.cpp:411] ip2 -> ip2
I1127 11:01:32.890416  5915 net.cpp:150] Setting up ip2
I1127 11:01:32.890439  5915 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:01:32.890450  5915 net.cpp:165] Memory required for data: 8078800
I1127 11:01:32.890465  5915 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:01:32.890482  5915 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:01:32.890493  5915 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:01:32.890506  5915 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:01:32.890524  5915 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:01:32.890578  5915 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:01:32.890596  5915 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:01:32.890609  5915 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:01:32.890619  5915 net.cpp:165] Memory required for data: 8086800
I1127 11:01:32.890630  5915 layer_factory.hpp:76] Creating layer accuracy
I1127 11:01:32.890641  5915 net.cpp:106] Creating Layer accuracy
I1127 11:01:32.890650  5915 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:01:32.890658  5915 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:01:32.890672  5915 net.cpp:411] accuracy -> accuracy
I1127 11:01:32.890705  5915 net.cpp:150] Setting up accuracy
I1127 11:01:32.890727  5915 net.cpp:157] Top shape: (1)
I1127 11:01:32.890735  5915 net.cpp:165] Memory required for data: 8086804
I1127 11:01:32.890743  5915 layer_factory.hpp:76] Creating layer loss
I1127 11:01:32.890756  5915 net.cpp:106] Creating Layer loss
I1127 11:01:32.890763  5915 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:01:32.890771  5915 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:01:32.890780  5915 net.cpp:411] loss -> loss
I1127 11:01:32.890794  5915 layer_factory.hpp:76] Creating layer loss
I1127 11:01:32.890929  5915 net.cpp:150] Setting up loss
I1127 11:01:32.890941  5915 net.cpp:157] Top shape: (1)
I1127 11:01:32.890947  5915 net.cpp:160]     with loss weight 1
I1127 11:01:32.890964  5915 net.cpp:165] Memory required for data: 8086808
I1127 11:01:32.890969  5915 net.cpp:226] loss needs backward computation.
I1127 11:01:32.890980  5915 net.cpp:228] accuracy does not need backward computation.
I1127 11:01:32.890993  5915 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:01:32.891000  5915 net.cpp:226] ip2 needs backward computation.
I1127 11:01:32.891006  5915 net.cpp:226] relu1 needs backward computation.
I1127 11:01:32.891013  5915 net.cpp:226] ip1 needs backward computation.
I1127 11:01:32.891019  5915 net.cpp:226] pool2 needs backward computation.
I1127 11:01:32.891026  5915 net.cpp:226] conv2 needs backward computation.
I1127 11:01:32.891033  5915 net.cpp:226] pool1 needs backward computation.
I1127 11:01:32.891041  5915 net.cpp:226] conv1 needs backward computation.
I1127 11:01:32.891047  5915 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:01:32.891054  5915 net.cpp:228] mnist does not need backward computation.
I1127 11:01:32.891060  5915 net.cpp:270] This network produces output accuracy
I1127 11:01:32.891067  5915 net.cpp:270] This network produces output loss
I1127 11:01:32.891083  5915 net.cpp:283] Network initialization done.
I1127 11:01:32.891149  5915 solver.cpp:59] Solver scaffolding done.
I1127 11:01:32.891436  5915 caffe.cpp:212] Starting Optimization
I1127 11:01:32.891448  5915 solver.cpp:287] Solving LeNet
I1127 11:01:32.891454  5915 solver.cpp:288] Learning Rate Policy: inv
I1127 11:01:32.892087  5915 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:01:35.975409  5915 solver.cpp:408]     Test net output #0: accuracy = 0.0791
I1127 11:01:35.975472  5915 solver.cpp:408]     Test net output #1: loss = 2.40391 (* 1 = 2.40391 loss)
I1127 11:01:36.008752  5915 solver.cpp:236] Iteration 0, loss = 2.41683
I1127 11:01:36.008822  5915 solver.cpp:252]     Train net output #0: loss = 2.41683 (* 1 = 2.41683 loss)
I1127 11:01:36.008851  5915 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:01:47.518920  5915 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:01:49.381465  5915 solver.cpp:408]     Test net output #0: accuracy = 0.9731
I1127 11:01:49.381496  5915 solver.cpp:408]     Test net output #1: loss = 0.0862563 (* 1 = 0.0862563 loss)
I1127 11:01:49.409862  5915 solver.cpp:236] Iteration 500, loss = 0.117891
I1127 11:01:49.409890  5915 solver.cpp:252]     Train net output #0: loss = 0.117891 (* 1 = 0.117891 loss)
I1127 11:01:49.409900  5915 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:02:02.066711  5915 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:02:02.078413  5915 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:02:02.087730  5915 solver.cpp:320] Iteration 1000, loss = 0.0710295
I1127 11:02:02.087795  5915 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:02:05.221719  5915 solver.cpp:408]     Test net output #0: accuracy = 0.9811
I1127 11:02:05.221884  5915 solver.cpp:408]     Test net output #1: loss = 0.0590996 (* 1 = 0.0590996 loss)
I1127 11:02:05.221894  5915 solver.cpp:325] Optimization Done.
I1127 11:02:05.221899  5915 caffe.cpp:215] Optimization Done.
I1127 11:02:05.315995  5942 caffe.cpp:184] Using GPUs 0
I1127 11:02:05.643486  5942 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:02:05.643667  5942 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:02:05.644163  5942 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:02:05.644201  5942 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:02:05.644342  5942 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:02:05.644453  5942 layer_factory.hpp:76] Creating layer mnist
I1127 11:02:05.686329  5942 net.cpp:106] Creating Layer mnist
I1127 11:02:05.686396  5942 net.cpp:411] mnist -> data
I1127 11:02:05.686434  5942 net.cpp:411] mnist -> label
I1127 11:02:05.687434  5946 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:02:05.702353  5942 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:02:05.703896  5942 net.cpp:150] Setting up mnist
I1127 11:02:05.703945  5942 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:02:05.703956  5942 net.cpp:157] Top shape: 64 (64)
I1127 11:02:05.703963  5942 net.cpp:165] Memory required for data: 200960
I1127 11:02:05.703977  5942 layer_factory.hpp:76] Creating layer conv1
I1127 11:02:05.704005  5942 net.cpp:106] Creating Layer conv1
I1127 11:02:05.704015  5942 net.cpp:454] conv1 <- data
I1127 11:02:05.704030  5942 net.cpp:411] conv1 -> conv1
I1127 11:02:05.704890  5942 net.cpp:150] Setting up conv1
I1127 11:02:05.704932  5942 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:02:05.704942  5942 net.cpp:165] Memory required for data: 3150080
I1127 11:02:05.704965  5942 layer_factory.hpp:76] Creating layer pool1
I1127 11:02:05.704991  5942 net.cpp:106] Creating Layer pool1
I1127 11:02:05.705003  5942 net.cpp:454] pool1 <- conv1
I1127 11:02:05.705015  5942 net.cpp:411] pool1 -> pool1
I1127 11:02:05.705157  5942 net.cpp:150] Setting up pool1
I1127 11:02:05.705184  5942 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:02:05.705200  5942 net.cpp:165] Memory required for data: 3887360
I1127 11:02:05.705216  5942 layer_factory.hpp:76] Creating layer conv2
I1127 11:02:05.705260  5942 net.cpp:106] Creating Layer conv2
I1127 11:02:05.705281  5942 net.cpp:454] conv2 <- pool1
I1127 11:02:05.705332  5942 net.cpp:411] conv2 -> conv2
I1127 11:02:05.705833  5942 net.cpp:150] Setting up conv2
I1127 11:02:05.705862  5942 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:02:05.705868  5942 net.cpp:165] Memory required for data: 4706560
I1127 11:02:05.705888  5942 layer_factory.hpp:76] Creating layer pool2
I1127 11:02:05.705909  5942 net.cpp:106] Creating Layer pool2
I1127 11:02:05.705917  5942 net.cpp:454] pool2 <- conv2
I1127 11:02:05.705931  5942 net.cpp:411] pool2 -> pool2
I1127 11:02:05.705983  5942 net.cpp:150] Setting up pool2
I1127 11:02:05.705996  5942 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:02:05.706024  5942 net.cpp:165] Memory required for data: 4911360
I1127 11:02:05.706033  5942 layer_factory.hpp:76] Creating layer ip1
I1127 11:02:05.706058  5942 net.cpp:106] Creating Layer ip1
I1127 11:02:05.706069  5942 net.cpp:454] ip1 <- pool2
I1127 11:02:05.706085  5942 net.cpp:411] ip1 -> ip1
I1127 11:02:05.710386  5942 net.cpp:150] Setting up ip1
I1127 11:02:05.710444  5942 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:02:05.710453  5942 net.cpp:165] Memory required for data: 5039360
I1127 11:02:05.710479  5942 layer_factory.hpp:76] Creating layer relu1
I1127 11:02:05.710501  5942 net.cpp:106] Creating Layer relu1
I1127 11:02:05.710512  5942 net.cpp:454] relu1 <- ip1
I1127 11:02:05.710526  5942 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:02:05.710554  5942 net.cpp:150] Setting up relu1
I1127 11:02:05.710566  5942 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:02:05.710573  5942 net.cpp:165] Memory required for data: 5167360
I1127 11:02:05.710580  5942 layer_factory.hpp:76] Creating layer ip2
I1127 11:02:05.710598  5942 net.cpp:106] Creating Layer ip2
I1127 11:02:05.710607  5942 net.cpp:454] ip2 <- ip1
I1127 11:02:05.710618  5942 net.cpp:411] ip2 -> ip2
I1127 11:02:05.711881  5942 net.cpp:150] Setting up ip2
I1127 11:02:05.711997  5942 net.cpp:157] Top shape: 64 10 (640)
I1127 11:02:05.712013  5942 net.cpp:165] Memory required for data: 5169920
I1127 11:02:05.712045  5942 layer_factory.hpp:76] Creating layer loss
I1127 11:02:05.712091  5942 net.cpp:106] Creating Layer loss
I1127 11:02:05.712110  5942 net.cpp:454] loss <- ip2
I1127 11:02:05.712129  5942 net.cpp:454] loss <- label
I1127 11:02:05.712162  5942 net.cpp:411] loss -> loss
I1127 11:02:05.712220  5942 layer_factory.hpp:76] Creating layer loss
I1127 11:02:05.712476  5942 net.cpp:150] Setting up loss
I1127 11:02:05.712493  5942 net.cpp:157] Top shape: (1)
I1127 11:02:05.712501  5942 net.cpp:160]     with loss weight 1
I1127 11:02:05.712543  5942 net.cpp:165] Memory required for data: 5169924
I1127 11:02:05.712558  5942 net.cpp:226] loss needs backward computation.
I1127 11:02:05.712575  5942 net.cpp:226] ip2 needs backward computation.
I1127 11:02:05.712599  5942 net.cpp:226] relu1 needs backward computation.
I1127 11:02:05.712617  5942 net.cpp:226] ip1 needs backward computation.
I1127 11:02:05.712637  5942 net.cpp:226] pool2 needs backward computation.
I1127 11:02:05.712647  5942 net.cpp:226] conv2 needs backward computation.
I1127 11:02:05.712656  5942 net.cpp:226] pool1 needs backward computation.
I1127 11:02:05.712662  5942 net.cpp:226] conv1 needs backward computation.
I1127 11:02:05.712671  5942 net.cpp:228] mnist does not need backward computation.
I1127 11:02:05.712677  5942 net.cpp:270] This network produces output loss
I1127 11:02:05.712694  5942 net.cpp:283] Network initialization done.
I1127 11:02:05.713203  5942 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:02:05.713295  5942 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:02:05.713517  5942 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:02:05.713645  5942 layer_factory.hpp:76] Creating layer mnist
I1127 11:02:05.713840  5942 net.cpp:106] Creating Layer mnist
I1127 11:02:05.713853  5942 net.cpp:411] mnist -> data
I1127 11:02:05.713871  5942 net.cpp:411] mnist -> label
I1127 11:02:05.719722  5948 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:02:05.722348  5942 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:02:05.724592  5942 net.cpp:150] Setting up mnist
I1127 11:02:05.724694  5942 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:02:05.724709  5942 net.cpp:157] Top shape: 100 (100)
I1127 11:02:05.724717  5942 net.cpp:165] Memory required for data: 314000
I1127 11:02:05.724732  5942 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:02:05.724761  5942 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:02:05.724773  5942 net.cpp:454] label_mnist_1_split <- label
I1127 11:02:05.724792  5942 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:02:05.724815  5942 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:02:05.724897  5942 net.cpp:150] Setting up label_mnist_1_split
I1127 11:02:05.724910  5942 net.cpp:157] Top shape: 100 (100)
I1127 11:02:05.724920  5942 net.cpp:157] Top shape: 100 (100)
I1127 11:02:05.724925  5942 net.cpp:165] Memory required for data: 314800
I1127 11:02:05.724932  5942 layer_factory.hpp:76] Creating layer conv1
I1127 11:02:05.724957  5942 net.cpp:106] Creating Layer conv1
I1127 11:02:05.724966  5942 net.cpp:454] conv1 <- data
I1127 11:02:05.724975  5942 net.cpp:411] conv1 -> conv1
I1127 11:02:05.725211  5942 net.cpp:150] Setting up conv1
I1127 11:02:05.725229  5942 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:02:05.725239  5942 net.cpp:165] Memory required for data: 4922800
I1127 11:02:05.725260  5942 layer_factory.hpp:76] Creating layer pool1
I1127 11:02:05.725280  5942 net.cpp:106] Creating Layer pool1
I1127 11:02:05.725291  5942 net.cpp:454] pool1 <- conv1
I1127 11:02:05.725340  5942 net.cpp:411] pool1 -> pool1
I1127 11:02:05.725407  5942 net.cpp:150] Setting up pool1
I1127 11:02:05.725421  5942 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:02:05.725430  5942 net.cpp:165] Memory required for data: 6074800
I1127 11:02:05.725437  5942 layer_factory.hpp:76] Creating layer conv2
I1127 11:02:05.725455  5942 net.cpp:106] Creating Layer conv2
I1127 11:02:05.725466  5942 net.cpp:454] conv2 <- pool1
I1127 11:02:05.725484  5942 net.cpp:411] conv2 -> conv2
I1127 11:02:05.725966  5942 net.cpp:150] Setting up conv2
I1127 11:02:05.725991  5942 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:02:05.726001  5942 net.cpp:165] Memory required for data: 7354800
I1127 11:02:05.726022  5942 layer_factory.hpp:76] Creating layer pool2
I1127 11:02:05.726038  5942 net.cpp:106] Creating Layer pool2
I1127 11:02:05.726049  5942 net.cpp:454] pool2 <- conv2
I1127 11:02:05.726063  5942 net.cpp:411] pool2 -> pool2
I1127 11:02:05.726121  5942 net.cpp:150] Setting up pool2
I1127 11:02:05.726135  5942 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:02:05.726155  5942 net.cpp:165] Memory required for data: 7674800
I1127 11:02:05.726164  5942 layer_factory.hpp:76] Creating layer ip1
I1127 11:02:05.726179  5942 net.cpp:106] Creating Layer ip1
I1127 11:02:05.726187  5942 net.cpp:454] ip1 <- pool2
I1127 11:02:05.726197  5942 net.cpp:411] ip1 -> ip1
I1127 11:02:05.731034  5942 net.cpp:150] Setting up ip1
I1127 11:02:05.731129  5942 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:02:05.731139  5942 net.cpp:165] Memory required for data: 7874800
I1127 11:02:05.731173  5942 layer_factory.hpp:76] Creating layer relu1
I1127 11:02:05.731214  5942 net.cpp:106] Creating Layer relu1
I1127 11:02:05.731230  5942 net.cpp:454] relu1 <- ip1
I1127 11:02:05.731250  5942 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:02:05.731276  5942 net.cpp:150] Setting up relu1
I1127 11:02:05.731287  5942 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:02:05.731297  5942 net.cpp:165] Memory required for data: 8074800
I1127 11:02:05.731304  5942 layer_factory.hpp:76] Creating layer ip2
I1127 11:02:05.731336  5942 net.cpp:106] Creating Layer ip2
I1127 11:02:05.731348  5942 net.cpp:454] ip2 <- ip1
I1127 11:02:05.731361  5942 net.cpp:411] ip2 -> ip2
I1127 11:02:05.731675  5942 net.cpp:150] Setting up ip2
I1127 11:02:05.731711  5942 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:02:05.731722  5942 net.cpp:165] Memory required for data: 8078800
I1127 11:02:05.731745  5942 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:02:05.731770  5942 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:02:05.731783  5942 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:02:05.731803  5942 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:02:05.731817  5942 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:02:05.731874  5942 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:02:05.731889  5942 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:02:05.731899  5942 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:02:05.731906  5942 net.cpp:165] Memory required for data: 8086800
I1127 11:02:05.731914  5942 layer_factory.hpp:76] Creating layer accuracy
I1127 11:02:05.731935  5942 net.cpp:106] Creating Layer accuracy
I1127 11:02:05.731943  5942 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:02:05.731955  5942 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:02:05.731969  5942 net.cpp:411] accuracy -> accuracy
I1127 11:02:05.731992  5942 net.cpp:150] Setting up accuracy
I1127 11:02:05.732002  5942 net.cpp:157] Top shape: (1)
I1127 11:02:05.732010  5942 net.cpp:165] Memory required for data: 8086804
I1127 11:02:05.732018  5942 layer_factory.hpp:76] Creating layer loss
I1127 11:02:05.732030  5942 net.cpp:106] Creating Layer loss
I1127 11:02:05.732038  5942 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:02:05.732048  5942 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:02:05.732059  5942 net.cpp:411] loss -> loss
I1127 11:02:05.732077  5942 layer_factory.hpp:76] Creating layer loss
I1127 11:02:05.732277  5942 net.cpp:150] Setting up loss
I1127 11:02:05.732296  5942 net.cpp:157] Top shape: (1)
I1127 11:02:05.732303  5942 net.cpp:160]     with loss weight 1
I1127 11:02:05.732336  5942 net.cpp:165] Memory required for data: 8086808
I1127 11:02:05.732347  5942 net.cpp:226] loss needs backward computation.
I1127 11:02:05.732367  5942 net.cpp:228] accuracy does not need backward computation.
I1127 11:02:05.732377  5942 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:02:05.732386  5942 net.cpp:226] ip2 needs backward computation.
I1127 11:02:05.732396  5942 net.cpp:226] relu1 needs backward computation.
I1127 11:02:05.732404  5942 net.cpp:226] ip1 needs backward computation.
I1127 11:02:05.732414  5942 net.cpp:226] pool2 needs backward computation.
I1127 11:02:05.732422  5942 net.cpp:226] conv2 needs backward computation.
I1127 11:02:05.732430  5942 net.cpp:226] pool1 needs backward computation.
I1127 11:02:05.732439  5942 net.cpp:226] conv1 needs backward computation.
I1127 11:02:05.732461  5942 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:02:05.732472  5942 net.cpp:228] mnist does not need backward computation.
I1127 11:02:05.732480  5942 net.cpp:270] This network produces output accuracy
I1127 11:02:05.732487  5942 net.cpp:270] This network produces output loss
I1127 11:02:05.732512  5942 net.cpp:283] Network initialization done.
I1127 11:02:05.732661  5942 solver.cpp:59] Solver scaffolding done.
I1127 11:02:05.733083  5942 caffe.cpp:212] Starting Optimization
I1127 11:02:05.733126  5942 solver.cpp:287] Solving LeNet
I1127 11:02:05.733147  5942 solver.cpp:288] Learning Rate Policy: inv
I1127 11:02:05.734272  5942 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:02:05.735420  5942 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:02:07.646107  5942 solver.cpp:408]     Test net output #0: accuracy = 0.0893
I1127 11:02:07.646148  5942 solver.cpp:408]     Test net output #1: loss = 2.42581 (* 1 = 2.42581 loss)
I1127 11:02:07.681138  5942 solver.cpp:236] Iteration 0, loss = 2.41326
I1127 11:02:07.681154  5942 solver.cpp:252]     Train net output #0: loss = 2.41326 (* 1 = 2.41326 loss)
I1127 11:02:07.681170  5942 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:02:20.627882  5942 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:02:23.424089  5942 solver.cpp:408]     Test net output #0: accuracy = 0.9728
I1127 11:02:23.424178  5942 solver.cpp:408]     Test net output #1: loss = 0.0849739 (* 1 = 0.0849739 loss)
I1127 11:02:23.436389  5942 solver.cpp:236] Iteration 500, loss = 0.0847606
I1127 11:02:23.436480  5942 solver.cpp:252]     Train net output #0: loss = 0.0847605 (* 1 = 0.0847605 loss)
I1127 11:02:23.436504  5942 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:02:34.951001  5942 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:02:34.970103  5942 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:02:34.983842  5942 solver.cpp:320] Iteration 1000, loss = 0.0602123
I1127 11:02:34.983988  5942 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:02:37.195958  5942 solver.cpp:408]     Test net output #0: accuracy = 0.9813
I1127 11:02:37.196075  5942 solver.cpp:408]     Test net output #1: loss = 0.0569118 (* 1 = 0.0569118 loss)
I1127 11:02:37.196084  5942 solver.cpp:325] Optimization Done.
I1127 11:02:37.196089  5942 caffe.cpp:215] Optimization Done.
I1127 11:02:37.263845  5971 caffe.cpp:184] Using GPUs 0
I1127 11:02:37.656457  5971 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:02:37.656599  5971 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:02:37.656976  5971 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:02:37.656999  5971 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:02:37.657133  5971 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:02:37.657218  5971 layer_factory.hpp:76] Creating layer mnist
I1127 11:02:37.657671  5971 net.cpp:106] Creating Layer mnist
I1127 11:02:37.657687  5971 net.cpp:411] mnist -> data
I1127 11:02:37.657721  5971 net.cpp:411] mnist -> label
I1127 11:02:37.658488  5974 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:02:37.692219  5971 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:02:37.710235  5971 net.cpp:150] Setting up mnist
I1127 11:02:37.710286  5971 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:02:37.710300  5971 net.cpp:157] Top shape: 64 (64)
I1127 11:02:37.710309  5971 net.cpp:165] Memory required for data: 200960
I1127 11:02:37.710325  5971 layer_factory.hpp:76] Creating layer conv1
I1127 11:02:37.710350  5971 net.cpp:106] Creating Layer conv1
I1127 11:02:37.710363  5971 net.cpp:454] conv1 <- data
I1127 11:02:37.710381  5971 net.cpp:411] conv1 -> conv1
I1127 11:02:37.711287  5971 net.cpp:150] Setting up conv1
I1127 11:02:37.711304  5971 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:02:37.711314  5971 net.cpp:165] Memory required for data: 3150080
I1127 11:02:37.711334  5971 layer_factory.hpp:76] Creating layer pool1
I1127 11:02:37.711349  5971 net.cpp:106] Creating Layer pool1
I1127 11:02:37.711359  5971 net.cpp:454] pool1 <- conv1
I1127 11:02:37.711380  5971 net.cpp:411] pool1 -> pool1
I1127 11:02:37.711460  5971 net.cpp:150] Setting up pool1
I1127 11:02:37.711474  5971 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:02:37.711483  5971 net.cpp:165] Memory required for data: 3887360
I1127 11:02:37.711490  5971 layer_factory.hpp:76] Creating layer conv2
I1127 11:02:37.711508  5971 net.cpp:106] Creating Layer conv2
I1127 11:02:37.711518  5971 net.cpp:454] conv2 <- pool1
I1127 11:02:37.711532  5971 net.cpp:411] conv2 -> conv2
I1127 11:02:37.711971  5971 net.cpp:150] Setting up conv2
I1127 11:02:37.711987  5971 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:02:37.712000  5971 net.cpp:165] Memory required for data: 4706560
I1127 11:02:37.712019  5971 layer_factory.hpp:76] Creating layer pool2
I1127 11:02:37.712035  5971 net.cpp:106] Creating Layer pool2
I1127 11:02:37.712045  5971 net.cpp:454] pool2 <- conv2
I1127 11:02:37.712056  5971 net.cpp:411] pool2 -> pool2
I1127 11:02:37.712103  5971 net.cpp:150] Setting up pool2
I1127 11:02:37.712119  5971 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:02:37.712128  5971 net.cpp:165] Memory required for data: 4911360
I1127 11:02:37.712136  5971 layer_factory.hpp:76] Creating layer ip1
I1127 11:02:37.712151  5971 net.cpp:106] Creating Layer ip1
I1127 11:02:37.712158  5971 net.cpp:454] ip1 <- pool2
I1127 11:02:37.712172  5971 net.cpp:411] ip1 -> ip1
I1127 11:02:37.715781  5971 net.cpp:150] Setting up ip1
I1127 11:02:37.715801  5971 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:02:37.715816  5971 net.cpp:165] Memory required for data: 5039360
I1127 11:02:37.715832  5971 layer_factory.hpp:76] Creating layer relu1
I1127 11:02:37.715847  5971 net.cpp:106] Creating Layer relu1
I1127 11:02:37.715857  5971 net.cpp:454] relu1 <- ip1
I1127 11:02:37.715867  5971 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:02:37.715883  5971 net.cpp:150] Setting up relu1
I1127 11:02:37.715893  5971 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:02:37.715901  5971 net.cpp:165] Memory required for data: 5167360
I1127 11:02:37.715909  5971 layer_factory.hpp:76] Creating layer ip2
I1127 11:02:37.715921  5971 net.cpp:106] Creating Layer ip2
I1127 11:02:37.715930  5971 net.cpp:454] ip2 <- ip1
I1127 11:02:37.715945  5971 net.cpp:411] ip2 -> ip2
I1127 11:02:37.716548  5971 net.cpp:150] Setting up ip2
I1127 11:02:37.716564  5971 net.cpp:157] Top shape: 64 10 (640)
I1127 11:02:37.716573  5971 net.cpp:165] Memory required for data: 5169920
I1127 11:02:37.716586  5971 layer_factory.hpp:76] Creating layer loss
I1127 11:02:37.716600  5971 net.cpp:106] Creating Layer loss
I1127 11:02:37.716609  5971 net.cpp:454] loss <- ip2
I1127 11:02:37.716620  5971 net.cpp:454] loss <- label
I1127 11:02:37.716634  5971 net.cpp:411] loss -> loss
I1127 11:02:37.716655  5971 layer_factory.hpp:76] Creating layer loss
I1127 11:02:37.716768  5971 net.cpp:150] Setting up loss
I1127 11:02:37.716781  5971 net.cpp:157] Top shape: (1)
I1127 11:02:37.716790  5971 net.cpp:160]     with loss weight 1
I1127 11:02:37.716814  5971 net.cpp:165] Memory required for data: 5169924
I1127 11:02:37.716823  5971 net.cpp:226] loss needs backward computation.
I1127 11:02:37.716832  5971 net.cpp:226] ip2 needs backward computation.
I1127 11:02:37.716840  5971 net.cpp:226] relu1 needs backward computation.
I1127 11:02:37.716847  5971 net.cpp:226] ip1 needs backward computation.
I1127 11:02:37.716856  5971 net.cpp:226] pool2 needs backward computation.
I1127 11:02:37.716863  5971 net.cpp:226] conv2 needs backward computation.
I1127 11:02:37.716871  5971 net.cpp:226] pool1 needs backward computation.
I1127 11:02:37.716879  5971 net.cpp:226] conv1 needs backward computation.
I1127 11:02:37.716887  5971 net.cpp:228] mnist does not need backward computation.
I1127 11:02:37.716895  5971 net.cpp:270] This network produces output loss
I1127 11:02:37.716912  5971 net.cpp:283] Network initialization done.
I1127 11:02:37.717285  5971 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:02:37.717320  5971 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:02:37.717499  5971 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:02:37.717598  5971 layer_factory.hpp:76] Creating layer mnist
I1127 11:02:37.717726  5971 net.cpp:106] Creating Layer mnist
I1127 11:02:37.717741  5971 net.cpp:411] mnist -> data
I1127 11:02:37.717758  5971 net.cpp:411] mnist -> label
I1127 11:02:37.718466  5976 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:02:37.718572  5971 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:02:37.722267  5971 net.cpp:150] Setting up mnist
I1127 11:02:37.722287  5971 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:02:37.722298  5971 net.cpp:157] Top shape: 100 (100)
I1127 11:02:37.722306  5971 net.cpp:165] Memory required for data: 314000
I1127 11:02:37.722314  5971 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:02:37.722326  5971 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:02:37.722334  5971 net.cpp:454] label_mnist_1_split <- label
I1127 11:02:37.722344  5971 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:02:37.722357  5971 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:02:37.722406  5971 net.cpp:150] Setting up label_mnist_1_split
I1127 11:02:37.722419  5971 net.cpp:157] Top shape: 100 (100)
I1127 11:02:37.722429  5971 net.cpp:157] Top shape: 100 (100)
I1127 11:02:37.722436  5971 net.cpp:165] Memory required for data: 314800
I1127 11:02:37.722443  5971 layer_factory.hpp:76] Creating layer conv1
I1127 11:02:37.722460  5971 net.cpp:106] Creating Layer conv1
I1127 11:02:37.722468  5971 net.cpp:454] conv1 <- data
I1127 11:02:37.722481  5971 net.cpp:411] conv1 -> conv1
I1127 11:02:37.722719  5971 net.cpp:150] Setting up conv1
I1127 11:02:37.722733  5971 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:02:37.722743  5971 net.cpp:165] Memory required for data: 4922800
I1127 11:02:37.722757  5971 layer_factory.hpp:76] Creating layer pool1
I1127 11:02:37.722769  5971 net.cpp:106] Creating Layer pool1
I1127 11:02:37.722776  5971 net.cpp:454] pool1 <- conv1
I1127 11:02:37.722796  5971 net.cpp:411] pool1 -> pool1
I1127 11:02:37.722842  5971 net.cpp:150] Setting up pool1
I1127 11:02:37.722854  5971 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:02:37.722863  5971 net.cpp:165] Memory required for data: 6074800
I1127 11:02:37.722872  5971 layer_factory.hpp:76] Creating layer conv2
I1127 11:02:37.722885  5971 net.cpp:106] Creating Layer conv2
I1127 11:02:37.722894  5971 net.cpp:454] conv2 <- pool1
I1127 11:02:37.722906  5971 net.cpp:411] conv2 -> conv2
I1127 11:02:37.723600  5971 net.cpp:150] Setting up conv2
I1127 11:02:37.723615  5971 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:02:37.723623  5971 net.cpp:165] Memory required for data: 7354800
I1127 11:02:37.723639  5971 layer_factory.hpp:76] Creating layer pool2
I1127 11:02:37.723651  5971 net.cpp:106] Creating Layer pool2
I1127 11:02:37.723660  5971 net.cpp:454] pool2 <- conv2
I1127 11:02:37.723670  5971 net.cpp:411] pool2 -> pool2
I1127 11:02:37.723717  5971 net.cpp:150] Setting up pool2
I1127 11:02:37.723732  5971 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:02:37.723742  5971 net.cpp:165] Memory required for data: 7674800
I1127 11:02:37.723749  5971 layer_factory.hpp:76] Creating layer ip1
I1127 11:02:37.723764  5971 net.cpp:106] Creating Layer ip1
I1127 11:02:37.723774  5971 net.cpp:454] ip1 <- pool2
I1127 11:02:37.723788  5971 net.cpp:411] ip1 -> ip1
I1127 11:02:37.727416  5971 net.cpp:150] Setting up ip1
I1127 11:02:37.727443  5971 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:02:37.727452  5971 net.cpp:165] Memory required for data: 7874800
I1127 11:02:37.727468  5971 layer_factory.hpp:76] Creating layer relu1
I1127 11:02:37.727481  5971 net.cpp:106] Creating Layer relu1
I1127 11:02:37.727490  5971 net.cpp:454] relu1 <- ip1
I1127 11:02:37.727504  5971 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:02:37.727517  5971 net.cpp:150] Setting up relu1
I1127 11:02:37.727529  5971 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:02:37.727536  5971 net.cpp:165] Memory required for data: 8074800
I1127 11:02:37.727543  5971 layer_factory.hpp:76] Creating layer ip2
I1127 11:02:37.727556  5971 net.cpp:106] Creating Layer ip2
I1127 11:02:37.727566  5971 net.cpp:454] ip2 <- ip1
I1127 11:02:37.727582  5971 net.cpp:411] ip2 -> ip2
I1127 11:02:37.727746  5971 net.cpp:150] Setting up ip2
I1127 11:02:37.727758  5971 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:02:37.727766  5971 net.cpp:165] Memory required for data: 8078800
I1127 11:02:37.727779  5971 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:02:37.727790  5971 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:02:37.727799  5971 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:02:37.727809  5971 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:02:37.727824  5971 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:02:37.727870  5971 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:02:37.727883  5971 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:02:37.727892  5971 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:02:37.727901  5971 net.cpp:165] Memory required for data: 8086800
I1127 11:02:37.727910  5971 layer_factory.hpp:76] Creating layer accuracy
I1127 11:02:37.727922  5971 net.cpp:106] Creating Layer accuracy
I1127 11:02:37.727931  5971 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:02:37.727941  5971 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:02:37.727952  5971 net.cpp:411] accuracy -> accuracy
I1127 11:02:37.727967  5971 net.cpp:150] Setting up accuracy
I1127 11:02:37.727977  5971 net.cpp:157] Top shape: (1)
I1127 11:02:37.727984  5971 net.cpp:165] Memory required for data: 8086804
I1127 11:02:37.727993  5971 layer_factory.hpp:76] Creating layer loss
I1127 11:02:37.728006  5971 net.cpp:106] Creating Layer loss
I1127 11:02:37.728016  5971 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:02:37.728025  5971 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:02:37.728035  5971 net.cpp:411] loss -> loss
I1127 11:02:37.728049  5971 layer_factory.hpp:76] Creating layer loss
I1127 11:02:37.728171  5971 net.cpp:150] Setting up loss
I1127 11:02:37.728183  5971 net.cpp:157] Top shape: (1)
I1127 11:02:37.728191  5971 net.cpp:160]     with loss weight 1
I1127 11:02:37.728206  5971 net.cpp:165] Memory required for data: 8086808
I1127 11:02:37.728215  5971 net.cpp:226] loss needs backward computation.
I1127 11:02:37.728226  5971 net.cpp:228] accuracy does not need backward computation.
I1127 11:02:37.728235  5971 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:02:37.728242  5971 net.cpp:226] ip2 needs backward computation.
I1127 11:02:37.728250  5971 net.cpp:226] relu1 needs backward computation.
I1127 11:02:37.728258  5971 net.cpp:226] ip1 needs backward computation.
I1127 11:02:37.728266  5971 net.cpp:226] pool2 needs backward computation.
I1127 11:02:37.728273  5971 net.cpp:226] conv2 needs backward computation.
I1127 11:02:37.728282  5971 net.cpp:226] pool1 needs backward computation.
I1127 11:02:37.728291  5971 net.cpp:226] conv1 needs backward computation.
I1127 11:02:37.728298  5971 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:02:37.728307  5971 net.cpp:228] mnist does not need backward computation.
I1127 11:02:37.728314  5971 net.cpp:270] This network produces output accuracy
I1127 11:02:37.728322  5971 net.cpp:270] This network produces output loss
I1127 11:02:37.728339  5971 net.cpp:283] Network initialization done.
I1127 11:02:37.728396  5971 solver.cpp:59] Solver scaffolding done.
I1127 11:02:37.728721  5971 caffe.cpp:212] Starting Optimization
I1127 11:02:37.728732  5971 solver.cpp:287] Solving LeNet
I1127 11:02:37.728740  5971 solver.cpp:288] Learning Rate Policy: inv
I1127 11:02:37.729208  5971 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:02:39.192965  5971 solver.cpp:408]     Test net output #0: accuracy = 0.0648
I1127 11:02:39.193089  5971 solver.cpp:408]     Test net output #1: loss = 2.44193 (* 1 = 2.44193 loss)
I1127 11:02:39.207986  5971 solver.cpp:236] Iteration 0, loss = 2.48176
I1127 11:02:39.208073  5971 solver.cpp:252]     Train net output #0: loss = 2.48176 (* 1 = 2.48176 loss)
I1127 11:02:39.208101  5971 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:02:52.623356  5971 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:02:55.073819  5971 solver.cpp:408]     Test net output #0: accuracy = 0.9699
I1127 11:02:55.073998  5971 solver.cpp:408]     Test net output #1: loss = 0.0914634 (* 1 = 0.0914634 loss)
I1127 11:02:55.105525  5971 solver.cpp:236] Iteration 500, loss = 0.152172
I1127 11:02:55.105633  5971 solver.cpp:252]     Train net output #0: loss = 0.152172 (* 1 = 0.152172 loss)
I1127 11:02:55.105655  5971 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:03:07.300241  5971 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:03:07.315083  5971 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:03:07.325390  5971 solver.cpp:320] Iteration 1000, loss = 0.0774173
I1127 11:03:07.325443  5971 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:03:09.600883  5971 solver.cpp:408]     Test net output #0: accuracy = 0.9803
I1127 11:03:09.600921  5971 solver.cpp:408]     Test net output #1: loss = 0.0612489 (* 1 = 0.0612489 loss)
I1127 11:03:09.600929  5971 solver.cpp:325] Optimization Done.
I1127 11:03:09.600934  5971 caffe.cpp:215] Optimization Done.
I1127 11:03:09.668154  6001 caffe.cpp:184] Using GPUs 0
I1127 11:03:10.035491  6001 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:03:10.035604  6001 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:03:10.035887  6001 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:03:10.035902  6001 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:03:10.035989  6001 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:03:10.036049  6001 layer_factory.hpp:76] Creating layer mnist
I1127 11:03:10.036357  6001 net.cpp:106] Creating Layer mnist
I1127 11:03:10.036380  6001 net.cpp:411] mnist -> data
I1127 11:03:10.036403  6001 net.cpp:411] mnist -> label
I1127 11:03:10.037160  6004 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:03:10.071671  6001 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:03:10.077924  6001 net.cpp:150] Setting up mnist
I1127 11:03:10.077945  6001 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:03:10.077952  6001 net.cpp:157] Top shape: 64 (64)
I1127 11:03:10.077957  6001 net.cpp:165] Memory required for data: 200960
I1127 11:03:10.077966  6001 layer_factory.hpp:76] Creating layer conv1
I1127 11:03:10.077985  6001 net.cpp:106] Creating Layer conv1
I1127 11:03:10.077991  6001 net.cpp:454] conv1 <- data
I1127 11:03:10.078061  6001 net.cpp:411] conv1 -> conv1
I1127 11:03:10.079006  6001 net.cpp:150] Setting up conv1
I1127 11:03:10.079018  6001 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:03:10.079023  6001 net.cpp:165] Memory required for data: 3150080
I1127 11:03:10.079037  6001 layer_factory.hpp:76] Creating layer pool1
I1127 11:03:10.079046  6001 net.cpp:106] Creating Layer pool1
I1127 11:03:10.079051  6001 net.cpp:454] pool1 <- conv1
I1127 11:03:10.079058  6001 net.cpp:411] pool1 -> pool1
I1127 11:03:10.079105  6001 net.cpp:150] Setting up pool1
I1127 11:03:10.079114  6001 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:03:10.079118  6001 net.cpp:165] Memory required for data: 3887360
I1127 11:03:10.079123  6001 layer_factory.hpp:76] Creating layer conv2
I1127 11:03:10.079130  6001 net.cpp:106] Creating Layer conv2
I1127 11:03:10.079135  6001 net.cpp:454] conv2 <- pool1
I1127 11:03:10.079143  6001 net.cpp:411] conv2 -> conv2
I1127 11:03:10.079394  6001 net.cpp:150] Setting up conv2
I1127 11:03:10.079403  6001 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:03:10.079408  6001 net.cpp:165] Memory required for data: 4706560
I1127 11:03:10.079416  6001 layer_factory.hpp:76] Creating layer pool2
I1127 11:03:10.079424  6001 net.cpp:106] Creating Layer pool2
I1127 11:03:10.079429  6001 net.cpp:454] pool2 <- conv2
I1127 11:03:10.079435  6001 net.cpp:411] pool2 -> pool2
I1127 11:03:10.079490  6001 net.cpp:150] Setting up pool2
I1127 11:03:10.079499  6001 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:03:10.079502  6001 net.cpp:165] Memory required for data: 4911360
I1127 11:03:10.079507  6001 layer_factory.hpp:76] Creating layer ip1
I1127 11:03:10.079517  6001 net.cpp:106] Creating Layer ip1
I1127 11:03:10.079522  6001 net.cpp:454] ip1 <- pool2
I1127 11:03:10.079529  6001 net.cpp:411] ip1 -> ip1
I1127 11:03:10.081627  6001 net.cpp:150] Setting up ip1
I1127 11:03:10.081640  6001 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:03:10.081645  6001 net.cpp:165] Memory required for data: 5039360
I1127 11:03:10.081652  6001 layer_factory.hpp:76] Creating layer relu1
I1127 11:03:10.081661  6001 net.cpp:106] Creating Layer relu1
I1127 11:03:10.081666  6001 net.cpp:454] relu1 <- ip1
I1127 11:03:10.081671  6001 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:03:10.081681  6001 net.cpp:150] Setting up relu1
I1127 11:03:10.081686  6001 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:03:10.081689  6001 net.cpp:165] Memory required for data: 5167360
I1127 11:03:10.081693  6001 layer_factory.hpp:76] Creating layer ip2
I1127 11:03:10.081704  6001 net.cpp:106] Creating Layer ip2
I1127 11:03:10.081709  6001 net.cpp:454] ip2 <- ip1
I1127 11:03:10.081717  6001 net.cpp:411] ip2 -> ip2
I1127 11:03:10.082110  6001 net.cpp:150] Setting up ip2
I1127 11:03:10.082120  6001 net.cpp:157] Top shape: 64 10 (640)
I1127 11:03:10.082124  6001 net.cpp:165] Memory required for data: 5169920
I1127 11:03:10.082131  6001 layer_factory.hpp:76] Creating layer loss
I1127 11:03:10.082145  6001 net.cpp:106] Creating Layer loss
I1127 11:03:10.082151  6001 net.cpp:454] loss <- ip2
I1127 11:03:10.082157  6001 net.cpp:454] loss <- label
I1127 11:03:10.082165  6001 net.cpp:411] loss -> loss
I1127 11:03:10.082177  6001 layer_factory.hpp:76] Creating layer loss
I1127 11:03:10.082243  6001 net.cpp:150] Setting up loss
I1127 11:03:10.082252  6001 net.cpp:157] Top shape: (1)
I1127 11:03:10.082255  6001 net.cpp:160]     with loss weight 1
I1127 11:03:10.082270  6001 net.cpp:165] Memory required for data: 5169924
I1127 11:03:10.082275  6001 net.cpp:226] loss needs backward computation.
I1127 11:03:10.082279  6001 net.cpp:226] ip2 needs backward computation.
I1127 11:03:10.082284  6001 net.cpp:226] relu1 needs backward computation.
I1127 11:03:10.082288  6001 net.cpp:226] ip1 needs backward computation.
I1127 11:03:10.082293  6001 net.cpp:226] pool2 needs backward computation.
I1127 11:03:10.082296  6001 net.cpp:226] conv2 needs backward computation.
I1127 11:03:10.082300  6001 net.cpp:226] pool1 needs backward computation.
I1127 11:03:10.082304  6001 net.cpp:226] conv1 needs backward computation.
I1127 11:03:10.082309  6001 net.cpp:228] mnist does not need backward computation.
I1127 11:03:10.082314  6001 net.cpp:270] This network produces output loss
I1127 11:03:10.082321  6001 net.cpp:283] Network initialization done.
I1127 11:03:10.082557  6001 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:03:10.082579  6001 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:03:10.082690  6001 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:03:10.082751  6001 layer_factory.hpp:76] Creating layer mnist
I1127 11:03:10.082834  6001 net.cpp:106] Creating Layer mnist
I1127 11:03:10.082844  6001 net.cpp:411] mnist -> data
I1127 11:03:10.082854  6001 net.cpp:411] mnist -> label
I1127 11:03:10.083578  6006 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:03:10.083653  6001 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:03:10.172186  6001 net.cpp:150] Setting up mnist
I1127 11:03:10.172227  6001 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:03:10.172235  6001 net.cpp:157] Top shape: 100 (100)
I1127 11:03:10.172238  6001 net.cpp:165] Memory required for data: 314000
I1127 11:03:10.172247  6001 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:03:10.172263  6001 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:03:10.172271  6001 net.cpp:454] label_mnist_1_split <- label
I1127 11:03:10.172278  6001 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:03:10.172289  6001 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:03:10.172416  6001 net.cpp:150] Setting up label_mnist_1_split
I1127 11:03:10.172427  6001 net.cpp:157] Top shape: 100 (100)
I1127 11:03:10.172432  6001 net.cpp:157] Top shape: 100 (100)
I1127 11:03:10.172436  6001 net.cpp:165] Memory required for data: 314800
I1127 11:03:10.172441  6001 layer_factory.hpp:76] Creating layer conv1
I1127 11:03:10.172454  6001 net.cpp:106] Creating Layer conv1
I1127 11:03:10.172459  6001 net.cpp:454] conv1 <- data
I1127 11:03:10.172466  6001 net.cpp:411] conv1 -> conv1
I1127 11:03:10.172642  6001 net.cpp:150] Setting up conv1
I1127 11:03:10.172652  6001 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:03:10.172657  6001 net.cpp:165] Memory required for data: 4922800
I1127 11:03:10.172667  6001 layer_factory.hpp:76] Creating layer pool1
I1127 11:03:10.172674  6001 net.cpp:106] Creating Layer pool1
I1127 11:03:10.172678  6001 net.cpp:454] pool1 <- conv1
I1127 11:03:10.172709  6001 net.cpp:411] pool1 -> pool1
I1127 11:03:10.172755  6001 net.cpp:150] Setting up pool1
I1127 11:03:10.172763  6001 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:03:10.172767  6001 net.cpp:165] Memory required for data: 6074800
I1127 11:03:10.172772  6001 layer_factory.hpp:76] Creating layer conv2
I1127 11:03:10.172783  6001 net.cpp:106] Creating Layer conv2
I1127 11:03:10.172788  6001 net.cpp:454] conv2 <- pool1
I1127 11:03:10.172794  6001 net.cpp:411] conv2 -> conv2
I1127 11:03:10.173077  6001 net.cpp:150] Setting up conv2
I1127 11:03:10.173086  6001 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:03:10.173091  6001 net.cpp:165] Memory required for data: 7354800
I1127 11:03:10.173099  6001 layer_factory.hpp:76] Creating layer pool2
I1127 11:03:10.173107  6001 net.cpp:106] Creating Layer pool2
I1127 11:03:10.173112  6001 net.cpp:454] pool2 <- conv2
I1127 11:03:10.173130  6001 net.cpp:411] pool2 -> pool2
I1127 11:03:10.173207  6001 net.cpp:150] Setting up pool2
I1127 11:03:10.173214  6001 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:03:10.173218  6001 net.cpp:165] Memory required for data: 7674800
I1127 11:03:10.173223  6001 layer_factory.hpp:76] Creating layer ip1
I1127 11:03:10.173233  6001 net.cpp:106] Creating Layer ip1
I1127 11:03:10.173238  6001 net.cpp:454] ip1 <- pool2
I1127 11:03:10.173244  6001 net.cpp:411] ip1 -> ip1
I1127 11:03:10.175680  6001 net.cpp:150] Setting up ip1
I1127 11:03:10.175698  6001 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:03:10.175703  6001 net.cpp:165] Memory required for data: 7874800
I1127 11:03:10.175711  6001 layer_factory.hpp:76] Creating layer relu1
I1127 11:03:10.175719  6001 net.cpp:106] Creating Layer relu1
I1127 11:03:10.175722  6001 net.cpp:454] relu1 <- ip1
I1127 11:03:10.175729  6001 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:03:10.175736  6001 net.cpp:150] Setting up relu1
I1127 11:03:10.175741  6001 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:03:10.175745  6001 net.cpp:165] Memory required for data: 8074800
I1127 11:03:10.175756  6001 layer_factory.hpp:76] Creating layer ip2
I1127 11:03:10.175767  6001 net.cpp:106] Creating Layer ip2
I1127 11:03:10.175771  6001 net.cpp:454] ip2 <- ip1
I1127 11:03:10.175778  6001 net.cpp:411] ip2 -> ip2
I1127 11:03:10.175874  6001 net.cpp:150] Setting up ip2
I1127 11:03:10.175884  6001 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:03:10.175887  6001 net.cpp:165] Memory required for data: 8078800
I1127 11:03:10.175894  6001 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:03:10.175900  6001 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:03:10.175904  6001 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:03:10.175910  6001 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:03:10.175916  6001 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:03:10.175945  6001 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:03:10.175951  6001 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:03:10.175956  6001 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:03:10.175961  6001 net.cpp:165] Memory required for data: 8086800
I1127 11:03:10.175966  6001 layer_factory.hpp:76] Creating layer accuracy
I1127 11:03:10.175973  6001 net.cpp:106] Creating Layer accuracy
I1127 11:03:10.175978  6001 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:03:10.175983  6001 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:03:10.175989  6001 net.cpp:411] accuracy -> accuracy
I1127 11:03:10.175997  6001 net.cpp:150] Setting up accuracy
I1127 11:03:10.176003  6001 net.cpp:157] Top shape: (1)
I1127 11:03:10.176007  6001 net.cpp:165] Memory required for data: 8086804
I1127 11:03:10.176012  6001 layer_factory.hpp:76] Creating layer loss
I1127 11:03:10.176019  6001 net.cpp:106] Creating Layer loss
I1127 11:03:10.176024  6001 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:03:10.176028  6001 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:03:10.176034  6001 net.cpp:411] loss -> loss
I1127 11:03:10.176043  6001 layer_factory.hpp:76] Creating layer loss
I1127 11:03:10.176131  6001 net.cpp:150] Setting up loss
I1127 11:03:10.176141  6001 net.cpp:157] Top shape: (1)
I1127 11:03:10.176144  6001 net.cpp:160]     with loss weight 1
I1127 11:03:10.176156  6001 net.cpp:165] Memory required for data: 8086808
I1127 11:03:10.176161  6001 net.cpp:226] loss needs backward computation.
I1127 11:03:10.176168  6001 net.cpp:228] accuracy does not need backward computation.
I1127 11:03:10.176173  6001 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:03:10.176177  6001 net.cpp:226] ip2 needs backward computation.
I1127 11:03:10.176182  6001 net.cpp:226] relu1 needs backward computation.
I1127 11:03:10.176185  6001 net.cpp:226] ip1 needs backward computation.
I1127 11:03:10.176189  6001 net.cpp:226] pool2 needs backward computation.
I1127 11:03:10.176193  6001 net.cpp:226] conv2 needs backward computation.
I1127 11:03:10.176198  6001 net.cpp:226] pool1 needs backward computation.
I1127 11:03:10.176203  6001 net.cpp:226] conv1 needs backward computation.
I1127 11:03:10.176206  6001 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:03:10.176211  6001 net.cpp:228] mnist does not need backward computation.
I1127 11:03:10.176215  6001 net.cpp:270] This network produces output accuracy
I1127 11:03:10.176219  6001 net.cpp:270] This network produces output loss
I1127 11:03:10.176230  6001 net.cpp:283] Network initialization done.
I1127 11:03:10.176271  6001 solver.cpp:59] Solver scaffolding done.
I1127 11:03:10.176460  6001 caffe.cpp:212] Starting Optimization
I1127 11:03:10.176466  6001 solver.cpp:287] Solving LeNet
I1127 11:03:10.176470  6001 solver.cpp:288] Learning Rate Policy: inv
I1127 11:03:10.176813  6001 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:03:11.539053  6001 solver.cpp:408]     Test net output #0: accuracy = 0.0989
I1127 11:03:11.539196  6001 solver.cpp:408]     Test net output #1: loss = 2.39256 (* 1 = 2.39256 loss)
I1127 11:03:11.555225  6001 solver.cpp:236] Iteration 0, loss = 2.46116
I1127 11:03:11.555397  6001 solver.cpp:252]     Train net output #0: loss = 2.46116 (* 1 = 2.46116 loss)
I1127 11:03:11.555472  6001 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:03:20.791651  6001 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:03:24.994750  6001 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:03:26.168041  6001 solver.cpp:408]     Test net output #0: accuracy = 0.9727
I1127 11:03:26.168077  6001 solver.cpp:408]     Test net output #1: loss = 0.0861385 (* 1 = 0.0861385 loss)
I1127 11:03:26.196420  6001 solver.cpp:236] Iteration 500, loss = 0.111641
I1127 11:03:26.196439  6001 solver.cpp:252]     Train net output #0: loss = 0.111641 (* 1 = 0.111641 loss)
I1127 11:03:26.196447  6001 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:03:39.882426  6001 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:03:39.902593  6001 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:03:39.930615  6001 solver.cpp:320] Iteration 1000, loss = 0.0753918
I1127 11:03:39.930635  6001 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:03:42.688370  6001 solver.cpp:408]     Test net output #0: accuracy = 0.9832
I1127 11:03:42.688592  6001 solver.cpp:408]     Test net output #1: loss = 0.0521628 (* 1 = 0.0521628 loss)
I1127 11:03:42.688619  6001 solver.cpp:325] Optimization Done.
I1127 11:03:42.688637  6001 caffe.cpp:215] Optimization Done.
I1127 11:03:42.813340  6029 caffe.cpp:184] Using GPUs 0
I1127 11:03:43.183392  6029 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:03:43.183629  6029 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:03:43.184082  6029 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:03:43.184111  6029 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:03:43.184273  6029 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:03:43.184391  6029 layer_factory.hpp:76] Creating layer mnist
I1127 11:03:43.185024  6029 net.cpp:106] Creating Layer mnist
I1127 11:03:43.185048  6029 net.cpp:411] mnist -> data
I1127 11:03:43.185101  6029 net.cpp:411] mnist -> label
I1127 11:03:43.186298  6033 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:03:43.194840  6029 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:03:43.196033  6029 net.cpp:150] Setting up mnist
I1127 11:03:43.196100  6029 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:03:43.196108  6029 net.cpp:157] Top shape: 64 (64)
I1127 11:03:43.196113  6029 net.cpp:165] Memory required for data: 200960
I1127 11:03:43.196125  6029 layer_factory.hpp:76] Creating layer conv1
I1127 11:03:43.196149  6029 net.cpp:106] Creating Layer conv1
I1127 11:03:43.196158  6029 net.cpp:454] conv1 <- data
I1127 11:03:43.196173  6029 net.cpp:411] conv1 -> conv1
I1127 11:03:43.197002  6029 net.cpp:150] Setting up conv1
I1127 11:03:43.197046  6029 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:03:43.197052  6029 net.cpp:165] Memory required for data: 3150080
I1127 11:03:43.197069  6029 layer_factory.hpp:76] Creating layer pool1
I1127 11:03:43.197082  6029 net.cpp:106] Creating Layer pool1
I1127 11:03:43.197088  6029 net.cpp:454] pool1 <- conv1
I1127 11:03:43.197095  6029 net.cpp:411] pool1 -> pool1
I1127 11:03:43.197150  6029 net.cpp:150] Setting up pool1
I1127 11:03:43.197159  6029 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:03:43.197162  6029 net.cpp:165] Memory required for data: 3887360
I1127 11:03:43.197167  6029 layer_factory.hpp:76] Creating layer conv2
I1127 11:03:43.197178  6029 net.cpp:106] Creating Layer conv2
I1127 11:03:43.197183  6029 net.cpp:454] conv2 <- pool1
I1127 11:03:43.197191  6029 net.cpp:411] conv2 -> conv2
I1127 11:03:43.197526  6029 net.cpp:150] Setting up conv2
I1127 11:03:43.197543  6029 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:03:43.197556  6029 net.cpp:165] Memory required for data: 4706560
I1127 11:03:43.197567  6029 layer_factory.hpp:76] Creating layer pool2
I1127 11:03:43.197579  6029 net.cpp:106] Creating Layer pool2
I1127 11:03:43.197585  6029 net.cpp:454] pool2 <- conv2
I1127 11:03:43.197594  6029 net.cpp:411] pool2 -> pool2
I1127 11:03:43.197638  6029 net.cpp:150] Setting up pool2
I1127 11:03:43.197650  6029 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:03:43.197657  6029 net.cpp:165] Memory required for data: 4911360
I1127 11:03:43.197664  6029 layer_factory.hpp:76] Creating layer ip1
I1127 11:03:43.197679  6029 net.cpp:106] Creating Layer ip1
I1127 11:03:43.197686  6029 net.cpp:454] ip1 <- pool2
I1127 11:03:43.197697  6029 net.cpp:411] ip1 -> ip1
I1127 11:03:43.201428  6029 net.cpp:150] Setting up ip1
I1127 11:03:43.201499  6029 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:03:43.201509  6029 net.cpp:165] Memory required for data: 5039360
I1127 11:03:43.201535  6029 layer_factory.hpp:76] Creating layer relu1
I1127 11:03:43.201556  6029 net.cpp:106] Creating Layer relu1
I1127 11:03:43.201565  6029 net.cpp:454] relu1 <- ip1
I1127 11:03:43.201581  6029 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:03:43.201608  6029 net.cpp:150] Setting up relu1
I1127 11:03:43.201617  6029 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:03:43.201622  6029 net.cpp:165] Memory required for data: 5167360
I1127 11:03:43.201625  6029 layer_factory.hpp:76] Creating layer ip2
I1127 11:03:43.201638  6029 net.cpp:106] Creating Layer ip2
I1127 11:03:43.201643  6029 net.cpp:454] ip2 <- ip1
I1127 11:03:43.201652  6029 net.cpp:411] ip2 -> ip2
I1127 11:03:43.202368  6029 net.cpp:150] Setting up ip2
I1127 11:03:43.202409  6029 net.cpp:157] Top shape: 64 10 (640)
I1127 11:03:43.202414  6029 net.cpp:165] Memory required for data: 5169920
I1127 11:03:43.202427  6029 layer_factory.hpp:76] Creating layer loss
I1127 11:03:43.202445  6029 net.cpp:106] Creating Layer loss
I1127 11:03:43.202464  6029 net.cpp:454] loss <- ip2
I1127 11:03:43.202473  6029 net.cpp:454] loss <- label
I1127 11:03:43.202488  6029 net.cpp:411] loss -> loss
I1127 11:03:43.202513  6029 layer_factory.hpp:76] Creating layer loss
I1127 11:03:43.202594  6029 net.cpp:150] Setting up loss
I1127 11:03:43.202602  6029 net.cpp:157] Top shape: (1)
I1127 11:03:43.202607  6029 net.cpp:160]     with loss weight 1
I1127 11:03:43.202635  6029 net.cpp:165] Memory required for data: 5169924
I1127 11:03:43.202639  6029 net.cpp:226] loss needs backward computation.
I1127 11:03:43.202644  6029 net.cpp:226] ip2 needs backward computation.
I1127 11:03:43.202648  6029 net.cpp:226] relu1 needs backward computation.
I1127 11:03:43.202652  6029 net.cpp:226] ip1 needs backward computation.
I1127 11:03:43.202657  6029 net.cpp:226] pool2 needs backward computation.
I1127 11:03:43.202661  6029 net.cpp:226] conv2 needs backward computation.
I1127 11:03:43.202666  6029 net.cpp:226] pool1 needs backward computation.
I1127 11:03:43.202672  6029 net.cpp:226] conv1 needs backward computation.
I1127 11:03:43.202675  6029 net.cpp:228] mnist does not need backward computation.
I1127 11:03:43.202679  6029 net.cpp:270] This network produces output loss
I1127 11:03:43.202692  6029 net.cpp:283] Network initialization done.
I1127 11:03:43.203032  6029 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:03:43.203065  6029 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:03:43.203200  6029 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:03:43.203287  6029 layer_factory.hpp:76] Creating layer mnist
I1127 11:03:43.203415  6029 net.cpp:106] Creating Layer mnist
I1127 11:03:43.203425  6029 net.cpp:411] mnist -> data
I1127 11:03:43.203434  6029 net.cpp:411] mnist -> label
I1127 11:03:43.207818  6035 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:03:43.210304  6029 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:03:43.226271  6029 net.cpp:150] Setting up mnist
I1127 11:03:43.226366  6029 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:03:43.226380  6029 net.cpp:157] Top shape: 100 (100)
I1127 11:03:43.226387  6029 net.cpp:165] Memory required for data: 314000
I1127 11:03:43.226402  6029 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:03:43.226431  6029 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:03:43.226444  6029 net.cpp:454] label_mnist_1_split <- label
I1127 11:03:43.226459  6029 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:03:43.226483  6029 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:03:43.226588  6029 net.cpp:150] Setting up label_mnist_1_split
I1127 11:03:43.226601  6029 net.cpp:157] Top shape: 100 (100)
I1127 11:03:43.226610  6029 net.cpp:157] Top shape: 100 (100)
I1127 11:03:43.226616  6029 net.cpp:165] Memory required for data: 314800
I1127 11:03:43.226625  6029 layer_factory.hpp:76] Creating layer conv1
I1127 11:03:43.226650  6029 net.cpp:106] Creating Layer conv1
I1127 11:03:43.226661  6029 net.cpp:454] conv1 <- data
I1127 11:03:43.226673  6029 net.cpp:411] conv1 -> conv1
I1127 11:03:43.226997  6029 net.cpp:150] Setting up conv1
I1127 11:03:43.227013  6029 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:03:43.227021  6029 net.cpp:165] Memory required for data: 4922800
I1127 11:03:43.227038  6029 layer_factory.hpp:76] Creating layer pool1
I1127 11:03:43.227052  6029 net.cpp:106] Creating Layer pool1
I1127 11:03:43.227058  6029 net.cpp:454] pool1 <- conv1
I1127 11:03:43.227095  6029 net.cpp:411] pool1 -> pool1
I1127 11:03:43.227140  6029 net.cpp:150] Setting up pool1
I1127 11:03:43.227154  6029 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:03:43.227160  6029 net.cpp:165] Memory required for data: 6074800
I1127 11:03:43.227166  6029 layer_factory.hpp:76] Creating layer conv2
I1127 11:03:43.227180  6029 net.cpp:106] Creating Layer conv2
I1127 11:03:43.227187  6029 net.cpp:454] conv2 <- pool1
I1127 11:03:43.227237  6029 net.cpp:411] conv2 -> conv2
I1127 11:03:43.227933  6029 net.cpp:150] Setting up conv2
I1127 11:03:43.227978  6029 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:03:43.227988  6029 net.cpp:165] Memory required for data: 7354800
I1127 11:03:43.228013  6029 layer_factory.hpp:76] Creating layer pool2
I1127 11:03:43.228044  6029 net.cpp:106] Creating Layer pool2
I1127 11:03:43.228057  6029 net.cpp:454] pool2 <- conv2
I1127 11:03:43.228070  6029 net.cpp:411] pool2 -> pool2
I1127 11:03:43.228127  6029 net.cpp:150] Setting up pool2
I1127 11:03:43.228138  6029 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:03:43.228144  6029 net.cpp:165] Memory required for data: 7674800
I1127 11:03:43.228152  6029 layer_factory.hpp:76] Creating layer ip1
I1127 11:03:43.228175  6029 net.cpp:106] Creating Layer ip1
I1127 11:03:43.228185  6029 net.cpp:454] ip1 <- pool2
I1127 11:03:43.228195  6029 net.cpp:411] ip1 -> ip1
I1127 11:03:43.231529  6029 net.cpp:150] Setting up ip1
I1127 11:03:43.231595  6029 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:03:43.231600  6029 net.cpp:165] Memory required for data: 7874800
I1127 11:03:43.231624  6029 layer_factory.hpp:76] Creating layer relu1
I1127 11:03:43.231642  6029 net.cpp:106] Creating Layer relu1
I1127 11:03:43.231649  6029 net.cpp:454] relu1 <- ip1
I1127 11:03:43.231658  6029 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:03:43.231690  6029 net.cpp:150] Setting up relu1
I1127 11:03:43.231696  6029 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:03:43.231701  6029 net.cpp:165] Memory required for data: 8074800
I1127 11:03:43.231706  6029 layer_factory.hpp:76] Creating layer ip2
I1127 11:03:43.231720  6029 net.cpp:106] Creating Layer ip2
I1127 11:03:43.231724  6029 net.cpp:454] ip2 <- ip1
I1127 11:03:43.231731  6029 net.cpp:411] ip2 -> ip2
I1127 11:03:43.231861  6029 net.cpp:150] Setting up ip2
I1127 11:03:43.231871  6029 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:03:43.231876  6029 net.cpp:165] Memory required for data: 8078800
I1127 11:03:43.231895  6029 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:03:43.231907  6029 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:03:43.231911  6029 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:03:43.231919  6029 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:03:43.231925  6029 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:03:43.231956  6029 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:03:43.231964  6029 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:03:43.231969  6029 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:03:43.231973  6029 net.cpp:165] Memory required for data: 8086800
I1127 11:03:43.231978  6029 layer_factory.hpp:76] Creating layer accuracy
I1127 11:03:43.231986  6029 net.cpp:106] Creating Layer accuracy
I1127 11:03:43.231992  6029 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:03:43.231997  6029 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:03:43.232002  6029 net.cpp:411] accuracy -> accuracy
I1127 11:03:43.232013  6029 net.cpp:150] Setting up accuracy
I1127 11:03:43.232019  6029 net.cpp:157] Top shape: (1)
I1127 11:03:43.232023  6029 net.cpp:165] Memory required for data: 8086804
I1127 11:03:43.232028  6029 layer_factory.hpp:76] Creating layer loss
I1127 11:03:43.232036  6029 net.cpp:106] Creating Layer loss
I1127 11:03:43.232040  6029 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:03:43.232045  6029 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:03:43.232053  6029 net.cpp:411] loss -> loss
I1127 11:03:43.232065  6029 layer_factory.hpp:76] Creating layer loss
I1127 11:03:43.232161  6029 net.cpp:150] Setting up loss
I1127 11:03:43.232169  6029 net.cpp:157] Top shape: (1)
I1127 11:03:43.232174  6029 net.cpp:160]     with loss weight 1
I1127 11:03:43.232194  6029 net.cpp:165] Memory required for data: 8086808
I1127 11:03:43.232198  6029 net.cpp:226] loss needs backward computation.
I1127 11:03:43.232210  6029 net.cpp:228] accuracy does not need backward computation.
I1127 11:03:43.232216  6029 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:03:43.232221  6029 net.cpp:226] ip2 needs backward computation.
I1127 11:03:43.232226  6029 net.cpp:226] relu1 needs backward computation.
I1127 11:03:43.232230  6029 net.cpp:226] ip1 needs backward computation.
I1127 11:03:43.232237  6029 net.cpp:226] pool2 needs backward computation.
I1127 11:03:43.232244  6029 net.cpp:226] conv2 needs backward computation.
I1127 11:03:43.232249  6029 net.cpp:226] pool1 needs backward computation.
I1127 11:03:43.232252  6029 net.cpp:226] conv1 needs backward computation.
I1127 11:03:43.232257  6029 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:03:43.232264  6029 net.cpp:228] mnist does not need backward computation.
I1127 11:03:43.232267  6029 net.cpp:270] This network produces output accuracy
I1127 11:03:43.232271  6029 net.cpp:270] This network produces output loss
I1127 11:03:43.232283  6029 net.cpp:283] Network initialization done.
I1127 11:03:43.232355  6029 solver.cpp:59] Solver scaffolding done.
I1127 11:03:43.232543  6029 caffe.cpp:212] Starting Optimization
I1127 11:03:43.232552  6029 solver.cpp:287] Solving LeNet
I1127 11:03:43.232555  6029 solver.cpp:288] Learning Rate Policy: inv
I1127 11:03:43.233022  6029 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:03:45.380050  6029 solver.cpp:408]     Test net output #0: accuracy = 0.0693
I1127 11:03:45.380089  6029 solver.cpp:408]     Test net output #1: loss = 2.35361 (* 1 = 2.35361 loss)
I1127 11:03:45.411322  6029 solver.cpp:236] Iteration 0, loss = 2.37209
I1127 11:03:45.411340  6029 solver.cpp:252]     Train net output #0: loss = 2.37209 (* 1 = 2.37209 loss)
I1127 11:03:45.411357  6029 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:03:57.899063  6029 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:03:58.989787  6029 solver.cpp:408]     Test net output #0: accuracy = 0.9718
I1127 11:03:58.989915  6029 solver.cpp:408]     Test net output #1: loss = 0.0876887 (* 1 = 0.0876887 loss)
I1127 11:03:59.002602  6029 solver.cpp:236] Iteration 500, loss = 0.118973
I1127 11:03:59.002790  6029 solver.cpp:252]     Train net output #0: loss = 0.118973 (* 1 = 0.118973 loss)
I1127 11:03:59.002820  6029 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:04:12.499060  6029 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:04:12.516800  6029 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:04:12.528542  6029 solver.cpp:320] Iteration 1000, loss = 0.128187
I1127 11:04:12.528650  6029 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:04:14.926379  6029 solver.cpp:408]     Test net output #0: accuracy = 0.9797
I1127 11:04:14.926743  6029 solver.cpp:408]     Test net output #1: loss = 0.0623012 (* 1 = 0.0623012 loss)
I1127 11:04:14.926758  6029 solver.cpp:325] Optimization Done.
I1127 11:04:14.926764  6029 caffe.cpp:215] Optimization Done.
I1127 11:04:15.016561  6075 caffe.cpp:184] Using GPUs 0
I1127 11:04:15.392149  6075 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:04:15.392280  6075 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:04:15.392649  6075 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:04:15.392670  6075 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:04:15.392796  6075 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:04:15.392869  6075 layer_factory.hpp:76] Creating layer mnist
I1127 11:04:15.393332  6075 net.cpp:106] Creating Layer mnist
I1127 11:04:15.393345  6075 net.cpp:411] mnist -> data
I1127 11:04:15.393373  6075 net.cpp:411] mnist -> label
I1127 11:04:15.394243  6080 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:04:15.428267  6075 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:04:15.435503  6075 net.cpp:150] Setting up mnist
I1127 11:04:15.435546  6075 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:04:15.435555  6075 net.cpp:157] Top shape: 64 (64)
I1127 11:04:15.435559  6075 net.cpp:165] Memory required for data: 200960
I1127 11:04:15.435570  6075 layer_factory.hpp:76] Creating layer conv1
I1127 11:04:15.435588  6075 net.cpp:106] Creating Layer conv1
I1127 11:04:15.435596  6075 net.cpp:454] conv1 <- data
I1127 11:04:15.435608  6075 net.cpp:411] conv1 -> conv1
I1127 11:04:15.436275  6075 net.cpp:150] Setting up conv1
I1127 11:04:15.436303  6075 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:04:15.436308  6075 net.cpp:165] Memory required for data: 3150080
I1127 11:04:15.436326  6075 layer_factory.hpp:76] Creating layer pool1
I1127 11:04:15.436342  6075 net.cpp:106] Creating Layer pool1
I1127 11:04:15.436347  6075 net.cpp:454] pool1 <- conv1
I1127 11:04:15.436355  6075 net.cpp:411] pool1 -> pool1
I1127 11:04:15.436430  6075 net.cpp:150] Setting up pool1
I1127 11:04:15.436440  6075 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:04:15.436444  6075 net.cpp:165] Memory required for data: 3887360
I1127 11:04:15.436450  6075 layer_factory.hpp:76] Creating layer conv2
I1127 11:04:15.436465  6075 net.cpp:106] Creating Layer conv2
I1127 11:04:15.436470  6075 net.cpp:454] conv2 <- pool1
I1127 11:04:15.436478  6075 net.cpp:411] conv2 -> conv2
I1127 11:04:15.436887  6075 net.cpp:150] Setting up conv2
I1127 11:04:15.436902  6075 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:04:15.436905  6075 net.cpp:165] Memory required for data: 4706560
I1127 11:04:15.436915  6075 layer_factory.hpp:76] Creating layer pool2
I1127 11:04:15.436923  6075 net.cpp:106] Creating Layer pool2
I1127 11:04:15.436928  6075 net.cpp:454] pool2 <- conv2
I1127 11:04:15.436933  6075 net.cpp:411] pool2 -> pool2
I1127 11:04:15.436961  6075 net.cpp:150] Setting up pool2
I1127 11:04:15.436970  6075 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:04:15.436975  6075 net.cpp:165] Memory required for data: 4911360
I1127 11:04:15.436980  6075 layer_factory.hpp:76] Creating layer ip1
I1127 11:04:15.436988  6075 net.cpp:106] Creating Layer ip1
I1127 11:04:15.436993  6075 net.cpp:454] ip1 <- pool2
I1127 11:04:15.437000  6075 net.cpp:411] ip1 -> ip1
I1127 11:04:15.439713  6075 net.cpp:150] Setting up ip1
I1127 11:04:15.439757  6075 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:04:15.439766  6075 net.cpp:165] Memory required for data: 5039360
I1127 11:04:15.439787  6075 layer_factory.hpp:76] Creating layer relu1
I1127 11:04:15.439805  6075 net.cpp:106] Creating Layer relu1
I1127 11:04:15.439816  6075 net.cpp:454] relu1 <- ip1
I1127 11:04:15.439833  6075 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:04:15.439857  6075 net.cpp:150] Setting up relu1
I1127 11:04:15.439867  6075 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:04:15.439875  6075 net.cpp:165] Memory required for data: 5167360
I1127 11:04:15.439884  6075 layer_factory.hpp:76] Creating layer ip2
I1127 11:04:15.439900  6075 net.cpp:106] Creating Layer ip2
I1127 11:04:15.439909  6075 net.cpp:454] ip2 <- ip1
I1127 11:04:15.439920  6075 net.cpp:411] ip2 -> ip2
I1127 11:04:15.440636  6075 net.cpp:150] Setting up ip2
I1127 11:04:15.440665  6075 net.cpp:157] Top shape: 64 10 (640)
I1127 11:04:15.440672  6075 net.cpp:165] Memory required for data: 5169920
I1127 11:04:15.440686  6075 layer_factory.hpp:76] Creating layer loss
I1127 11:04:15.440706  6075 net.cpp:106] Creating Layer loss
I1127 11:04:15.440713  6075 net.cpp:454] loss <- ip2
I1127 11:04:15.440723  6075 net.cpp:454] loss <- label
I1127 11:04:15.440737  6075 net.cpp:411] loss -> loss
I1127 11:04:15.440757  6075 layer_factory.hpp:76] Creating layer loss
I1127 11:04:15.440865  6075 net.cpp:150] Setting up loss
I1127 11:04:15.440877  6075 net.cpp:157] Top shape: (1)
I1127 11:04:15.440883  6075 net.cpp:160]     with loss weight 1
I1127 11:04:15.440917  6075 net.cpp:165] Memory required for data: 5169924
I1127 11:04:15.440924  6075 net.cpp:226] loss needs backward computation.
I1127 11:04:15.440944  6075 net.cpp:226] ip2 needs backward computation.
I1127 11:04:15.440953  6075 net.cpp:226] relu1 needs backward computation.
I1127 11:04:15.440959  6075 net.cpp:226] ip1 needs backward computation.
I1127 11:04:15.440965  6075 net.cpp:226] pool2 needs backward computation.
I1127 11:04:15.440973  6075 net.cpp:226] conv2 needs backward computation.
I1127 11:04:15.440979  6075 net.cpp:226] pool1 needs backward computation.
I1127 11:04:15.440986  6075 net.cpp:226] conv1 needs backward computation.
I1127 11:04:15.440994  6075 net.cpp:228] mnist does not need backward computation.
I1127 11:04:15.441000  6075 net.cpp:270] This network produces output loss
I1127 11:04:15.441015  6075 net.cpp:283] Network initialization done.
I1127 11:04:15.441433  6075 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:04:15.441498  6075 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:04:15.441694  6075 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:04:15.441809  6075 layer_factory.hpp:76] Creating layer mnist
I1127 11:04:15.441972  6075 net.cpp:106] Creating Layer mnist
I1127 11:04:15.441990  6075 net.cpp:411] mnist -> data
I1127 11:04:15.442015  6075 net.cpp:411] mnist -> label
I1127 11:04:15.443290  6082 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:04:15.443629  6075 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:04:15.491838  6075 net.cpp:150] Setting up mnist
I1127 11:04:15.491901  6075 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:04:15.491909  6075 net.cpp:157] Top shape: 100 (100)
I1127 11:04:15.491912  6075 net.cpp:165] Memory required for data: 314000
I1127 11:04:15.491924  6075 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:04:15.491945  6075 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:04:15.491952  6075 net.cpp:454] label_mnist_1_split <- label
I1127 11:04:15.491963  6075 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:04:15.491992  6075 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:04:15.492059  6075 net.cpp:150] Setting up label_mnist_1_split
I1127 11:04:15.492070  6075 net.cpp:157] Top shape: 100 (100)
I1127 11:04:15.492075  6075 net.cpp:157] Top shape: 100 (100)
I1127 11:04:15.492079  6075 net.cpp:165] Memory required for data: 314800
I1127 11:04:15.492084  6075 layer_factory.hpp:76] Creating layer conv1
I1127 11:04:15.492100  6075 net.cpp:106] Creating Layer conv1
I1127 11:04:15.492105  6075 net.cpp:454] conv1 <- data
I1127 11:04:15.492113  6075 net.cpp:411] conv1 -> conv1
I1127 11:04:15.492288  6075 net.cpp:150] Setting up conv1
I1127 11:04:15.492298  6075 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:04:15.492302  6075 net.cpp:165] Memory required for data: 4922800
I1127 11:04:15.492312  6075 layer_factory.hpp:76] Creating layer pool1
I1127 11:04:15.492322  6075 net.cpp:106] Creating Layer pool1
I1127 11:04:15.492327  6075 net.cpp:454] pool1 <- conv1
I1127 11:04:15.492346  6075 net.cpp:411] pool1 -> pool1
I1127 11:04:15.492377  6075 net.cpp:150] Setting up pool1
I1127 11:04:15.492384  6075 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:04:15.492388  6075 net.cpp:165] Memory required for data: 6074800
I1127 11:04:15.492393  6075 layer_factory.hpp:76] Creating layer conv2
I1127 11:04:15.492403  6075 net.cpp:106] Creating Layer conv2
I1127 11:04:15.492408  6075 net.cpp:454] conv2 <- pool1
I1127 11:04:15.492414  6075 net.cpp:411] conv2 -> conv2
I1127 11:04:15.492671  6075 net.cpp:150] Setting up conv2
I1127 11:04:15.492681  6075 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:04:15.492686  6075 net.cpp:165] Memory required for data: 7354800
I1127 11:04:15.492694  6075 layer_factory.hpp:76] Creating layer pool2
I1127 11:04:15.492704  6075 net.cpp:106] Creating Layer pool2
I1127 11:04:15.492709  6075 net.cpp:454] pool2 <- conv2
I1127 11:04:15.492715  6075 net.cpp:411] pool2 -> pool2
I1127 11:04:15.492763  6075 net.cpp:150] Setting up pool2
I1127 11:04:15.492772  6075 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:04:15.492776  6075 net.cpp:165] Memory required for data: 7674800
I1127 11:04:15.492781  6075 layer_factory.hpp:76] Creating layer ip1
I1127 11:04:15.492794  6075 net.cpp:106] Creating Layer ip1
I1127 11:04:15.492799  6075 net.cpp:454] ip1 <- pool2
I1127 11:04:15.492805  6075 net.cpp:411] ip1 -> ip1
I1127 11:04:15.496579  6075 net.cpp:150] Setting up ip1
I1127 11:04:15.496652  6075 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:04:15.496664  6075 net.cpp:165] Memory required for data: 7874800
I1127 11:04:15.496695  6075 layer_factory.hpp:76] Creating layer relu1
I1127 11:04:15.496723  6075 net.cpp:106] Creating Layer relu1
I1127 11:04:15.496737  6075 net.cpp:454] relu1 <- ip1
I1127 11:04:15.496754  6075 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:04:15.496775  6075 net.cpp:150] Setting up relu1
I1127 11:04:15.496788  6075 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:04:15.496795  6075 net.cpp:165] Memory required for data: 8074800
I1127 11:04:15.496803  6075 layer_factory.hpp:76] Creating layer ip2
I1127 11:04:15.496827  6075 net.cpp:106] Creating Layer ip2
I1127 11:04:15.496836  6075 net.cpp:454] ip2 <- ip1
I1127 11:04:15.496860  6075 net.cpp:411] ip2 -> ip2
I1127 11:04:15.497092  6075 net.cpp:150] Setting up ip2
I1127 11:04:15.497109  6075 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:04:15.497118  6075 net.cpp:165] Memory required for data: 8078800
I1127 11:04:15.497133  6075 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:04:15.497148  6075 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:04:15.497164  6075 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:04:15.497179  6075 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:04:15.497192  6075 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:04:15.497249  6075 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:04:15.497262  6075 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:04:15.497272  6075 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:04:15.497297  6075 net.cpp:165] Memory required for data: 8086800
I1127 11:04:15.497309  6075 layer_factory.hpp:76] Creating layer accuracy
I1127 11:04:15.497325  6075 net.cpp:106] Creating Layer accuracy
I1127 11:04:15.497334  6075 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:04:15.497344  6075 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:04:15.497357  6075 net.cpp:411] accuracy -> accuracy
I1127 11:04:15.497375  6075 net.cpp:150] Setting up accuracy
I1127 11:04:15.497386  6075 net.cpp:157] Top shape: (1)
I1127 11:04:15.497393  6075 net.cpp:165] Memory required for data: 8086804
I1127 11:04:15.497409  6075 layer_factory.hpp:76] Creating layer loss
I1127 11:04:15.497422  6075 net.cpp:106] Creating Layer loss
I1127 11:04:15.497431  6075 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:04:15.497441  6075 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:04:15.497452  6075 net.cpp:411] loss -> loss
I1127 11:04:15.497467  6075 layer_factory.hpp:76] Creating layer loss
I1127 11:04:15.497622  6075 net.cpp:150] Setting up loss
I1127 11:04:15.497637  6075 net.cpp:157] Top shape: (1)
I1127 11:04:15.497647  6075 net.cpp:160]     with loss weight 1
I1127 11:04:15.497679  6075 net.cpp:165] Memory required for data: 8086808
I1127 11:04:15.497689  6075 net.cpp:226] loss needs backward computation.
I1127 11:04:15.497705  6075 net.cpp:228] accuracy does not need backward computation.
I1127 11:04:15.497715  6075 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:04:15.497723  6075 net.cpp:226] ip2 needs backward computation.
I1127 11:04:15.497731  6075 net.cpp:226] relu1 needs backward computation.
I1127 11:04:15.497740  6075 net.cpp:226] ip1 needs backward computation.
I1127 11:04:15.497748  6075 net.cpp:226] pool2 needs backward computation.
I1127 11:04:15.497757  6075 net.cpp:226] conv2 needs backward computation.
I1127 11:04:15.497766  6075 net.cpp:226] pool1 needs backward computation.
I1127 11:04:15.497781  6075 net.cpp:226] conv1 needs backward computation.
I1127 11:04:15.497791  6075 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:04:15.497799  6075 net.cpp:228] mnist does not need backward computation.
I1127 11:04:15.497807  6075 net.cpp:270] This network produces output accuracy
I1127 11:04:15.497814  6075 net.cpp:270] This network produces output loss
I1127 11:04:15.497836  6075 net.cpp:283] Network initialization done.
I1127 11:04:15.497972  6075 solver.cpp:59] Solver scaffolding done.
I1127 11:04:15.498332  6075 caffe.cpp:212] Starting Optimization
I1127 11:04:15.498345  6075 solver.cpp:287] Solving LeNet
I1127 11:04:15.498349  6075 solver.cpp:288] Learning Rate Policy: inv
I1127 11:04:15.498741  6075 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:04:16.761308  6075 solver.cpp:408]     Test net output #0: accuracy = 0.118
I1127 11:04:16.761396  6075 solver.cpp:408]     Test net output #1: loss = 2.35079 (* 1 = 2.35079 loss)
I1127 11:04:16.777632  6075 solver.cpp:236] Iteration 0, loss = 2.33004
I1127 11:04:16.777766  6075 solver.cpp:252]     Train net output #0: loss = 2.33004 (* 1 = 2.33004 loss)
I1127 11:04:16.777806  6075 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:04:30.269772  6075 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:04:30.857813  6075 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:04:31.380285  6075 solver.cpp:408]     Test net output #0: accuracy = 0.9714
I1127 11:04:31.380386  6075 solver.cpp:408]     Test net output #1: loss = 0.0885918 (* 1 = 0.0885918 loss)
I1127 11:04:31.390715  6075 solver.cpp:236] Iteration 500, loss = 0.106075
I1127 11:04:31.390813  6075 solver.cpp:252]     Train net output #0: loss = 0.106075 (* 1 = 0.106075 loss)
I1127 11:04:31.390830  6075 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:04:44.862701  6075 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:04:44.877919  6075 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:04:44.888073  6075 solver.cpp:320] Iteration 1000, loss = 0.0663218
I1127 11:04:44.888144  6075 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:04:46.293535  6075 solver.cpp:408]     Test net output #0: accuracy = 0.9808
I1127 11:04:46.293611  6075 solver.cpp:408]     Test net output #1: loss = 0.0606897 (* 1 = 0.0606897 loss)
I1127 11:04:46.293618  6075 solver.cpp:325] Optimization Done.
I1127 11:04:46.293623  6075 caffe.cpp:215] Optimization Done.
I1127 11:04:46.362162  6157 caffe.cpp:184] Using GPUs 0
I1127 11:04:46.802618  6157 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:04:46.802750  6157 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:04:46.803123  6157 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:04:46.803143  6157 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:04:46.803269  6157 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:04:46.803336  6157 layer_factory.hpp:76] Creating layer mnist
I1127 11:04:46.803827  6157 net.cpp:106] Creating Layer mnist
I1127 11:04:46.803853  6157 net.cpp:411] mnist -> data
I1127 11:04:46.803882  6157 net.cpp:411] mnist -> label
I1127 11:04:46.804632  6161 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:04:46.843546  6157 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:04:46.850759  6157 net.cpp:150] Setting up mnist
I1127 11:04:46.850782  6157 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:04:46.850791  6157 net.cpp:157] Top shape: 64 (64)
I1127 11:04:46.850798  6157 net.cpp:165] Memory required for data: 200960
I1127 11:04:46.850810  6157 layer_factory.hpp:76] Creating layer conv1
I1127 11:04:46.850829  6157 net.cpp:106] Creating Layer conv1
I1127 11:04:46.850838  6157 net.cpp:454] conv1 <- data
I1127 11:04:46.850859  6157 net.cpp:411] conv1 -> conv1
I1127 11:04:46.851634  6157 net.cpp:150] Setting up conv1
I1127 11:04:46.851649  6157 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:04:46.851656  6157 net.cpp:165] Memory required for data: 3150080
I1127 11:04:46.851675  6157 layer_factory.hpp:76] Creating layer pool1
I1127 11:04:46.851688  6157 net.cpp:106] Creating Layer pool1
I1127 11:04:46.851696  6157 net.cpp:454] pool1 <- conv1
I1127 11:04:46.851706  6157 net.cpp:411] pool1 -> pool1
I1127 11:04:46.851999  6157 net.cpp:150] Setting up pool1
I1127 11:04:46.852011  6157 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:04:46.852017  6157 net.cpp:165] Memory required for data: 3887360
I1127 11:04:46.852025  6157 layer_factory.hpp:76] Creating layer conv2
I1127 11:04:46.852038  6157 net.cpp:106] Creating Layer conv2
I1127 11:04:46.852046  6157 net.cpp:454] conv2 <- pool1
I1127 11:04:46.852054  6157 net.cpp:411] conv2 -> conv2
I1127 11:04:46.852468  6157 net.cpp:150] Setting up conv2
I1127 11:04:46.852480  6157 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:04:46.852496  6157 net.cpp:165] Memory required for data: 4706560
I1127 11:04:46.852509  6157 layer_factory.hpp:76] Creating layer pool2
I1127 11:04:46.852519  6157 net.cpp:106] Creating Layer pool2
I1127 11:04:46.852526  6157 net.cpp:454] pool2 <- conv2
I1127 11:04:46.852535  6157 net.cpp:411] pool2 -> pool2
I1127 11:04:46.852619  6157 net.cpp:150] Setting up pool2
I1127 11:04:46.852630  6157 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:04:46.852638  6157 net.cpp:165] Memory required for data: 4911360
I1127 11:04:46.852644  6157 layer_factory.hpp:76] Creating layer ip1
I1127 11:04:46.852654  6157 net.cpp:106] Creating Layer ip1
I1127 11:04:46.852661  6157 net.cpp:454] ip1 <- pool2
I1127 11:04:46.852672  6157 net.cpp:411] ip1 -> ip1
I1127 11:04:46.856227  6157 net.cpp:150] Setting up ip1
I1127 11:04:46.856247  6157 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:04:46.856257  6157 net.cpp:165] Memory required for data: 5039360
I1127 11:04:46.856273  6157 layer_factory.hpp:76] Creating layer relu1
I1127 11:04:46.856286  6157 net.cpp:106] Creating Layer relu1
I1127 11:04:46.856294  6157 net.cpp:454] relu1 <- ip1
I1127 11:04:46.856305  6157 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:04:46.856319  6157 net.cpp:150] Setting up relu1
I1127 11:04:46.856330  6157 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:04:46.856338  6157 net.cpp:165] Memory required for data: 5167360
I1127 11:04:46.856346  6157 layer_factory.hpp:76] Creating layer ip2
I1127 11:04:46.856361  6157 net.cpp:106] Creating Layer ip2
I1127 11:04:46.856369  6157 net.cpp:454] ip2 <- ip1
I1127 11:04:46.856384  6157 net.cpp:411] ip2 -> ip2
I1127 11:04:46.856991  6157 net.cpp:150] Setting up ip2
I1127 11:04:46.857005  6157 net.cpp:157] Top shape: 64 10 (640)
I1127 11:04:46.857013  6157 net.cpp:165] Memory required for data: 5169920
I1127 11:04:46.857026  6157 layer_factory.hpp:76] Creating layer loss
I1127 11:04:46.857043  6157 net.cpp:106] Creating Layer loss
I1127 11:04:46.857051  6157 net.cpp:454] loss <- ip2
I1127 11:04:46.857061  6157 net.cpp:454] loss <- label
I1127 11:04:46.857074  6157 net.cpp:411] loss -> loss
I1127 11:04:46.857090  6157 layer_factory.hpp:76] Creating layer loss
I1127 11:04:46.857193  6157 net.cpp:150] Setting up loss
I1127 11:04:46.857205  6157 net.cpp:157] Top shape: (1)
I1127 11:04:46.857213  6157 net.cpp:160]     with loss weight 1
I1127 11:04:46.857231  6157 net.cpp:165] Memory required for data: 5169924
I1127 11:04:46.857239  6157 net.cpp:226] loss needs backward computation.
I1127 11:04:46.857245  6157 net.cpp:226] ip2 needs backward computation.
I1127 11:04:46.857252  6157 net.cpp:226] relu1 needs backward computation.
I1127 11:04:46.857259  6157 net.cpp:226] ip1 needs backward computation.
I1127 11:04:46.857265  6157 net.cpp:226] pool2 needs backward computation.
I1127 11:04:46.857272  6157 net.cpp:226] conv2 needs backward computation.
I1127 11:04:46.857278  6157 net.cpp:226] pool1 needs backward computation.
I1127 11:04:46.857285  6157 net.cpp:226] conv1 needs backward computation.
I1127 11:04:46.857298  6157 net.cpp:228] mnist does not need backward computation.
I1127 11:04:46.857306  6157 net.cpp:270] This network produces output loss
I1127 11:04:46.857318  6157 net.cpp:283] Network initialization done.
I1127 11:04:46.857671  6157 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:04:46.857710  6157 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:04:46.857869  6157 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:04:46.857952  6157 layer_factory.hpp:76] Creating layer mnist
I1127 11:04:46.858065  6157 net.cpp:106] Creating Layer mnist
I1127 11:04:46.858078  6157 net.cpp:411] mnist -> data
I1127 11:04:46.858090  6157 net.cpp:411] mnist -> label
I1127 11:04:46.858788  6163 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:04:46.858886  6157 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:04:46.862855  6157 net.cpp:150] Setting up mnist
I1127 11:04:46.862872  6157 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:04:46.862881  6157 net.cpp:157] Top shape: 100 (100)
I1127 11:04:46.862887  6157 net.cpp:165] Memory required for data: 314000
I1127 11:04:46.862895  6157 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:04:46.862905  6157 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:04:46.862912  6157 net.cpp:454] label_mnist_1_split <- label
I1127 11:04:46.862923  6157 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:04:46.862936  6157 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:04:46.862977  6157 net.cpp:150] Setting up label_mnist_1_split
I1127 11:04:46.862987  6157 net.cpp:157] Top shape: 100 (100)
I1127 11:04:46.862994  6157 net.cpp:157] Top shape: 100 (100)
I1127 11:04:46.863000  6157 net.cpp:165] Memory required for data: 314800
I1127 11:04:46.863008  6157 layer_factory.hpp:76] Creating layer conv1
I1127 11:04:46.863026  6157 net.cpp:106] Creating Layer conv1
I1127 11:04:46.863034  6157 net.cpp:454] conv1 <- data
I1127 11:04:46.863044  6157 net.cpp:411] conv1 -> conv1
I1127 11:04:46.863255  6157 net.cpp:150] Setting up conv1
I1127 11:04:46.863267  6157 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:04:46.863273  6157 net.cpp:165] Memory required for data: 4922800
I1127 11:04:46.863287  6157 layer_factory.hpp:76] Creating layer pool1
I1127 11:04:46.863297  6157 net.cpp:106] Creating Layer pool1
I1127 11:04:46.863304  6157 net.cpp:454] pool1 <- conv1
I1127 11:04:46.863320  6157 net.cpp:411] pool1 -> pool1
I1127 11:04:46.863361  6157 net.cpp:150] Setting up pool1
I1127 11:04:46.863373  6157 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:04:46.863379  6157 net.cpp:165] Memory required for data: 6074800
I1127 11:04:46.863385  6157 layer_factory.hpp:76] Creating layer conv2
I1127 11:04:46.863397  6157 net.cpp:106] Creating Layer conv2
I1127 11:04:46.863404  6157 net.cpp:454] conv2 <- pool1
I1127 11:04:46.863415  6157 net.cpp:411] conv2 -> conv2
I1127 11:04:46.864166  6157 net.cpp:150] Setting up conv2
I1127 11:04:46.864177  6157 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:04:46.864184  6157 net.cpp:165] Memory required for data: 7354800
I1127 11:04:46.864197  6157 layer_factory.hpp:76] Creating layer pool2
I1127 11:04:46.864207  6157 net.cpp:106] Creating Layer pool2
I1127 11:04:46.864213  6157 net.cpp:454] pool2 <- conv2
I1127 11:04:46.864223  6157 net.cpp:411] pool2 -> pool2
I1127 11:04:46.864264  6157 net.cpp:150] Setting up pool2
I1127 11:04:46.864274  6157 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:04:46.864281  6157 net.cpp:165] Memory required for data: 7674800
I1127 11:04:46.864287  6157 layer_factory.hpp:76] Creating layer ip1
I1127 11:04:46.864297  6157 net.cpp:106] Creating Layer ip1
I1127 11:04:46.864305  6157 net.cpp:454] ip1 <- pool2
I1127 11:04:46.864315  6157 net.cpp:411] ip1 -> ip1
I1127 11:04:46.867941  6157 net.cpp:150] Setting up ip1
I1127 11:04:46.867962  6157 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:04:46.867969  6157 net.cpp:165] Memory required for data: 7874800
I1127 11:04:46.867983  6157 layer_factory.hpp:76] Creating layer relu1
I1127 11:04:46.867993  6157 net.cpp:106] Creating Layer relu1
I1127 11:04:46.868001  6157 net.cpp:454] relu1 <- ip1
I1127 11:04:46.868010  6157 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:04:46.868021  6157 net.cpp:150] Setting up relu1
I1127 11:04:46.868028  6157 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:04:46.868036  6157 net.cpp:165] Memory required for data: 8074800
I1127 11:04:46.868041  6157 layer_factory.hpp:76] Creating layer ip2
I1127 11:04:46.868054  6157 net.cpp:106] Creating Layer ip2
I1127 11:04:46.868062  6157 net.cpp:454] ip2 <- ip1
I1127 11:04:46.868072  6157 net.cpp:411] ip2 -> ip2
I1127 11:04:46.868214  6157 net.cpp:150] Setting up ip2
I1127 11:04:46.868226  6157 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:04:46.868232  6157 net.cpp:165] Memory required for data: 8078800
I1127 11:04:46.868242  6157 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:04:46.868252  6157 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:04:46.868258  6157 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:04:46.868268  6157 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:04:46.868278  6157 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:04:46.868316  6157 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:04:46.868326  6157 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:04:46.868335  6157 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:04:46.868340  6157 net.cpp:165] Memory required for data: 8086800
I1127 11:04:46.868347  6157 layer_factory.hpp:76] Creating layer accuracy
I1127 11:04:46.868357  6157 net.cpp:106] Creating Layer accuracy
I1127 11:04:46.868363  6157 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:04:46.868371  6157 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:04:46.868383  6157 net.cpp:411] accuracy -> accuracy
I1127 11:04:46.868396  6157 net.cpp:150] Setting up accuracy
I1127 11:04:46.868409  6157 net.cpp:157] Top shape: (1)
I1127 11:04:46.868417  6157 net.cpp:165] Memory required for data: 8086804
I1127 11:04:46.868422  6157 layer_factory.hpp:76] Creating layer loss
I1127 11:04:46.868432  6157 net.cpp:106] Creating Layer loss
I1127 11:04:46.868438  6157 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:04:46.868445  6157 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:04:46.868456  6157 net.cpp:411] loss -> loss
I1127 11:04:46.868468  6157 layer_factory.hpp:76] Creating layer loss
I1127 11:04:46.868564  6157 net.cpp:150] Setting up loss
I1127 11:04:46.868574  6157 net.cpp:157] Top shape: (1)
I1127 11:04:46.868582  6157 net.cpp:160]     with loss weight 1
I1127 11:04:46.868593  6157 net.cpp:165] Memory required for data: 8086808
I1127 11:04:46.868600  6157 net.cpp:226] loss needs backward computation.
I1127 11:04:46.868609  6157 net.cpp:228] accuracy does not need backward computation.
I1127 11:04:46.868618  6157 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:04:46.868623  6157 net.cpp:226] ip2 needs backward computation.
I1127 11:04:46.868630  6157 net.cpp:226] relu1 needs backward computation.
I1127 11:04:46.868636  6157 net.cpp:226] ip1 needs backward computation.
I1127 11:04:46.868643  6157 net.cpp:226] pool2 needs backward computation.
I1127 11:04:46.868650  6157 net.cpp:226] conv2 needs backward computation.
I1127 11:04:46.868656  6157 net.cpp:226] pool1 needs backward computation.
I1127 11:04:46.868665  6157 net.cpp:226] conv1 needs backward computation.
I1127 11:04:46.868672  6157 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:04:46.868680  6157 net.cpp:228] mnist does not need backward computation.
I1127 11:04:46.868685  6157 net.cpp:270] This network produces output accuracy
I1127 11:04:46.868692  6157 net.cpp:270] This network produces output loss
I1127 11:04:46.868706  6157 net.cpp:283] Network initialization done.
I1127 11:04:46.868757  6157 solver.cpp:59] Solver scaffolding done.
I1127 11:04:46.869045  6157 caffe.cpp:212] Starting Optimization
I1127 11:04:46.869055  6157 solver.cpp:287] Solving LeNet
I1127 11:04:46.869060  6157 solver.cpp:288] Learning Rate Policy: inv
I1127 11:04:46.869474  6157 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:04:49.112390  6157 solver.cpp:408]     Test net output #0: accuracy = 0.1777
I1127 11:04:49.112478  6157 solver.cpp:408]     Test net output #1: loss = 2.27991 (* 1 = 2.27991 loss)
I1127 11:04:49.124109  6157 solver.cpp:236] Iteration 0, loss = 2.23169
I1127 11:04:49.124238  6157 solver.cpp:252]     Train net output #0: loss = 2.23169 (* 1 = 2.23169 loss)
I1127 11:04:49.124267  6157 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:05:02.606004  6157 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:05:02.663522  6157 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:05:03.693168  6157 solver.cpp:408]     Test net output #0: accuracy = 0.9735
I1127 11:05:03.693245  6157 solver.cpp:408]     Test net output #1: loss = 0.0832604 (* 1 = 0.0832604 loss)
I1127 11:05:03.702355  6157 solver.cpp:236] Iteration 500, loss = 0.0763958
I1127 11:05:03.702427  6157 solver.cpp:252]     Train net output #0: loss = 0.0763959 (* 1 = 0.0763959 loss)
I1127 11:05:03.702441  6157 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:05:17.231997  6157 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:05:17.251994  6157 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:05:17.264315  6157 solver.cpp:320] Iteration 1000, loss = 0.0912531
I1127 11:05:17.264492  6157 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:05:18.817526  6157 solver.cpp:408]     Test net output #0: accuracy = 0.9813
I1127 11:05:18.817565  6157 solver.cpp:408]     Test net output #1: loss = 0.0598929 (* 1 = 0.0598929 loss)
I1127 11:05:18.817572  6157 solver.cpp:325] Optimization Done.
I1127 11:05:18.817577  6157 caffe.cpp:215] Optimization Done.
I1127 11:05:18.882494  6204 caffe.cpp:184] Using GPUs 0
I1127 11:05:19.313977  6204 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:05:19.314112  6204 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:05:19.314527  6204 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:05:19.314550  6204 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:05:19.314687  6204 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:05:19.314757  6204 layer_factory.hpp:76] Creating layer mnist
I1127 11:05:19.315201  6204 net.cpp:106] Creating Layer mnist
I1127 11:05:19.315217  6204 net.cpp:411] mnist -> data
I1127 11:05:19.315248  6204 net.cpp:411] mnist -> label
I1127 11:05:19.315943  6207 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:05:19.348973  6204 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:05:19.356119  6204 net.cpp:150] Setting up mnist
I1127 11:05:19.356143  6204 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:05:19.356153  6204 net.cpp:157] Top shape: 64 (64)
I1127 11:05:19.356160  6204 net.cpp:165] Memory required for data: 200960
I1127 11:05:19.356173  6204 layer_factory.hpp:76] Creating layer conv1
I1127 11:05:19.356194  6204 net.cpp:106] Creating Layer conv1
I1127 11:05:19.356204  6204 net.cpp:454] conv1 <- data
I1127 11:05:19.356230  6204 net.cpp:411] conv1 -> conv1
I1127 11:05:19.356989  6204 net.cpp:150] Setting up conv1
I1127 11:05:19.357005  6204 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:05:19.357013  6204 net.cpp:165] Memory required for data: 3150080
I1127 11:05:19.357031  6204 layer_factory.hpp:76] Creating layer pool1
I1127 11:05:19.357045  6204 net.cpp:106] Creating Layer pool1
I1127 11:05:19.357053  6204 net.cpp:454] pool1 <- conv1
I1127 11:05:19.357071  6204 net.cpp:411] pool1 -> pool1
I1127 11:05:19.357298  6204 net.cpp:150] Setting up pool1
I1127 11:05:19.357311  6204 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:05:19.357319  6204 net.cpp:165] Memory required for data: 3887360
I1127 11:05:19.357326  6204 layer_factory.hpp:76] Creating layer conv2
I1127 11:05:19.357339  6204 net.cpp:106] Creating Layer conv2
I1127 11:05:19.357347  6204 net.cpp:454] conv2 <- pool1
I1127 11:05:19.357359  6204 net.cpp:411] conv2 -> conv2
I1127 11:05:19.357780  6204 net.cpp:150] Setting up conv2
I1127 11:05:19.357795  6204 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:05:19.357802  6204 net.cpp:165] Memory required for data: 4706560
I1127 11:05:19.357816  6204 layer_factory.hpp:76] Creating layer pool2
I1127 11:05:19.357827  6204 net.cpp:106] Creating Layer pool2
I1127 11:05:19.357834  6204 net.cpp:454] pool2 <- conv2
I1127 11:05:19.357844  6204 net.cpp:411] pool2 -> pool2
I1127 11:05:19.357884  6204 net.cpp:150] Setting up pool2
I1127 11:05:19.357895  6204 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:05:19.357903  6204 net.cpp:165] Memory required for data: 4911360
I1127 11:05:19.357909  6204 layer_factory.hpp:76] Creating layer ip1
I1127 11:05:19.357923  6204 net.cpp:106] Creating Layer ip1
I1127 11:05:19.357935  6204 net.cpp:454] ip1 <- pool2
I1127 11:05:19.357949  6204 net.cpp:411] ip1 -> ip1
I1127 11:05:19.361517  6204 net.cpp:150] Setting up ip1
I1127 11:05:19.361533  6204 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:05:19.361541  6204 net.cpp:165] Memory required for data: 5039360
I1127 11:05:19.361554  6204 layer_factory.hpp:76] Creating layer relu1
I1127 11:05:19.361565  6204 net.cpp:106] Creating Layer relu1
I1127 11:05:19.361574  6204 net.cpp:454] relu1 <- ip1
I1127 11:05:19.361587  6204 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:05:19.361600  6204 net.cpp:150] Setting up relu1
I1127 11:05:19.361610  6204 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:05:19.361618  6204 net.cpp:165] Memory required for data: 5167360
I1127 11:05:19.361624  6204 layer_factory.hpp:76] Creating layer ip2
I1127 11:05:19.361634  6204 net.cpp:106] Creating Layer ip2
I1127 11:05:19.361641  6204 net.cpp:454] ip2 <- ip1
I1127 11:05:19.361652  6204 net.cpp:411] ip2 -> ip2
I1127 11:05:19.362247  6204 net.cpp:150] Setting up ip2
I1127 11:05:19.362262  6204 net.cpp:157] Top shape: 64 10 (640)
I1127 11:05:19.362272  6204 net.cpp:165] Memory required for data: 5169920
I1127 11:05:19.362283  6204 layer_factory.hpp:76] Creating layer loss
I1127 11:05:19.362303  6204 net.cpp:106] Creating Layer loss
I1127 11:05:19.362310  6204 net.cpp:454] loss <- ip2
I1127 11:05:19.362318  6204 net.cpp:454] loss <- label
I1127 11:05:19.362329  6204 net.cpp:411] loss -> loss
I1127 11:05:19.362346  6204 layer_factory.hpp:76] Creating layer loss
I1127 11:05:19.362447  6204 net.cpp:150] Setting up loss
I1127 11:05:19.362460  6204 net.cpp:157] Top shape: (1)
I1127 11:05:19.362468  6204 net.cpp:160]     with loss weight 1
I1127 11:05:19.362490  6204 net.cpp:165] Memory required for data: 5169924
I1127 11:05:19.362498  6204 net.cpp:226] loss needs backward computation.
I1127 11:05:19.362506  6204 net.cpp:226] ip2 needs backward computation.
I1127 11:05:19.362514  6204 net.cpp:226] relu1 needs backward computation.
I1127 11:05:19.362519  6204 net.cpp:226] ip1 needs backward computation.
I1127 11:05:19.362526  6204 net.cpp:226] pool2 needs backward computation.
I1127 11:05:19.362534  6204 net.cpp:226] conv2 needs backward computation.
I1127 11:05:19.362540  6204 net.cpp:226] pool1 needs backward computation.
I1127 11:05:19.362547  6204 net.cpp:226] conv1 needs backward computation.
I1127 11:05:19.362555  6204 net.cpp:228] mnist does not need backward computation.
I1127 11:05:19.362561  6204 net.cpp:270] This network produces output loss
I1127 11:05:19.362574  6204 net.cpp:283] Network initialization done.
I1127 11:05:19.362943  6204 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:05:19.362977  6204 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:05:19.363149  6204 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:05:19.363235  6204 layer_factory.hpp:76] Creating layer mnist
I1127 11:05:19.458258  6204 net.cpp:106] Creating Layer mnist
I1127 11:05:19.458307  6204 net.cpp:411] mnist -> data
I1127 11:05:19.458329  6204 net.cpp:411] mnist -> label
I1127 11:05:19.459101  6209 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:05:19.459254  6204 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:05:19.462710  6204 net.cpp:150] Setting up mnist
I1127 11:05:19.462730  6204 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:05:19.462740  6204 net.cpp:157] Top shape: 100 (100)
I1127 11:05:19.462748  6204 net.cpp:165] Memory required for data: 314000
I1127 11:05:19.462756  6204 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:05:19.462767  6204 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:05:19.462775  6204 net.cpp:454] label_mnist_1_split <- label
I1127 11:05:19.462786  6204 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:05:19.462798  6204 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:05:19.462843  6204 net.cpp:150] Setting up label_mnist_1_split
I1127 11:05:19.462854  6204 net.cpp:157] Top shape: 100 (100)
I1127 11:05:19.462862  6204 net.cpp:157] Top shape: 100 (100)
I1127 11:05:19.462868  6204 net.cpp:165] Memory required for data: 314800
I1127 11:05:19.462874  6204 layer_factory.hpp:76] Creating layer conv1
I1127 11:05:19.462891  6204 net.cpp:106] Creating Layer conv1
I1127 11:05:19.462899  6204 net.cpp:454] conv1 <- data
I1127 11:05:19.462908  6204 net.cpp:411] conv1 -> conv1
I1127 11:05:19.463142  6204 net.cpp:150] Setting up conv1
I1127 11:05:19.463155  6204 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:05:19.463161  6204 net.cpp:165] Memory required for data: 4922800
I1127 11:05:19.463176  6204 layer_factory.hpp:76] Creating layer pool1
I1127 11:05:19.463187  6204 net.cpp:106] Creating Layer pool1
I1127 11:05:19.463203  6204 net.cpp:454] pool1 <- conv1
I1127 11:05:19.463225  6204 net.cpp:411] pool1 -> pool1
I1127 11:05:19.463268  6204 net.cpp:150] Setting up pool1
I1127 11:05:19.463277  6204 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:05:19.463284  6204 net.cpp:165] Memory required for data: 6074800
I1127 11:05:19.463290  6204 layer_factory.hpp:76] Creating layer conv2
I1127 11:05:19.463305  6204 net.cpp:106] Creating Layer conv2
I1127 11:05:19.463312  6204 net.cpp:454] conv2 <- pool1
I1127 11:05:19.463322  6204 net.cpp:411] conv2 -> conv2
I1127 11:05:19.464015  6204 net.cpp:150] Setting up conv2
I1127 11:05:19.464027  6204 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:05:19.464035  6204 net.cpp:165] Memory required for data: 7354800
I1127 11:05:19.464047  6204 layer_factory.hpp:76] Creating layer pool2
I1127 11:05:19.464057  6204 net.cpp:106] Creating Layer pool2
I1127 11:05:19.464064  6204 net.cpp:454] pool2 <- conv2
I1127 11:05:19.464073  6204 net.cpp:411] pool2 -> pool2
I1127 11:05:19.464113  6204 net.cpp:150] Setting up pool2
I1127 11:05:19.464123  6204 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:05:19.464130  6204 net.cpp:165] Memory required for data: 7674800
I1127 11:05:19.464136  6204 layer_factory.hpp:76] Creating layer ip1
I1127 11:05:19.464149  6204 net.cpp:106] Creating Layer ip1
I1127 11:05:19.464160  6204 net.cpp:454] ip1 <- pool2
I1127 11:05:19.464171  6204 net.cpp:411] ip1 -> ip1
I1127 11:05:19.467795  6204 net.cpp:150] Setting up ip1
I1127 11:05:19.467818  6204 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:05:19.467828  6204 net.cpp:165] Memory required for data: 7874800
I1127 11:05:19.467844  6204 layer_factory.hpp:76] Creating layer relu1
I1127 11:05:19.467855  6204 net.cpp:106] Creating Layer relu1
I1127 11:05:19.467864  6204 net.cpp:454] relu1 <- ip1
I1127 11:05:19.467875  6204 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:05:19.467888  6204 net.cpp:150] Setting up relu1
I1127 11:05:19.467898  6204 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:05:19.467905  6204 net.cpp:165] Memory required for data: 8074800
I1127 11:05:19.467913  6204 layer_factory.hpp:76] Creating layer ip2
I1127 11:05:19.467931  6204 net.cpp:106] Creating Layer ip2
I1127 11:05:19.467939  6204 net.cpp:454] ip2 <- ip1
I1127 11:05:19.467952  6204 net.cpp:411] ip2 -> ip2
I1127 11:05:19.468108  6204 net.cpp:150] Setting up ip2
I1127 11:05:19.468122  6204 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:05:19.468129  6204 net.cpp:165] Memory required for data: 8078800
I1127 11:05:19.468142  6204 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:05:19.468152  6204 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:05:19.468160  6204 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:05:19.468170  6204 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:05:19.468183  6204 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:05:19.468227  6204 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:05:19.468240  6204 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:05:19.468250  6204 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:05:19.468256  6204 net.cpp:165] Memory required for data: 8086800
I1127 11:05:19.468264  6204 layer_factory.hpp:76] Creating layer accuracy
I1127 11:05:19.468276  6204 net.cpp:106] Creating Layer accuracy
I1127 11:05:19.468283  6204 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:05:19.468294  6204 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:05:19.468305  6204 net.cpp:411] accuracy -> accuracy
I1127 11:05:19.468322  6204 net.cpp:150] Setting up accuracy
I1127 11:05:19.468332  6204 net.cpp:157] Top shape: (1)
I1127 11:05:19.468339  6204 net.cpp:165] Memory required for data: 8086804
I1127 11:05:19.468348  6204 layer_factory.hpp:76] Creating layer loss
I1127 11:05:19.468360  6204 net.cpp:106] Creating Layer loss
I1127 11:05:19.468369  6204 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:05:19.468379  6204 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:05:19.468389  6204 net.cpp:411] loss -> loss
I1127 11:05:19.468402  6204 layer_factory.hpp:76] Creating layer loss
I1127 11:05:19.468518  6204 net.cpp:150] Setting up loss
I1127 11:05:19.468529  6204 net.cpp:157] Top shape: (1)
I1127 11:05:19.468538  6204 net.cpp:160]     with loss weight 1
I1127 11:05:19.468552  6204 net.cpp:165] Memory required for data: 8086808
I1127 11:05:19.468560  6204 net.cpp:226] loss needs backward computation.
I1127 11:05:19.468572  6204 net.cpp:228] accuracy does not need backward computation.
I1127 11:05:19.468581  6204 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:05:19.468590  6204 net.cpp:226] ip2 needs backward computation.
I1127 11:05:19.468597  6204 net.cpp:226] relu1 needs backward computation.
I1127 11:05:19.468605  6204 net.cpp:226] ip1 needs backward computation.
I1127 11:05:19.468612  6204 net.cpp:226] pool2 needs backward computation.
I1127 11:05:19.468621  6204 net.cpp:226] conv2 needs backward computation.
I1127 11:05:19.468628  6204 net.cpp:226] pool1 needs backward computation.
I1127 11:05:19.468636  6204 net.cpp:226] conv1 needs backward computation.
I1127 11:05:19.468644  6204 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:05:19.468652  6204 net.cpp:228] mnist does not need backward computation.
I1127 11:05:19.468659  6204 net.cpp:270] This network produces output accuracy
I1127 11:05:19.468667  6204 net.cpp:270] This network produces output loss
I1127 11:05:19.468685  6204 net.cpp:283] Network initialization done.
I1127 11:05:19.468750  6204 solver.cpp:59] Solver scaffolding done.
I1127 11:05:19.469054  6204 caffe.cpp:212] Starting Optimization
I1127 11:05:19.469065  6204 solver.cpp:287] Solving LeNet
I1127 11:05:19.469072  6204 solver.cpp:288] Learning Rate Policy: inv
I1127 11:05:19.469542  6204 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:05:21.504184  6204 solver.cpp:408]     Test net output #0: accuracy = 0.0799
I1127 11:05:21.504315  6204 solver.cpp:408]     Test net output #1: loss = 2.41362 (* 1 = 2.41362 loss)
I1127 11:05:21.519222  6204 solver.cpp:236] Iteration 0, loss = 2.45172
I1127 11:05:21.519372  6204 solver.cpp:252]     Train net output #0: loss = 2.45172 (* 1 = 2.45172 loss)
I1127 11:05:21.519428  6204 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:05:32.071522  6204 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:05:33.474829  6204 solver.cpp:408]     Test net output #0: accuracy = 0.9719
I1127 11:05:33.474946  6204 solver.cpp:408]     Test net output #1: loss = 0.0879356 (* 1 = 0.0879356 loss)
I1127 11:05:33.488529  6204 solver.cpp:236] Iteration 500, loss = 0.10863
I1127 11:05:33.488661  6204 solver.cpp:252]     Train net output #0: loss = 0.108629 (* 1 = 0.108629 loss)
I1127 11:05:33.488687  6204 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:05:42.903578  6204 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:05:42.914284  6204 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:05:42.923558  6204 solver.cpp:320] Iteration 1000, loss = 0.0956565
I1127 11:05:42.923601  6204 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:05:44.223008  6204 solver.cpp:408]     Test net output #0: accuracy = 0.9806
I1127 11:05:44.223130  6204 solver.cpp:408]     Test net output #1: loss = 0.0576191 (* 1 = 0.0576191 loss)
I1127 11:05:44.223141  6204 solver.cpp:325] Optimization Done.
I1127 11:05:44.223148  6204 caffe.cpp:215] Optimization Done.
I1127 11:05:44.360637  6258 caffe.cpp:184] Using GPUs 0
I1127 11:05:44.709830  6258 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:05:44.710139  6258 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:05:44.710583  6258 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:05:44.710614  6258 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:05:44.710713  6258 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:05:44.710777  6258 layer_factory.hpp:76] Creating layer mnist
I1127 11:05:44.711127  6258 net.cpp:106] Creating Layer mnist
I1127 11:05:44.711149  6258 net.cpp:411] mnist -> data
I1127 11:05:44.711176  6258 net.cpp:411] mnist -> label
I1127 11:05:44.712404  6262 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:05:44.749820  6258 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:05:44.753403  6258 net.cpp:150] Setting up mnist
I1127 11:05:44.753495  6258 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:05:44.753504  6258 net.cpp:157] Top shape: 64 (64)
I1127 11:05:44.753509  6258 net.cpp:165] Memory required for data: 200960
I1127 11:05:44.753526  6258 layer_factory.hpp:76] Creating layer conv1
I1127 11:05:44.753566  6258 net.cpp:106] Creating Layer conv1
I1127 11:05:44.753576  6258 net.cpp:454] conv1 <- data
I1127 11:05:44.753592  6258 net.cpp:411] conv1 -> conv1
I1127 11:05:44.754684  6258 net.cpp:150] Setting up conv1
I1127 11:05:44.754765  6258 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:05:44.754775  6258 net.cpp:165] Memory required for data: 3150080
I1127 11:05:44.754806  6258 layer_factory.hpp:76] Creating layer pool1
I1127 11:05:44.754840  6258 net.cpp:106] Creating Layer pool1
I1127 11:05:44.754853  6258 net.cpp:454] pool1 <- conv1
I1127 11:05:44.754870  6258 net.cpp:411] pool1 -> pool1
I1127 11:05:44.754994  6258 net.cpp:150] Setting up pool1
I1127 11:05:44.755009  6258 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:05:44.755017  6258 net.cpp:165] Memory required for data: 3887360
I1127 11:05:44.755025  6258 layer_factory.hpp:76] Creating layer conv2
I1127 11:05:44.755044  6258 net.cpp:106] Creating Layer conv2
I1127 11:05:44.755053  6258 net.cpp:454] conv2 <- pool1
I1127 11:05:44.755064  6258 net.cpp:411] conv2 -> conv2
I1127 11:05:44.755517  6258 net.cpp:150] Setting up conv2
I1127 11:05:44.755553  6258 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:05:44.755560  6258 net.cpp:165] Memory required for data: 4706560
I1127 11:05:44.755581  6258 layer_factory.hpp:76] Creating layer pool2
I1127 11:05:44.755600  6258 net.cpp:106] Creating Layer pool2
I1127 11:05:44.755609  6258 net.cpp:454] pool2 <- conv2
I1127 11:05:44.755620  6258 net.cpp:411] pool2 -> pool2
I1127 11:05:44.755673  6258 net.cpp:150] Setting up pool2
I1127 11:05:44.755686  6258 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:05:44.755693  6258 net.cpp:165] Memory required for data: 4911360
I1127 11:05:44.755702  6258 layer_factory.hpp:76] Creating layer ip1
I1127 11:05:44.755719  6258 net.cpp:106] Creating Layer ip1
I1127 11:05:44.755728  6258 net.cpp:454] ip1 <- pool2
I1127 11:05:44.755743  6258 net.cpp:411] ip1 -> ip1
I1127 11:05:44.759783  6258 net.cpp:150] Setting up ip1
I1127 11:05:44.759870  6258 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:05:44.759878  6258 net.cpp:165] Memory required for data: 5039360
I1127 11:05:44.759903  6258 layer_factory.hpp:76] Creating layer relu1
I1127 11:05:44.759922  6258 net.cpp:106] Creating Layer relu1
I1127 11:05:44.759932  6258 net.cpp:454] relu1 <- ip1
I1127 11:05:44.759946  6258 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:05:44.759964  6258 net.cpp:150] Setting up relu1
I1127 11:05:44.759971  6258 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:05:44.759975  6258 net.cpp:165] Memory required for data: 5167360
I1127 11:05:44.759980  6258 layer_factory.hpp:76] Creating layer ip2
I1127 11:05:44.759999  6258 net.cpp:106] Creating Layer ip2
I1127 11:05:44.760007  6258 net.cpp:454] ip2 <- ip1
I1127 11:05:44.760020  6258 net.cpp:411] ip2 -> ip2
I1127 11:05:44.761080  6258 net.cpp:150] Setting up ip2
I1127 11:05:44.761122  6258 net.cpp:157] Top shape: 64 10 (640)
I1127 11:05:44.761128  6258 net.cpp:165] Memory required for data: 5169920
I1127 11:05:44.761139  6258 layer_factory.hpp:76] Creating layer loss
I1127 11:05:44.761157  6258 net.cpp:106] Creating Layer loss
I1127 11:05:44.761168  6258 net.cpp:454] loss <- ip2
I1127 11:05:44.761175  6258 net.cpp:454] loss <- label
I1127 11:05:44.761188  6258 net.cpp:411] loss -> loss
I1127 11:05:44.761207  6258 layer_factory.hpp:76] Creating layer loss
I1127 11:05:44.761301  6258 net.cpp:150] Setting up loss
I1127 11:05:44.761312  6258 net.cpp:157] Top shape: (1)
I1127 11:05:44.761317  6258 net.cpp:160]     with loss weight 1
I1127 11:05:44.761339  6258 net.cpp:165] Memory required for data: 5169924
I1127 11:05:44.761348  6258 net.cpp:226] loss needs backward computation.
I1127 11:05:44.761355  6258 net.cpp:226] ip2 needs backward computation.
I1127 11:05:44.761360  6258 net.cpp:226] relu1 needs backward computation.
I1127 11:05:44.761365  6258 net.cpp:226] ip1 needs backward computation.
I1127 11:05:44.761369  6258 net.cpp:226] pool2 needs backward computation.
I1127 11:05:44.761373  6258 net.cpp:226] conv2 needs backward computation.
I1127 11:05:44.761380  6258 net.cpp:226] pool1 needs backward computation.
I1127 11:05:44.761387  6258 net.cpp:226] conv1 needs backward computation.
I1127 11:05:44.761396  6258 net.cpp:228] mnist does not need backward computation.
I1127 11:05:44.761402  6258 net.cpp:270] This network produces output loss
I1127 11:05:44.761411  6258 net.cpp:283] Network initialization done.
I1127 11:05:44.761696  6258 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:05:44.761723  6258 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:05:44.761873  6258 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:05:44.761960  6258 layer_factory.hpp:76] Creating layer mnist
I1127 11:05:44.762131  6258 net.cpp:106] Creating Layer mnist
I1127 11:05:44.762158  6258 net.cpp:411] mnist -> data
I1127 11:05:44.762177  6258 net.cpp:411] mnist -> label
I1127 11:05:44.763422  6264 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:05:44.763761  6258 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:05:44.765990  6258 net.cpp:150] Setting up mnist
I1127 11:05:44.766053  6258 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:05:44.766062  6258 net.cpp:157] Top shape: 100 (100)
I1127 11:05:44.766067  6258 net.cpp:165] Memory required for data: 314000
I1127 11:05:44.766078  6258 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:05:44.766106  6258 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:05:44.766113  6258 net.cpp:454] label_mnist_1_split <- label
I1127 11:05:44.766125  6258 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:05:44.766160  6258 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:05:44.766216  6258 net.cpp:150] Setting up label_mnist_1_split
I1127 11:05:44.766227  6258 net.cpp:157] Top shape: 100 (100)
I1127 11:05:44.766232  6258 net.cpp:157] Top shape: 100 (100)
I1127 11:05:44.766237  6258 net.cpp:165] Memory required for data: 314800
I1127 11:05:44.766242  6258 layer_factory.hpp:76] Creating layer conv1
I1127 11:05:44.766263  6258 net.cpp:106] Creating Layer conv1
I1127 11:05:44.766268  6258 net.cpp:454] conv1 <- data
I1127 11:05:44.766276  6258 net.cpp:411] conv1 -> conv1
I1127 11:05:44.766469  6258 net.cpp:150] Setting up conv1
I1127 11:05:44.766482  6258 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:05:44.766487  6258 net.cpp:165] Memory required for data: 4922800
I1127 11:05:44.766499  6258 layer_factory.hpp:76] Creating layer pool1
I1127 11:05:44.766508  6258 net.cpp:106] Creating Layer pool1
I1127 11:05:44.766513  6258 net.cpp:454] pool1 <- conv1
I1127 11:05:44.766544  6258 net.cpp:411] pool1 -> pool1
I1127 11:05:44.766584  6258 net.cpp:150] Setting up pool1
I1127 11:05:44.766593  6258 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:05:44.766597  6258 net.cpp:165] Memory required for data: 6074800
I1127 11:05:44.766602  6258 layer_factory.hpp:76] Creating layer conv2
I1127 11:05:44.766615  6258 net.cpp:106] Creating Layer conv2
I1127 11:05:44.766621  6258 net.cpp:454] conv2 <- pool1
I1127 11:05:44.766630  6258 net.cpp:411] conv2 -> conv2
I1127 11:05:44.766921  6258 net.cpp:150] Setting up conv2
I1127 11:05:44.766937  6258 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:05:44.766942  6258 net.cpp:165] Memory required for data: 7354800
I1127 11:05:44.766952  6258 layer_factory.hpp:76] Creating layer pool2
I1127 11:05:44.766960  6258 net.cpp:106] Creating Layer pool2
I1127 11:05:44.766965  6258 net.cpp:454] pool2 <- conv2
I1127 11:05:44.766971  6258 net.cpp:411] pool2 -> pool2
I1127 11:05:44.767000  6258 net.cpp:150] Setting up pool2
I1127 11:05:44.767007  6258 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:05:44.767011  6258 net.cpp:165] Memory required for data: 7674800
I1127 11:05:44.767015  6258 layer_factory.hpp:76] Creating layer ip1
I1127 11:05:44.767025  6258 net.cpp:106] Creating Layer ip1
I1127 11:05:44.767030  6258 net.cpp:454] ip1 <- pool2
I1127 11:05:44.767037  6258 net.cpp:411] ip1 -> ip1
I1127 11:05:44.771060  6258 net.cpp:150] Setting up ip1
I1127 11:05:44.771152  6258 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:05:44.771158  6258 net.cpp:165] Memory required for data: 7874800
I1127 11:05:44.771181  6258 layer_factory.hpp:76] Creating layer relu1
I1127 11:05:44.771200  6258 net.cpp:106] Creating Layer relu1
I1127 11:05:44.771209  6258 net.cpp:454] relu1 <- ip1
I1127 11:05:44.771219  6258 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:05:44.771237  6258 net.cpp:150] Setting up relu1
I1127 11:05:44.771244  6258 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:05:44.771247  6258 net.cpp:165] Memory required for data: 8074800
I1127 11:05:44.771252  6258 layer_factory.hpp:76] Creating layer ip2
I1127 11:05:44.771270  6258 net.cpp:106] Creating Layer ip2
I1127 11:05:44.771275  6258 net.cpp:454] ip2 <- ip1
I1127 11:05:44.771292  6258 net.cpp:411] ip2 -> ip2
I1127 11:05:44.771446  6258 net.cpp:150] Setting up ip2
I1127 11:05:44.771457  6258 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:05:44.771461  6258 net.cpp:165] Memory required for data: 8078800
I1127 11:05:44.771469  6258 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:05:44.771478  6258 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:05:44.771483  6258 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:05:44.771489  6258 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:05:44.771497  6258 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:05:44.771536  6258 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:05:44.771545  6258 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:05:44.771550  6258 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:05:44.771555  6258 net.cpp:165] Memory required for data: 8086800
I1127 11:05:44.771560  6258 layer_factory.hpp:76] Creating layer accuracy
I1127 11:05:44.771571  6258 net.cpp:106] Creating Layer accuracy
I1127 11:05:44.771576  6258 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:05:44.771582  6258 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:05:44.771589  6258 net.cpp:411] accuracy -> accuracy
I1127 11:05:44.771602  6258 net.cpp:150] Setting up accuracy
I1127 11:05:44.771608  6258 net.cpp:157] Top shape: (1)
I1127 11:05:44.771612  6258 net.cpp:165] Memory required for data: 8086804
I1127 11:05:44.771617  6258 layer_factory.hpp:76] Creating layer loss
I1127 11:05:44.771630  6258 net.cpp:106] Creating Layer loss
I1127 11:05:44.771634  6258 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:05:44.771639  6258 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:05:44.771646  6258 net.cpp:411] loss -> loss
I1127 11:05:44.771657  6258 layer_factory.hpp:76] Creating layer loss
I1127 11:05:44.771781  6258 net.cpp:150] Setting up loss
I1127 11:05:44.771795  6258 net.cpp:157] Top shape: (1)
I1127 11:05:44.771800  6258 net.cpp:160]     with loss weight 1
I1127 11:05:44.771823  6258 net.cpp:165] Memory required for data: 8086808
I1127 11:05:44.771828  6258 net.cpp:226] loss needs backward computation.
I1127 11:05:44.771844  6258 net.cpp:228] accuracy does not need backward computation.
I1127 11:05:44.771852  6258 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:05:44.771857  6258 net.cpp:226] ip2 needs backward computation.
I1127 11:05:44.771869  6258 net.cpp:226] relu1 needs backward computation.
I1127 11:05:44.771873  6258 net.cpp:226] ip1 needs backward computation.
I1127 11:05:44.771879  6258 net.cpp:226] pool2 needs backward computation.
I1127 11:05:44.771884  6258 net.cpp:226] conv2 needs backward computation.
I1127 11:05:44.771889  6258 net.cpp:226] pool1 needs backward computation.
I1127 11:05:44.771894  6258 net.cpp:226] conv1 needs backward computation.
I1127 11:05:44.771901  6258 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:05:44.771906  6258 net.cpp:228] mnist does not need backward computation.
I1127 11:05:44.771910  6258 net.cpp:270] This network produces output accuracy
I1127 11:05:44.771916  6258 net.cpp:270] This network produces output loss
I1127 11:05:44.771934  6258 net.cpp:283] Network initialization done.
I1127 11:05:44.772013  6258 solver.cpp:59] Solver scaffolding done.
I1127 11:05:44.772238  6258 caffe.cpp:212] Starting Optimization
I1127 11:05:44.772248  6258 solver.cpp:287] Solving LeNet
I1127 11:05:44.772251  6258 solver.cpp:288] Learning Rate Policy: inv
I1127 11:05:44.772866  6258 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:05:46.236320  6258 solver.cpp:408]     Test net output #0: accuracy = 0.1443
I1127 11:05:46.236387  6258 solver.cpp:408]     Test net output #1: loss = 2.37621 (* 1 = 2.37621 loss)
I1127 11:05:46.246541  6258 solver.cpp:236] Iteration 0, loss = 2.35048
I1127 11:05:46.246618  6258 solver.cpp:252]     Train net output #0: loss = 2.35048 (* 1 = 2.35048 loss)
I1127 11:05:46.246646  6258 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:05:55.481685  6258 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:05:56.981348  6258 solver.cpp:408]     Test net output #0: accuracy = 0.9726
I1127 11:05:56.981462  6258 solver.cpp:408]     Test net output #1: loss = 0.0861269 (* 1 = 0.0861269 loss)
I1127 11:05:56.992964  6258 solver.cpp:236] Iteration 500, loss = 0.0979437
I1127 11:05:56.993026  6258 solver.cpp:252]     Train net output #0: loss = 0.0979437 (* 1 = 0.0979437 loss)
I1127 11:05:56.993047  6258 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:06:06.222865  6258 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:06:06.280200  6258 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:06:06.307752  6258 solver.cpp:320] Iteration 1000, loss = 0.0790696
I1127 11:06:06.307787  6258 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:06:07.732575  6258 solver.cpp:408]     Test net output #0: accuracy = 0.9808
I1127 11:06:07.732647  6258 solver.cpp:408]     Test net output #1: loss = 0.0583215 (* 1 = 0.0583215 loss)
I1127 11:06:07.732656  6258 solver.cpp:325] Optimization Done.
I1127 11:06:07.732661  6258 caffe.cpp:215] Optimization Done.
I1127 11:06:07.810348  6325 caffe.cpp:184] Using GPUs 0
I1127 11:06:08.103023  6325 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:06:08.103353  6325 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:06:08.104034  6325 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:06:08.104126  6325 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:06:08.104456  6325 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:06:08.104668  6325 layer_factory.hpp:76] Creating layer mnist
I1127 11:06:08.105468  6325 net.cpp:106] Creating Layer mnist
I1127 11:06:08.105541  6325 net.cpp:411] mnist -> data
I1127 11:06:08.105635  6325 net.cpp:411] mnist -> label
I1127 11:06:08.107656  6329 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:06:08.120079  6325 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:06:08.234988  6325 net.cpp:150] Setting up mnist
I1127 11:06:08.235033  6325 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:06:08.235043  6325 net.cpp:157] Top shape: 64 (64)
I1127 11:06:08.235046  6325 net.cpp:165] Memory required for data: 200960
I1127 11:06:08.235057  6325 layer_factory.hpp:76] Creating layer conv1
I1127 11:06:08.235079  6325 net.cpp:106] Creating Layer conv1
I1127 11:06:08.235086  6325 net.cpp:454] conv1 <- data
I1127 11:06:08.235098  6325 net.cpp:411] conv1 -> conv1
I1127 11:06:08.235800  6325 net.cpp:150] Setting up conv1
I1127 11:06:08.235812  6325 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:06:08.235816  6325 net.cpp:165] Memory required for data: 3150080
I1127 11:06:08.235829  6325 layer_factory.hpp:76] Creating layer pool1
I1127 11:06:08.235838  6325 net.cpp:106] Creating Layer pool1
I1127 11:06:08.235843  6325 net.cpp:454] pool1 <- conv1
I1127 11:06:08.235852  6325 net.cpp:411] pool1 -> pool1
I1127 11:06:08.235899  6325 net.cpp:150] Setting up pool1
I1127 11:06:08.235906  6325 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:06:08.235910  6325 net.cpp:165] Memory required for data: 3887360
I1127 11:06:08.235915  6325 layer_factory.hpp:76] Creating layer conv2
I1127 11:06:08.235925  6325 net.cpp:106] Creating Layer conv2
I1127 11:06:08.235931  6325 net.cpp:454] conv2 <- pool1
I1127 11:06:08.235937  6325 net.cpp:411] conv2 -> conv2
I1127 11:06:08.236245  6325 net.cpp:150] Setting up conv2
I1127 11:06:08.236255  6325 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:06:08.236259  6325 net.cpp:165] Memory required for data: 4706560
I1127 11:06:08.236268  6325 layer_factory.hpp:76] Creating layer pool2
I1127 11:06:08.236275  6325 net.cpp:106] Creating Layer pool2
I1127 11:06:08.236280  6325 net.cpp:454] pool2 <- conv2
I1127 11:06:08.236285  6325 net.cpp:411] pool2 -> pool2
I1127 11:06:08.236330  6325 net.cpp:150] Setting up pool2
I1127 11:06:08.236337  6325 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:06:08.236342  6325 net.cpp:165] Memory required for data: 4911360
I1127 11:06:08.236352  6325 layer_factory.hpp:76] Creating layer ip1
I1127 11:06:08.236362  6325 net.cpp:106] Creating Layer ip1
I1127 11:06:08.236367  6325 net.cpp:454] ip1 <- pool2
I1127 11:06:08.236373  6325 net.cpp:411] ip1 -> ip1
I1127 11:06:08.238595  6325 net.cpp:150] Setting up ip1
I1127 11:06:08.238610  6325 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:06:08.238615  6325 net.cpp:165] Memory required for data: 5039360
I1127 11:06:08.238623  6325 layer_factory.hpp:76] Creating layer relu1
I1127 11:06:08.238631  6325 net.cpp:106] Creating Layer relu1
I1127 11:06:08.238636  6325 net.cpp:454] relu1 <- ip1
I1127 11:06:08.238641  6325 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:06:08.238651  6325 net.cpp:150] Setting up relu1
I1127 11:06:08.238656  6325 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:06:08.238661  6325 net.cpp:165] Memory required for data: 5167360
I1127 11:06:08.238664  6325 layer_factory.hpp:76] Creating layer ip2
I1127 11:06:08.238672  6325 net.cpp:106] Creating Layer ip2
I1127 11:06:08.238677  6325 net.cpp:454] ip2 <- ip1
I1127 11:06:08.238683  6325 net.cpp:411] ip2 -> ip2
I1127 11:06:08.239105  6325 net.cpp:150] Setting up ip2
I1127 11:06:08.239115  6325 net.cpp:157] Top shape: 64 10 (640)
I1127 11:06:08.239120  6325 net.cpp:165] Memory required for data: 5169920
I1127 11:06:08.239126  6325 layer_factory.hpp:76] Creating layer loss
I1127 11:06:08.239135  6325 net.cpp:106] Creating Layer loss
I1127 11:06:08.239140  6325 net.cpp:454] loss <- ip2
I1127 11:06:08.239145  6325 net.cpp:454] loss <- label
I1127 11:06:08.239153  6325 net.cpp:411] loss -> loss
I1127 11:06:08.239166  6325 layer_factory.hpp:76] Creating layer loss
I1127 11:06:08.239233  6325 net.cpp:150] Setting up loss
I1127 11:06:08.239241  6325 net.cpp:157] Top shape: (1)
I1127 11:06:08.239245  6325 net.cpp:160]     with loss weight 1
I1127 11:06:08.239262  6325 net.cpp:165] Memory required for data: 5169924
I1127 11:06:08.239267  6325 net.cpp:226] loss needs backward computation.
I1127 11:06:08.239272  6325 net.cpp:226] ip2 needs backward computation.
I1127 11:06:08.239276  6325 net.cpp:226] relu1 needs backward computation.
I1127 11:06:08.239280  6325 net.cpp:226] ip1 needs backward computation.
I1127 11:06:08.239284  6325 net.cpp:226] pool2 needs backward computation.
I1127 11:06:08.239289  6325 net.cpp:226] conv2 needs backward computation.
I1127 11:06:08.239294  6325 net.cpp:226] pool1 needs backward computation.
I1127 11:06:08.239297  6325 net.cpp:226] conv1 needs backward computation.
I1127 11:06:08.239303  6325 net.cpp:228] mnist does not need backward computation.
I1127 11:06:08.239307  6325 net.cpp:270] This network produces output loss
I1127 11:06:08.239316  6325 net.cpp:283] Network initialization done.
I1127 11:06:08.239552  6325 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:06:08.239574  6325 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:06:08.239686  6325 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:06:08.239751  6325 layer_factory.hpp:76] Creating layer mnist
I1127 11:06:08.239833  6325 net.cpp:106] Creating Layer mnist
I1127 11:06:08.239842  6325 net.cpp:411] mnist -> data
I1127 11:06:08.239852  6325 net.cpp:411] mnist -> label
I1127 11:06:08.240558  6332 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:06:08.240635  6325 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:06:08.243890  6325 net.cpp:150] Setting up mnist
I1127 11:06:08.243904  6325 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:06:08.243911  6325 net.cpp:157] Top shape: 100 (100)
I1127 11:06:08.243914  6325 net.cpp:165] Memory required for data: 314000
I1127 11:06:08.243919  6325 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:06:08.243927  6325 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:06:08.243932  6325 net.cpp:454] label_mnist_1_split <- label
I1127 11:06:08.243937  6325 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:06:08.243945  6325 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:06:08.243980  6325 net.cpp:150] Setting up label_mnist_1_split
I1127 11:06:08.243988  6325 net.cpp:157] Top shape: 100 (100)
I1127 11:06:08.243993  6325 net.cpp:157] Top shape: 100 (100)
I1127 11:06:08.243998  6325 net.cpp:165] Memory required for data: 314800
I1127 11:06:08.244001  6325 layer_factory.hpp:76] Creating layer conv1
I1127 11:06:08.244011  6325 net.cpp:106] Creating Layer conv1
I1127 11:06:08.244016  6325 net.cpp:454] conv1 <- data
I1127 11:06:08.244024  6325 net.cpp:411] conv1 -> conv1
I1127 11:06:08.244171  6325 net.cpp:150] Setting up conv1
I1127 11:06:08.244180  6325 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:06:08.244184  6325 net.cpp:165] Memory required for data: 4922800
I1127 11:06:08.244194  6325 layer_factory.hpp:76] Creating layer pool1
I1127 11:06:08.244200  6325 net.cpp:106] Creating Layer pool1
I1127 11:06:08.244204  6325 net.cpp:454] pool1 <- conv1
I1127 11:06:08.244217  6325 net.cpp:411] pool1 -> pool1
I1127 11:06:08.244246  6325 net.cpp:150] Setting up pool1
I1127 11:06:08.244253  6325 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:06:08.244257  6325 net.cpp:165] Memory required for data: 6074800
I1127 11:06:08.244262  6325 layer_factory.hpp:76] Creating layer conv2
I1127 11:06:08.244271  6325 net.cpp:106] Creating Layer conv2
I1127 11:06:08.244276  6325 net.cpp:454] conv2 <- pool1
I1127 11:06:08.244282  6325 net.cpp:411] conv2 -> conv2
I1127 11:06:08.244534  6325 net.cpp:150] Setting up conv2
I1127 11:06:08.244544  6325 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:06:08.244547  6325 net.cpp:165] Memory required for data: 7354800
I1127 11:06:08.244555  6325 layer_factory.hpp:76] Creating layer pool2
I1127 11:06:08.244561  6325 net.cpp:106] Creating Layer pool2
I1127 11:06:08.244566  6325 net.cpp:454] pool2 <- conv2
I1127 11:06:08.244571  6325 net.cpp:411] pool2 -> pool2
I1127 11:06:08.244598  6325 net.cpp:150] Setting up pool2
I1127 11:06:08.244606  6325 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:06:08.244614  6325 net.cpp:165] Memory required for data: 7674800
I1127 11:06:08.244618  6325 layer_factory.hpp:76] Creating layer ip1
I1127 11:06:08.244626  6325 net.cpp:106] Creating Layer ip1
I1127 11:06:08.244631  6325 net.cpp:454] ip1 <- pool2
I1127 11:06:08.244637  6325 net.cpp:411] ip1 -> ip1
I1127 11:06:08.246790  6325 net.cpp:150] Setting up ip1
I1127 11:06:08.246806  6325 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:06:08.246811  6325 net.cpp:165] Memory required for data: 7874800
I1127 11:06:08.246821  6325 layer_factory.hpp:76] Creating layer relu1
I1127 11:06:08.246829  6325 net.cpp:106] Creating Layer relu1
I1127 11:06:08.246834  6325 net.cpp:454] relu1 <- ip1
I1127 11:06:08.246840  6325 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:06:08.246848  6325 net.cpp:150] Setting up relu1
I1127 11:06:08.246853  6325 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:06:08.246857  6325 net.cpp:165] Memory required for data: 8074800
I1127 11:06:08.246861  6325 layer_factory.hpp:76] Creating layer ip2
I1127 11:06:08.246870  6325 net.cpp:106] Creating Layer ip2
I1127 11:06:08.246875  6325 net.cpp:454] ip2 <- ip1
I1127 11:06:08.246882  6325 net.cpp:411] ip2 -> ip2
I1127 11:06:08.246981  6325 net.cpp:150] Setting up ip2
I1127 11:06:08.246989  6325 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:06:08.246994  6325 net.cpp:165] Memory required for data: 8078800
I1127 11:06:08.246999  6325 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:06:08.247006  6325 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:06:08.247010  6325 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:06:08.247016  6325 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:06:08.247022  6325 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:06:08.247048  6325 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:06:08.247056  6325 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:06:08.247061  6325 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:06:08.247066  6325 net.cpp:165] Memory required for data: 8086800
I1127 11:06:08.247071  6325 layer_factory.hpp:76] Creating layer accuracy
I1127 11:06:08.247077  6325 net.cpp:106] Creating Layer accuracy
I1127 11:06:08.247081  6325 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:06:08.247087  6325 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:06:08.247092  6325 net.cpp:411] accuracy -> accuracy
I1127 11:06:08.247100  6325 net.cpp:150] Setting up accuracy
I1127 11:06:08.247107  6325 net.cpp:157] Top shape: (1)
I1127 11:06:08.247110  6325 net.cpp:165] Memory required for data: 8086804
I1127 11:06:08.247114  6325 layer_factory.hpp:76] Creating layer loss
I1127 11:06:08.247123  6325 net.cpp:106] Creating Layer loss
I1127 11:06:08.247126  6325 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:06:08.247131  6325 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:06:08.247138  6325 net.cpp:411] loss -> loss
I1127 11:06:08.247145  6325 layer_factory.hpp:76] Creating layer loss
I1127 11:06:08.247215  6325 net.cpp:150] Setting up loss
I1127 11:06:08.247221  6325 net.cpp:157] Top shape: (1)
I1127 11:06:08.247225  6325 net.cpp:160]     with loss weight 1
I1127 11:06:08.247234  6325 net.cpp:165] Memory required for data: 8086808
I1127 11:06:08.247239  6325 net.cpp:226] loss needs backward computation.
I1127 11:06:08.247244  6325 net.cpp:228] accuracy does not need backward computation.
I1127 11:06:08.247249  6325 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:06:08.247253  6325 net.cpp:226] ip2 needs backward computation.
I1127 11:06:08.247258  6325 net.cpp:226] relu1 needs backward computation.
I1127 11:06:08.247262  6325 net.cpp:226] ip1 needs backward computation.
I1127 11:06:08.247267  6325 net.cpp:226] pool2 needs backward computation.
I1127 11:06:08.247270  6325 net.cpp:226] conv2 needs backward computation.
I1127 11:06:08.247274  6325 net.cpp:226] pool1 needs backward computation.
I1127 11:06:08.247279  6325 net.cpp:226] conv1 needs backward computation.
I1127 11:06:08.247283  6325 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:06:08.247292  6325 net.cpp:228] mnist does not need backward computation.
I1127 11:06:08.247297  6325 net.cpp:270] This network produces output accuracy
I1127 11:06:08.247301  6325 net.cpp:270] This network produces output loss
I1127 11:06:08.247311  6325 net.cpp:283] Network initialization done.
I1127 11:06:08.247346  6325 solver.cpp:59] Solver scaffolding done.
I1127 11:06:08.247537  6325 caffe.cpp:212] Starting Optimization
I1127 11:06:08.247544  6325 solver.cpp:287] Solving LeNet
I1127 11:06:08.247547  6325 solver.cpp:288] Learning Rate Policy: inv
I1127 11:06:08.247858  6325 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:06:09.678882  6325 solver.cpp:408]     Test net output #0: accuracy = 0.1016
I1127 11:06:09.678946  6325 solver.cpp:408]     Test net output #1: loss = 2.3541 (* 1 = 2.3541 loss)
I1127 11:06:09.691031  6325 solver.cpp:236] Iteration 0, loss = 2.37491
I1127 11:06:09.691126  6325 solver.cpp:252]     Train net output #0: loss = 2.37491 (* 1 = 2.37491 loss)
I1127 11:06:09.691171  6325 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:06:20.971185  6325 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:06:23.241204  6325 solver.cpp:408]     Test net output #0: accuracy = 0.9762
I1127 11:06:23.241251  6325 solver.cpp:408]     Test net output #1: loss = 0.0790569 (* 1 = 0.0790569 loss)
I1127 11:06:23.252665  6325 solver.cpp:236] Iteration 500, loss = 0.0973875
I1127 11:06:23.252821  6325 solver.cpp:252]     Train net output #0: loss = 0.0973875 (* 1 = 0.0973875 loss)
I1127 11:06:23.252858  6325 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:06:36.632681  6325 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:06:36.648869  6325 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:06:36.659358  6325 solver.cpp:320] Iteration 1000, loss = 0.0907546
I1127 11:06:36.659466  6325 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:06:37.774832  6325 solver.cpp:408]     Test net output #0: accuracy = 0.9815
I1127 11:06:37.774875  6325 solver.cpp:408]     Test net output #1: loss = 0.0573371 (* 1 = 0.0573371 loss)
I1127 11:06:37.774883  6325 solver.cpp:325] Optimization Done.
I1127 11:06:37.774896  6325 caffe.cpp:215] Optimization Done.
I1127 11:06:37.841271  6367 caffe.cpp:184] Using GPUs 0
I1127 11:06:38.256477  6367 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:06:38.256587  6367 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:06:38.256841  6367 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:06:38.256856  6367 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:06:38.256940  6367 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:06:38.257000  6367 layer_factory.hpp:76] Creating layer mnist
I1127 11:06:38.257315  6367 net.cpp:106] Creating Layer mnist
I1127 11:06:38.257328  6367 net.cpp:411] mnist -> data
I1127 11:06:38.257351  6367 net.cpp:411] mnist -> label
I1127 11:06:38.258105  6370 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:06:38.293759  6367 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:06:38.300117  6367 net.cpp:150] Setting up mnist
I1127 11:06:38.300135  6367 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:06:38.300143  6367 net.cpp:157] Top shape: 64 (64)
I1127 11:06:38.300146  6367 net.cpp:165] Memory required for data: 200960
I1127 11:06:38.300154  6367 layer_factory.hpp:76] Creating layer conv1
I1127 11:06:38.300168  6367 net.cpp:106] Creating Layer conv1
I1127 11:06:38.300175  6367 net.cpp:454] conv1 <- data
I1127 11:06:38.300185  6367 net.cpp:411] conv1 -> conv1
I1127 11:06:38.300901  6367 net.cpp:150] Setting up conv1
I1127 11:06:38.300916  6367 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:06:38.300921  6367 net.cpp:165] Memory required for data: 3150080
I1127 11:06:38.300935  6367 layer_factory.hpp:76] Creating layer pool1
I1127 11:06:38.300947  6367 net.cpp:106] Creating Layer pool1
I1127 11:06:38.300952  6367 net.cpp:454] pool1 <- conv1
I1127 11:06:38.300958  6367 net.cpp:411] pool1 -> pool1
I1127 11:06:38.301003  6367 net.cpp:150] Setting up pool1
I1127 11:06:38.301012  6367 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:06:38.301017  6367 net.cpp:165] Memory required for data: 3887360
I1127 11:06:38.301020  6367 layer_factory.hpp:76] Creating layer conv2
I1127 11:06:38.301029  6367 net.cpp:106] Creating Layer conv2
I1127 11:06:38.301034  6367 net.cpp:454] conv2 <- pool1
I1127 11:06:38.301040  6367 net.cpp:411] conv2 -> conv2
I1127 11:06:38.301501  6367 net.cpp:150] Setting up conv2
I1127 11:06:38.301515  6367 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:06:38.301520  6367 net.cpp:165] Memory required for data: 4706560
I1127 11:06:38.301530  6367 layer_factory.hpp:76] Creating layer pool2
I1127 11:06:38.301538  6367 net.cpp:106] Creating Layer pool2
I1127 11:06:38.301543  6367 net.cpp:454] pool2 <- conv2
I1127 11:06:38.301551  6367 net.cpp:411] pool2 -> pool2
I1127 11:06:38.301581  6367 net.cpp:150] Setting up pool2
I1127 11:06:38.301589  6367 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:06:38.301592  6367 net.cpp:165] Memory required for data: 4911360
I1127 11:06:38.301597  6367 layer_factory.hpp:76] Creating layer ip1
I1127 11:06:38.301606  6367 net.cpp:106] Creating Layer ip1
I1127 11:06:38.301611  6367 net.cpp:454] ip1 <- pool2
I1127 11:06:38.301620  6367 net.cpp:411] ip1 -> ip1
I1127 11:06:38.303843  6367 net.cpp:150] Setting up ip1
I1127 11:06:38.303856  6367 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:06:38.303860  6367 net.cpp:165] Memory required for data: 5039360
I1127 11:06:38.303870  6367 layer_factory.hpp:76] Creating layer relu1
I1127 11:06:38.303877  6367 net.cpp:106] Creating Layer relu1
I1127 11:06:38.303886  6367 net.cpp:454] relu1 <- ip1
I1127 11:06:38.303894  6367 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:06:38.303902  6367 net.cpp:150] Setting up relu1
I1127 11:06:38.303907  6367 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:06:38.303911  6367 net.cpp:165] Memory required for data: 5167360
I1127 11:06:38.303916  6367 layer_factory.hpp:76] Creating layer ip2
I1127 11:06:38.303925  6367 net.cpp:106] Creating Layer ip2
I1127 11:06:38.303930  6367 net.cpp:454] ip2 <- ip1
I1127 11:06:38.303936  6367 net.cpp:411] ip2 -> ip2
I1127 11:06:38.304342  6367 net.cpp:150] Setting up ip2
I1127 11:06:38.304352  6367 net.cpp:157] Top shape: 64 10 (640)
I1127 11:06:38.304357  6367 net.cpp:165] Memory required for data: 5169920
I1127 11:06:38.304363  6367 layer_factory.hpp:76] Creating layer loss
I1127 11:06:38.304373  6367 net.cpp:106] Creating Layer loss
I1127 11:06:38.304378  6367 net.cpp:454] loss <- ip2
I1127 11:06:38.304383  6367 net.cpp:454] loss <- label
I1127 11:06:38.304389  6367 net.cpp:411] loss -> loss
I1127 11:06:38.304401  6367 layer_factory.hpp:76] Creating layer loss
I1127 11:06:38.304469  6367 net.cpp:150] Setting up loss
I1127 11:06:38.304477  6367 net.cpp:157] Top shape: (1)
I1127 11:06:38.304481  6367 net.cpp:160]     with loss weight 1
I1127 11:06:38.304494  6367 net.cpp:165] Memory required for data: 5169924
I1127 11:06:38.304499  6367 net.cpp:226] loss needs backward computation.
I1127 11:06:38.304504  6367 net.cpp:226] ip2 needs backward computation.
I1127 11:06:38.304508  6367 net.cpp:226] relu1 needs backward computation.
I1127 11:06:38.304512  6367 net.cpp:226] ip1 needs backward computation.
I1127 11:06:38.304517  6367 net.cpp:226] pool2 needs backward computation.
I1127 11:06:38.304522  6367 net.cpp:226] conv2 needs backward computation.
I1127 11:06:38.304525  6367 net.cpp:226] pool1 needs backward computation.
I1127 11:06:38.304530  6367 net.cpp:226] conv1 needs backward computation.
I1127 11:06:38.304534  6367 net.cpp:228] mnist does not need backward computation.
I1127 11:06:38.304538  6367 net.cpp:270] This network produces output loss
I1127 11:06:38.304548  6367 net.cpp:283] Network initialization done.
I1127 11:06:38.304772  6367 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:06:38.304793  6367 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:06:38.304901  6367 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:06:38.304961  6367 layer_factory.hpp:76] Creating layer mnist
I1127 11:06:38.305049  6367 net.cpp:106] Creating Layer mnist
I1127 11:06:38.305059  6367 net.cpp:411] mnist -> data
I1127 11:06:38.305068  6367 net.cpp:411] mnist -> label
I1127 11:06:38.305727  6372 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:06:38.305831  6367 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:06:38.309525  6367 net.cpp:150] Setting up mnist
I1127 11:06:38.309537  6367 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:06:38.309543  6367 net.cpp:157] Top shape: 100 (100)
I1127 11:06:38.309547  6367 net.cpp:165] Memory required for data: 314000
I1127 11:06:38.309552  6367 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:06:38.309559  6367 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:06:38.309564  6367 net.cpp:454] label_mnist_1_split <- label
I1127 11:06:38.309571  6367 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:06:38.309581  6367 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:06:38.309613  6367 net.cpp:150] Setting up label_mnist_1_split
I1127 11:06:38.309622  6367 net.cpp:157] Top shape: 100 (100)
I1127 11:06:38.309626  6367 net.cpp:157] Top shape: 100 (100)
I1127 11:06:38.309630  6367 net.cpp:165] Memory required for data: 314800
I1127 11:06:38.309634  6367 layer_factory.hpp:76] Creating layer conv1
I1127 11:06:38.309644  6367 net.cpp:106] Creating Layer conv1
I1127 11:06:38.309649  6367 net.cpp:454] conv1 <- data
I1127 11:06:38.309658  6367 net.cpp:411] conv1 -> conv1
I1127 11:06:38.309803  6367 net.cpp:150] Setting up conv1
I1127 11:06:38.309811  6367 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:06:38.309815  6367 net.cpp:165] Memory required for data: 4922800
I1127 11:06:38.309825  6367 layer_factory.hpp:76] Creating layer pool1
I1127 11:06:38.309831  6367 net.cpp:106] Creating Layer pool1
I1127 11:06:38.309836  6367 net.cpp:454] pool1 <- conv1
I1127 11:06:38.309850  6367 net.cpp:411] pool1 -> pool1
I1127 11:06:38.309880  6367 net.cpp:150] Setting up pool1
I1127 11:06:38.309886  6367 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:06:38.309890  6367 net.cpp:165] Memory required for data: 6074800
I1127 11:06:38.309895  6367 layer_factory.hpp:76] Creating layer conv2
I1127 11:06:38.309903  6367 net.cpp:106] Creating Layer conv2
I1127 11:06:38.309908  6367 net.cpp:454] conv2 <- pool1
I1127 11:06:38.309916  6367 net.cpp:411] conv2 -> conv2
I1127 11:06:38.310170  6367 net.cpp:150] Setting up conv2
I1127 11:06:38.310180  6367 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:06:38.310184  6367 net.cpp:165] Memory required for data: 7354800
I1127 11:06:38.310192  6367 layer_factory.hpp:76] Creating layer pool2
I1127 11:06:38.310199  6367 net.cpp:106] Creating Layer pool2
I1127 11:06:38.310204  6367 net.cpp:454] pool2 <- conv2
I1127 11:06:38.310209  6367 net.cpp:411] pool2 -> pool2
I1127 11:06:38.310250  6367 net.cpp:150] Setting up pool2
I1127 11:06:38.310259  6367 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:06:38.310263  6367 net.cpp:165] Memory required for data: 7674800
I1127 11:06:38.310268  6367 layer_factory.hpp:76] Creating layer ip1
I1127 11:06:38.310276  6367 net.cpp:106] Creating Layer ip1
I1127 11:06:38.310281  6367 net.cpp:454] ip1 <- pool2
I1127 11:06:38.310287  6367 net.cpp:411] ip1 -> ip1
I1127 11:06:38.313027  6367 net.cpp:150] Setting up ip1
I1127 11:06:38.313045  6367 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:06:38.313050  6367 net.cpp:165] Memory required for data: 7874800
I1127 11:06:38.313063  6367 layer_factory.hpp:76] Creating layer relu1
I1127 11:06:38.313071  6367 net.cpp:106] Creating Layer relu1
I1127 11:06:38.313076  6367 net.cpp:454] relu1 <- ip1
I1127 11:06:38.313082  6367 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:06:38.313091  6367 net.cpp:150] Setting up relu1
I1127 11:06:38.313096  6367 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:06:38.313099  6367 net.cpp:165] Memory required for data: 8074800
I1127 11:06:38.313103  6367 layer_factory.hpp:76] Creating layer ip2
I1127 11:06:38.313115  6367 net.cpp:106] Creating Layer ip2
I1127 11:06:38.313120  6367 net.cpp:454] ip2 <- ip1
I1127 11:06:38.313127  6367 net.cpp:411] ip2 -> ip2
I1127 11:06:38.313221  6367 net.cpp:150] Setting up ip2
I1127 11:06:38.313230  6367 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:06:38.313233  6367 net.cpp:165] Memory required for data: 8078800
I1127 11:06:38.313241  6367 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:06:38.313246  6367 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:06:38.313251  6367 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:06:38.313256  6367 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:06:38.313263  6367 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:06:38.313292  6367 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:06:38.313298  6367 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:06:38.313303  6367 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:06:38.313308  6367 net.cpp:165] Memory required for data: 8086800
I1127 11:06:38.313311  6367 layer_factory.hpp:76] Creating layer accuracy
I1127 11:06:38.313318  6367 net.cpp:106] Creating Layer accuracy
I1127 11:06:38.313323  6367 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:06:38.313328  6367 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:06:38.313335  6367 net.cpp:411] accuracy -> accuracy
I1127 11:06:38.313344  6367 net.cpp:150] Setting up accuracy
I1127 11:06:38.313350  6367 net.cpp:157] Top shape: (1)
I1127 11:06:38.313354  6367 net.cpp:165] Memory required for data: 8086804
I1127 11:06:38.313359  6367 layer_factory.hpp:76] Creating layer loss
I1127 11:06:38.313364  6367 net.cpp:106] Creating Layer loss
I1127 11:06:38.313369  6367 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:06:38.313374  6367 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:06:38.313380  6367 net.cpp:411] loss -> loss
I1127 11:06:38.313390  6367 layer_factory.hpp:76] Creating layer loss
I1127 11:06:38.313457  6367 net.cpp:150] Setting up loss
I1127 11:06:38.313463  6367 net.cpp:157] Top shape: (1)
I1127 11:06:38.313467  6367 net.cpp:160]     with loss weight 1
I1127 11:06:38.313475  6367 net.cpp:165] Memory required for data: 8086808
I1127 11:06:38.313480  6367 net.cpp:226] loss needs backward computation.
I1127 11:06:38.313487  6367 net.cpp:228] accuracy does not need backward computation.
I1127 11:06:38.313491  6367 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:06:38.313495  6367 net.cpp:226] ip2 needs backward computation.
I1127 11:06:38.313499  6367 net.cpp:226] relu1 needs backward computation.
I1127 11:06:38.313503  6367 net.cpp:226] ip1 needs backward computation.
I1127 11:06:38.313508  6367 net.cpp:226] pool2 needs backward computation.
I1127 11:06:38.313513  6367 net.cpp:226] conv2 needs backward computation.
I1127 11:06:38.313516  6367 net.cpp:226] pool1 needs backward computation.
I1127 11:06:38.313520  6367 net.cpp:226] conv1 needs backward computation.
I1127 11:06:38.313525  6367 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:06:38.313531  6367 net.cpp:228] mnist does not need backward computation.
I1127 11:06:38.313535  6367 net.cpp:270] This network produces output accuracy
I1127 11:06:38.313539  6367 net.cpp:270] This network produces output loss
I1127 11:06:38.313549  6367 net.cpp:283] Network initialization done.
I1127 11:06:38.313580  6367 solver.cpp:59] Solver scaffolding done.
I1127 11:06:38.313766  6367 caffe.cpp:212] Starting Optimization
I1127 11:06:38.313772  6367 solver.cpp:287] Solving LeNet
I1127 11:06:38.313776  6367 solver.cpp:288] Learning Rate Policy: inv
I1127 11:06:38.314090  6367 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:06:40.847226  6367 solver.cpp:408]     Test net output #0: accuracy = 0.0964
I1127 11:06:40.847301  6367 solver.cpp:408]     Test net output #1: loss = 2.42481 (* 1 = 2.42481 loss)
I1127 11:06:40.863867  6367 solver.cpp:236] Iteration 0, loss = 2.41866
I1127 11:06:40.864023  6367 solver.cpp:252]     Train net output #0: loss = 2.41866 (* 1 = 2.41866 loss)
I1127 11:06:40.864083  6367 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:06:52.281635  6367 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:06:55.270570  6367 solver.cpp:408]     Test net output #0: accuracy = 0.9738
I1127 11:06:55.270612  6367 solver.cpp:408]     Test net output #1: loss = 0.0845977 (* 1 = 0.0845977 loss)
I1127 11:06:55.300252  6367 solver.cpp:236] Iteration 500, loss = 0.139027
I1127 11:06:55.300268  6367 solver.cpp:252]     Train net output #0: loss = 0.139027 (* 1 = 0.139027 loss)
I1127 11:06:55.300278  6367 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:07:08.757108  6367 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:07:08.770678  6367 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:07:08.780390  6367 solver.cpp:320] Iteration 1000, loss = 0.0775201
I1127 11:07:08.780458  6367 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:07:09.399741  6367 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:07:09.853631  6367 solver.cpp:408]     Test net output #0: accuracy = 0.9814
I1127 11:07:09.853685  6367 solver.cpp:408]     Test net output #1: loss = 0.0568304 (* 1 = 0.0568304 loss)
I1127 11:07:09.853693  6367 solver.cpp:325] Optimization Done.
I1127 11:07:09.853698  6367 caffe.cpp:215] Optimization Done.
I1127 11:07:09.984706  6395 caffe.cpp:184] Using GPUs 0
I1127 11:07:10.414574  6395 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:07:10.414683  6395 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:07:10.414939  6395 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:07:10.414953  6395 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:07:10.415041  6395 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:07:10.415098  6395 layer_factory.hpp:76] Creating layer mnist
I1127 11:07:10.415408  6395 net.cpp:106] Creating Layer mnist
I1127 11:07:10.415419  6395 net.cpp:411] mnist -> data
I1127 11:07:10.415441  6395 net.cpp:411] mnist -> label
I1127 11:07:10.416188  6399 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:07:10.450528  6395 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:07:10.528228  6395 net.cpp:150] Setting up mnist
I1127 11:07:10.528275  6395 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:07:10.528282  6395 net.cpp:157] Top shape: 64 (64)
I1127 11:07:10.528287  6395 net.cpp:165] Memory required for data: 200960
I1127 11:07:10.528298  6395 layer_factory.hpp:76] Creating layer conv1
I1127 11:07:10.528319  6395 net.cpp:106] Creating Layer conv1
I1127 11:07:10.528326  6395 net.cpp:454] conv1 <- data
I1127 11:07:10.528339  6395 net.cpp:411] conv1 -> conv1
I1127 11:07:10.529307  6395 net.cpp:150] Setting up conv1
I1127 11:07:10.529325  6395 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:07:10.529330  6395 net.cpp:165] Memory required for data: 3150080
I1127 11:07:10.529343  6395 layer_factory.hpp:76] Creating layer pool1
I1127 11:07:10.529355  6395 net.cpp:106] Creating Layer pool1
I1127 11:07:10.529359  6395 net.cpp:454] pool1 <- conv1
I1127 11:07:10.529366  6395 net.cpp:411] pool1 -> pool1
I1127 11:07:10.529414  6395 net.cpp:150] Setting up pool1
I1127 11:07:10.529423  6395 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:07:10.529428  6395 net.cpp:165] Memory required for data: 3887360
I1127 11:07:10.529433  6395 layer_factory.hpp:76] Creating layer conv2
I1127 11:07:10.529441  6395 net.cpp:106] Creating Layer conv2
I1127 11:07:10.529445  6395 net.cpp:454] conv2 <- pool1
I1127 11:07:10.529453  6395 net.cpp:411] conv2 -> conv2
I1127 11:07:10.529747  6395 net.cpp:150] Setting up conv2
I1127 11:07:10.529757  6395 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:07:10.529760  6395 net.cpp:165] Memory required for data: 4706560
I1127 11:07:10.529769  6395 layer_factory.hpp:76] Creating layer pool2
I1127 11:07:10.529779  6395 net.cpp:106] Creating Layer pool2
I1127 11:07:10.529784  6395 net.cpp:454] pool2 <- conv2
I1127 11:07:10.529790  6395 net.cpp:411] pool2 -> pool2
I1127 11:07:10.529817  6395 net.cpp:150] Setting up pool2
I1127 11:07:10.529824  6395 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:07:10.529829  6395 net.cpp:165] Memory required for data: 4911360
I1127 11:07:10.529834  6395 layer_factory.hpp:76] Creating layer ip1
I1127 11:07:10.529844  6395 net.cpp:106] Creating Layer ip1
I1127 11:07:10.529850  6395 net.cpp:454] ip1 <- pool2
I1127 11:07:10.529855  6395 net.cpp:411] ip1 -> ip1
I1127 11:07:10.532073  6395 net.cpp:150] Setting up ip1
I1127 11:07:10.532084  6395 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:07:10.532088  6395 net.cpp:165] Memory required for data: 5039360
I1127 11:07:10.532099  6395 layer_factory.hpp:76] Creating layer relu1
I1127 11:07:10.532106  6395 net.cpp:106] Creating Layer relu1
I1127 11:07:10.532111  6395 net.cpp:454] relu1 <- ip1
I1127 11:07:10.532117  6395 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:07:10.532125  6395 net.cpp:150] Setting up relu1
I1127 11:07:10.532131  6395 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:07:10.532135  6395 net.cpp:165] Memory required for data: 5167360
I1127 11:07:10.532140  6395 layer_factory.hpp:76] Creating layer ip2
I1127 11:07:10.532148  6395 net.cpp:106] Creating Layer ip2
I1127 11:07:10.532157  6395 net.cpp:454] ip2 <- ip1
I1127 11:07:10.532163  6395 net.cpp:411] ip2 -> ip2
I1127 11:07:10.532590  6395 net.cpp:150] Setting up ip2
I1127 11:07:10.532600  6395 net.cpp:157] Top shape: 64 10 (640)
I1127 11:07:10.532604  6395 net.cpp:165] Memory required for data: 5169920
I1127 11:07:10.532611  6395 layer_factory.hpp:76] Creating layer loss
I1127 11:07:10.532620  6395 net.cpp:106] Creating Layer loss
I1127 11:07:10.532625  6395 net.cpp:454] loss <- ip2
I1127 11:07:10.532630  6395 net.cpp:454] loss <- label
I1127 11:07:10.532639  6395 net.cpp:411] loss -> loss
I1127 11:07:10.532649  6395 layer_factory.hpp:76] Creating layer loss
I1127 11:07:10.532716  6395 net.cpp:150] Setting up loss
I1127 11:07:10.532724  6395 net.cpp:157] Top shape: (1)
I1127 11:07:10.532728  6395 net.cpp:160]     with loss weight 1
I1127 11:07:10.532745  6395 net.cpp:165] Memory required for data: 5169924
I1127 11:07:10.532749  6395 net.cpp:226] loss needs backward computation.
I1127 11:07:10.532754  6395 net.cpp:226] ip2 needs backward computation.
I1127 11:07:10.532759  6395 net.cpp:226] relu1 needs backward computation.
I1127 11:07:10.532763  6395 net.cpp:226] ip1 needs backward computation.
I1127 11:07:10.532768  6395 net.cpp:226] pool2 needs backward computation.
I1127 11:07:10.532771  6395 net.cpp:226] conv2 needs backward computation.
I1127 11:07:10.532775  6395 net.cpp:226] pool1 needs backward computation.
I1127 11:07:10.532779  6395 net.cpp:226] conv1 needs backward computation.
I1127 11:07:10.532784  6395 net.cpp:228] mnist does not need backward computation.
I1127 11:07:10.532788  6395 net.cpp:270] This network produces output loss
I1127 11:07:10.532798  6395 net.cpp:283] Network initialization done.
I1127 11:07:10.533031  6395 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:07:10.533053  6395 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:07:10.533167  6395 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:07:10.533229  6395 layer_factory.hpp:76] Creating layer mnist
I1127 11:07:10.533318  6395 net.cpp:106] Creating Layer mnist
I1127 11:07:10.533327  6395 net.cpp:411] mnist -> data
I1127 11:07:10.533335  6395 net.cpp:411] mnist -> label
I1127 11:07:10.534040  6401 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:07:10.534114  6395 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:07:10.537570  6395 net.cpp:150] Setting up mnist
I1127 11:07:10.537582  6395 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:07:10.537588  6395 net.cpp:157] Top shape: 100 (100)
I1127 11:07:10.537592  6395 net.cpp:165] Memory required for data: 314000
I1127 11:07:10.537597  6395 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:07:10.537606  6395 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:07:10.537611  6395 net.cpp:454] label_mnist_1_split <- label
I1127 11:07:10.537616  6395 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:07:10.537624  6395 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:07:10.537657  6395 net.cpp:150] Setting up label_mnist_1_split
I1127 11:07:10.537663  6395 net.cpp:157] Top shape: 100 (100)
I1127 11:07:10.537668  6395 net.cpp:157] Top shape: 100 (100)
I1127 11:07:10.537672  6395 net.cpp:165] Memory required for data: 314800
I1127 11:07:10.537677  6395 layer_factory.hpp:76] Creating layer conv1
I1127 11:07:10.537686  6395 net.cpp:106] Creating Layer conv1
I1127 11:07:10.537690  6395 net.cpp:454] conv1 <- data
I1127 11:07:10.537698  6395 net.cpp:411] conv1 -> conv1
I1127 11:07:10.537839  6395 net.cpp:150] Setting up conv1
I1127 11:07:10.537847  6395 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:07:10.537853  6395 net.cpp:165] Memory required for data: 4922800
I1127 11:07:10.537860  6395 layer_factory.hpp:76] Creating layer pool1
I1127 11:07:10.537868  6395 net.cpp:106] Creating Layer pool1
I1127 11:07:10.537873  6395 net.cpp:454] pool1 <- conv1
I1127 11:07:10.537888  6395 net.cpp:411] pool1 -> pool1
I1127 11:07:10.537914  6395 net.cpp:150] Setting up pool1
I1127 11:07:10.537920  6395 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:07:10.537925  6395 net.cpp:165] Memory required for data: 6074800
I1127 11:07:10.537930  6395 layer_factory.hpp:76] Creating layer conv2
I1127 11:07:10.537938  6395 net.cpp:106] Creating Layer conv2
I1127 11:07:10.537943  6395 net.cpp:454] conv2 <- pool1
I1127 11:07:10.537950  6395 net.cpp:411] conv2 -> conv2
I1127 11:07:10.538208  6395 net.cpp:150] Setting up conv2
I1127 11:07:10.538218  6395 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:07:10.538221  6395 net.cpp:165] Memory required for data: 7354800
I1127 11:07:10.538229  6395 layer_factory.hpp:76] Creating layer pool2
I1127 11:07:10.538235  6395 net.cpp:106] Creating Layer pool2
I1127 11:07:10.538240  6395 net.cpp:454] pool2 <- conv2
I1127 11:07:10.538251  6395 net.cpp:411] pool2 -> pool2
I1127 11:07:10.538307  6395 net.cpp:150] Setting up pool2
I1127 11:07:10.538316  6395 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:07:10.538319  6395 net.cpp:165] Memory required for data: 7674800
I1127 11:07:10.538324  6395 layer_factory.hpp:76] Creating layer ip1
I1127 11:07:10.538332  6395 net.cpp:106] Creating Layer ip1
I1127 11:07:10.538336  6395 net.cpp:454] ip1 <- pool2
I1127 11:07:10.538344  6395 net.cpp:411] ip1 -> ip1
I1127 11:07:10.541046  6395 net.cpp:150] Setting up ip1
I1127 11:07:10.541060  6395 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:07:10.541065  6395 net.cpp:165] Memory required for data: 7874800
I1127 11:07:10.541074  6395 layer_factory.hpp:76] Creating layer relu1
I1127 11:07:10.541081  6395 net.cpp:106] Creating Layer relu1
I1127 11:07:10.541086  6395 net.cpp:454] relu1 <- ip1
I1127 11:07:10.541093  6395 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:07:10.541101  6395 net.cpp:150] Setting up relu1
I1127 11:07:10.541107  6395 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:07:10.541111  6395 net.cpp:165] Memory required for data: 8074800
I1127 11:07:10.541115  6395 layer_factory.hpp:76] Creating layer ip2
I1127 11:07:10.541128  6395 net.cpp:106] Creating Layer ip2
I1127 11:07:10.541133  6395 net.cpp:454] ip2 <- ip1
I1127 11:07:10.541141  6395 net.cpp:411] ip2 -> ip2
I1127 11:07:10.541237  6395 net.cpp:150] Setting up ip2
I1127 11:07:10.541245  6395 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:07:10.541250  6395 net.cpp:165] Memory required for data: 8078800
I1127 11:07:10.541257  6395 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:07:10.541263  6395 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:07:10.541267  6395 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:07:10.541273  6395 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:07:10.541283  6395 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:07:10.541311  6395 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:07:10.541316  6395 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:07:10.541322  6395 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:07:10.541326  6395 net.cpp:165] Memory required for data: 8086800
I1127 11:07:10.541331  6395 layer_factory.hpp:76] Creating layer accuracy
I1127 11:07:10.541338  6395 net.cpp:106] Creating Layer accuracy
I1127 11:07:10.541343  6395 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:07:10.541348  6395 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:07:10.541353  6395 net.cpp:411] accuracy -> accuracy
I1127 11:07:10.541363  6395 net.cpp:150] Setting up accuracy
I1127 11:07:10.541368  6395 net.cpp:157] Top shape: (1)
I1127 11:07:10.541373  6395 net.cpp:165] Memory required for data: 8086804
I1127 11:07:10.541376  6395 layer_factory.hpp:76] Creating layer loss
I1127 11:07:10.541384  6395 net.cpp:106] Creating Layer loss
I1127 11:07:10.541388  6395 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:07:10.541393  6395 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:07:10.541399  6395 net.cpp:411] loss -> loss
I1127 11:07:10.541406  6395 layer_factory.hpp:76] Creating layer loss
I1127 11:07:10.541476  6395 net.cpp:150] Setting up loss
I1127 11:07:10.541483  6395 net.cpp:157] Top shape: (1)
I1127 11:07:10.541488  6395 net.cpp:160]     with loss weight 1
I1127 11:07:10.541496  6395 net.cpp:165] Memory required for data: 8086808
I1127 11:07:10.541501  6395 net.cpp:226] loss needs backward computation.
I1127 11:07:10.541507  6395 net.cpp:228] accuracy does not need backward computation.
I1127 11:07:10.541512  6395 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:07:10.541515  6395 net.cpp:226] ip2 needs backward computation.
I1127 11:07:10.541519  6395 net.cpp:226] relu1 needs backward computation.
I1127 11:07:10.541523  6395 net.cpp:226] ip1 needs backward computation.
I1127 11:07:10.541528  6395 net.cpp:226] pool2 needs backward computation.
I1127 11:07:10.541532  6395 net.cpp:226] conv2 needs backward computation.
I1127 11:07:10.541537  6395 net.cpp:226] pool1 needs backward computation.
I1127 11:07:10.541540  6395 net.cpp:226] conv1 needs backward computation.
I1127 11:07:10.541545  6395 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:07:10.541549  6395 net.cpp:228] mnist does not need backward computation.
I1127 11:07:10.541553  6395 net.cpp:270] This network produces output accuracy
I1127 11:07:10.541558  6395 net.cpp:270] This network produces output loss
I1127 11:07:10.541569  6395 net.cpp:283] Network initialization done.
I1127 11:07:10.541601  6395 solver.cpp:59] Solver scaffolding done.
I1127 11:07:10.541785  6395 caffe.cpp:212] Starting Optimization
I1127 11:07:10.541791  6395 solver.cpp:287] Solving LeNet
I1127 11:07:10.541795  6395 solver.cpp:288] Learning Rate Policy: inv
I1127 11:07:10.542115  6395 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:07:13.083806  6395 solver.cpp:408]     Test net output #0: accuracy = 0.1571
I1127 11:07:13.083937  6395 solver.cpp:408]     Test net output #1: loss = 2.33896 (* 1 = 2.33896 loss)
I1127 11:07:13.096019  6395 solver.cpp:236] Iteration 0, loss = 2.28605
I1127 11:07:13.096159  6395 solver.cpp:252]     Train net output #0: loss = 2.28605 (* 1 = 2.28605 loss)
I1127 11:07:13.096218  6395 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:07:24.650588  6395 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:07:27.722959  6395 solver.cpp:408]     Test net output #0: accuracy = 0.9727
I1127 11:07:27.722998  6395 solver.cpp:408]     Test net output #1: loss = 0.0856081 (* 1 = 0.0856081 loss)
I1127 11:07:27.752743  6395 solver.cpp:236] Iteration 500, loss = 0.111074
I1127 11:07:27.752760  6395 solver.cpp:252]     Train net output #0: loss = 0.111074 (* 1 = 0.111074 loss)
I1127 11:07:27.752770  6395 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:07:40.684711  6395 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:07:40.704939  6395 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:07:40.731628  6395 solver.cpp:320] Iteration 1000, loss = 0.0771703
I1127 11:07:40.731665  6395 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:07:42.208433  6395 solver.cpp:408]     Test net output #0: accuracy = 0.9817
I1127 11:07:42.208565  6395 solver.cpp:408]     Test net output #1: loss = 0.0597192 (* 1 = 0.0597192 loss)
I1127 11:07:42.208605  6395 solver.cpp:325] Optimization Done.
I1127 11:07:42.208626  6395 caffe.cpp:215] Optimization Done.
I1127 11:07:42.316347  6423 caffe.cpp:184] Using GPUs 0
I1127 11:07:42.545966  6423 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:07:42.546123  6423 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:07:42.546433  6423 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:07:42.546452  6423 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:07:42.546546  6423 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:07:42.546617  6423 layer_factory.hpp:76] Creating layer mnist
I1127 11:07:42.546969  6423 net.cpp:106] Creating Layer mnist
I1127 11:07:42.546986  6423 net.cpp:411] mnist -> data
I1127 11:07:42.547018  6423 net.cpp:411] mnist -> label
I1127 11:07:42.547862  6426 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:07:42.553925  6423 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:07:42.555197  6423 net.cpp:150] Setting up mnist
I1127 11:07:42.555268  6423 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:07:42.555274  6423 net.cpp:157] Top shape: 64 (64)
I1127 11:07:42.555279  6423 net.cpp:165] Memory required for data: 200960
I1127 11:07:42.555292  6423 layer_factory.hpp:76] Creating layer conv1
I1127 11:07:42.555320  6423 net.cpp:106] Creating Layer conv1
I1127 11:07:42.555330  6423 net.cpp:454] conv1 <- data
I1127 11:07:42.555346  6423 net.cpp:411] conv1 -> conv1
I1127 11:07:42.556165  6423 net.cpp:150] Setting up conv1
I1127 11:07:42.556212  6423 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:07:42.556217  6423 net.cpp:165] Memory required for data: 3150080
I1127 11:07:42.556237  6423 layer_factory.hpp:76] Creating layer pool1
I1127 11:07:42.556253  6423 net.cpp:106] Creating Layer pool1
I1127 11:07:42.556260  6423 net.cpp:454] pool1 <- conv1
I1127 11:07:42.556268  6423 net.cpp:411] pool1 -> pool1
I1127 11:07:42.556437  6423 net.cpp:150] Setting up pool1
I1127 11:07:42.556454  6423 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:07:42.556462  6423 net.cpp:165] Memory required for data: 3887360
I1127 11:07:42.556468  6423 layer_factory.hpp:76] Creating layer conv2
I1127 11:07:42.556484  6423 net.cpp:106] Creating Layer conv2
I1127 11:07:42.556490  6423 net.cpp:454] conv2 <- pool1
I1127 11:07:42.556498  6423 net.cpp:411] conv2 -> conv2
I1127 11:07:42.556777  6423 net.cpp:150] Setting up conv2
I1127 11:07:42.556787  6423 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:07:42.556792  6423 net.cpp:165] Memory required for data: 4706560
I1127 11:07:42.556802  6423 layer_factory.hpp:76] Creating layer pool2
I1127 11:07:42.556810  6423 net.cpp:106] Creating Layer pool2
I1127 11:07:42.556815  6423 net.cpp:454] pool2 <- conv2
I1127 11:07:42.556823  6423 net.cpp:411] pool2 -> pool2
I1127 11:07:42.556851  6423 net.cpp:150] Setting up pool2
I1127 11:07:42.556859  6423 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:07:42.556864  6423 net.cpp:165] Memory required for data: 4911360
I1127 11:07:42.556869  6423 layer_factory.hpp:76] Creating layer ip1
I1127 11:07:42.556879  6423 net.cpp:106] Creating Layer ip1
I1127 11:07:42.556885  6423 net.cpp:454] ip1 <- pool2
I1127 11:07:42.556891  6423 net.cpp:411] ip1 -> ip1
I1127 11:07:42.559310  6423 net.cpp:150] Setting up ip1
I1127 11:07:42.559376  6423 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:07:42.559381  6423 net.cpp:165] Memory required for data: 5039360
I1127 11:07:42.559402  6423 layer_factory.hpp:76] Creating layer relu1
I1127 11:07:42.559422  6423 net.cpp:106] Creating Layer relu1
I1127 11:07:42.559429  6423 net.cpp:454] relu1 <- ip1
I1127 11:07:42.559442  6423 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:07:42.559461  6423 net.cpp:150] Setting up relu1
I1127 11:07:42.559468  6423 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:07:42.559471  6423 net.cpp:165] Memory required for data: 5167360
I1127 11:07:42.559476  6423 layer_factory.hpp:76] Creating layer ip2
I1127 11:07:42.559489  6423 net.cpp:106] Creating Layer ip2
I1127 11:07:42.559494  6423 net.cpp:454] ip2 <- ip1
I1127 11:07:42.559500  6423 net.cpp:411] ip2 -> ip2
I1127 11:07:42.560168  6423 net.cpp:150] Setting up ip2
I1127 11:07:42.560186  6423 net.cpp:157] Top shape: 64 10 (640)
I1127 11:07:42.560190  6423 net.cpp:165] Memory required for data: 5169920
I1127 11:07:42.560199  6423 layer_factory.hpp:76] Creating layer loss
I1127 11:07:42.560211  6423 net.cpp:106] Creating Layer loss
I1127 11:07:42.560217  6423 net.cpp:454] loss <- ip2
I1127 11:07:42.560223  6423 net.cpp:454] loss <- label
I1127 11:07:42.560233  6423 net.cpp:411] loss -> loss
I1127 11:07:42.560261  6423 layer_factory.hpp:76] Creating layer loss
I1127 11:07:42.560348  6423 net.cpp:150] Setting up loss
I1127 11:07:42.560355  6423 net.cpp:157] Top shape: (1)
I1127 11:07:42.560359  6423 net.cpp:160]     with loss weight 1
I1127 11:07:42.560389  6423 net.cpp:165] Memory required for data: 5169924
I1127 11:07:42.560395  6423 net.cpp:226] loss needs backward computation.
I1127 11:07:42.560400  6423 net.cpp:226] ip2 needs backward computation.
I1127 11:07:42.560403  6423 net.cpp:226] relu1 needs backward computation.
I1127 11:07:42.560407  6423 net.cpp:226] ip1 needs backward computation.
I1127 11:07:42.560412  6423 net.cpp:226] pool2 needs backward computation.
I1127 11:07:42.560417  6423 net.cpp:226] conv2 needs backward computation.
I1127 11:07:42.560421  6423 net.cpp:226] pool1 needs backward computation.
I1127 11:07:42.560425  6423 net.cpp:226] conv1 needs backward computation.
I1127 11:07:42.560431  6423 net.cpp:228] mnist does not need backward computation.
I1127 11:07:42.560436  6423 net.cpp:270] This network produces output loss
I1127 11:07:42.560446  6423 net.cpp:283] Network initialization done.
I1127 11:07:42.560761  6423 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:07:42.560796  6423 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:07:42.560936  6423 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:07:42.561007  6423 layer_factory.hpp:76] Creating layer mnist
I1127 11:07:42.561158  6423 net.cpp:106] Creating Layer mnist
I1127 11:07:42.561167  6423 net.cpp:411] mnist -> data
I1127 11:07:42.561177  6423 net.cpp:411] mnist -> label
I1127 11:07:42.562093  6429 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:07:42.562265  6423 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:07:42.568272  6423 net.cpp:150] Setting up mnist
I1127 11:07:42.568318  6423 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:07:42.568349  6423 net.cpp:157] Top shape: 100 (100)
I1127 11:07:42.568358  6423 net.cpp:165] Memory required for data: 314000
I1127 11:07:42.568373  6423 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:07:42.568404  6423 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:07:42.568416  6423 net.cpp:454] label_mnist_1_split <- label
I1127 11:07:42.568436  6423 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:07:42.568456  6423 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:07:42.568522  6423 net.cpp:150] Setting up label_mnist_1_split
I1127 11:07:42.568533  6423 net.cpp:157] Top shape: 100 (100)
I1127 11:07:42.568544  6423 net.cpp:157] Top shape: 100 (100)
I1127 11:07:42.568552  6423 net.cpp:165] Memory required for data: 314800
I1127 11:07:42.568560  6423 layer_factory.hpp:76] Creating layer conv1
I1127 11:07:42.568588  6423 net.cpp:106] Creating Layer conv1
I1127 11:07:42.568598  6423 net.cpp:454] conv1 <- data
I1127 11:07:42.568608  6423 net.cpp:411] conv1 -> conv1
I1127 11:07:42.568984  6423 net.cpp:150] Setting up conv1
I1127 11:07:42.569363  6423 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:07:42.569373  6423 net.cpp:165] Memory required for data: 4922800
I1127 11:07:42.569391  6423 layer_factory.hpp:76] Creating layer pool1
I1127 11:07:42.569407  6423 net.cpp:106] Creating Layer pool1
I1127 11:07:42.569416  6423 net.cpp:454] pool1 <- conv1
I1127 11:07:42.569444  6423 net.cpp:411] pool1 -> pool1
I1127 11:07:42.569499  6423 net.cpp:150] Setting up pool1
I1127 11:07:42.569511  6423 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:07:42.569519  6423 net.cpp:165] Memory required for data: 6074800
I1127 11:07:42.569526  6423 layer_factory.hpp:76] Creating layer conv2
I1127 11:07:42.569545  6423 net.cpp:106] Creating Layer conv2
I1127 11:07:42.569552  6423 net.cpp:454] conv2 <- pool1
I1127 11:07:42.569566  6423 net.cpp:411] conv2 -> conv2
I1127 11:07:42.570004  6423 net.cpp:150] Setting up conv2
I1127 11:07:42.570024  6423 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:07:42.570031  6423 net.cpp:165] Memory required for data: 7354800
I1127 11:07:42.570046  6423 layer_factory.hpp:76] Creating layer pool2
I1127 11:07:42.570058  6423 net.cpp:106] Creating Layer pool2
I1127 11:07:42.570065  6423 net.cpp:454] pool2 <- conv2
I1127 11:07:42.570076  6423 net.cpp:411] pool2 -> pool2
I1127 11:07:42.570119  6423 net.cpp:150] Setting up pool2
I1127 11:07:42.570132  6423 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:07:42.570139  6423 net.cpp:165] Memory required for data: 7674800
I1127 11:07:42.570158  6423 layer_factory.hpp:76] Creating layer ip1
I1127 11:07:42.570173  6423 net.cpp:106] Creating Layer ip1
I1127 11:07:42.570180  6423 net.cpp:454] ip1 <- pool2
I1127 11:07:42.570193  6423 net.cpp:411] ip1 -> ip1
I1127 11:07:42.573448  6423 net.cpp:150] Setting up ip1
I1127 11:07:42.573526  6423 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:07:42.573534  6423 net.cpp:165] Memory required for data: 7874800
I1127 11:07:42.573552  6423 layer_factory.hpp:76] Creating layer relu1
I1127 11:07:42.573565  6423 net.cpp:106] Creating Layer relu1
I1127 11:07:42.573571  6423 net.cpp:454] relu1 <- ip1
I1127 11:07:42.573580  6423 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:07:42.573593  6423 net.cpp:150] Setting up relu1
I1127 11:07:42.573599  6423 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:07:42.573603  6423 net.cpp:165] Memory required for data: 8074800
I1127 11:07:42.573607  6423 layer_factory.hpp:76] Creating layer ip2
I1127 11:07:42.573624  6423 net.cpp:106] Creating Layer ip2
I1127 11:07:42.573629  6423 net.cpp:454] ip2 <- ip1
I1127 11:07:42.573637  6423 net.cpp:411] ip2 -> ip2
I1127 11:07:42.573755  6423 net.cpp:150] Setting up ip2
I1127 11:07:42.573765  6423 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:07:42.573768  6423 net.cpp:165] Memory required for data: 8078800
I1127 11:07:42.573776  6423 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:07:42.573784  6423 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:07:42.573788  6423 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:07:42.573808  6423 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:07:42.573817  6423 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:07:42.573846  6423 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:07:42.573853  6423 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:07:42.573859  6423 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:07:42.573863  6423 net.cpp:165] Memory required for data: 8086800
I1127 11:07:42.573868  6423 layer_factory.hpp:76] Creating layer accuracy
I1127 11:07:42.573876  6423 net.cpp:106] Creating Layer accuracy
I1127 11:07:42.573881  6423 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:07:42.573886  6423 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:07:42.573894  6423 net.cpp:411] accuracy -> accuracy
I1127 11:07:42.573904  6423 net.cpp:150] Setting up accuracy
I1127 11:07:42.573909  6423 net.cpp:157] Top shape: (1)
I1127 11:07:42.573914  6423 net.cpp:165] Memory required for data: 8086804
I1127 11:07:42.573917  6423 layer_factory.hpp:76] Creating layer loss
I1127 11:07:42.573925  6423 net.cpp:106] Creating Layer loss
I1127 11:07:42.573930  6423 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:07:42.573935  6423 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:07:42.573942  6423 net.cpp:411] loss -> loss
I1127 11:07:42.573954  6423 layer_factory.hpp:76] Creating layer loss
I1127 11:07:42.574041  6423 net.cpp:150] Setting up loss
I1127 11:07:42.574049  6423 net.cpp:157] Top shape: (1)
I1127 11:07:42.574054  6423 net.cpp:160]     with loss weight 1
I1127 11:07:42.574067  6423 net.cpp:165] Memory required for data: 8086808
I1127 11:07:42.574072  6423 net.cpp:226] loss needs backward computation.
I1127 11:07:42.574081  6423 net.cpp:228] accuracy does not need backward computation.
I1127 11:07:42.574087  6423 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:07:42.574091  6423 net.cpp:226] ip2 needs backward computation.
I1127 11:07:42.574095  6423 net.cpp:226] relu1 needs backward computation.
I1127 11:07:42.574100  6423 net.cpp:226] ip1 needs backward computation.
I1127 11:07:42.574103  6423 net.cpp:226] pool2 needs backward computation.
I1127 11:07:42.574108  6423 net.cpp:226] conv2 needs backward computation.
I1127 11:07:42.574121  6423 net.cpp:226] pool1 needs backward computation.
I1127 11:07:42.574126  6423 net.cpp:226] conv1 needs backward computation.
I1127 11:07:42.574131  6423 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:07:42.574136  6423 net.cpp:228] mnist does not need backward computation.
I1127 11:07:42.574139  6423 net.cpp:270] This network produces output accuracy
I1127 11:07:42.574154  6423 net.cpp:270] This network produces output loss
I1127 11:07:42.574164  6423 net.cpp:283] Network initialization done.
I1127 11:07:42.574232  6423 solver.cpp:59] Solver scaffolding done.
I1127 11:07:42.574450  6423 caffe.cpp:212] Starting Optimization
I1127 11:07:42.574457  6423 solver.cpp:287] Solving LeNet
I1127 11:07:42.574461  6423 solver.cpp:288] Learning Rate Policy: inv
I1127 11:07:42.574985  6423 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:07:44.876365  6423 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:07:45.690893  6423 solver.cpp:408]     Test net output #0: accuracy = 0.0441
I1127 11:07:45.691010  6423 solver.cpp:408]     Test net output #1: loss = 2.36337 (* 1 = 2.36337 loss)
I1127 11:07:45.705711  6423 solver.cpp:236] Iteration 0, loss = 2.36622
I1127 11:07:45.705780  6423 solver.cpp:252]     Train net output #0: loss = 2.36622 (* 1 = 2.36622 loss)
I1127 11:07:45.705816  6423 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:07:57.255245  6423 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:08:00.390782  6423 solver.cpp:408]     Test net output #0: accuracy = 0.9747
I1127 11:08:00.390919  6423 solver.cpp:408]     Test net output #1: loss = 0.0811754 (* 1 = 0.0811754 loss)
I1127 11:08:00.400234  6423 solver.cpp:236] Iteration 500, loss = 0.12769
I1127 11:08:00.400355  6423 solver.cpp:252]     Train net output #0: loss = 0.12769 (* 1 = 0.12769 loss)
I1127 11:08:00.400394  6423 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:08:11.885583  6423 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:08:11.905544  6423 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:08:11.931978  6423 solver.cpp:320] Iteration 1000, loss = 0.108447
I1127 11:08:11.931999  6423 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:08:15.008919  6423 solver.cpp:408]     Test net output #0: accuracy = 0.9815
I1127 11:08:15.009101  6423 solver.cpp:408]     Test net output #1: loss = 0.0577267 (* 1 = 0.0577267 loss)
I1127 11:08:15.009111  6423 solver.cpp:325] Optimization Done.
I1127 11:08:15.009116  6423 caffe.cpp:215] Optimization Done.
I1127 11:08:15.109531  6466 caffe.cpp:184] Using GPUs 0
I1127 11:08:15.405956  6466 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:08:15.406096  6466 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:08:15.406401  6466 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:08:15.406419  6466 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:08:15.406524  6466 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:08:15.406594  6466 layer_factory.hpp:76] Creating layer mnist
I1127 11:08:15.406992  6466 net.cpp:106] Creating Layer mnist
I1127 11:08:15.407009  6466 net.cpp:411] mnist -> data
I1127 11:08:15.407045  6466 net.cpp:411] mnist -> label
I1127 11:08:15.408231  6469 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:08:15.419697  6466 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:08:15.421888  6466 net.cpp:150] Setting up mnist
I1127 11:08:15.422003  6466 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:08:15.422019  6466 net.cpp:157] Top shape: 64 (64)
I1127 11:08:15.422026  6466 net.cpp:165] Memory required for data: 200960
I1127 11:08:15.422046  6466 layer_factory.hpp:76] Creating layer conv1
I1127 11:08:15.422086  6466 net.cpp:106] Creating Layer conv1
I1127 11:08:15.422102  6466 net.cpp:454] conv1 <- data
I1127 11:08:15.422128  6466 net.cpp:411] conv1 -> conv1
I1127 11:08:15.423713  6466 net.cpp:150] Setting up conv1
I1127 11:08:15.423784  6466 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:08:15.423800  6466 net.cpp:165] Memory required for data: 3150080
I1127 11:08:15.423840  6466 layer_factory.hpp:76] Creating layer pool1
I1127 11:08:15.423878  6466 net.cpp:106] Creating Layer pool1
I1127 11:08:15.423898  6466 net.cpp:454] pool1 <- conv1
I1127 11:08:15.423918  6466 net.cpp:411] pool1 -> pool1
I1127 11:08:15.424060  6466 net.cpp:150] Setting up pool1
I1127 11:08:15.424078  6466 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:08:15.424088  6466 net.cpp:165] Memory required for data: 3887360
I1127 11:08:15.424095  6466 layer_factory.hpp:76] Creating layer conv2
I1127 11:08:15.424123  6466 net.cpp:106] Creating Layer conv2
I1127 11:08:15.424134  6466 net.cpp:454] conv2 <- pool1
I1127 11:08:15.424149  6466 net.cpp:411] conv2 -> conv2
I1127 11:08:15.424653  6466 net.cpp:150] Setting up conv2
I1127 11:08:15.424680  6466 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:08:15.424690  6466 net.cpp:165] Memory required for data: 4706560
I1127 11:08:15.424711  6466 layer_factory.hpp:76] Creating layer pool2
I1127 11:08:15.424732  6466 net.cpp:106] Creating Layer pool2
I1127 11:08:15.424744  6466 net.cpp:454] pool2 <- conv2
I1127 11:08:15.424757  6466 net.cpp:411] pool2 -> pool2
I1127 11:08:15.424805  6466 net.cpp:150] Setting up pool2
I1127 11:08:15.424814  6466 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:08:15.424819  6466 net.cpp:165] Memory required for data: 4911360
I1127 11:08:15.424823  6466 layer_factory.hpp:76] Creating layer ip1
I1127 11:08:15.424835  6466 net.cpp:106] Creating Layer ip1
I1127 11:08:15.424840  6466 net.cpp:454] ip1 <- pool2
I1127 11:08:15.424846  6466 net.cpp:411] ip1 -> ip1
I1127 11:08:15.427434  6466 net.cpp:150] Setting up ip1
I1127 11:08:15.427503  6466 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:08:15.427515  6466 net.cpp:165] Memory required for data: 5039360
I1127 11:08:15.427539  6466 layer_factory.hpp:76] Creating layer relu1
I1127 11:08:15.427563  6466 net.cpp:106] Creating Layer relu1
I1127 11:08:15.427578  6466 net.cpp:454] relu1 <- ip1
I1127 11:08:15.427593  6466 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:08:15.427623  6466 net.cpp:150] Setting up relu1
I1127 11:08:15.427635  6466 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:08:15.427644  6466 net.cpp:165] Memory required for data: 5167360
I1127 11:08:15.427650  6466 layer_factory.hpp:76] Creating layer ip2
I1127 11:08:15.427667  6466 net.cpp:106] Creating Layer ip2
I1127 11:08:15.427676  6466 net.cpp:454] ip2 <- ip1
I1127 11:08:15.427687  6466 net.cpp:411] ip2 -> ip2
I1127 11:08:15.428529  6466 net.cpp:150] Setting up ip2
I1127 11:08:15.428555  6466 net.cpp:157] Top shape: 64 10 (640)
I1127 11:08:15.428560  6466 net.cpp:165] Memory required for data: 5169920
I1127 11:08:15.428570  6466 layer_factory.hpp:76] Creating layer loss
I1127 11:08:15.428585  6466 net.cpp:106] Creating Layer loss
I1127 11:08:15.428591  6466 net.cpp:454] loss <- ip2
I1127 11:08:15.428598  6466 net.cpp:454] loss <- label
I1127 11:08:15.428613  6466 net.cpp:411] loss -> loss
I1127 11:08:15.428640  6466 layer_factory.hpp:76] Creating layer loss
I1127 11:08:15.428777  6466 net.cpp:150] Setting up loss
I1127 11:08:15.428793  6466 net.cpp:157] Top shape: (1)
I1127 11:08:15.428798  6466 net.cpp:160]     with loss weight 1
I1127 11:08:15.428833  6466 net.cpp:165] Memory required for data: 5169924
I1127 11:08:15.428838  6466 net.cpp:226] loss needs backward computation.
I1127 11:08:15.428845  6466 net.cpp:226] ip2 needs backward computation.
I1127 11:08:15.428864  6466 net.cpp:226] relu1 needs backward computation.
I1127 11:08:15.428870  6466 net.cpp:226] ip1 needs backward computation.
I1127 11:08:15.428875  6466 net.cpp:226] pool2 needs backward computation.
I1127 11:08:15.428880  6466 net.cpp:226] conv2 needs backward computation.
I1127 11:08:15.428885  6466 net.cpp:226] pool1 needs backward computation.
I1127 11:08:15.428889  6466 net.cpp:226] conv1 needs backward computation.
I1127 11:08:15.428895  6466 net.cpp:228] mnist does not need backward computation.
I1127 11:08:15.428900  6466 net.cpp:270] This network produces output loss
I1127 11:08:15.428915  6466 net.cpp:283] Network initialization done.
I1127 11:08:15.429304  6466 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:08:15.429347  6466 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:08:15.429489  6466 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:08:15.429572  6466 layer_factory.hpp:76] Creating layer mnist
I1127 11:08:15.429687  6466 net.cpp:106] Creating Layer mnist
I1127 11:08:15.429695  6466 net.cpp:411] mnist -> data
I1127 11:08:15.429707  6466 net.cpp:411] mnist -> label
I1127 11:08:15.430830  6471 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:08:15.431057  6466 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:08:15.432433  6466 net.cpp:150] Setting up mnist
I1127 11:08:15.432487  6466 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:08:15.432499  6466 net.cpp:157] Top shape: 100 (100)
I1127 11:08:15.432507  6466 net.cpp:165] Memory required for data: 314000
I1127 11:08:15.432517  6466 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:08:15.432538  6466 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:08:15.432548  6466 net.cpp:454] label_mnist_1_split <- label
I1127 11:08:15.432559  6466 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:08:15.432579  6466 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:08:15.432660  6466 net.cpp:150] Setting up label_mnist_1_split
I1127 11:08:15.432678  6466 net.cpp:157] Top shape: 100 (100)
I1127 11:08:15.432685  6466 net.cpp:157] Top shape: 100 (100)
I1127 11:08:15.432690  6466 net.cpp:165] Memory required for data: 314800
I1127 11:08:15.432698  6466 layer_factory.hpp:76] Creating layer conv1
I1127 11:08:15.432720  6466 net.cpp:106] Creating Layer conv1
I1127 11:08:15.432730  6466 net.cpp:454] conv1 <- data
I1127 11:08:15.432742  6466 net.cpp:411] conv1 -> conv1
I1127 11:08:15.432984  6466 net.cpp:150] Setting up conv1
I1127 11:08:15.432999  6466 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:08:15.433004  6466 net.cpp:165] Memory required for data: 4922800
I1127 11:08:15.433017  6466 layer_factory.hpp:76] Creating layer pool1
I1127 11:08:15.433028  6466 net.cpp:106] Creating Layer pool1
I1127 11:08:15.433035  6466 net.cpp:454] pool1 <- conv1
I1127 11:08:15.433058  6466 net.cpp:411] pool1 -> pool1
I1127 11:08:15.433106  6466 net.cpp:150] Setting up pool1
I1127 11:08:15.433117  6466 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:08:15.433121  6466 net.cpp:165] Memory required for data: 6074800
I1127 11:08:15.433126  6466 layer_factory.hpp:76] Creating layer conv2
I1127 11:08:15.433138  6466 net.cpp:106] Creating Layer conv2
I1127 11:08:15.433143  6466 net.cpp:454] conv2 <- pool1
I1127 11:08:15.433153  6466 net.cpp:411] conv2 -> conv2
I1127 11:08:15.433454  6466 net.cpp:150] Setting up conv2
I1127 11:08:15.433465  6466 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:08:15.433470  6466 net.cpp:165] Memory required for data: 7354800
I1127 11:08:15.433481  6466 layer_factory.hpp:76] Creating layer pool2
I1127 11:08:15.433491  6466 net.cpp:106] Creating Layer pool2
I1127 11:08:15.433497  6466 net.cpp:454] pool2 <- conv2
I1127 11:08:15.433504  6466 net.cpp:411] pool2 -> pool2
I1127 11:08:15.433548  6466 net.cpp:150] Setting up pool2
I1127 11:08:15.433558  6466 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:08:15.433563  6466 net.cpp:165] Memory required for data: 7674800
I1127 11:08:15.433568  6466 layer_factory.hpp:76] Creating layer ip1
I1127 11:08:15.433581  6466 net.cpp:106] Creating Layer ip1
I1127 11:08:15.433588  6466 net.cpp:454] ip1 <- pool2
I1127 11:08:15.433596  6466 net.cpp:411] ip1 -> ip1
I1127 11:08:15.436182  6466 net.cpp:150] Setting up ip1
I1127 11:08:15.436223  6466 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:08:15.436231  6466 net.cpp:165] Memory required for data: 7874800
I1127 11:08:15.436247  6466 layer_factory.hpp:76] Creating layer relu1
I1127 11:08:15.436262  6466 net.cpp:106] Creating Layer relu1
I1127 11:08:15.436270  6466 net.cpp:454] relu1 <- ip1
I1127 11:08:15.436281  6466 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:08:15.436296  6466 net.cpp:150] Setting up relu1
I1127 11:08:15.436306  6466 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:08:15.436311  6466 net.cpp:165] Memory required for data: 8074800
I1127 11:08:15.436316  6466 layer_factory.hpp:76] Creating layer ip2
I1127 11:08:15.436337  6466 net.cpp:106] Creating Layer ip2
I1127 11:08:15.436344  6466 net.cpp:454] ip2 <- ip1
I1127 11:08:15.436354  6466 net.cpp:411] ip2 -> ip2
I1127 11:08:15.436516  6466 net.cpp:150] Setting up ip2
I1127 11:08:15.436534  6466 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:08:15.436538  6466 net.cpp:165] Memory required for data: 8078800
I1127 11:08:15.436547  6466 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:08:15.436560  6466 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:08:15.436566  6466 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:08:15.436573  6466 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:08:15.436583  6466 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:08:15.436624  6466 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:08:15.436636  6466 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:08:15.436642  6466 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:08:15.436646  6466 net.cpp:165] Memory required for data: 8086800
I1127 11:08:15.436651  6466 layer_factory.hpp:76] Creating layer accuracy
I1127 11:08:15.436672  6466 net.cpp:106] Creating Layer accuracy
I1127 11:08:15.436678  6466 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:08:15.436684  6466 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:08:15.436691  6466 net.cpp:411] accuracy -> accuracy
I1127 11:08:15.436703  6466 net.cpp:150] Setting up accuracy
I1127 11:08:15.436710  6466 net.cpp:157] Top shape: (1)
I1127 11:08:15.436714  6466 net.cpp:165] Memory required for data: 8086804
I1127 11:08:15.436719  6466 layer_factory.hpp:76] Creating layer loss
I1127 11:08:15.436728  6466 net.cpp:106] Creating Layer loss
I1127 11:08:15.436733  6466 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:08:15.436739  6466 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:08:15.436748  6466 net.cpp:411] loss -> loss
I1127 11:08:15.436758  6466 layer_factory.hpp:76] Creating layer loss
I1127 11:08:15.436858  6466 net.cpp:150] Setting up loss
I1127 11:08:15.436868  6466 net.cpp:157] Top shape: (1)
I1127 11:08:15.436872  6466 net.cpp:160]     with loss weight 1
I1127 11:08:15.436887  6466 net.cpp:165] Memory required for data: 8086808
I1127 11:08:15.436892  6466 net.cpp:226] loss needs backward computation.
I1127 11:08:15.436904  6466 net.cpp:228] accuracy does not need backward computation.
I1127 11:08:15.436910  6466 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:08:15.436914  6466 net.cpp:226] ip2 needs backward computation.
I1127 11:08:15.436919  6466 net.cpp:226] relu1 needs backward computation.
I1127 11:08:15.436923  6466 net.cpp:226] ip1 needs backward computation.
I1127 11:08:15.436928  6466 net.cpp:226] pool2 needs backward computation.
I1127 11:08:15.436933  6466 net.cpp:226] conv2 needs backward computation.
I1127 11:08:15.436938  6466 net.cpp:226] pool1 needs backward computation.
I1127 11:08:15.436941  6466 net.cpp:226] conv1 needs backward computation.
I1127 11:08:15.436946  6466 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:08:15.436951  6466 net.cpp:228] mnist does not need backward computation.
I1127 11:08:15.436955  6466 net.cpp:270] This network produces output accuracy
I1127 11:08:15.436960  6466 net.cpp:270] This network produces output loss
I1127 11:08:15.436971  6466 net.cpp:283] Network initialization done.
I1127 11:08:15.437036  6466 solver.cpp:59] Solver scaffolding done.
I1127 11:08:15.437258  6466 caffe.cpp:212] Starting Optimization
I1127 11:08:15.437266  6466 solver.cpp:287] Solving LeNet
I1127 11:08:15.437271  6466 solver.cpp:288] Learning Rate Policy: inv
I1127 11:08:15.437752  6466 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:08:15.980849  6466 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:08:17.190979  6466 solver.cpp:408]     Test net output #0: accuracy = 0.1144
I1127 11:08:17.191129  6466 solver.cpp:408]     Test net output #1: loss = 2.31103 (* 1 = 2.31103 loss)
I1127 11:08:17.223861  6466 solver.cpp:236] Iteration 0, loss = 2.31334
I1127 11:08:17.223978  6466 solver.cpp:252]     Train net output #0: loss = 2.31334 (* 1 = 2.31334 loss)
I1127 11:08:17.224006  6466 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:08:30.352955  6466 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:08:33.097086  6466 solver.cpp:408]     Test net output #0: accuracy = 0.9695
I1127 11:08:33.097245  6466 solver.cpp:408]     Test net output #1: loss = 0.0958249 (* 1 = 0.0958249 loss)
I1127 11:08:33.111274  6466 solver.cpp:236] Iteration 500, loss = 0.103559
I1127 11:08:33.111404  6466 solver.cpp:252]     Train net output #0: loss = 0.103559 (* 1 = 0.103559 loss)
I1127 11:08:33.111433  6466 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:08:44.594979  6466 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:08:44.612047  6466 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:08:44.622617  6466 solver.cpp:320] Iteration 1000, loss = 0.100879
I1127 11:08:44.622704  6466 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:08:46.104202  6466 solver.cpp:408]     Test net output #0: accuracy = 0.978
I1127 11:08:46.104274  6466 solver.cpp:408]     Test net output #1: loss = 0.0683929 (* 1 = 0.0683929 loss)
I1127 11:08:46.104281  6466 solver.cpp:325] Optimization Done.
I1127 11:08:46.104286  6466 caffe.cpp:215] Optimization Done.
I1127 11:08:46.176300  6494 caffe.cpp:184] Using GPUs 0
I1127 11:08:46.599949  6494 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:08:46.600061  6494 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:08:46.600309  6494 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:08:46.600323  6494 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:08:46.600406  6494 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:08:46.600457  6494 layer_factory.hpp:76] Creating layer mnist
I1127 11:08:46.646252  6494 net.cpp:106] Creating Layer mnist
I1127 11:08:46.646296  6494 net.cpp:411] mnist -> data
I1127 11:08:46.646327  6494 net.cpp:411] mnist -> label
I1127 11:08:46.647119  6497 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:08:46.680086  6494 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:08:46.686421  6494 net.cpp:150] Setting up mnist
I1127 11:08:46.686441  6494 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:08:46.686447  6494 net.cpp:157] Top shape: 64 (64)
I1127 11:08:46.686452  6494 net.cpp:165] Memory required for data: 200960
I1127 11:08:46.686461  6494 layer_factory.hpp:76] Creating layer conv1
I1127 11:08:46.686477  6494 net.cpp:106] Creating Layer conv1
I1127 11:08:46.686485  6494 net.cpp:454] conv1 <- data
I1127 11:08:46.686496  6494 net.cpp:411] conv1 -> conv1
I1127 11:08:46.687124  6494 net.cpp:150] Setting up conv1
I1127 11:08:46.687141  6494 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:08:46.687147  6494 net.cpp:165] Memory required for data: 3150080
I1127 11:08:46.687160  6494 layer_factory.hpp:76] Creating layer pool1
I1127 11:08:46.687168  6494 net.cpp:106] Creating Layer pool1
I1127 11:08:46.687172  6494 net.cpp:454] pool1 <- conv1
I1127 11:08:46.687178  6494 net.cpp:411] pool1 -> pool1
I1127 11:08:46.687224  6494 net.cpp:150] Setting up pool1
I1127 11:08:46.687232  6494 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:08:46.687237  6494 net.cpp:165] Memory required for data: 3887360
I1127 11:08:46.687240  6494 layer_factory.hpp:76] Creating layer conv2
I1127 11:08:46.687250  6494 net.cpp:106] Creating Layer conv2
I1127 11:08:46.687254  6494 net.cpp:454] conv2 <- pool1
I1127 11:08:46.687260  6494 net.cpp:411] conv2 -> conv2
I1127 11:08:46.687651  6494 net.cpp:150] Setting up conv2
I1127 11:08:46.687661  6494 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:08:46.687665  6494 net.cpp:165] Memory required for data: 4706560
I1127 11:08:46.687674  6494 layer_factory.hpp:76] Creating layer pool2
I1127 11:08:46.687681  6494 net.cpp:106] Creating Layer pool2
I1127 11:08:46.687685  6494 net.cpp:454] pool2 <- conv2
I1127 11:08:46.687691  6494 net.cpp:411] pool2 -> pool2
I1127 11:08:46.687719  6494 net.cpp:150] Setting up pool2
I1127 11:08:46.687726  6494 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:08:46.687731  6494 net.cpp:165] Memory required for data: 4911360
I1127 11:08:46.687736  6494 layer_factory.hpp:76] Creating layer ip1
I1127 11:08:46.687744  6494 net.cpp:106] Creating Layer ip1
I1127 11:08:46.687749  6494 net.cpp:454] ip1 <- pool2
I1127 11:08:46.687755  6494 net.cpp:411] ip1 -> ip1
I1127 11:08:46.689865  6494 net.cpp:150] Setting up ip1
I1127 11:08:46.689877  6494 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:08:46.689882  6494 net.cpp:165] Memory required for data: 5039360
I1127 11:08:46.689892  6494 layer_factory.hpp:76] Creating layer relu1
I1127 11:08:46.689899  6494 net.cpp:106] Creating Layer relu1
I1127 11:08:46.689904  6494 net.cpp:454] relu1 <- ip1
I1127 11:08:46.689910  6494 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:08:46.689919  6494 net.cpp:150] Setting up relu1
I1127 11:08:46.689924  6494 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:08:46.689929  6494 net.cpp:165] Memory required for data: 5167360
I1127 11:08:46.689932  6494 layer_factory.hpp:76] Creating layer ip2
I1127 11:08:46.689940  6494 net.cpp:106] Creating Layer ip2
I1127 11:08:46.689945  6494 net.cpp:454] ip2 <- ip1
I1127 11:08:46.689951  6494 net.cpp:411] ip2 -> ip2
I1127 11:08:46.690356  6494 net.cpp:150] Setting up ip2
I1127 11:08:46.690367  6494 net.cpp:157] Top shape: 64 10 (640)
I1127 11:08:46.690371  6494 net.cpp:165] Memory required for data: 5169920
I1127 11:08:46.690378  6494 layer_factory.hpp:76] Creating layer loss
I1127 11:08:46.690387  6494 net.cpp:106] Creating Layer loss
I1127 11:08:46.690392  6494 net.cpp:454] loss <- ip2
I1127 11:08:46.690397  6494 net.cpp:454] loss <- label
I1127 11:08:46.690404  6494 net.cpp:411] loss -> loss
I1127 11:08:46.690417  6494 layer_factory.hpp:76] Creating layer loss
I1127 11:08:46.690481  6494 net.cpp:150] Setting up loss
I1127 11:08:46.690488  6494 net.cpp:157] Top shape: (1)
I1127 11:08:46.690492  6494 net.cpp:160]     with loss weight 1
I1127 11:08:46.690508  6494 net.cpp:165] Memory required for data: 5169924
I1127 11:08:46.690512  6494 net.cpp:226] loss needs backward computation.
I1127 11:08:46.690517  6494 net.cpp:226] ip2 needs backward computation.
I1127 11:08:46.690521  6494 net.cpp:226] relu1 needs backward computation.
I1127 11:08:46.690526  6494 net.cpp:226] ip1 needs backward computation.
I1127 11:08:46.690529  6494 net.cpp:226] pool2 needs backward computation.
I1127 11:08:46.690533  6494 net.cpp:226] conv2 needs backward computation.
I1127 11:08:46.690538  6494 net.cpp:226] pool1 needs backward computation.
I1127 11:08:46.690542  6494 net.cpp:226] conv1 needs backward computation.
I1127 11:08:46.690547  6494 net.cpp:228] mnist does not need backward computation.
I1127 11:08:46.690556  6494 net.cpp:270] This network produces output loss
I1127 11:08:46.690564  6494 net.cpp:283] Network initialization done.
I1127 11:08:46.690795  6494 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:08:46.690817  6494 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:08:46.690925  6494 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:08:46.690984  6494 layer_factory.hpp:76] Creating layer mnist
I1127 11:08:46.691068  6494 net.cpp:106] Creating Layer mnist
I1127 11:08:46.691076  6494 net.cpp:411] mnist -> data
I1127 11:08:46.691084  6494 net.cpp:411] mnist -> label
I1127 11:08:46.691761  6499 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:08:46.691834  6494 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:08:46.695332  6494 net.cpp:150] Setting up mnist
I1127 11:08:46.695343  6494 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:08:46.695349  6494 net.cpp:157] Top shape: 100 (100)
I1127 11:08:46.695353  6494 net.cpp:165] Memory required for data: 314000
I1127 11:08:46.695358  6494 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:08:46.695365  6494 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:08:46.695369  6494 net.cpp:454] label_mnist_1_split <- label
I1127 11:08:46.695375  6494 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:08:46.695384  6494 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:08:46.695415  6494 net.cpp:150] Setting up label_mnist_1_split
I1127 11:08:46.695421  6494 net.cpp:157] Top shape: 100 (100)
I1127 11:08:46.695428  6494 net.cpp:157] Top shape: 100 (100)
I1127 11:08:46.695432  6494 net.cpp:165] Memory required for data: 314800
I1127 11:08:46.695436  6494 layer_factory.hpp:76] Creating layer conv1
I1127 11:08:46.695444  6494 net.cpp:106] Creating Layer conv1
I1127 11:08:46.695448  6494 net.cpp:454] conv1 <- data
I1127 11:08:46.695461  6494 net.cpp:411] conv1 -> conv1
I1127 11:08:46.695605  6494 net.cpp:150] Setting up conv1
I1127 11:08:46.695612  6494 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:08:46.695616  6494 net.cpp:165] Memory required for data: 4922800
I1127 11:08:46.695626  6494 layer_factory.hpp:76] Creating layer pool1
I1127 11:08:46.695633  6494 net.cpp:106] Creating Layer pool1
I1127 11:08:46.695638  6494 net.cpp:454] pool1 <- conv1
I1127 11:08:46.695650  6494 net.cpp:411] pool1 -> pool1
I1127 11:08:46.695677  6494 net.cpp:150] Setting up pool1
I1127 11:08:46.695685  6494 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:08:46.695689  6494 net.cpp:165] Memory required for data: 6074800
I1127 11:08:46.695693  6494 layer_factory.hpp:76] Creating layer conv2
I1127 11:08:46.695703  6494 net.cpp:106] Creating Layer conv2
I1127 11:08:46.695706  6494 net.cpp:454] conv2 <- pool1
I1127 11:08:46.695713  6494 net.cpp:411] conv2 -> conv2
I1127 11:08:46.695956  6494 net.cpp:150] Setting up conv2
I1127 11:08:46.695966  6494 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:08:46.695969  6494 net.cpp:165] Memory required for data: 7354800
I1127 11:08:46.695978  6494 layer_factory.hpp:76] Creating layer pool2
I1127 11:08:46.695986  6494 net.cpp:106] Creating Layer pool2
I1127 11:08:46.695991  6494 net.cpp:454] pool2 <- conv2
I1127 11:08:46.695996  6494 net.cpp:411] pool2 -> pool2
I1127 11:08:46.696033  6494 net.cpp:150] Setting up pool2
I1127 11:08:46.696040  6494 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:08:46.696045  6494 net.cpp:165] Memory required for data: 7674800
I1127 11:08:46.696049  6494 layer_factory.hpp:76] Creating layer ip1
I1127 11:08:46.696056  6494 net.cpp:106] Creating Layer ip1
I1127 11:08:46.696060  6494 net.cpp:454] ip1 <- pool2
I1127 11:08:46.696069  6494 net.cpp:411] ip1 -> ip1
I1127 11:08:46.698642  6494 net.cpp:150] Setting up ip1
I1127 11:08:46.698654  6494 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:08:46.698658  6494 net.cpp:165] Memory required for data: 7874800
I1127 11:08:46.698668  6494 layer_factory.hpp:76] Creating layer relu1
I1127 11:08:46.698675  6494 net.cpp:106] Creating Layer relu1
I1127 11:08:46.698680  6494 net.cpp:454] relu1 <- ip1
I1127 11:08:46.698686  6494 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:08:46.698693  6494 net.cpp:150] Setting up relu1
I1127 11:08:46.698699  6494 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:08:46.698704  6494 net.cpp:165] Memory required for data: 8074800
I1127 11:08:46.698707  6494 layer_factory.hpp:76] Creating layer ip2
I1127 11:08:46.698715  6494 net.cpp:106] Creating Layer ip2
I1127 11:08:46.698720  6494 net.cpp:454] ip2 <- ip1
I1127 11:08:46.698726  6494 net.cpp:411] ip2 -> ip2
I1127 11:08:46.698818  6494 net.cpp:150] Setting up ip2
I1127 11:08:46.698827  6494 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:08:46.698830  6494 net.cpp:165] Memory required for data: 8078800
I1127 11:08:46.698837  6494 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:08:46.698843  6494 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:08:46.698848  6494 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:08:46.698854  6494 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:08:46.698861  6494 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:08:46.698886  6494 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:08:46.698894  6494 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:08:46.698899  6494 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:08:46.698902  6494 net.cpp:165] Memory required for data: 8086800
I1127 11:08:46.698906  6494 layer_factory.hpp:76] Creating layer accuracy
I1127 11:08:46.698915  6494 net.cpp:106] Creating Layer accuracy
I1127 11:08:46.698920  6494 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:08:46.698925  6494 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:08:46.698931  6494 net.cpp:411] accuracy -> accuracy
I1127 11:08:46.698940  6494 net.cpp:150] Setting up accuracy
I1127 11:08:46.698953  6494 net.cpp:157] Top shape: (1)
I1127 11:08:46.698958  6494 net.cpp:165] Memory required for data: 8086804
I1127 11:08:46.698966  6494 layer_factory.hpp:76] Creating layer loss
I1127 11:08:46.698971  6494 net.cpp:106] Creating Layer loss
I1127 11:08:46.698976  6494 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:08:46.698982  6494 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:08:46.698987  6494 net.cpp:411] loss -> loss
I1127 11:08:46.698994  6494 layer_factory.hpp:76] Creating layer loss
I1127 11:08:46.699059  6494 net.cpp:150] Setting up loss
I1127 11:08:46.699067  6494 net.cpp:157] Top shape: (1)
I1127 11:08:46.699071  6494 net.cpp:160]     with loss weight 1
I1127 11:08:46.699079  6494 net.cpp:165] Memory required for data: 8086808
I1127 11:08:46.699084  6494 net.cpp:226] loss needs backward computation.
I1127 11:08:46.699090  6494 net.cpp:228] accuracy does not need backward computation.
I1127 11:08:46.699095  6494 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:08:46.699098  6494 net.cpp:226] ip2 needs backward computation.
I1127 11:08:46.699103  6494 net.cpp:226] relu1 needs backward computation.
I1127 11:08:46.699107  6494 net.cpp:226] ip1 needs backward computation.
I1127 11:08:46.699111  6494 net.cpp:226] pool2 needs backward computation.
I1127 11:08:46.699115  6494 net.cpp:226] conv2 needs backward computation.
I1127 11:08:46.699120  6494 net.cpp:226] pool1 needs backward computation.
I1127 11:08:46.699125  6494 net.cpp:226] conv1 needs backward computation.
I1127 11:08:46.699128  6494 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:08:46.699133  6494 net.cpp:228] mnist does not need backward computation.
I1127 11:08:46.699137  6494 net.cpp:270] This network produces output accuracy
I1127 11:08:46.699141  6494 net.cpp:270] This network produces output loss
I1127 11:08:46.699152  6494 net.cpp:283] Network initialization done.
I1127 11:08:46.699185  6494 solver.cpp:59] Solver scaffolding done.
I1127 11:08:46.699404  6494 caffe.cpp:212] Starting Optimization
I1127 11:08:46.699410  6494 solver.cpp:287] Solving LeNet
I1127 11:08:46.699414  6494 solver.cpp:288] Learning Rate Policy: inv
I1127 11:08:46.699734  6494 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:08:48.866643  6494 solver.cpp:408]     Test net output #0: accuracy = 0.0776
I1127 11:08:48.866689  6494 solver.cpp:408]     Test net output #1: loss = 2.35979 (* 1 = 2.35979 loss)
I1127 11:08:48.877259  6494 solver.cpp:236] Iteration 0, loss = 2.35216
I1127 11:08:48.877307  6494 solver.cpp:252]     Train net output #0: loss = 2.35216 (* 1 = 2.35216 loss)
I1127 11:08:48.877323  6494 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:09:02.354621  6494 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:09:03.430346  6494 solver.cpp:408]     Test net output #0: accuracy = 0.9725
I1127 11:09:03.430423  6494 solver.cpp:408]     Test net output #1: loss = 0.0815014 (* 1 = 0.0815014 loss)
I1127 11:09:03.439496  6494 solver.cpp:236] Iteration 500, loss = 0.138816
I1127 11:09:03.439570  6494 solver.cpp:252]     Train net output #0: loss = 0.138816 (* 1 = 0.138816 loss)
I1127 11:09:03.439584  6494 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:09:16.886803  6494 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:09:17.005049  6494 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:09:17.031656  6494 solver.cpp:320] Iteration 1000, loss = 0.102436
I1127 11:09:17.031678  6494 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:09:20.082470  6494 solver.cpp:408]     Test net output #0: accuracy = 0.9811
I1127 11:09:20.082669  6494 solver.cpp:408]     Test net output #1: loss = 0.0584742 (* 1 = 0.0584742 loss)
I1127 11:09:20.082703  6494 solver.cpp:325] Optimization Done.
I1127 11:09:20.082723  6494 caffe.cpp:215] Optimization Done.
I1127 11:09:20.257139  6525 caffe.cpp:184] Using GPUs 0
I1127 11:09:20.634677  6525 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:09:20.634915  6525 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:09:20.635423  6525 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:09:20.635459  6525 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:09:20.635651  6525 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:09:20.635782  6525 layer_factory.hpp:76] Creating layer mnist
I1127 11:09:20.674424  6525 net.cpp:106] Creating Layer mnist
I1127 11:09:20.674554  6525 net.cpp:411] mnist -> data
I1127 11:09:20.674640  6525 net.cpp:411] mnist -> label
I1127 11:09:20.680153  6528 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:09:20.699297  6525 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:09:20.701144  6525 net.cpp:150] Setting up mnist
I1127 11:09:20.701221  6525 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:09:20.701234  6525 net.cpp:157] Top shape: 64 (64)
I1127 11:09:20.701242  6525 net.cpp:165] Memory required for data: 200960
I1127 11:09:20.701254  6525 layer_factory.hpp:76] Creating layer conv1
I1127 11:09:20.701277  6525 net.cpp:106] Creating Layer conv1
I1127 11:09:20.701287  6525 net.cpp:454] conv1 <- data
I1127 11:09:20.701303  6525 net.cpp:411] conv1 -> conv1
I1127 11:09:20.702685  6525 net.cpp:150] Setting up conv1
I1127 11:09:20.702764  6525 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:09:20.702775  6525 net.cpp:165] Memory required for data: 3150080
I1127 11:09:20.702805  6525 layer_factory.hpp:76] Creating layer pool1
I1127 11:09:20.702834  6525 net.cpp:106] Creating Layer pool1
I1127 11:09:20.702846  6525 net.cpp:454] pool1 <- conv1
I1127 11:09:20.702860  6525 net.cpp:411] pool1 -> pool1
I1127 11:09:20.702951  6525 net.cpp:150] Setting up pool1
I1127 11:09:20.702965  6525 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:09:20.702980  6525 net.cpp:165] Memory required for data: 3887360
I1127 11:09:20.702988  6525 layer_factory.hpp:76] Creating layer conv2
I1127 11:09:20.703008  6525 net.cpp:106] Creating Layer conv2
I1127 11:09:20.703018  6525 net.cpp:454] conv2 <- pool1
I1127 11:09:20.703032  6525 net.cpp:411] conv2 -> conv2
I1127 11:09:20.703691  6525 net.cpp:150] Setting up conv2
I1127 11:09:20.703770  6525 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:09:20.703781  6525 net.cpp:165] Memory required for data: 4706560
I1127 11:09:20.703816  6525 layer_factory.hpp:76] Creating layer pool2
I1127 11:09:20.703855  6525 net.cpp:106] Creating Layer pool2
I1127 11:09:20.703871  6525 net.cpp:454] pool2 <- conv2
I1127 11:09:20.703896  6525 net.cpp:411] pool2 -> pool2
I1127 11:09:20.703985  6525 net.cpp:150] Setting up pool2
I1127 11:09:20.704001  6525 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:09:20.704010  6525 net.cpp:165] Memory required for data: 4911360
I1127 11:09:20.704018  6525 layer_factory.hpp:76] Creating layer ip1
I1127 11:09:20.704040  6525 net.cpp:106] Creating Layer ip1
I1127 11:09:20.704048  6525 net.cpp:454] ip1 <- pool2
I1127 11:09:20.704066  6525 net.cpp:411] ip1 -> ip1
I1127 11:09:20.710520  6525 net.cpp:150] Setting up ip1
I1127 11:09:20.710640  6525 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:09:20.710652  6525 net.cpp:165] Memory required for data: 5039360
I1127 11:09:20.710688  6525 layer_factory.hpp:76] Creating layer relu1
I1127 11:09:20.710721  6525 net.cpp:106] Creating Layer relu1
I1127 11:09:20.710732  6525 net.cpp:454] relu1 <- ip1
I1127 11:09:20.710752  6525 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:09:20.710790  6525 net.cpp:150] Setting up relu1
I1127 11:09:20.710800  6525 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:09:20.710808  6525 net.cpp:165] Memory required for data: 5167360
I1127 11:09:20.710814  6525 layer_factory.hpp:76] Creating layer ip2
I1127 11:09:20.710831  6525 net.cpp:106] Creating Layer ip2
I1127 11:09:20.710839  6525 net.cpp:454] ip2 <- ip1
I1127 11:09:20.710852  6525 net.cpp:411] ip2 -> ip2
I1127 11:09:20.712357  6525 net.cpp:150] Setting up ip2
I1127 11:09:20.712401  6525 net.cpp:157] Top shape: 64 10 (640)
I1127 11:09:20.712409  6525 net.cpp:165] Memory required for data: 5169920
I1127 11:09:20.712422  6525 layer_factory.hpp:76] Creating layer loss
I1127 11:09:20.712438  6525 net.cpp:106] Creating Layer loss
I1127 11:09:20.712446  6525 net.cpp:454] loss <- ip2
I1127 11:09:20.712455  6525 net.cpp:454] loss <- label
I1127 11:09:20.712468  6525 net.cpp:411] loss -> loss
I1127 11:09:20.712488  6525 layer_factory.hpp:76] Creating layer loss
I1127 11:09:20.712587  6525 net.cpp:150] Setting up loss
I1127 11:09:20.712599  6525 net.cpp:157] Top shape: (1)
I1127 11:09:20.712604  6525 net.cpp:160]     with loss weight 1
I1127 11:09:20.712630  6525 net.cpp:165] Memory required for data: 5169924
I1127 11:09:20.712636  6525 net.cpp:226] loss needs backward computation.
I1127 11:09:20.712643  6525 net.cpp:226] ip2 needs backward computation.
I1127 11:09:20.712649  6525 net.cpp:226] relu1 needs backward computation.
I1127 11:09:20.712656  6525 net.cpp:226] ip1 needs backward computation.
I1127 11:09:20.712662  6525 net.cpp:226] pool2 needs backward computation.
I1127 11:09:20.712669  6525 net.cpp:226] conv2 needs backward computation.
I1127 11:09:20.712676  6525 net.cpp:226] pool1 needs backward computation.
I1127 11:09:20.712682  6525 net.cpp:226] conv1 needs backward computation.
I1127 11:09:20.712688  6525 net.cpp:228] mnist does not need backward computation.
I1127 11:09:20.712694  6525 net.cpp:270] This network produces output loss
I1127 11:09:20.712709  6525 net.cpp:283] Network initialization done.
I1127 11:09:20.713150  6525 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:09:20.713210  6525 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:09:20.713439  6525 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:09:20.713553  6525 layer_factory.hpp:76] Creating layer mnist
I1127 11:09:20.713745  6525 net.cpp:106] Creating Layer mnist
I1127 11:09:20.713774  6525 net.cpp:411] mnist -> data
I1127 11:09:20.713810  6525 net.cpp:411] mnist -> label
I1127 11:09:20.715725  6530 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:09:20.718487  6525 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:09:20.721312  6525 net.cpp:150] Setting up mnist
I1127 11:09:20.721436  6525 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:09:20.721458  6525 net.cpp:157] Top shape: 100 (100)
I1127 11:09:20.721469  6525 net.cpp:165] Memory required for data: 314000
I1127 11:09:20.721490  6525 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:09:20.721529  6525 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:09:20.721545  6525 net.cpp:454] label_mnist_1_split <- label
I1127 11:09:20.721565  6525 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:09:20.721591  6525 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:09:20.721715  6525 net.cpp:150] Setting up label_mnist_1_split
I1127 11:09:20.721732  6525 net.cpp:157] Top shape: 100 (100)
I1127 11:09:20.721741  6525 net.cpp:157] Top shape: 100 (100)
I1127 11:09:20.721748  6525 net.cpp:165] Memory required for data: 314800
I1127 11:09:20.721756  6525 layer_factory.hpp:76] Creating layer conv1
I1127 11:09:20.721786  6525 net.cpp:106] Creating Layer conv1
I1127 11:09:20.721796  6525 net.cpp:454] conv1 <- data
I1127 11:09:20.721810  6525 net.cpp:411] conv1 -> conv1
I1127 11:09:20.723901  6525 net.cpp:150] Setting up conv1
I1127 11:09:20.724005  6525 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:09:20.724020  6525 net.cpp:165] Memory required for data: 4922800
I1127 11:09:20.724086  6525 layer_factory.hpp:76] Creating layer pool1
I1127 11:09:20.724117  6525 net.cpp:106] Creating Layer pool1
I1127 11:09:20.724130  6525 net.cpp:454] pool1 <- conv1
I1127 11:09:20.724174  6525 net.cpp:411] pool1 -> pool1
I1127 11:09:20.725255  6525 net.cpp:150] Setting up pool1
I1127 11:09:20.725337  6525 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:09:20.725345  6525 net.cpp:165] Memory required for data: 6074800
I1127 11:09:20.725355  6525 layer_factory.hpp:76] Creating layer conv2
I1127 11:09:20.725747  6525 net.cpp:106] Creating Layer conv2
I1127 11:09:20.725771  6525 net.cpp:454] conv2 <- pool1
I1127 11:09:20.726452  6525 net.cpp:411] conv2 -> conv2
I1127 11:09:20.727430  6525 net.cpp:150] Setting up conv2
I1127 11:09:20.727543  6525 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:09:20.727551  6525 net.cpp:165] Memory required for data: 7354800
I1127 11:09:20.727584  6525 layer_factory.hpp:76] Creating layer pool2
I1127 11:09:20.727620  6525 net.cpp:106] Creating Layer pool2
I1127 11:09:20.727632  6525 net.cpp:454] pool2 <- conv2
I1127 11:09:20.727648  6525 net.cpp:411] pool2 -> pool2
I1127 11:09:20.727743  6525 net.cpp:150] Setting up pool2
I1127 11:09:20.727757  6525 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:09:20.727763  6525 net.cpp:165] Memory required for data: 7674800
I1127 11:09:20.727771  6525 layer_factory.hpp:76] Creating layer ip1
I1127 11:09:20.727792  6525 net.cpp:106] Creating Layer ip1
I1127 11:09:20.727799  6525 net.cpp:454] ip1 <- pool2
I1127 11:09:20.727810  6525 net.cpp:411] ip1 -> ip1
I1127 11:09:20.732972  6525 net.cpp:150] Setting up ip1
I1127 11:09:20.733042  6525 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:09:20.733052  6525 net.cpp:165] Memory required for data: 7874800
I1127 11:09:20.733078  6525 layer_factory.hpp:76] Creating layer relu1
I1127 11:09:20.733103  6525 net.cpp:106] Creating Layer relu1
I1127 11:09:20.733115  6525 net.cpp:454] relu1 <- ip1
I1127 11:09:20.733129  6525 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:09:20.733150  6525 net.cpp:150] Setting up relu1
I1127 11:09:20.733160  6525 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:09:20.733167  6525 net.cpp:165] Memory required for data: 8074800
I1127 11:09:20.733175  6525 layer_factory.hpp:76] Creating layer ip2
I1127 11:09:20.733191  6525 net.cpp:106] Creating Layer ip2
I1127 11:09:20.733201  6525 net.cpp:454] ip2 <- ip1
I1127 11:09:20.733216  6525 net.cpp:411] ip2 -> ip2
I1127 11:09:20.733428  6525 net.cpp:150] Setting up ip2
I1127 11:09:20.733443  6525 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:09:20.733450  6525 net.cpp:165] Memory required for data: 8078800
I1127 11:09:20.733461  6525 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:09:20.733474  6525 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:09:20.733480  6525 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:09:20.733491  6525 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:09:20.733503  6525 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:09:20.733551  6525 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:09:20.733564  6525 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:09:20.733572  6525 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:09:20.733578  6525 net.cpp:165] Memory required for data: 8086800
I1127 11:09:20.733584  6525 layer_factory.hpp:76] Creating layer accuracy
I1127 11:09:20.733599  6525 net.cpp:106] Creating Layer accuracy
I1127 11:09:20.733608  6525 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:09:20.733618  6525 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:09:20.733626  6525 net.cpp:411] accuracy -> accuracy
I1127 11:09:20.733646  6525 net.cpp:150] Setting up accuracy
I1127 11:09:20.733656  6525 net.cpp:157] Top shape: (1)
I1127 11:09:20.733662  6525 net.cpp:165] Memory required for data: 8086804
I1127 11:09:20.733669  6525 layer_factory.hpp:76] Creating layer loss
I1127 11:09:20.733685  6525 net.cpp:106] Creating Layer loss
I1127 11:09:20.733693  6525 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:09:20.733703  6525 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:09:20.733714  6525 net.cpp:411] loss -> loss
I1127 11:09:20.733734  6525 layer_factory.hpp:76] Creating layer loss
I1127 11:09:20.733935  6525 net.cpp:150] Setting up loss
I1127 11:09:20.733952  6525 net.cpp:157] Top shape: (1)
I1127 11:09:20.733963  6525 net.cpp:160]     with loss weight 1
I1127 11:09:20.734009  6525 net.cpp:165] Memory required for data: 8086808
I1127 11:09:20.734019  6525 net.cpp:226] loss needs backward computation.
I1127 11:09:20.734036  6525 net.cpp:228] accuracy does not need backward computation.
I1127 11:09:20.734050  6525 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:09:20.734060  6525 net.cpp:226] ip2 needs backward computation.
I1127 11:09:20.734068  6525 net.cpp:226] relu1 needs backward computation.
I1127 11:09:20.734078  6525 net.cpp:226] ip1 needs backward computation.
I1127 11:09:20.734087  6525 net.cpp:226] pool2 needs backward computation.
I1127 11:09:20.734097  6525 net.cpp:226] conv2 needs backward computation.
I1127 11:09:20.734107  6525 net.cpp:226] pool1 needs backward computation.
I1127 11:09:20.734115  6525 net.cpp:226] conv1 needs backward computation.
I1127 11:09:20.734125  6525 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:09:20.734135  6525 net.cpp:228] mnist does not need backward computation.
I1127 11:09:20.734159  6525 net.cpp:270] This network produces output accuracy
I1127 11:09:20.734170  6525 net.cpp:270] This network produces output loss
I1127 11:09:20.734187  6525 net.cpp:283] Network initialization done.
I1127 11:09:20.734289  6525 solver.cpp:59] Solver scaffolding done.
I1127 11:09:20.734669  6525 caffe.cpp:212] Starting Optimization
I1127 11:09:20.734685  6525 solver.cpp:287] Solving LeNet
I1127 11:09:20.734693  6525 solver.cpp:288] Learning Rate Policy: inv
I1127 11:09:20.735566  6525 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:09:20.736759  6525 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:09:21.917457  6525 solver.cpp:408]     Test net output #0: accuracy = 0.0984
I1127 11:09:21.917594  6525 solver.cpp:408]     Test net output #1: loss = 2.42469 (* 1 = 2.42469 loss)
I1127 11:09:21.933931  6525 solver.cpp:236] Iteration 0, loss = 2.32802
I1127 11:09:21.934077  6525 solver.cpp:252]     Train net output #0: loss = 2.32802 (* 1 = 2.32802 loss)
I1127 11:09:21.934120  6525 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:09:35.177466  6525 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:09:36.514611  6525 solver.cpp:408]     Test net output #0: accuracy = 0.9727
I1127 11:09:36.514721  6525 solver.cpp:408]     Test net output #1: loss = 0.0841261 (* 1 = 0.0841261 loss)
I1127 11:09:36.524679  6525 solver.cpp:236] Iteration 500, loss = 0.104912
I1127 11:09:36.524771  6525 solver.cpp:252]     Train net output #0: loss = 0.104912 (* 1 = 0.104912 loss)
I1127 11:09:36.524790  6525 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:09:49.968300  6525 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:09:49.982136  6525 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:09:49.998237  6525 solver.cpp:320] Iteration 1000, loss = 0.0867125
I1127 11:09:49.998394  6525 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:09:52.743379  6525 solver.cpp:408]     Test net output #0: accuracy = 0.9815
I1127 11:09:52.743574  6525 solver.cpp:408]     Test net output #1: loss = 0.0589314 (* 1 = 0.0589314 loss)
I1127 11:09:52.743584  6525 solver.cpp:325] Optimization Done.
I1127 11:09:52.743590  6525 caffe.cpp:215] Optimization Done.
I1127 11:09:52.860723  6552 caffe.cpp:184] Using GPUs 0
I1127 11:09:53.212323  6552 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:09:53.213106  6552 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:09:53.213724  6552 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:09:53.213795  6552 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:09:53.214058  6552 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:09:53.214221  6552 layer_factory.hpp:76] Creating layer mnist
I1127 11:09:53.214920  6552 net.cpp:106] Creating Layer mnist
I1127 11:09:53.214998  6552 net.cpp:411] mnist -> data
I1127 11:09:53.215081  6552 net.cpp:411] mnist -> label
I1127 11:09:53.219843  6555 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:09:53.234132  6552 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:09:53.236768  6552 net.cpp:150] Setting up mnist
I1127 11:09:53.236908  6552 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:09:53.236927  6552 net.cpp:157] Top shape: 64 (64)
I1127 11:09:53.236934  6552 net.cpp:165] Memory required for data: 200960
I1127 11:09:53.236960  6552 layer_factory.hpp:76] Creating layer conv1
I1127 11:09:53.237020  6552 net.cpp:106] Creating Layer conv1
I1127 11:09:53.237040  6552 net.cpp:454] conv1 <- data
I1127 11:09:53.237063  6552 net.cpp:411] conv1 -> conv1
I1127 11:09:53.238492  6552 net.cpp:150] Setting up conv1
I1127 11:09:53.238550  6552 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:09:53.238559  6552 net.cpp:165] Memory required for data: 3150080
I1127 11:09:53.238591  6552 layer_factory.hpp:76] Creating layer pool1
I1127 11:09:53.238620  6552 net.cpp:106] Creating Layer pool1
I1127 11:09:53.238631  6552 net.cpp:454] pool1 <- conv1
I1127 11:09:53.238644  6552 net.cpp:411] pool1 -> pool1
I1127 11:09:53.238761  6552 net.cpp:150] Setting up pool1
I1127 11:09:53.238775  6552 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:09:53.238782  6552 net.cpp:165] Memory required for data: 3887360
I1127 11:09:53.238790  6552 layer_factory.hpp:76] Creating layer conv2
I1127 11:09:53.238812  6552 net.cpp:106] Creating Layer conv2
I1127 11:09:53.238821  6552 net.cpp:454] conv2 <- pool1
I1127 11:09:53.238831  6552 net.cpp:411] conv2 -> conv2
I1127 11:09:53.239367  6552 net.cpp:150] Setting up conv2
I1127 11:09:53.239424  6552 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:09:53.239433  6552 net.cpp:165] Memory required for data: 4706560
I1127 11:09:53.239476  6552 layer_factory.hpp:76] Creating layer pool2
I1127 11:09:53.239505  6552 net.cpp:106] Creating Layer pool2
I1127 11:09:53.239513  6552 net.cpp:454] pool2 <- conv2
I1127 11:09:53.239528  6552 net.cpp:411] pool2 -> pool2
I1127 11:09:53.239603  6552 net.cpp:150] Setting up pool2
I1127 11:09:53.239619  6552 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:09:53.239626  6552 net.cpp:165] Memory required for data: 4911360
I1127 11:09:53.239634  6552 layer_factory.hpp:76] Creating layer ip1
I1127 11:09:53.239657  6552 net.cpp:106] Creating Layer ip1
I1127 11:09:53.239666  6552 net.cpp:454] ip1 <- pool2
I1127 11:09:53.239681  6552 net.cpp:411] ip1 -> ip1
I1127 11:09:53.248090  6552 net.cpp:150] Setting up ip1
I1127 11:09:53.248184  6552 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:09:53.248206  6552 net.cpp:165] Memory required for data: 5039360
I1127 11:09:53.248258  6552 layer_factory.hpp:76] Creating layer relu1
I1127 11:09:53.248301  6552 net.cpp:106] Creating Layer relu1
I1127 11:09:53.248324  6552 net.cpp:454] relu1 <- ip1
I1127 11:09:53.248348  6552 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:09:53.248394  6552 net.cpp:150] Setting up relu1
I1127 11:09:53.248417  6552 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:09:53.248431  6552 net.cpp:165] Memory required for data: 5167360
I1127 11:09:53.248445  6552 layer_factory.hpp:76] Creating layer ip2
I1127 11:09:53.248484  6552 net.cpp:106] Creating Layer ip2
I1127 11:09:53.248500  6552 net.cpp:454] ip2 <- ip1
I1127 11:09:53.248529  6552 net.cpp:411] ip2 -> ip2
I1127 11:09:53.250298  6552 net.cpp:150] Setting up ip2
I1127 11:09:53.250399  6552 net.cpp:157] Top shape: 64 10 (640)
I1127 11:09:53.250409  6552 net.cpp:165] Memory required for data: 5169920
I1127 11:09:53.250433  6552 layer_factory.hpp:76] Creating layer loss
I1127 11:09:53.250488  6552 net.cpp:106] Creating Layer loss
I1127 11:09:53.250500  6552 net.cpp:454] loss <- ip2
I1127 11:09:53.250515  6552 net.cpp:454] loss <- label
I1127 11:09:53.250538  6552 net.cpp:411] loss -> loss
I1127 11:09:53.250582  6552 layer_factory.hpp:76] Creating layer loss
I1127 11:09:53.250792  6552 net.cpp:150] Setting up loss
I1127 11:09:53.250811  6552 net.cpp:157] Top shape: (1)
I1127 11:09:53.250818  6552 net.cpp:160]     with loss weight 1
I1127 11:09:53.250876  6552 net.cpp:165] Memory required for data: 5169924
I1127 11:09:53.250886  6552 net.cpp:226] loss needs backward computation.
I1127 11:09:53.250895  6552 net.cpp:226] ip2 needs backward computation.
I1127 11:09:53.250902  6552 net.cpp:226] relu1 needs backward computation.
I1127 11:09:53.250910  6552 net.cpp:226] ip1 needs backward computation.
I1127 11:09:53.250917  6552 net.cpp:226] pool2 needs backward computation.
I1127 11:09:53.250926  6552 net.cpp:226] conv2 needs backward computation.
I1127 11:09:53.250933  6552 net.cpp:226] pool1 needs backward computation.
I1127 11:09:53.250941  6552 net.cpp:226] conv1 needs backward computation.
I1127 11:09:53.250949  6552 net.cpp:228] mnist does not need backward computation.
I1127 11:09:53.250957  6552 net.cpp:270] This network produces output loss
I1127 11:09:53.250973  6552 net.cpp:283] Network initialization done.
I1127 11:09:53.251469  6552 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:09:53.251603  6552 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:09:53.251888  6552 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:09:53.252058  6552 layer_factory.hpp:76] Creating layer mnist
I1127 11:09:53.252338  6552 net.cpp:106] Creating Layer mnist
I1127 11:09:53.252363  6552 net.cpp:411] mnist -> data
I1127 11:09:53.252398  6552 net.cpp:411] mnist -> label
I1127 11:09:53.255483  6557 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:09:53.258472  6552 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:09:53.261082  6552 net.cpp:150] Setting up mnist
I1127 11:09:53.261149  6552 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:09:53.261164  6552 net.cpp:157] Top shape: 100 (100)
I1127 11:09:53.261173  6552 net.cpp:165] Memory required for data: 314000
I1127 11:09:53.261185  6552 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:09:53.261204  6552 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:09:53.261214  6552 net.cpp:454] label_mnist_1_split <- label
I1127 11:09:53.261224  6552 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:09:53.261243  6552 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:09:53.261314  6552 net.cpp:150] Setting up label_mnist_1_split
I1127 11:09:53.261334  6552 net.cpp:157] Top shape: 100 (100)
I1127 11:09:53.261348  6552 net.cpp:157] Top shape: 100 (100)
I1127 11:09:53.261358  6552 net.cpp:165] Memory required for data: 314800
I1127 11:09:53.261368  6552 layer_factory.hpp:76] Creating layer conv1
I1127 11:09:53.261397  6552 net.cpp:106] Creating Layer conv1
I1127 11:09:53.261412  6552 net.cpp:454] conv1 <- data
I1127 11:09:53.261430  6552 net.cpp:411] conv1 -> conv1
I1127 11:09:53.261777  6552 net.cpp:150] Setting up conv1
I1127 11:09:53.261796  6552 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:09:53.261803  6552 net.cpp:165] Memory required for data: 4922800
I1127 11:09:53.261821  6552 layer_factory.hpp:76] Creating layer pool1
I1127 11:09:53.261834  6552 net.cpp:106] Creating Layer pool1
I1127 11:09:53.261842  6552 net.cpp:454] pool1 <- conv1
I1127 11:09:53.261873  6552 net.cpp:411] pool1 -> pool1
I1127 11:09:53.261927  6552 net.cpp:150] Setting up pool1
I1127 11:09:53.261941  6552 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:09:53.261948  6552 net.cpp:165] Memory required for data: 6074800
I1127 11:09:53.261960  6552 layer_factory.hpp:76] Creating layer conv2
I1127 11:09:53.261981  6552 net.cpp:106] Creating Layer conv2
I1127 11:09:53.261991  6552 net.cpp:454] conv2 <- pool1
I1127 11:09:53.262003  6552 net.cpp:411] conv2 -> conv2
I1127 11:09:53.263989  6552 net.cpp:150] Setting up conv2
I1127 11:09:53.264061  6552 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:09:53.264083  6552 net.cpp:165] Memory required for data: 7354800
I1127 11:09:53.264106  6552 layer_factory.hpp:76] Creating layer pool2
I1127 11:09:53.264127  6552 net.cpp:106] Creating Layer pool2
I1127 11:09:53.264138  6552 net.cpp:454] pool2 <- conv2
I1127 11:09:53.264153  6552 net.cpp:411] pool2 -> pool2
I1127 11:09:53.264211  6552 net.cpp:150] Setting up pool2
I1127 11:09:53.264225  6552 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:09:53.264231  6552 net.cpp:165] Memory required for data: 7674800
I1127 11:09:53.264240  6552 layer_factory.hpp:76] Creating layer ip1
I1127 11:09:53.264252  6552 net.cpp:106] Creating Layer ip1
I1127 11:09:53.264262  6552 net.cpp:454] ip1 <- pool2
I1127 11:09:53.264276  6552 net.cpp:411] ip1 -> ip1
I1127 11:09:53.268640  6552 net.cpp:150] Setting up ip1
I1127 11:09:53.268755  6552 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:09:53.268766  6552 net.cpp:165] Memory required for data: 7874800
I1127 11:09:53.268801  6552 layer_factory.hpp:76] Creating layer relu1
I1127 11:09:53.268831  6552 net.cpp:106] Creating Layer relu1
I1127 11:09:53.268844  6552 net.cpp:454] relu1 <- ip1
I1127 11:09:53.268860  6552 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:09:53.268883  6552 net.cpp:150] Setting up relu1
I1127 11:09:53.268893  6552 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:09:53.268901  6552 net.cpp:165] Memory required for data: 8074800
I1127 11:09:53.268910  6552 layer_factory.hpp:76] Creating layer ip2
I1127 11:09:53.268939  6552 net.cpp:106] Creating Layer ip2
I1127 11:09:53.268949  6552 net.cpp:454] ip2 <- ip1
I1127 11:09:53.268961  6552 net.cpp:411] ip2 -> ip2
I1127 11:09:53.269204  6552 net.cpp:150] Setting up ip2
I1127 11:09:53.269229  6552 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:09:53.269242  6552 net.cpp:165] Memory required for data: 8078800
I1127 11:09:53.269259  6552 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:09:53.269279  6552 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:09:53.269287  6552 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:09:53.269295  6552 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:09:53.269309  6552 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:09:53.269362  6552 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:09:53.269376  6552 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:09:53.269387  6552 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:09:53.269394  6552 net.cpp:165] Memory required for data: 8086800
I1127 11:09:53.269402  6552 layer_factory.hpp:76] Creating layer accuracy
I1127 11:09:53.269418  6552 net.cpp:106] Creating Layer accuracy
I1127 11:09:53.269426  6552 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:09:53.269435  6552 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:09:53.269450  6552 net.cpp:411] accuracy -> accuracy
I1127 11:09:53.269469  6552 net.cpp:150] Setting up accuracy
I1127 11:09:53.269480  6552 net.cpp:157] Top shape: (1)
I1127 11:09:53.269489  6552 net.cpp:165] Memory required for data: 8086804
I1127 11:09:53.269496  6552 layer_factory.hpp:76] Creating layer loss
I1127 11:09:53.269510  6552 net.cpp:106] Creating Layer loss
I1127 11:09:53.269517  6552 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:09:53.269527  6552 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:09:53.269542  6552 net.cpp:411] loss -> loss
I1127 11:09:53.269561  6552 layer_factory.hpp:76] Creating layer loss
I1127 11:09:53.269739  6552 net.cpp:150] Setting up loss
I1127 11:09:53.269757  6552 net.cpp:157] Top shape: (1)
I1127 11:09:53.269764  6552 net.cpp:160]     with loss weight 1
I1127 11:09:53.269795  6552 net.cpp:165] Memory required for data: 8086808
I1127 11:09:53.269804  6552 net.cpp:226] loss needs backward computation.
I1127 11:09:53.269819  6552 net.cpp:228] accuracy does not need backward computation.
I1127 11:09:53.269829  6552 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:09:53.269837  6552 net.cpp:226] ip2 needs backward computation.
I1127 11:09:53.269845  6552 net.cpp:226] relu1 needs backward computation.
I1127 11:09:53.269861  6552 net.cpp:226] ip1 needs backward computation.
I1127 11:09:53.269870  6552 net.cpp:226] pool2 needs backward computation.
I1127 11:09:53.269879  6552 net.cpp:226] conv2 needs backward computation.
I1127 11:09:53.269886  6552 net.cpp:226] pool1 needs backward computation.
I1127 11:09:53.269894  6552 net.cpp:226] conv1 needs backward computation.
I1127 11:09:53.269901  6552 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:09:53.269914  6552 net.cpp:228] mnist does not need backward computation.
I1127 11:09:53.269922  6552 net.cpp:270] This network produces output accuracy
I1127 11:09:53.269928  6552 net.cpp:270] This network produces output loss
I1127 11:09:53.269944  6552 net.cpp:283] Network initialization done.
I1127 11:09:53.270048  6552 solver.cpp:59] Solver scaffolding done.
I1127 11:09:53.270388  6552 caffe.cpp:212] Starting Optimization
I1127 11:09:53.270406  6552 solver.cpp:287] Solving LeNet
I1127 11:09:53.270412  6552 solver.cpp:288] Learning Rate Policy: inv
I1127 11:09:53.271319  6552 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:09:54.361124  6552 solver.cpp:408]     Test net output #0: accuracy = 0.0956
I1127 11:09:54.361198  6552 solver.cpp:408]     Test net output #1: loss = 2.37848 (* 1 = 2.37848 loss)
I1127 11:09:54.375588  6552 solver.cpp:236] Iteration 0, loss = 2.43338
I1127 11:09:54.375756  6552 solver.cpp:252]     Train net output #0: loss = 2.43338 (* 1 = 2.43338 loss)
I1127 11:09:54.375857  6552 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:10:07.797524  6552 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:10:08.657341  6552 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:10:09.055424  6552 solver.cpp:408]     Test net output #0: accuracy = 0.9745
I1127 11:10:09.055570  6552 solver.cpp:408]     Test net output #1: loss = 0.0824592 (* 1 = 0.0824592 loss)
I1127 11:10:09.065784  6552 solver.cpp:236] Iteration 500, loss = 0.0866225
I1127 11:10:09.065902  6552 solver.cpp:252]     Train net output #0: loss = 0.0866224 (* 1 = 0.0866224 loss)
I1127 11:10:09.065922  6552 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:10:22.501198  6552 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:10:22.521466  6552 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:10:22.548749  6552 solver.cpp:320] Iteration 1000, loss = 0.0789525
I1127 11:10:22.548769  6552 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:10:23.647796  6552 solver.cpp:408]     Test net output #0: accuracy = 0.9816
I1127 11:10:23.648030  6552 solver.cpp:408]     Test net output #1: loss = 0.0586883 (* 1 = 0.0586883 loss)
I1127 11:10:23.648049  6552 solver.cpp:325] Optimization Done.
I1127 11:10:23.648057  6552 caffe.cpp:215] Optimization Done.
I1127 11:10:23.773843  6581 caffe.cpp:184] Using GPUs 0
I1127 11:10:24.059484  6581 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:10:24.059625  6581 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:10:24.060010  6581 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:10:24.060034  6581 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:10:24.060170  6581 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:10:24.060251  6581 layer_factory.hpp:76] Creating layer mnist
I1127 11:10:24.060699  6581 net.cpp:106] Creating Layer mnist
I1127 11:10:24.060715  6581 net.cpp:411] mnist -> data
I1127 11:10:24.060750  6581 net.cpp:411] mnist -> label
I1127 11:10:24.061498  6585 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:10:24.094511  6581 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:10:24.101671  6581 net.cpp:150] Setting up mnist
I1127 11:10:24.101696  6581 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:10:24.101707  6581 net.cpp:157] Top shape: 64 (64)
I1127 11:10:24.101716  6581 net.cpp:165] Memory required for data: 200960
I1127 11:10:24.101730  6581 layer_factory.hpp:76] Creating layer conv1
I1127 11:10:24.101757  6581 net.cpp:106] Creating Layer conv1
I1127 11:10:24.101768  6581 net.cpp:454] conv1 <- data
I1127 11:10:24.101783  6581 net.cpp:411] conv1 -> conv1
I1127 11:10:24.102566  6581 net.cpp:150] Setting up conv1
I1127 11:10:24.102582  6581 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:10:24.102591  6581 net.cpp:165] Memory required for data: 3150080
I1127 11:10:24.102610  6581 layer_factory.hpp:76] Creating layer pool1
I1127 11:10:24.102625  6581 net.cpp:106] Creating Layer pool1
I1127 11:10:24.102634  6581 net.cpp:454] pool1 <- conv1
I1127 11:10:24.102646  6581 net.cpp:411] pool1 -> pool1
I1127 11:10:24.102874  6581 net.cpp:150] Setting up pool1
I1127 11:10:24.102888  6581 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:10:24.102897  6581 net.cpp:165] Memory required for data: 3887360
I1127 11:10:24.102910  6581 layer_factory.hpp:76] Creating layer conv2
I1127 11:10:24.102928  6581 net.cpp:106] Creating Layer conv2
I1127 11:10:24.102937  6581 net.cpp:454] conv2 <- pool1
I1127 11:10:24.102952  6581 net.cpp:411] conv2 -> conv2
I1127 11:10:24.103358  6581 net.cpp:150] Setting up conv2
I1127 11:10:24.103374  6581 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:10:24.103385  6581 net.cpp:165] Memory required for data: 4706560
I1127 11:10:24.103404  6581 layer_factory.hpp:76] Creating layer pool2
I1127 11:10:24.103420  6581 net.cpp:106] Creating Layer pool2
I1127 11:10:24.103428  6581 net.cpp:454] pool2 <- conv2
I1127 11:10:24.103440  6581 net.cpp:411] pool2 -> pool2
I1127 11:10:24.103488  6581 net.cpp:150] Setting up pool2
I1127 11:10:24.103504  6581 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:10:24.103513  6581 net.cpp:165] Memory required for data: 4911360
I1127 11:10:24.103528  6581 layer_factory.hpp:76] Creating layer ip1
I1127 11:10:24.103540  6581 net.cpp:106] Creating Layer ip1
I1127 11:10:24.103549  6581 net.cpp:454] ip1 <- pool2
I1127 11:10:24.103564  6581 net.cpp:411] ip1 -> ip1
I1127 11:10:24.107153  6581 net.cpp:150] Setting up ip1
I1127 11:10:24.107169  6581 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:10:24.107178  6581 net.cpp:165] Memory required for data: 5039360
I1127 11:10:24.107194  6581 layer_factory.hpp:76] Creating layer relu1
I1127 11:10:24.107208  6581 net.cpp:106] Creating Layer relu1
I1127 11:10:24.107216  6581 net.cpp:454] relu1 <- ip1
I1127 11:10:24.107226  6581 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:10:24.107240  6581 net.cpp:150] Setting up relu1
I1127 11:10:24.107251  6581 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:10:24.107259  6581 net.cpp:165] Memory required for data: 5167360
I1127 11:10:24.107267  6581 layer_factory.hpp:76] Creating layer ip2
I1127 11:10:24.107278  6581 net.cpp:106] Creating Layer ip2
I1127 11:10:24.107287  6581 net.cpp:454] ip2 <- ip1
I1127 11:10:24.107300  6581 net.cpp:411] ip2 -> ip2
I1127 11:10:24.107885  6581 net.cpp:150] Setting up ip2
I1127 11:10:24.107900  6581 net.cpp:157] Top shape: 64 10 (640)
I1127 11:10:24.107908  6581 net.cpp:165] Memory required for data: 5169920
I1127 11:10:24.107921  6581 layer_factory.hpp:76] Creating layer loss
I1127 11:10:24.107935  6581 net.cpp:106] Creating Layer loss
I1127 11:10:24.107944  6581 net.cpp:454] loss <- ip2
I1127 11:10:24.107954  6581 net.cpp:454] loss <- label
I1127 11:10:24.107965  6581 net.cpp:411] loss -> loss
I1127 11:10:24.107985  6581 layer_factory.hpp:76] Creating layer loss
I1127 11:10:24.108103  6581 net.cpp:150] Setting up loss
I1127 11:10:24.108116  6581 net.cpp:157] Top shape: (1)
I1127 11:10:24.108124  6581 net.cpp:160]     with loss weight 1
I1127 11:10:24.108146  6581 net.cpp:165] Memory required for data: 5169924
I1127 11:10:24.108155  6581 net.cpp:226] loss needs backward computation.
I1127 11:10:24.108163  6581 net.cpp:226] ip2 needs backward computation.
I1127 11:10:24.108171  6581 net.cpp:226] relu1 needs backward computation.
I1127 11:10:24.108178  6581 net.cpp:226] ip1 needs backward computation.
I1127 11:10:24.108186  6581 net.cpp:226] pool2 needs backward computation.
I1127 11:10:24.108193  6581 net.cpp:226] conv2 needs backward computation.
I1127 11:10:24.108201  6581 net.cpp:226] pool1 needs backward computation.
I1127 11:10:24.108208  6581 net.cpp:226] conv1 needs backward computation.
I1127 11:10:24.108217  6581 net.cpp:228] mnist does not need backward computation.
I1127 11:10:24.108223  6581 net.cpp:270] This network produces output loss
I1127 11:10:24.108239  6581 net.cpp:283] Network initialization done.
I1127 11:10:24.108610  6581 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:10:24.108644  6581 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:10:24.108819  6581 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:10:24.108919  6581 layer_factory.hpp:76] Creating layer mnist
I1127 11:10:24.109042  6581 net.cpp:106] Creating Layer mnist
I1127 11:10:24.109056  6581 net.cpp:411] mnist -> data
I1127 11:10:24.109073  6581 net.cpp:411] mnist -> label
I1127 11:10:24.109772  6587 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:10:24.109877  6581 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:10:24.113699  6581 net.cpp:150] Setting up mnist
I1127 11:10:24.113729  6581 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:10:24.113741  6581 net.cpp:157] Top shape: 100 (100)
I1127 11:10:24.113749  6581 net.cpp:165] Memory required for data: 314000
I1127 11:10:24.113756  6581 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:10:24.113767  6581 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:10:24.113776  6581 net.cpp:454] label_mnist_1_split <- label
I1127 11:10:24.113785  6581 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:10:24.113798  6581 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:10:24.113845  6581 net.cpp:150] Setting up label_mnist_1_split
I1127 11:10:24.113857  6581 net.cpp:157] Top shape: 100 (100)
I1127 11:10:24.113867  6581 net.cpp:157] Top shape: 100 (100)
I1127 11:10:24.113874  6581 net.cpp:165] Memory required for data: 314800
I1127 11:10:24.113881  6581 layer_factory.hpp:76] Creating layer conv1
I1127 11:10:24.113896  6581 net.cpp:106] Creating Layer conv1
I1127 11:10:24.113905  6581 net.cpp:454] conv1 <- data
I1127 11:10:24.113919  6581 net.cpp:411] conv1 -> conv1
I1127 11:10:24.114153  6581 net.cpp:150] Setting up conv1
I1127 11:10:24.114168  6581 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:10:24.114176  6581 net.cpp:165] Memory required for data: 4922800
I1127 11:10:24.114192  6581 layer_factory.hpp:76] Creating layer pool1
I1127 11:10:24.114202  6581 net.cpp:106] Creating Layer pool1
I1127 11:10:24.114210  6581 net.cpp:454] pool1 <- conv1
I1127 11:10:24.114230  6581 net.cpp:411] pool1 -> pool1
I1127 11:10:24.114275  6581 net.cpp:150] Setting up pool1
I1127 11:10:24.114289  6581 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:10:24.114296  6581 net.cpp:165] Memory required for data: 6074800
I1127 11:10:24.114303  6581 layer_factory.hpp:76] Creating layer conv2
I1127 11:10:24.114317  6581 net.cpp:106] Creating Layer conv2
I1127 11:10:24.114325  6581 net.cpp:454] conv2 <- pool1
I1127 11:10:24.114334  6581 net.cpp:411] conv2 -> conv2
I1127 11:10:24.115013  6581 net.cpp:150] Setting up conv2
I1127 11:10:24.115025  6581 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:10:24.115032  6581 net.cpp:165] Memory required for data: 7354800
I1127 11:10:24.115046  6581 layer_factory.hpp:76] Creating layer pool2
I1127 11:10:24.115054  6581 net.cpp:106] Creating Layer pool2
I1127 11:10:24.115061  6581 net.cpp:454] pool2 <- conv2
I1127 11:10:24.115070  6581 net.cpp:411] pool2 -> pool2
I1127 11:10:24.115109  6581 net.cpp:150] Setting up pool2
I1127 11:10:24.115121  6581 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:10:24.115133  6581 net.cpp:165] Memory required for data: 7674800
I1127 11:10:24.115139  6581 layer_factory.hpp:76] Creating layer ip1
I1127 11:10:24.115149  6581 net.cpp:106] Creating Layer ip1
I1127 11:10:24.115155  6581 net.cpp:454] ip1 <- pool2
I1127 11:10:24.115166  6581 net.cpp:411] ip1 -> ip1
I1127 11:10:24.118741  6581 net.cpp:150] Setting up ip1
I1127 11:10:24.118757  6581 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:10:24.118764  6581 net.cpp:165] Memory required for data: 7874800
I1127 11:10:24.118777  6581 layer_factory.hpp:76] Creating layer relu1
I1127 11:10:24.118788  6581 net.cpp:106] Creating Layer relu1
I1127 11:10:24.118794  6581 net.cpp:454] relu1 <- ip1
I1127 11:10:24.118805  6581 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:10:24.118815  6581 net.cpp:150] Setting up relu1
I1127 11:10:24.118824  6581 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:10:24.118830  6581 net.cpp:165] Memory required for data: 8074800
I1127 11:10:24.118837  6581 layer_factory.hpp:76] Creating layer ip2
I1127 11:10:24.118849  6581 net.cpp:106] Creating Layer ip2
I1127 11:10:24.118854  6581 net.cpp:454] ip2 <- ip1
I1127 11:10:24.118865  6581 net.cpp:411] ip2 -> ip2
I1127 11:10:24.119007  6581 net.cpp:150] Setting up ip2
I1127 11:10:24.119019  6581 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:10:24.119025  6581 net.cpp:165] Memory required for data: 8078800
I1127 11:10:24.119035  6581 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:10:24.119045  6581 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:10:24.119050  6581 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:10:24.119060  6581 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:10:24.119068  6581 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:10:24.119105  6581 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:10:24.119115  6581 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:10:24.119123  6581 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:10:24.119129  6581 net.cpp:165] Memory required for data: 8086800
I1127 11:10:24.119135  6581 layer_factory.hpp:76] Creating layer accuracy
I1127 11:10:24.119146  6581 net.cpp:106] Creating Layer accuracy
I1127 11:10:24.119153  6581 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:10:24.119161  6581 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:10:24.119170  6581 net.cpp:411] accuracy -> accuracy
I1127 11:10:24.119182  6581 net.cpp:150] Setting up accuracy
I1127 11:10:24.119190  6581 net.cpp:157] Top shape: (1)
I1127 11:10:24.119196  6581 net.cpp:165] Memory required for data: 8086804
I1127 11:10:24.119204  6581 layer_factory.hpp:76] Creating layer loss
I1127 11:10:24.119213  6581 net.cpp:106] Creating Layer loss
I1127 11:10:24.119220  6581 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:10:24.119228  6581 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:10:24.119236  6581 net.cpp:411] loss -> loss
I1127 11:10:24.119247  6581 layer_factory.hpp:76] Creating layer loss
I1127 11:10:24.119346  6581 net.cpp:150] Setting up loss
I1127 11:10:24.119356  6581 net.cpp:157] Top shape: (1)
I1127 11:10:24.119364  6581 net.cpp:160]     with loss weight 1
I1127 11:10:24.119374  6581 net.cpp:165] Memory required for data: 8086808
I1127 11:10:24.119381  6581 net.cpp:226] loss needs backward computation.
I1127 11:10:24.119390  6581 net.cpp:228] accuracy does not need backward computation.
I1127 11:10:24.119397  6581 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:10:24.119403  6581 net.cpp:226] ip2 needs backward computation.
I1127 11:10:24.119410  6581 net.cpp:226] relu1 needs backward computation.
I1127 11:10:24.119416  6581 net.cpp:226] ip1 needs backward computation.
I1127 11:10:24.119423  6581 net.cpp:226] pool2 needs backward computation.
I1127 11:10:24.119429  6581 net.cpp:226] conv2 needs backward computation.
I1127 11:10:24.119436  6581 net.cpp:226] pool1 needs backward computation.
I1127 11:10:24.119442  6581 net.cpp:226] conv1 needs backward computation.
I1127 11:10:24.119449  6581 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:10:24.119462  6581 net.cpp:228] mnist does not need backward computation.
I1127 11:10:24.119467  6581 net.cpp:270] This network produces output accuracy
I1127 11:10:24.119474  6581 net.cpp:270] This network produces output loss
I1127 11:10:24.119490  6581 net.cpp:283] Network initialization done.
I1127 11:10:24.119541  6581 solver.cpp:59] Solver scaffolding done.
I1127 11:10:24.119819  6581 caffe.cpp:212] Starting Optimization
I1127 11:10:24.119828  6581 solver.cpp:287] Solving LeNet
I1127 11:10:24.119834  6581 solver.cpp:288] Learning Rate Policy: inv
I1127 11:10:24.120244  6581 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:10:27.160459  6581 solver.cpp:408]     Test net output #0: accuracy = 0.0661
I1127 11:10:27.160614  6581 solver.cpp:408]     Test net output #1: loss = 2.52926 (* 1 = 2.52926 loss)
I1127 11:10:27.174253  6581 solver.cpp:236] Iteration 0, loss = 2.56881
I1127 11:10:27.174383  6581 solver.cpp:252]     Train net output #0: loss = 2.56881 (* 1 = 2.56881 loss)
I1127 11:10:27.174423  6581 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:10:40.604981  6581 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:10:41.683521  6581 solver.cpp:408]     Test net output #0: accuracy = 0.9714
I1127 11:10:41.683580  6581 solver.cpp:408]     Test net output #1: loss = 0.0915185 (* 1 = 0.0915185 loss)
I1127 11:10:41.694569  6581 solver.cpp:236] Iteration 500, loss = 0.0815683
I1127 11:10:41.694629  6581 solver.cpp:252]     Train net output #0: loss = 0.0815683 (* 1 = 0.0815683 loss)
I1127 11:10:41.694646  6581 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:10:54.945451  6581 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:10:54.966284  6581 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:10:54.995421  6581 solver.cpp:320] Iteration 1000, loss = 0.0720481
I1127 11:10:54.995448  6581 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:10:56.314103  6581 solver.cpp:408]     Test net output #0: accuracy = 0.9816
I1127 11:10:56.314226  6581 solver.cpp:408]     Test net output #1: loss = 0.0573509 (* 1 = 0.0573509 loss)
I1127 11:10:56.314241  6581 solver.cpp:325] Optimization Done.
I1127 11:10:56.314251  6581 caffe.cpp:215] Optimization Done.
I1127 11:10:56.442175  6608 caffe.cpp:184] Using GPUs 0
I1127 11:10:56.890312  6608 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:10:56.890686  6608 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:10:56.891609  6608 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:10:56.891752  6608 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:10:56.892078  6608 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:10:56.892312  6608 layer_factory.hpp:76] Creating layer mnist
I1127 11:10:56.893070  6608 net.cpp:106] Creating Layer mnist
I1127 11:10:56.893103  6608 net.cpp:411] mnist -> data
I1127 11:10:56.893147  6608 net.cpp:411] mnist -> label
I1127 11:10:56.895793  6611 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:10:56.913872  6608 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:10:56.920850  6608 net.cpp:150] Setting up mnist
I1127 11:10:56.921003  6608 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:10:56.921046  6608 net.cpp:157] Top shape: 64 (64)
I1127 11:10:56.921069  6608 net.cpp:165] Memory required for data: 200960
I1127 11:10:56.921111  6608 layer_factory.hpp:76] Creating layer conv1
I1127 11:10:56.921193  6608 net.cpp:106] Creating Layer conv1
I1127 11:10:56.921231  6608 net.cpp:454] conv1 <- data
I1127 11:10:56.921290  6608 net.cpp:411] conv1 -> conv1
I1127 11:10:56.923593  6608 net.cpp:150] Setting up conv1
I1127 11:10:56.923682  6608 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:10:56.923707  6608 net.cpp:165] Memory required for data: 3150080
I1127 11:10:56.923759  6608 layer_factory.hpp:76] Creating layer pool1
I1127 11:10:56.923796  6608 net.cpp:106] Creating Layer pool1
I1127 11:10:56.923813  6608 net.cpp:454] pool1 <- conv1
I1127 11:10:56.923831  6608 net.cpp:411] pool1 -> pool1
I1127 11:10:56.923976  6608 net.cpp:150] Setting up pool1
I1127 11:10:56.924000  6608 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:10:56.924008  6608 net.cpp:165] Memory required for data: 3887360
I1127 11:10:56.924018  6608 layer_factory.hpp:76] Creating layer conv2
I1127 11:10:56.924047  6608 net.cpp:106] Creating Layer conv2
I1127 11:10:56.924062  6608 net.cpp:454] conv2 <- pool1
I1127 11:10:56.924080  6608 net.cpp:411] conv2 -> conv2
I1127 11:10:56.924620  6608 net.cpp:150] Setting up conv2
I1127 11:10:56.924649  6608 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:10:56.924664  6608 net.cpp:165] Memory required for data: 4706560
I1127 11:10:56.924685  6608 layer_factory.hpp:76] Creating layer pool2
I1127 11:10:56.924708  6608 net.cpp:106] Creating Layer pool2
I1127 11:10:56.924718  6608 net.cpp:454] pool2 <- conv2
I1127 11:10:56.924736  6608 net.cpp:411] pool2 -> pool2
I1127 11:10:56.924805  6608 net.cpp:150] Setting up pool2
I1127 11:10:56.924821  6608 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:10:56.924829  6608 net.cpp:165] Memory required for data: 4911360
I1127 11:10:56.924837  6608 layer_factory.hpp:76] Creating layer ip1
I1127 11:10:56.924860  6608 net.cpp:106] Creating Layer ip1
I1127 11:10:56.924871  6608 net.cpp:454] ip1 <- pool2
I1127 11:10:56.924886  6608 net.cpp:411] ip1 -> ip1
I1127 11:10:56.929595  6608 net.cpp:150] Setting up ip1
I1127 11:10:56.929702  6608 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:10:56.929718  6608 net.cpp:165] Memory required for data: 5039360
I1127 11:10:56.929762  6608 layer_factory.hpp:76] Creating layer relu1
I1127 11:10:56.929805  6608 net.cpp:106] Creating Layer relu1
I1127 11:10:56.929831  6608 net.cpp:454] relu1 <- ip1
I1127 11:10:56.929847  6608 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:10:56.929879  6608 net.cpp:150] Setting up relu1
I1127 11:10:56.929890  6608 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:10:56.929896  6608 net.cpp:165] Memory required for data: 5167360
I1127 11:10:56.929904  6608 layer_factory.hpp:76] Creating layer ip2
I1127 11:10:56.929926  6608 net.cpp:106] Creating Layer ip2
I1127 11:10:56.929934  6608 net.cpp:454] ip2 <- ip1
I1127 11:10:56.929945  6608 net.cpp:411] ip2 -> ip2
I1127 11:10:56.938271  6608 net.cpp:150] Setting up ip2
I1127 11:10:56.938326  6608 net.cpp:157] Top shape: 64 10 (640)
I1127 11:10:56.938339  6608 net.cpp:165] Memory required for data: 5169920
I1127 11:10:56.938365  6608 layer_factory.hpp:76] Creating layer loss
I1127 11:10:56.938401  6608 net.cpp:106] Creating Layer loss
I1127 11:10:56.938417  6608 net.cpp:454] loss <- ip2
I1127 11:10:56.938433  6608 net.cpp:454] loss <- label
I1127 11:10:56.938455  6608 net.cpp:411] loss -> loss
I1127 11:10:56.938495  6608 layer_factory.hpp:76] Creating layer loss
I1127 11:10:56.938670  6608 net.cpp:150] Setting up loss
I1127 11:10:56.938684  6608 net.cpp:157] Top shape: (1)
I1127 11:10:56.938690  6608 net.cpp:160]     with loss weight 1
I1127 11:10:56.938717  6608 net.cpp:165] Memory required for data: 5169924
I1127 11:10:56.938724  6608 net.cpp:226] loss needs backward computation.
I1127 11:10:56.938730  6608 net.cpp:226] ip2 needs backward computation.
I1127 11:10:56.938737  6608 net.cpp:226] relu1 needs backward computation.
I1127 11:10:56.938743  6608 net.cpp:226] ip1 needs backward computation.
I1127 11:10:56.938750  6608 net.cpp:226] pool2 needs backward computation.
I1127 11:10:56.938756  6608 net.cpp:226] conv2 needs backward computation.
I1127 11:10:56.938763  6608 net.cpp:226] pool1 needs backward computation.
I1127 11:10:56.938769  6608 net.cpp:226] conv1 needs backward computation.
I1127 11:10:56.938776  6608 net.cpp:228] mnist does not need backward computation.
I1127 11:10:56.938782  6608 net.cpp:270] This network produces output loss
I1127 11:10:56.938796  6608 net.cpp:283] Network initialization done.
I1127 11:10:56.939198  6608 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:10:56.939239  6608 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:10:56.939451  6608 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:10:56.939575  6608 layer_factory.hpp:76] Creating layer mnist
I1127 11:10:56.939780  6608 net.cpp:106] Creating Layer mnist
I1127 11:10:56.939796  6608 net.cpp:411] mnist -> data
I1127 11:10:56.939816  6608 net.cpp:411] mnist -> label
I1127 11:10:56.944003  6613 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:10:56.946437  6608 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:10:56.949264  6608 net.cpp:150] Setting up mnist
I1127 11:10:56.949388  6608 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:10:56.949426  6608 net.cpp:157] Top shape: 100 (100)
I1127 11:10:56.949445  6608 net.cpp:165] Memory required for data: 314000
I1127 11:10:56.949470  6608 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:10:56.949519  6608 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:10:56.949542  6608 net.cpp:454] label_mnist_1_split <- label
I1127 11:10:56.949575  6608 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:10:56.949623  6608 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:10:56.954368  6608 net.cpp:150] Setting up label_mnist_1_split
I1127 11:10:56.954484  6608 net.cpp:157] Top shape: 100 (100)
I1127 11:10:56.954515  6608 net.cpp:157] Top shape: 100 (100)
I1127 11:10:56.954532  6608 net.cpp:165] Memory required for data: 314800
I1127 11:10:56.954556  6608 layer_factory.hpp:76] Creating layer conv1
I1127 11:10:56.954596  6608 net.cpp:106] Creating Layer conv1
I1127 11:10:56.954613  6608 net.cpp:454] conv1 <- data
I1127 11:10:56.954638  6608 net.cpp:411] conv1 -> conv1
I1127 11:10:56.955014  6608 net.cpp:150] Setting up conv1
I1127 11:10:56.955044  6608 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:10:56.955054  6608 net.cpp:165] Memory required for data: 4922800
I1127 11:10:56.955077  6608 layer_factory.hpp:76] Creating layer pool1
I1127 11:10:56.955102  6608 net.cpp:106] Creating Layer pool1
I1127 11:10:56.955112  6608 net.cpp:454] pool1 <- conv1
I1127 11:10:56.955170  6608 net.cpp:411] pool1 -> pool1
I1127 11:10:56.955246  6608 net.cpp:150] Setting up pool1
I1127 11:10:56.955263  6608 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:10:56.955271  6608 net.cpp:165] Memory required for data: 6074800
I1127 11:10:56.955281  6608 layer_factory.hpp:76] Creating layer conv2
I1127 11:10:56.955307  6608 net.cpp:106] Creating Layer conv2
I1127 11:10:56.955317  6608 net.cpp:454] conv2 <- pool1
I1127 11:10:56.955334  6608 net.cpp:411] conv2 -> conv2
I1127 11:10:56.955961  6608 net.cpp:150] Setting up conv2
I1127 11:10:56.956018  6608 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:10:56.956032  6608 net.cpp:165] Memory required for data: 7354800
I1127 11:10:56.956066  6608 layer_factory.hpp:76] Creating layer pool2
I1127 11:10:56.956100  6608 net.cpp:106] Creating Layer pool2
I1127 11:10:56.956116  6608 net.cpp:454] pool2 <- conv2
I1127 11:10:56.956135  6608 net.cpp:411] pool2 -> pool2
I1127 11:10:56.956230  6608 net.cpp:150] Setting up pool2
I1127 11:10:56.956250  6608 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:10:56.956259  6608 net.cpp:165] Memory required for data: 7674800
I1127 11:10:56.956271  6608 layer_factory.hpp:76] Creating layer ip1
I1127 11:10:56.956296  6608 net.cpp:106] Creating Layer ip1
I1127 11:10:56.956307  6608 net.cpp:454] ip1 <- pool2
I1127 11:10:56.956326  6608 net.cpp:411] ip1 -> ip1
I1127 11:10:56.961427  6608 net.cpp:150] Setting up ip1
I1127 11:10:56.961566  6608 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:10:56.961580  6608 net.cpp:165] Memory required for data: 7874800
I1127 11:10:56.961624  6608 layer_factory.hpp:76] Creating layer relu1
I1127 11:10:56.961679  6608 net.cpp:106] Creating Layer relu1
I1127 11:10:56.961696  6608 net.cpp:454] relu1 <- ip1
I1127 11:10:56.961715  6608 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:10:56.961747  6608 net.cpp:150] Setting up relu1
I1127 11:10:56.961763  6608 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:10:56.961774  6608 net.cpp:165] Memory required for data: 8074800
I1127 11:10:56.961786  6608 layer_factory.hpp:76] Creating layer ip2
I1127 11:10:56.961828  6608 net.cpp:106] Creating Layer ip2
I1127 11:10:56.961840  6608 net.cpp:454] ip2 <- ip1
I1127 11:10:56.961860  6608 net.cpp:411] ip2 -> ip2
I1127 11:10:56.962203  6608 net.cpp:150] Setting up ip2
I1127 11:10:56.962223  6608 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:10:56.962229  6608 net.cpp:165] Memory required for data: 8078800
I1127 11:10:56.962241  6608 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:10:56.962255  6608 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:10:56.962261  6608 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:10:56.962272  6608 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:10:56.962285  6608 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:10:56.962330  6608 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:10:56.962342  6608 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:10:56.962350  6608 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:10:56.962357  6608 net.cpp:165] Memory required for data: 8086800
I1127 11:10:56.962364  6608 layer_factory.hpp:76] Creating layer accuracy
I1127 11:10:56.962379  6608 net.cpp:106] Creating Layer accuracy
I1127 11:10:56.962386  6608 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:10:56.962396  6608 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:10:56.962409  6608 net.cpp:411] accuracy -> accuracy
I1127 11:10:56.962426  6608 net.cpp:150] Setting up accuracy
I1127 11:10:56.962435  6608 net.cpp:157] Top shape: (1)
I1127 11:10:56.962442  6608 net.cpp:165] Memory required for data: 8086804
I1127 11:10:56.962450  6608 layer_factory.hpp:76] Creating layer loss
I1127 11:10:56.962462  6608 net.cpp:106] Creating Layer loss
I1127 11:10:56.962469  6608 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:10:56.962477  6608 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:10:56.962489  6608 net.cpp:411] loss -> loss
I1127 11:10:56.962507  6608 layer_factory.hpp:76] Creating layer loss
I1127 11:10:56.962663  6608 net.cpp:150] Setting up loss
I1127 11:10:56.962678  6608 net.cpp:157] Top shape: (1)
I1127 11:10:56.962685  6608 net.cpp:160]     with loss weight 1
I1127 11:10:56.962715  6608 net.cpp:165] Memory required for data: 8086808
I1127 11:10:56.962723  6608 net.cpp:226] loss needs backward computation.
I1127 11:10:56.962738  6608 net.cpp:228] accuracy does not need backward computation.
I1127 11:10:56.962745  6608 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:10:56.962752  6608 net.cpp:226] ip2 needs backward computation.
I1127 11:10:56.962759  6608 net.cpp:226] relu1 needs backward computation.
I1127 11:10:56.962765  6608 net.cpp:226] ip1 needs backward computation.
I1127 11:10:56.962771  6608 net.cpp:226] pool2 needs backward computation.
I1127 11:10:56.962779  6608 net.cpp:226] conv2 needs backward computation.
I1127 11:10:56.962786  6608 net.cpp:226] pool1 needs backward computation.
I1127 11:10:56.962795  6608 net.cpp:226] conv1 needs backward computation.
I1127 11:10:56.962801  6608 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:10:56.962810  6608 net.cpp:228] mnist does not need backward computation.
I1127 11:10:56.962816  6608 net.cpp:270] This network produces output accuracy
I1127 11:10:56.962823  6608 net.cpp:270] This network produces output loss
I1127 11:10:56.962838  6608 net.cpp:283] Network initialization done.
I1127 11:10:56.962962  6608 solver.cpp:59] Solver scaffolding done.
I1127 11:10:56.963289  6608 caffe.cpp:212] Starting Optimization
I1127 11:10:56.963312  6608 solver.cpp:287] Solving LeNet
I1127 11:10:56.963318  6608 solver.cpp:288] Learning Rate Policy: inv
I1127 11:10:56.964680  6608 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:10:59.585520  6608 solver.cpp:408]     Test net output #0: accuracy = 0.1001
I1127 11:10:59.585559  6608 solver.cpp:408]     Test net output #1: loss = 2.38648 (* 1 = 2.38648 loss)
I1127 11:10:59.617524  6608 solver.cpp:236] Iteration 0, loss = 2.35634
I1127 11:10:59.617542  6608 solver.cpp:252]     Train net output #0: loss = 2.35634 (* 1 = 2.35634 loss)
I1127 11:10:59.617558  6608 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:11:12.538547  6608 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:11:14.621137  6608 solver.cpp:408]     Test net output #0: accuracy = 0.9713
I1127 11:11:14.621256  6608 solver.cpp:408]     Test net output #1: loss = 0.0877846 (* 1 = 0.0877846 loss)
I1127 11:11:14.631233  6608 solver.cpp:236] Iteration 500, loss = 0.120554
I1127 11:11:14.631333  6608 solver.cpp:252]     Train net output #0: loss = 0.120554 (* 1 = 0.120554 loss)
I1127 11:11:14.631363  6608 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:11:28.119055  6608 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:11:28.133882  6608 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:11:28.144911  6608 solver.cpp:320] Iteration 1000, loss = 0.0981077
I1127 11:11:28.144984  6608 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:11:28.330672  6608 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:11:29.241091  6608 solver.cpp:408]     Test net output #0: accuracy = 0.9806
I1127 11:11:29.241180  6608 solver.cpp:408]     Test net output #1: loss = 0.0596469 (* 1 = 0.0596469 loss)
I1127 11:11:29.241189  6608 solver.cpp:325] Optimization Done.
I1127 11:11:29.241195  6608 caffe.cpp:215] Optimization Done.
I1127 11:11:29.346128  6636 caffe.cpp:184] Using GPUs 0
I1127 11:11:29.707422  6636 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:11:29.707535  6636 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:11:29.707799  6636 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:11:29.707816  6636 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:11:29.707901  6636 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:11:29.707967  6636 layer_factory.hpp:76] Creating layer mnist
I1127 11:11:29.708278  6636 net.cpp:106] Creating Layer mnist
I1127 11:11:29.708300  6636 net.cpp:411] mnist -> data
I1127 11:11:29.708325  6636 net.cpp:411] mnist -> label
I1127 11:11:29.709058  6640 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:11:29.741844  6636 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:11:29.748230  6636 net.cpp:150] Setting up mnist
I1127 11:11:29.748250  6636 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:11:29.748257  6636 net.cpp:157] Top shape: 64 (64)
I1127 11:11:29.748261  6636 net.cpp:165] Memory required for data: 200960
I1127 11:11:29.748270  6636 layer_factory.hpp:76] Creating layer conv1
I1127 11:11:29.748286  6636 net.cpp:106] Creating Layer conv1
I1127 11:11:29.748293  6636 net.cpp:454] conv1 <- data
I1127 11:11:29.748303  6636 net.cpp:411] conv1 -> conv1
I1127 11:11:29.748888  6636 net.cpp:150] Setting up conv1
I1127 11:11:29.748898  6636 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:11:29.748903  6636 net.cpp:165] Memory required for data: 3150080
I1127 11:11:29.748915  6636 layer_factory.hpp:76] Creating layer pool1
I1127 11:11:29.748924  6636 net.cpp:106] Creating Layer pool1
I1127 11:11:29.748929  6636 net.cpp:454] pool1 <- conv1
I1127 11:11:29.748936  6636 net.cpp:411] pool1 -> pool1
I1127 11:11:29.748982  6636 net.cpp:150] Setting up pool1
I1127 11:11:29.748991  6636 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:11:29.748996  6636 net.cpp:165] Memory required for data: 3887360
I1127 11:11:29.749001  6636 layer_factory.hpp:76] Creating layer conv2
I1127 11:11:29.749008  6636 net.cpp:106] Creating Layer conv2
I1127 11:11:29.749012  6636 net.cpp:454] conv2 <- pool1
I1127 11:11:29.749019  6636 net.cpp:411] conv2 -> conv2
I1127 11:11:29.749265  6636 net.cpp:150] Setting up conv2
I1127 11:11:29.749274  6636 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:11:29.749279  6636 net.cpp:165] Memory required for data: 4706560
I1127 11:11:29.749287  6636 layer_factory.hpp:76] Creating layer pool2
I1127 11:11:29.749296  6636 net.cpp:106] Creating Layer pool2
I1127 11:11:29.749300  6636 net.cpp:454] pool2 <- conv2
I1127 11:11:29.749306  6636 net.cpp:411] pool2 -> pool2
I1127 11:11:29.749474  6636 net.cpp:150] Setting up pool2
I1127 11:11:29.749490  6636 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:11:29.749496  6636 net.cpp:165] Memory required for data: 4911360
I1127 11:11:29.749503  6636 layer_factory.hpp:76] Creating layer ip1
I1127 11:11:29.749516  6636 net.cpp:106] Creating Layer ip1
I1127 11:11:29.749523  6636 net.cpp:454] ip1 <- pool2
I1127 11:11:29.749531  6636 net.cpp:411] ip1 -> ip1
I1127 11:11:29.751642  6636 net.cpp:150] Setting up ip1
I1127 11:11:29.751653  6636 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:11:29.751658  6636 net.cpp:165] Memory required for data: 5039360
I1127 11:11:29.751667  6636 layer_factory.hpp:76] Creating layer relu1
I1127 11:11:29.751677  6636 net.cpp:106] Creating Layer relu1
I1127 11:11:29.751682  6636 net.cpp:454] relu1 <- ip1
I1127 11:11:29.751688  6636 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:11:29.751695  6636 net.cpp:150] Setting up relu1
I1127 11:11:29.751701  6636 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:11:29.751705  6636 net.cpp:165] Memory required for data: 5167360
I1127 11:11:29.751709  6636 layer_factory.hpp:76] Creating layer ip2
I1127 11:11:29.751718  6636 net.cpp:106] Creating Layer ip2
I1127 11:11:29.751721  6636 net.cpp:454] ip2 <- ip1
I1127 11:11:29.751732  6636 net.cpp:411] ip2 -> ip2
I1127 11:11:29.752127  6636 net.cpp:150] Setting up ip2
I1127 11:11:29.752137  6636 net.cpp:157] Top shape: 64 10 (640)
I1127 11:11:29.752141  6636 net.cpp:165] Memory required for data: 5169920
I1127 11:11:29.752149  6636 layer_factory.hpp:76] Creating layer loss
I1127 11:11:29.752157  6636 net.cpp:106] Creating Layer loss
I1127 11:11:29.752161  6636 net.cpp:454] loss <- ip2
I1127 11:11:29.752167  6636 net.cpp:454] loss <- label
I1127 11:11:29.752176  6636 net.cpp:411] loss -> loss
I1127 11:11:29.752187  6636 layer_factory.hpp:76] Creating layer loss
I1127 11:11:29.752251  6636 net.cpp:150] Setting up loss
I1127 11:11:29.752259  6636 net.cpp:157] Top shape: (1)
I1127 11:11:29.752264  6636 net.cpp:160]     with loss weight 1
I1127 11:11:29.752279  6636 net.cpp:165] Memory required for data: 5169924
I1127 11:11:29.752285  6636 net.cpp:226] loss needs backward computation.
I1127 11:11:29.752288  6636 net.cpp:226] ip2 needs backward computation.
I1127 11:11:29.752293  6636 net.cpp:226] relu1 needs backward computation.
I1127 11:11:29.752297  6636 net.cpp:226] ip1 needs backward computation.
I1127 11:11:29.752301  6636 net.cpp:226] pool2 needs backward computation.
I1127 11:11:29.752305  6636 net.cpp:226] conv2 needs backward computation.
I1127 11:11:29.752310  6636 net.cpp:226] pool1 needs backward computation.
I1127 11:11:29.752315  6636 net.cpp:226] conv1 needs backward computation.
I1127 11:11:29.752320  6636 net.cpp:228] mnist does not need backward computation.
I1127 11:11:29.752323  6636 net.cpp:270] This network produces output loss
I1127 11:11:29.752331  6636 net.cpp:283] Network initialization done.
I1127 11:11:29.752568  6636 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:11:29.752589  6636 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:11:29.752697  6636 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:11:29.752759  6636 layer_factory.hpp:76] Creating layer mnist
I1127 11:11:29.752842  6636 net.cpp:106] Creating Layer mnist
I1127 11:11:29.752852  6636 net.cpp:411] mnist -> data
I1127 11:11:29.752861  6636 net.cpp:411] mnist -> label
I1127 11:11:29.753545  6642 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:11:29.753651  6636 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:11:29.802237  6636 net.cpp:150] Setting up mnist
I1127 11:11:29.802276  6636 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:11:29.802284  6636 net.cpp:157] Top shape: 100 (100)
I1127 11:11:29.802289  6636 net.cpp:165] Memory required for data: 314000
I1127 11:11:29.802295  6636 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:11:29.802312  6636 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:11:29.802320  6636 net.cpp:454] label_mnist_1_split <- label
I1127 11:11:29.802327  6636 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:11:29.802338  6636 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:11:29.802435  6636 net.cpp:150] Setting up label_mnist_1_split
I1127 11:11:29.802445  6636 net.cpp:157] Top shape: 100 (100)
I1127 11:11:29.802450  6636 net.cpp:157] Top shape: 100 (100)
I1127 11:11:29.802454  6636 net.cpp:165] Memory required for data: 314800
I1127 11:11:29.802459  6636 layer_factory.hpp:76] Creating layer conv1
I1127 11:11:29.802472  6636 net.cpp:106] Creating Layer conv1
I1127 11:11:29.802477  6636 net.cpp:454] conv1 <- data
I1127 11:11:29.802486  6636 net.cpp:411] conv1 -> conv1
I1127 11:11:29.802662  6636 net.cpp:150] Setting up conv1
I1127 11:11:29.802672  6636 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:11:29.802676  6636 net.cpp:165] Memory required for data: 4922800
I1127 11:11:29.802686  6636 layer_factory.hpp:76] Creating layer pool1
I1127 11:11:29.802695  6636 net.cpp:106] Creating Layer pool1
I1127 11:11:29.802700  6636 net.cpp:454] pool1 <- conv1
I1127 11:11:29.802719  6636 net.cpp:411] pool1 -> pool1
I1127 11:11:29.802752  6636 net.cpp:150] Setting up pool1
I1127 11:11:29.802758  6636 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:11:29.802762  6636 net.cpp:165] Memory required for data: 6074800
I1127 11:11:29.802767  6636 layer_factory.hpp:76] Creating layer conv2
I1127 11:11:29.802777  6636 net.cpp:106] Creating Layer conv2
I1127 11:11:29.802781  6636 net.cpp:454] conv2 <- pool1
I1127 11:11:29.802788  6636 net.cpp:411] conv2 -> conv2
I1127 11:11:29.803043  6636 net.cpp:150] Setting up conv2
I1127 11:11:29.803051  6636 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:11:29.803056  6636 net.cpp:165] Memory required for data: 7354800
I1127 11:11:29.803066  6636 layer_factory.hpp:76] Creating layer pool2
I1127 11:11:29.803072  6636 net.cpp:106] Creating Layer pool2
I1127 11:11:29.803076  6636 net.cpp:454] pool2 <- conv2
I1127 11:11:29.803083  6636 net.cpp:411] pool2 -> pool2
I1127 11:11:29.803127  6636 net.cpp:150] Setting up pool2
I1127 11:11:29.803134  6636 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:11:29.803138  6636 net.cpp:165] Memory required for data: 7674800
I1127 11:11:29.803143  6636 layer_factory.hpp:76] Creating layer ip1
I1127 11:11:29.803153  6636 net.cpp:106] Creating Layer ip1
I1127 11:11:29.803158  6636 net.cpp:454] ip1 <- pool2
I1127 11:11:29.803167  6636 net.cpp:411] ip1 -> ip1
I1127 11:11:29.805423  6636 net.cpp:150] Setting up ip1
I1127 11:11:29.805438  6636 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:11:29.805443  6636 net.cpp:165] Memory required for data: 7874800
I1127 11:11:29.805451  6636 layer_factory.hpp:76] Creating layer relu1
I1127 11:11:29.805459  6636 net.cpp:106] Creating Layer relu1
I1127 11:11:29.805464  6636 net.cpp:454] relu1 <- ip1
I1127 11:11:29.805470  6636 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:11:29.805479  6636 net.cpp:150] Setting up relu1
I1127 11:11:29.805483  6636 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:11:29.805488  6636 net.cpp:165] Memory required for data: 8074800
I1127 11:11:29.805492  6636 layer_factory.hpp:76] Creating layer ip2
I1127 11:11:29.805505  6636 net.cpp:106] Creating Layer ip2
I1127 11:11:29.805510  6636 net.cpp:454] ip2 <- ip1
I1127 11:11:29.805517  6636 net.cpp:411] ip2 -> ip2
I1127 11:11:29.805613  6636 net.cpp:150] Setting up ip2
I1127 11:11:29.805621  6636 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:11:29.805626  6636 net.cpp:165] Memory required for data: 8078800
I1127 11:11:29.805632  6636 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:11:29.805639  6636 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:11:29.805644  6636 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:11:29.805649  6636 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:11:29.805657  6636 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:11:29.805685  6636 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:11:29.805691  6636 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:11:29.805697  6636 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:11:29.805701  6636 net.cpp:165] Memory required for data: 8086800
I1127 11:11:29.805706  6636 layer_factory.hpp:76] Creating layer accuracy
I1127 11:11:29.805713  6636 net.cpp:106] Creating Layer accuracy
I1127 11:11:29.805718  6636 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:11:29.805724  6636 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:11:29.805730  6636 net.cpp:411] accuracy -> accuracy
I1127 11:11:29.805739  6636 net.cpp:150] Setting up accuracy
I1127 11:11:29.805744  6636 net.cpp:157] Top shape: (1)
I1127 11:11:29.805748  6636 net.cpp:165] Memory required for data: 8086804
I1127 11:11:29.805752  6636 layer_factory.hpp:76] Creating layer loss
I1127 11:11:29.805760  6636 net.cpp:106] Creating Layer loss
I1127 11:11:29.805765  6636 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:11:29.805770  6636 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:11:29.805776  6636 net.cpp:411] loss -> loss
I1127 11:11:29.805785  6636 layer_factory.hpp:76] Creating layer loss
I1127 11:11:29.805863  6636 net.cpp:150] Setting up loss
I1127 11:11:29.805871  6636 net.cpp:157] Top shape: (1)
I1127 11:11:29.805876  6636 net.cpp:160]     with loss weight 1
I1127 11:11:29.805892  6636 net.cpp:165] Memory required for data: 8086808
I1127 11:11:29.805897  6636 net.cpp:226] loss needs backward computation.
I1127 11:11:29.805904  6636 net.cpp:228] accuracy does not need backward computation.
I1127 11:11:29.805910  6636 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:11:29.805914  6636 net.cpp:226] ip2 needs backward computation.
I1127 11:11:29.805919  6636 net.cpp:226] relu1 needs backward computation.
I1127 11:11:29.805923  6636 net.cpp:226] ip1 needs backward computation.
I1127 11:11:29.805927  6636 net.cpp:226] pool2 needs backward computation.
I1127 11:11:29.805932  6636 net.cpp:226] conv2 needs backward computation.
I1127 11:11:29.805937  6636 net.cpp:226] pool1 needs backward computation.
I1127 11:11:29.805940  6636 net.cpp:226] conv1 needs backward computation.
I1127 11:11:29.805944  6636 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:11:29.805949  6636 net.cpp:228] mnist does not need backward computation.
I1127 11:11:29.805953  6636 net.cpp:270] This network produces output accuracy
I1127 11:11:29.805958  6636 net.cpp:270] This network produces output loss
I1127 11:11:29.805970  6636 net.cpp:283] Network initialization done.
I1127 11:11:29.806013  6636 solver.cpp:59] Solver scaffolding done.
I1127 11:11:29.806210  6636 caffe.cpp:212] Starting Optimization
I1127 11:11:29.806217  6636 solver.cpp:287] Solving LeNet
I1127 11:11:29.806221  6636 solver.cpp:288] Learning Rate Policy: inv
I1127 11:11:29.806565  6636 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:11:32.570483  6636 solver.cpp:408]     Test net output #0: accuracy = 0.1144
I1127 11:11:32.570617  6636 solver.cpp:408]     Test net output #1: loss = 2.26583 (* 1 = 2.26583 loss)
I1127 11:11:32.585892  6636 solver.cpp:236] Iteration 0, loss = 2.22313
I1127 11:11:32.586005  6636 solver.cpp:252]     Train net output #0: loss = 2.22313 (* 1 = 2.22313 loss)
I1127 11:11:32.586035  6636 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:11:44.025918  6636 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:11:45.439491  6636 solver.cpp:408]     Test net output #0: accuracy = 0.9742
I1127 11:11:45.439566  6636 solver.cpp:408]     Test net output #1: loss = 0.0826607 (* 1 = 0.0826607 loss)
I1127 11:11:45.469128  6636 solver.cpp:236] Iteration 500, loss = 0.0818404
I1127 11:11:45.469193  6636 solver.cpp:252]     Train net output #0: loss = 0.0818405 (* 1 = 0.0818405 loss)
I1127 11:11:45.469207  6636 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:11:58.566599  6636 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:11:58.579114  6636 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:11:58.589546  6636 solver.cpp:320] Iteration 1000, loss = 0.0996957
I1127 11:11:58.589664  6636 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:12:01.727471  6636 solver.cpp:408]     Test net output #0: accuracy = 0.98
I1127 11:12:01.727565  6636 solver.cpp:408]     Test net output #1: loss = 0.0570546 (* 1 = 0.0570546 loss)
I1127 11:12:01.727574  6636 solver.cpp:325] Optimization Done.
I1127 11:12:01.727579  6636 caffe.cpp:215] Optimization Done.
I1127 11:12:01.820135  6665 caffe.cpp:184] Using GPUs 0
I1127 11:12:02.122776  6665 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:12:02.122930  6665 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:12:02.123303  6665 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:12:02.123334  6665 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:12:02.123466  6665 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:12:02.123597  6665 layer_factory.hpp:76] Creating layer mnist
I1127 11:12:02.124132  6665 net.cpp:106] Creating Layer mnist
I1127 11:12:02.124173  6665 net.cpp:411] mnist -> data
I1127 11:12:02.124228  6665 net.cpp:411] mnist -> label
I1127 11:12:02.125542  6668 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:12:02.133602  6665 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:12:02.134686  6665 net.cpp:150] Setting up mnist
I1127 11:12:02.134794  6665 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:12:02.134809  6665 net.cpp:157] Top shape: 64 (64)
I1127 11:12:02.134817  6665 net.cpp:165] Memory required for data: 200960
I1127 11:12:02.134834  6665 layer_factory.hpp:76] Creating layer conv1
I1127 11:12:02.134867  6665 net.cpp:106] Creating Layer conv1
I1127 11:12:02.134884  6665 net.cpp:454] conv1 <- data
I1127 11:12:02.134908  6665 net.cpp:411] conv1 -> conv1
I1127 11:12:02.136276  6665 net.cpp:150] Setting up conv1
I1127 11:12:02.136339  6665 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:12:02.136349  6665 net.cpp:165] Memory required for data: 3150080
I1127 11:12:02.136389  6665 layer_factory.hpp:76] Creating layer pool1
I1127 11:12:02.136427  6665 net.cpp:106] Creating Layer pool1
I1127 11:12:02.136438  6665 net.cpp:454] pool1 <- conv1
I1127 11:12:02.136454  6665 net.cpp:411] pool1 -> pool1
I1127 11:12:02.136567  6665 net.cpp:150] Setting up pool1
I1127 11:12:02.136581  6665 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:12:02.136589  6665 net.cpp:165] Memory required for data: 3887360
I1127 11:12:02.136596  6665 layer_factory.hpp:76] Creating layer conv2
I1127 11:12:02.136622  6665 net.cpp:106] Creating Layer conv2
I1127 11:12:02.136632  6665 net.cpp:454] conv2 <- pool1
I1127 11:12:02.136644  6665 net.cpp:411] conv2 -> conv2
I1127 11:12:02.137259  6665 net.cpp:150] Setting up conv2
I1127 11:12:02.137277  6665 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:12:02.137285  6665 net.cpp:165] Memory required for data: 4706560
I1127 11:12:02.137301  6665 layer_factory.hpp:76] Creating layer pool2
I1127 11:12:02.137316  6665 net.cpp:106] Creating Layer pool2
I1127 11:12:02.137325  6665 net.cpp:454] pool2 <- conv2
I1127 11:12:02.137341  6665 net.cpp:411] pool2 -> pool2
I1127 11:12:02.137384  6665 net.cpp:150] Setting up pool2
I1127 11:12:02.137397  6665 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:12:02.137403  6665 net.cpp:165] Memory required for data: 4911360
I1127 11:12:02.137410  6665 layer_factory.hpp:76] Creating layer ip1
I1127 11:12:02.137430  6665 net.cpp:106] Creating Layer ip1
I1127 11:12:02.137441  6665 net.cpp:454] ip1 <- pool2
I1127 11:12:02.137454  6665 net.cpp:411] ip1 -> ip1
I1127 11:12:02.141479  6665 net.cpp:150] Setting up ip1
I1127 11:12:02.141554  6665 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:12:02.141564  6665 net.cpp:165] Memory required for data: 5039360
I1127 11:12:02.141592  6665 layer_factory.hpp:76] Creating layer relu1
I1127 11:12:02.141618  6665 net.cpp:106] Creating Layer relu1
I1127 11:12:02.141629  6665 net.cpp:454] relu1 <- ip1
I1127 11:12:02.141644  6665 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:12:02.141674  6665 net.cpp:150] Setting up relu1
I1127 11:12:02.141683  6665 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:12:02.141690  6665 net.cpp:165] Memory required for data: 5167360
I1127 11:12:02.141696  6665 layer_factory.hpp:76] Creating layer ip2
I1127 11:12:02.141711  6665 net.cpp:106] Creating Layer ip2
I1127 11:12:02.141721  6665 net.cpp:454] ip2 <- ip1
I1127 11:12:02.141729  6665 net.cpp:411] ip2 -> ip2
I1127 11:12:02.142956  6665 net.cpp:150] Setting up ip2
I1127 11:12:02.143007  6665 net.cpp:157] Top shape: 64 10 (640)
I1127 11:12:02.143017  6665 net.cpp:165] Memory required for data: 5169920
I1127 11:12:02.143033  6665 layer_factory.hpp:76] Creating layer loss
I1127 11:12:02.143055  6665 net.cpp:106] Creating Layer loss
I1127 11:12:02.143065  6665 net.cpp:454] loss <- ip2
I1127 11:12:02.143074  6665 net.cpp:454] loss <- label
I1127 11:12:02.143091  6665 net.cpp:411] loss -> loss
I1127 11:12:02.143131  6665 layer_factory.hpp:76] Creating layer loss
I1127 11:12:02.143234  6665 net.cpp:150] Setting up loss
I1127 11:12:02.143246  6665 net.cpp:157] Top shape: (1)
I1127 11:12:02.143252  6665 net.cpp:160]     with loss weight 1
I1127 11:12:02.143278  6665 net.cpp:165] Memory required for data: 5169924
I1127 11:12:02.143285  6665 net.cpp:226] loss needs backward computation.
I1127 11:12:02.143292  6665 net.cpp:226] ip2 needs backward computation.
I1127 11:12:02.143299  6665 net.cpp:226] relu1 needs backward computation.
I1127 11:12:02.143306  6665 net.cpp:226] ip1 needs backward computation.
I1127 11:12:02.143312  6665 net.cpp:226] pool2 needs backward computation.
I1127 11:12:02.143319  6665 net.cpp:226] conv2 needs backward computation.
I1127 11:12:02.143326  6665 net.cpp:226] pool1 needs backward computation.
I1127 11:12:02.143332  6665 net.cpp:226] conv1 needs backward computation.
I1127 11:12:02.143338  6665 net.cpp:228] mnist does not need backward computation.
I1127 11:12:02.143344  6665 net.cpp:270] This network produces output loss
I1127 11:12:02.143357  6665 net.cpp:283] Network initialization done.
I1127 11:12:02.143743  6665 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:12:02.143793  6665 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:12:02.143990  6665 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:12:02.144084  6665 layer_factory.hpp:76] Creating layer mnist
I1127 11:12:02.144246  6665 net.cpp:106] Creating Layer mnist
I1127 11:12:02.144261  6665 net.cpp:411] mnist -> data
I1127 11:12:02.144278  6665 net.cpp:411] mnist -> label
I1127 11:12:02.145400  6670 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:12:02.145578  6665 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:12:02.147307  6665 net.cpp:150] Setting up mnist
I1127 11:12:02.147394  6665 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:12:02.147410  6665 net.cpp:157] Top shape: 100 (100)
I1127 11:12:02.147441  6665 net.cpp:165] Memory required for data: 314000
I1127 11:12:02.147454  6665 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:12:02.147477  6665 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:12:02.147485  6665 net.cpp:454] label_mnist_1_split <- label
I1127 11:12:02.147498  6665 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:12:02.147512  6665 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:12:02.147568  6665 net.cpp:150] Setting up label_mnist_1_split
I1127 11:12:02.147579  6665 net.cpp:157] Top shape: 100 (100)
I1127 11:12:02.147586  6665 net.cpp:157] Top shape: 100 (100)
I1127 11:12:02.147593  6665 net.cpp:165] Memory required for data: 314800
I1127 11:12:02.147598  6665 layer_factory.hpp:76] Creating layer conv1
I1127 11:12:02.147616  6665 net.cpp:106] Creating Layer conv1
I1127 11:12:02.147624  6665 net.cpp:454] conv1 <- data
I1127 11:12:02.147635  6665 net.cpp:411] conv1 -> conv1
I1127 11:12:02.147872  6665 net.cpp:150] Setting up conv1
I1127 11:12:02.147886  6665 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:12:02.147891  6665 net.cpp:165] Memory required for data: 4922800
I1127 11:12:02.147905  6665 layer_factory.hpp:76] Creating layer pool1
I1127 11:12:02.147918  6665 net.cpp:106] Creating Layer pool1
I1127 11:12:02.147925  6665 net.cpp:454] pool1 <- conv1
I1127 11:12:02.147966  6665 net.cpp:411] pool1 -> pool1
I1127 11:12:02.148023  6665 net.cpp:150] Setting up pool1
I1127 11:12:02.148038  6665 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:12:02.148046  6665 net.cpp:165] Memory required for data: 6074800
I1127 11:12:02.148054  6665 layer_factory.hpp:76] Creating layer conv2
I1127 11:12:02.148074  6665 net.cpp:106] Creating Layer conv2
I1127 11:12:02.148084  6665 net.cpp:454] conv2 <- pool1
I1127 11:12:02.148099  6665 net.cpp:411] conv2 -> conv2
I1127 11:12:02.148777  6665 net.cpp:150] Setting up conv2
I1127 11:12:02.148802  6665 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:12:02.148810  6665 net.cpp:165] Memory required for data: 7354800
I1127 11:12:02.148828  6665 layer_factory.hpp:76] Creating layer pool2
I1127 11:12:02.148844  6665 net.cpp:106] Creating Layer pool2
I1127 11:12:02.148854  6665 net.cpp:454] pool2 <- conv2
I1127 11:12:02.148867  6665 net.cpp:411] pool2 -> pool2
I1127 11:12:02.148916  6665 net.cpp:150] Setting up pool2
I1127 11:12:02.148929  6665 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:12:02.148936  6665 net.cpp:165] Memory required for data: 7674800
I1127 11:12:02.148944  6665 layer_factory.hpp:76] Creating layer ip1
I1127 11:12:02.148967  6665 net.cpp:106] Creating Layer ip1
I1127 11:12:02.148975  6665 net.cpp:454] ip1 <- pool2
I1127 11:12:02.148990  6665 net.cpp:411] ip1 -> ip1
I1127 11:12:02.153331  6665 net.cpp:150] Setting up ip1
I1127 11:12:02.153445  6665 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:12:02.153460  6665 net.cpp:165] Memory required for data: 7874800
I1127 11:12:02.153493  6665 layer_factory.hpp:76] Creating layer relu1
I1127 11:12:02.153518  6665 net.cpp:106] Creating Layer relu1
I1127 11:12:02.153532  6665 net.cpp:454] relu1 <- ip1
I1127 11:12:02.153548  6665 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:12:02.153573  6665 net.cpp:150] Setting up relu1
I1127 11:12:02.153583  6665 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:12:02.153589  6665 net.cpp:165] Memory required for data: 8074800
I1127 11:12:02.153595  6665 layer_factory.hpp:76] Creating layer ip2
I1127 11:12:02.153615  6665 net.cpp:106] Creating Layer ip2
I1127 11:12:02.153623  6665 net.cpp:454] ip2 <- ip1
I1127 11:12:02.153638  6665 net.cpp:411] ip2 -> ip2
I1127 11:12:02.153830  6665 net.cpp:150] Setting up ip2
I1127 11:12:02.153844  6665 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:12:02.153851  6665 net.cpp:165] Memory required for data: 8078800
I1127 11:12:02.153861  6665 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:12:02.153874  6665 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:12:02.153880  6665 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:12:02.153908  6665 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:12:02.153919  6665 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:12:02.153966  6665 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:12:02.153978  6665 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:12:02.153986  6665 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:12:02.153992  6665 net.cpp:165] Memory required for data: 8086800
I1127 11:12:02.153998  6665 layer_factory.hpp:76] Creating layer accuracy
I1127 11:12:02.154014  6665 net.cpp:106] Creating Layer accuracy
I1127 11:12:02.154021  6665 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:12:02.154029  6665 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:12:02.154038  6665 net.cpp:411] accuracy -> accuracy
I1127 11:12:02.154054  6665 net.cpp:150] Setting up accuracy
I1127 11:12:02.154067  6665 net.cpp:157] Top shape: (1)
I1127 11:12:02.154075  6665 net.cpp:165] Memory required for data: 8086804
I1127 11:12:02.154084  6665 layer_factory.hpp:76] Creating layer loss
I1127 11:12:02.154103  6665 net.cpp:106] Creating Layer loss
I1127 11:12:02.154114  6665 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:12:02.154124  6665 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:12:02.154136  6665 net.cpp:411] loss -> loss
I1127 11:12:02.154173  6665 layer_factory.hpp:76] Creating layer loss
I1127 11:12:02.154332  6665 net.cpp:150] Setting up loss
I1127 11:12:02.154346  6665 net.cpp:157] Top shape: (1)
I1127 11:12:02.154355  6665 net.cpp:160]     with loss weight 1
I1127 11:12:02.154381  6665 net.cpp:165] Memory required for data: 8086808
I1127 11:12:02.154388  6665 net.cpp:226] loss needs backward computation.
I1127 11:12:02.154404  6665 net.cpp:228] accuracy does not need backward computation.
I1127 11:12:02.154414  6665 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:12:02.154422  6665 net.cpp:226] ip2 needs backward computation.
I1127 11:12:02.154428  6665 net.cpp:226] relu1 needs backward computation.
I1127 11:12:02.154436  6665 net.cpp:226] ip1 needs backward computation.
I1127 11:12:02.154445  6665 net.cpp:226] pool2 needs backward computation.
I1127 11:12:02.154453  6665 net.cpp:226] conv2 needs backward computation.
I1127 11:12:02.154461  6665 net.cpp:226] pool1 needs backward computation.
I1127 11:12:02.154469  6665 net.cpp:226] conv1 needs backward computation.
I1127 11:12:02.154479  6665 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:12:02.154489  6665 net.cpp:228] mnist does not need backward computation.
I1127 11:12:02.154495  6665 net.cpp:270] This network produces output accuracy
I1127 11:12:02.154503  6665 net.cpp:270] This network produces output loss
I1127 11:12:02.154527  6665 net.cpp:283] Network initialization done.
I1127 11:12:02.154642  6665 solver.cpp:59] Solver scaffolding done.
I1127 11:12:02.155030  6665 caffe.cpp:212] Starting Optimization
I1127 11:12:02.155056  6665 solver.cpp:287] Solving LeNet
I1127 11:12:02.155069  6665 solver.cpp:288] Learning Rate Policy: inv
I1127 11:12:02.156010  6665 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:12:03.439882  6665 solver.cpp:408]     Test net output #0: accuracy = 0.091
I1127 11:12:03.439920  6665 solver.cpp:408]     Test net output #1: loss = 2.38302 (* 1 = 2.38302 loss)
I1127 11:12:03.471400  6665 solver.cpp:236] Iteration 0, loss = 2.33016
I1127 11:12:03.471418  6665 solver.cpp:252]     Train net output #0: loss = 2.33016 (* 1 = 2.33016 loss)
I1127 11:12:03.471434  6665 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:12:16.695616  6665 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:12:18.732373  6665 solver.cpp:408]     Test net output #0: accuracy = 0.9733
I1127 11:12:18.732413  6665 solver.cpp:408]     Test net output #1: loss = 0.0857854 (* 1 = 0.0857854 loss)
I1127 11:12:18.760911  6665 solver.cpp:236] Iteration 500, loss = 0.094633
I1127 11:12:18.760929  6665 solver.cpp:252]     Train net output #0: loss = 0.0946331 (* 1 = 0.0946331 loss)
I1127 11:12:18.760939  6665 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:12:31.355857  6665 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:12:31.370298  6665 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:12:31.380234  6665 solver.cpp:320] Iteration 1000, loss = 0.0876675
I1127 11:12:31.380322  6665 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:12:32.472081  6665 solver.cpp:408]     Test net output #0: accuracy = 0.9813
I1127 11:12:32.472580  6665 solver.cpp:408]     Test net output #1: loss = 0.058195 (* 1 = 0.058195 loss)
I1127 11:12:32.472618  6665 solver.cpp:325] Optimization Done.
I1127 11:12:32.472637  6665 caffe.cpp:215] Optimization Done.
I1127 11:12:32.588289  6691 caffe.cpp:184] Using GPUs 0
I1127 11:12:33.000218  6691 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:12:33.000532  6691 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:12:33.001094  6691 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:12:33.001135  6691 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:12:33.001312  6691 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:12:33.001435  6691 layer_factory.hpp:76] Creating layer mnist
I1127 11:12:33.046363  6691 net.cpp:106] Creating Layer mnist
I1127 11:12:33.046493  6691 net.cpp:411] mnist -> data
I1127 11:12:33.046597  6691 net.cpp:411] mnist -> label
I1127 11:12:33.047806  6695 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:12:33.089066  6691 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:12:33.097661  6691 net.cpp:150] Setting up mnist
I1127 11:12:33.097844  6691 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:12:33.097941  6691 net.cpp:157] Top shape: 64 (64)
I1127 11:12:33.097970  6691 net.cpp:165] Memory required for data: 200960
I1127 11:12:33.098013  6691 layer_factory.hpp:76] Creating layer conv1
I1127 11:12:33.098093  6691 net.cpp:106] Creating Layer conv1
I1127 11:12:33.098129  6691 net.cpp:454] conv1 <- data
I1127 11:12:33.098192  6691 net.cpp:411] conv1 -> conv1
I1127 11:12:33.101011  6691 net.cpp:150] Setting up conv1
I1127 11:12:33.101122  6691 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:12:33.101141  6691 net.cpp:165] Memory required for data: 3150080
I1127 11:12:33.101189  6691 layer_factory.hpp:76] Creating layer pool1
I1127 11:12:33.101233  6691 net.cpp:106] Creating Layer pool1
I1127 11:12:33.101248  6691 net.cpp:454] pool1 <- conv1
I1127 11:12:33.101272  6691 net.cpp:411] pool1 -> pool1
I1127 11:12:33.101429  6691 net.cpp:150] Setting up pool1
I1127 11:12:33.101451  6691 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:12:33.101457  6691 net.cpp:165] Memory required for data: 3887360
I1127 11:12:33.101469  6691 layer_factory.hpp:76] Creating layer conv2
I1127 11:12:33.101495  6691 net.cpp:106] Creating Layer conv2
I1127 11:12:33.101503  6691 net.cpp:454] conv2 <- pool1
I1127 11:12:33.101518  6691 net.cpp:411] conv2 -> conv2
I1127 11:12:33.102035  6691 net.cpp:150] Setting up conv2
I1127 11:12:33.102118  6691 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:12:33.102128  6691 net.cpp:165] Memory required for data: 4706560
I1127 11:12:33.102192  6691 layer_factory.hpp:76] Creating layer pool2
I1127 11:12:33.102224  6691 net.cpp:106] Creating Layer pool2
I1127 11:12:33.102236  6691 net.cpp:454] pool2 <- conv2
I1127 11:12:33.102252  6691 net.cpp:411] pool2 -> pool2
I1127 11:12:33.102351  6691 net.cpp:150] Setting up pool2
I1127 11:12:33.102375  6691 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:12:33.102391  6691 net.cpp:165] Memory required for data: 4911360
I1127 11:12:33.102411  6691 layer_factory.hpp:76] Creating layer ip1
I1127 11:12:33.102452  6691 net.cpp:106] Creating Layer ip1
I1127 11:12:33.102470  6691 net.cpp:454] ip1 <- pool2
I1127 11:12:33.102504  6691 net.cpp:411] ip1 -> ip1
I1127 11:12:33.108045  6691 net.cpp:150] Setting up ip1
I1127 11:12:33.108141  6691 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:12:33.108152  6691 net.cpp:165] Memory required for data: 5039360
I1127 11:12:33.108186  6691 layer_factory.hpp:76] Creating layer relu1
I1127 11:12:33.108213  6691 net.cpp:106] Creating Layer relu1
I1127 11:12:33.108229  6691 net.cpp:454] relu1 <- ip1
I1127 11:12:33.108243  6691 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:12:33.108278  6691 net.cpp:150] Setting up relu1
I1127 11:12:33.108291  6691 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:12:33.108299  6691 net.cpp:165] Memory required for data: 5167360
I1127 11:12:33.108305  6691 layer_factory.hpp:76] Creating layer ip2
I1127 11:12:33.108324  6691 net.cpp:106] Creating Layer ip2
I1127 11:12:33.108333  6691 net.cpp:454] ip2 <- ip1
I1127 11:12:33.108347  6691 net.cpp:411] ip2 -> ip2
I1127 11:12:33.109838  6691 net.cpp:150] Setting up ip2
I1127 11:12:33.109962  6691 net.cpp:157] Top shape: 64 10 (640)
I1127 11:12:33.109971  6691 net.cpp:165] Memory required for data: 5169920
I1127 11:12:33.109997  6691 layer_factory.hpp:76] Creating layer loss
I1127 11:12:33.110039  6691 net.cpp:106] Creating Layer loss
I1127 11:12:33.110052  6691 net.cpp:454] loss <- ip2
I1127 11:12:33.110069  6691 net.cpp:454] loss <- label
I1127 11:12:33.110093  6691 net.cpp:411] loss -> loss
I1127 11:12:33.110160  6691 layer_factory.hpp:76] Creating layer loss
I1127 11:12:33.110340  6691 net.cpp:150] Setting up loss
I1127 11:12:33.110357  6691 net.cpp:157] Top shape: (1)
I1127 11:12:33.110364  6691 net.cpp:160]     with loss weight 1
I1127 11:12:33.110430  6691 net.cpp:165] Memory required for data: 5169924
I1127 11:12:33.110438  6691 net.cpp:226] loss needs backward computation.
I1127 11:12:33.110447  6691 net.cpp:226] ip2 needs backward computation.
I1127 11:12:33.110455  6691 net.cpp:226] relu1 needs backward computation.
I1127 11:12:33.110462  6691 net.cpp:226] ip1 needs backward computation.
I1127 11:12:33.110494  6691 net.cpp:226] pool2 needs backward computation.
I1127 11:12:33.110503  6691 net.cpp:226] conv2 needs backward computation.
I1127 11:12:33.110510  6691 net.cpp:226] pool1 needs backward computation.
I1127 11:12:33.110518  6691 net.cpp:226] conv1 needs backward computation.
I1127 11:12:33.110527  6691 net.cpp:228] mnist does not need backward computation.
I1127 11:12:33.110534  6691 net.cpp:270] This network produces output loss
I1127 11:12:33.110554  6691 net.cpp:283] Network initialization done.
I1127 11:12:33.111160  6691 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:12:33.111268  6691 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:12:33.111600  6691 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:12:33.111824  6691 layer_factory.hpp:76] Creating layer mnist
I1127 11:12:33.112196  6691 net.cpp:106] Creating Layer mnist
I1127 11:12:33.112218  6691 net.cpp:411] mnist -> data
I1127 11:12:33.112246  6691 net.cpp:411] mnist -> label
I1127 11:12:33.115500  6698 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:12:33.118350  6691 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:12:33.123569  6691 net.cpp:150] Setting up mnist
I1127 11:12:33.123617  6691 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:12:33.123626  6691 net.cpp:157] Top shape: 100 (100)
I1127 11:12:33.123631  6691 net.cpp:165] Memory required for data: 314000
I1127 11:12:33.123643  6691 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:12:33.123670  6691 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:12:33.123678  6691 net.cpp:454] label_mnist_1_split <- label
I1127 11:12:33.123689  6691 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:12:33.123708  6691 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:12:33.123810  6691 net.cpp:150] Setting up label_mnist_1_split
I1127 11:12:33.123829  6691 net.cpp:157] Top shape: 100 (100)
I1127 11:12:33.123836  6691 net.cpp:157] Top shape: 100 (100)
I1127 11:12:33.123839  6691 net.cpp:165] Memory required for data: 314800
I1127 11:12:33.123844  6691 layer_factory.hpp:76] Creating layer conv1
I1127 11:12:33.123860  6691 net.cpp:106] Creating Layer conv1
I1127 11:12:33.123865  6691 net.cpp:454] conv1 <- data
I1127 11:12:33.123878  6691 net.cpp:411] conv1 -> conv1
I1127 11:12:33.124091  6691 net.cpp:150] Setting up conv1
I1127 11:12:33.124106  6691 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:12:33.124110  6691 net.cpp:165] Memory required for data: 4922800
I1127 11:12:33.124124  6691 layer_factory.hpp:76] Creating layer pool1
I1127 11:12:33.124136  6691 net.cpp:106] Creating Layer pool1
I1127 11:12:33.124141  6691 net.cpp:454] pool1 <- conv1
I1127 11:12:33.124168  6691 net.cpp:411] pool1 -> pool1
I1127 11:12:33.124205  6691 net.cpp:150] Setting up pool1
I1127 11:12:33.124212  6691 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:12:33.124217  6691 net.cpp:165] Memory required for data: 6074800
I1127 11:12:33.124222  6691 layer_factory.hpp:76] Creating layer conv2
I1127 11:12:33.124233  6691 net.cpp:106] Creating Layer conv2
I1127 11:12:33.124238  6691 net.cpp:454] conv2 <- pool1
I1127 11:12:33.124243  6691 net.cpp:411] conv2 -> conv2
I1127 11:12:33.124521  6691 net.cpp:150] Setting up conv2
I1127 11:12:33.124531  6691 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:12:33.124536  6691 net.cpp:165] Memory required for data: 7354800
I1127 11:12:33.124546  6691 layer_factory.hpp:76] Creating layer pool2
I1127 11:12:33.124553  6691 net.cpp:106] Creating Layer pool2
I1127 11:12:33.124558  6691 net.cpp:454] pool2 <- conv2
I1127 11:12:33.124563  6691 net.cpp:411] pool2 -> pool2
I1127 11:12:33.124590  6691 net.cpp:150] Setting up pool2
I1127 11:12:33.124598  6691 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:12:33.124603  6691 net.cpp:165] Memory required for data: 7674800
I1127 11:12:33.124608  6691 layer_factory.hpp:76] Creating layer ip1
I1127 11:12:33.124618  6691 net.cpp:106] Creating Layer ip1
I1127 11:12:33.124622  6691 net.cpp:454] ip1 <- pool2
I1127 11:12:33.124630  6691 net.cpp:411] ip1 -> ip1
I1127 11:12:33.128422  6691 net.cpp:150] Setting up ip1
I1127 11:12:33.128547  6691 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:12:33.128559  6691 net.cpp:165] Memory required for data: 7874800
I1127 11:12:33.128594  6691 layer_factory.hpp:76] Creating layer relu1
I1127 11:12:33.128626  6691 net.cpp:106] Creating Layer relu1
I1127 11:12:33.128643  6691 net.cpp:454] relu1 <- ip1
I1127 11:12:33.128660  6691 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:12:33.128690  6691 net.cpp:150] Setting up relu1
I1127 11:12:33.128703  6691 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:12:33.128711  6691 net.cpp:165] Memory required for data: 8074800
I1127 11:12:33.128721  6691 layer_factory.hpp:76] Creating layer ip2
I1127 11:12:33.128751  6691 net.cpp:106] Creating Layer ip2
I1127 11:12:33.128762  6691 net.cpp:454] ip2 <- ip1
I1127 11:12:33.128780  6691 net.cpp:411] ip2 -> ip2
I1127 11:12:33.129082  6691 net.cpp:150] Setting up ip2
I1127 11:12:33.129117  6691 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:12:33.129125  6691 net.cpp:165] Memory required for data: 8078800
I1127 11:12:33.129142  6691 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:12:33.129160  6691 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:12:33.129169  6691 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:12:33.129182  6691 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:12:33.129245  6691 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:12:33.129307  6691 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:12:33.129319  6691 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:12:33.129328  6691 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:12:33.129334  6691 net.cpp:165] Memory required for data: 8086800
I1127 11:12:33.129341  6691 layer_factory.hpp:76] Creating layer accuracy
I1127 11:12:33.129358  6691 net.cpp:106] Creating Layer accuracy
I1127 11:12:33.129389  6691 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:12:33.129400  6691 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:12:33.129411  6691 net.cpp:411] accuracy -> accuracy
I1127 11:12:33.129428  6691 net.cpp:150] Setting up accuracy
I1127 11:12:33.129438  6691 net.cpp:157] Top shape: (1)
I1127 11:12:33.129446  6691 net.cpp:165] Memory required for data: 8086804
I1127 11:12:33.129454  6691 layer_factory.hpp:76] Creating layer loss
I1127 11:12:33.129468  6691 net.cpp:106] Creating Layer loss
I1127 11:12:33.129477  6691 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:12:33.129485  6691 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:12:33.129497  6691 net.cpp:411] loss -> loss
I1127 11:12:33.129516  6691 layer_factory.hpp:76] Creating layer loss
I1127 11:12:33.129703  6691 net.cpp:150] Setting up loss
I1127 11:12:33.129719  6691 net.cpp:157] Top shape: (1)
I1127 11:12:33.129725  6691 net.cpp:160]     with loss weight 1
I1127 11:12:33.129763  6691 net.cpp:165] Memory required for data: 8086808
I1127 11:12:33.129771  6691 net.cpp:226] loss needs backward computation.
I1127 11:12:33.129789  6691 net.cpp:228] accuracy does not need backward computation.
I1127 11:12:33.129799  6691 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:12:33.129807  6691 net.cpp:226] ip2 needs backward computation.
I1127 11:12:33.129814  6691 net.cpp:226] relu1 needs backward computation.
I1127 11:12:33.129822  6691 net.cpp:226] ip1 needs backward computation.
I1127 11:12:33.129830  6691 net.cpp:226] pool2 needs backward computation.
I1127 11:12:33.129838  6691 net.cpp:226] conv2 needs backward computation.
I1127 11:12:33.129848  6691 net.cpp:226] pool1 needs backward computation.
I1127 11:12:33.129855  6691 net.cpp:226] conv1 needs backward computation.
I1127 11:12:33.129864  6691 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:12:33.129875  6691 net.cpp:228] mnist does not need backward computation.
I1127 11:12:33.129884  6691 net.cpp:270] This network produces output accuracy
I1127 11:12:33.129891  6691 net.cpp:270] This network produces output loss
I1127 11:12:33.129909  6691 net.cpp:283] Network initialization done.
I1127 11:12:33.130034  6691 solver.cpp:59] Solver scaffolding done.
I1127 11:12:33.130404  6691 caffe.cpp:212] Starting Optimization
I1127 11:12:33.130425  6691 solver.cpp:287] Solving LeNet
I1127 11:12:33.130434  6691 solver.cpp:288] Learning Rate Policy: inv
I1127 11:12:33.131355  6691 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:12:36.002372  6691 solver.cpp:408]     Test net output #0: accuracy = 0.1297
I1127 11:12:36.002475  6691 solver.cpp:408]     Test net output #1: loss = 2.31833 (* 1 = 2.31833 loss)
I1127 11:12:36.013622  6691 solver.cpp:236] Iteration 0, loss = 2.34358
I1127 11:12:36.013710  6691 solver.cpp:252]     Train net output #0: loss = 2.34358 (* 1 = 2.34358 loss)
I1127 11:12:36.013743  6691 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:12:49.427774  6691 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:12:50.509359  6691 solver.cpp:408]     Test net output #0: accuracy = 0.9731
I1127 11:12:50.509409  6691 solver.cpp:408]     Test net output #1: loss = 0.0850107 (* 1 = 0.0850107 loss)
I1127 11:12:50.519327  6691 solver.cpp:236] Iteration 500, loss = 0.0876844
I1127 11:12:50.519425  6691 solver.cpp:252]     Train net output #0: loss = 0.0876844 (* 1 = 0.0876844 loss)
I1127 11:12:50.519449  6691 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:13:04.012495  6691 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:13:04.024567  6691 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:13:04.035838  6691 solver.cpp:320] Iteration 1000, loss = 0.0872012
I1127 11:13:04.035956  6691 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:13:04.919025  6691 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:13:05.182857  6691 solver.cpp:408]     Test net output #0: accuracy = 0.9817
I1127 11:13:05.182912  6691 solver.cpp:408]     Test net output #1: loss = 0.0579736 (* 1 = 0.0579736 loss)
I1127 11:13:05.182920  6691 solver.cpp:325] Optimization Done.
I1127 11:13:05.182924  6691 caffe.cpp:215] Optimization Done.
I1127 11:13:05.249580  6721 caffe.cpp:184] Using GPUs 0
I1127 11:13:05.707754  6721 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:13:05.707886  6721 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:13:05.708268  6721 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:13:05.708289  6721 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:13:05.708423  6721 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:13:05.708498  6721 layer_factory.hpp:76] Creating layer mnist
I1127 11:13:05.708958  6721 net.cpp:106] Creating Layer mnist
I1127 11:13:05.708974  6721 net.cpp:411] mnist -> data
I1127 11:13:05.709004  6721 net.cpp:411] mnist -> label
I1127 11:13:05.709708  6724 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:13:05.745285  6721 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:13:05.802259  6721 net.cpp:150] Setting up mnist
I1127 11:13:05.802314  6721 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:13:05.802330  6721 net.cpp:157] Top shape: 64 (64)
I1127 11:13:05.802340  6721 net.cpp:165] Memory required for data: 200960
I1127 11:13:05.802355  6721 layer_factory.hpp:76] Creating layer conv1
I1127 11:13:05.802377  6721 net.cpp:106] Creating Layer conv1
I1127 11:13:05.802387  6721 net.cpp:454] conv1 <- data
I1127 11:13:05.802405  6721 net.cpp:411] conv1 -> conv1
I1127 11:13:05.803318  6721 net.cpp:150] Setting up conv1
I1127 11:13:05.803335  6721 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:13:05.803352  6721 net.cpp:165] Memory required for data: 3150080
I1127 11:13:05.803372  6721 layer_factory.hpp:76] Creating layer pool1
I1127 11:13:05.803385  6721 net.cpp:106] Creating Layer pool1
I1127 11:13:05.803393  6721 net.cpp:454] pool1 <- conv1
I1127 11:13:05.803403  6721 net.cpp:411] pool1 -> pool1
I1127 11:13:05.803478  6721 net.cpp:150] Setting up pool1
I1127 11:13:05.803491  6721 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:13:05.803498  6721 net.cpp:165] Memory required for data: 3887360
I1127 11:13:05.803506  6721 layer_factory.hpp:76] Creating layer conv2
I1127 11:13:05.803521  6721 net.cpp:106] Creating Layer conv2
I1127 11:13:05.803532  6721 net.cpp:454] conv2 <- pool1
I1127 11:13:05.803544  6721 net.cpp:411] conv2 -> conv2
I1127 11:13:05.803972  6721 net.cpp:150] Setting up conv2
I1127 11:13:05.803988  6721 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:13:05.803995  6721 net.cpp:165] Memory required for data: 4706560
I1127 11:13:05.804010  6721 layer_factory.hpp:76] Creating layer pool2
I1127 11:13:05.804024  6721 net.cpp:106] Creating Layer pool2
I1127 11:13:05.804031  6721 net.cpp:454] pool2 <- conv2
I1127 11:13:05.804041  6721 net.cpp:411] pool2 -> pool2
I1127 11:13:05.804113  6721 net.cpp:150] Setting up pool2
I1127 11:13:05.804126  6721 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:13:05.804133  6721 net.cpp:165] Memory required for data: 4911360
I1127 11:13:05.804141  6721 layer_factory.hpp:76] Creating layer ip1
I1127 11:13:05.804155  6721 net.cpp:106] Creating Layer ip1
I1127 11:13:05.804163  6721 net.cpp:454] ip1 <- pool2
I1127 11:13:05.804173  6721 net.cpp:411] ip1 -> ip1
I1127 11:13:05.807914  6721 net.cpp:150] Setting up ip1
I1127 11:13:05.807931  6721 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:13:05.807939  6721 net.cpp:165] Memory required for data: 5039360
I1127 11:13:05.807953  6721 layer_factory.hpp:76] Creating layer relu1
I1127 11:13:05.807966  6721 net.cpp:106] Creating Layer relu1
I1127 11:13:05.807976  6721 net.cpp:454] relu1 <- ip1
I1127 11:13:05.807989  6721 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:13:05.808002  6721 net.cpp:150] Setting up relu1
I1127 11:13:05.808012  6721 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:13:05.808018  6721 net.cpp:165] Memory required for data: 5167360
I1127 11:13:05.808024  6721 layer_factory.hpp:76] Creating layer ip2
I1127 11:13:05.808035  6721 net.cpp:106] Creating Layer ip2
I1127 11:13:05.808043  6721 net.cpp:454] ip2 <- ip1
I1127 11:13:05.808053  6721 net.cpp:411] ip2 -> ip2
I1127 11:13:05.808660  6721 net.cpp:150] Setting up ip2
I1127 11:13:05.808675  6721 net.cpp:157] Top shape: 64 10 (640)
I1127 11:13:05.808682  6721 net.cpp:165] Memory required for data: 5169920
I1127 11:13:05.808694  6721 layer_factory.hpp:76] Creating layer loss
I1127 11:13:05.808712  6721 net.cpp:106] Creating Layer loss
I1127 11:13:05.808720  6721 net.cpp:454] loss <- ip2
I1127 11:13:05.808730  6721 net.cpp:454] loss <- label
I1127 11:13:05.808743  6721 net.cpp:411] loss -> loss
I1127 11:13:05.808760  6721 layer_factory.hpp:76] Creating layer loss
I1127 11:13:05.808867  6721 net.cpp:150] Setting up loss
I1127 11:13:05.808881  6721 net.cpp:157] Top shape: (1)
I1127 11:13:05.808888  6721 net.cpp:160]     with loss weight 1
I1127 11:13:05.808909  6721 net.cpp:165] Memory required for data: 5169924
I1127 11:13:05.808917  6721 net.cpp:226] loss needs backward computation.
I1127 11:13:05.808924  6721 net.cpp:226] ip2 needs backward computation.
I1127 11:13:05.808931  6721 net.cpp:226] relu1 needs backward computation.
I1127 11:13:05.808938  6721 net.cpp:226] ip1 needs backward computation.
I1127 11:13:05.808945  6721 net.cpp:226] pool2 needs backward computation.
I1127 11:13:05.808953  6721 net.cpp:226] conv2 needs backward computation.
I1127 11:13:05.808959  6721 net.cpp:226] pool1 needs backward computation.
I1127 11:13:05.808966  6721 net.cpp:226] conv1 needs backward computation.
I1127 11:13:05.808974  6721 net.cpp:228] mnist does not need backward computation.
I1127 11:13:05.808980  6721 net.cpp:270] This network produces output loss
I1127 11:13:05.809000  6721 net.cpp:283] Network initialization done.
I1127 11:13:05.809375  6721 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:13:05.809407  6721 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:13:05.809581  6721 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:13:05.809676  6721 layer_factory.hpp:76] Creating layer mnist
I1127 11:13:05.809798  6721 net.cpp:106] Creating Layer mnist
I1127 11:13:05.809811  6721 net.cpp:411] mnist -> data
I1127 11:13:05.809823  6721 net.cpp:411] mnist -> label
I1127 11:13:05.810516  6726 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:13:05.810616  6721 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:13:05.814225  6721 net.cpp:150] Setting up mnist
I1127 11:13:05.814241  6721 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:13:05.814252  6721 net.cpp:157] Top shape: 100 (100)
I1127 11:13:05.814260  6721 net.cpp:165] Memory required for data: 314000
I1127 11:13:05.814267  6721 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:13:05.814278  6721 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:13:05.814287  6721 net.cpp:454] label_mnist_1_split <- label
I1127 11:13:05.814298  6721 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:13:05.814311  6721 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:13:05.814355  6721 net.cpp:150] Setting up label_mnist_1_split
I1127 11:13:05.814368  6721 net.cpp:157] Top shape: 100 (100)
I1127 11:13:05.814376  6721 net.cpp:157] Top shape: 100 (100)
I1127 11:13:05.814383  6721 net.cpp:165] Memory required for data: 314800
I1127 11:13:05.814389  6721 layer_factory.hpp:76] Creating layer conv1
I1127 11:13:05.814400  6721 net.cpp:106] Creating Layer conv1
I1127 11:13:05.814407  6721 net.cpp:454] conv1 <- data
I1127 11:13:05.814421  6721 net.cpp:411] conv1 -> conv1
I1127 11:13:05.814648  6721 net.cpp:150] Setting up conv1
I1127 11:13:05.814666  6721 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:13:05.814673  6721 net.cpp:165] Memory required for data: 4922800
I1127 11:13:05.814687  6721 layer_factory.hpp:76] Creating layer pool1
I1127 11:13:05.814699  6721 net.cpp:106] Creating Layer pool1
I1127 11:13:05.814707  6721 net.cpp:454] pool1 <- conv1
I1127 11:13:05.814726  6721 net.cpp:411] pool1 -> pool1
I1127 11:13:05.814771  6721 net.cpp:150] Setting up pool1
I1127 11:13:05.814782  6721 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:13:05.814788  6721 net.cpp:165] Memory required for data: 6074800
I1127 11:13:05.814795  6721 layer_factory.hpp:76] Creating layer conv2
I1127 11:13:05.814808  6721 net.cpp:106] Creating Layer conv2
I1127 11:13:05.814815  6721 net.cpp:454] conv2 <- pool1
I1127 11:13:05.814826  6721 net.cpp:411] conv2 -> conv2
I1127 11:13:05.815533  6721 net.cpp:150] Setting up conv2
I1127 11:13:05.815548  6721 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:13:05.815562  6721 net.cpp:165] Memory required for data: 7354800
I1127 11:13:05.815577  6721 layer_factory.hpp:76] Creating layer pool2
I1127 11:13:05.815594  6721 net.cpp:106] Creating Layer pool2
I1127 11:13:05.815603  6721 net.cpp:454] pool2 <- conv2
I1127 11:13:05.815615  6721 net.cpp:411] pool2 -> pool2
I1127 11:13:05.815656  6721 net.cpp:150] Setting up pool2
I1127 11:13:05.815667  6721 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:13:05.815675  6721 net.cpp:165] Memory required for data: 7674800
I1127 11:13:05.815681  6721 layer_factory.hpp:76] Creating layer ip1
I1127 11:13:05.815697  6721 net.cpp:106] Creating Layer ip1
I1127 11:13:05.815709  6721 net.cpp:454] ip1 <- pool2
I1127 11:13:05.815723  6721 net.cpp:411] ip1 -> ip1
I1127 11:13:05.819478  6721 net.cpp:150] Setting up ip1
I1127 11:13:05.819495  6721 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:13:05.819502  6721 net.cpp:165] Memory required for data: 7874800
I1127 11:13:05.819516  6721 layer_factory.hpp:76] Creating layer relu1
I1127 11:13:05.819530  6721 net.cpp:106] Creating Layer relu1
I1127 11:13:05.819540  6721 net.cpp:454] relu1 <- ip1
I1127 11:13:05.819550  6721 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:13:05.819561  6721 net.cpp:150] Setting up relu1
I1127 11:13:05.819572  6721 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:13:05.819581  6721 net.cpp:165] Memory required for data: 8074800
I1127 11:13:05.819588  6721 layer_factory.hpp:76] Creating layer ip2
I1127 11:13:05.819602  6721 net.cpp:106] Creating Layer ip2
I1127 11:13:05.819610  6721 net.cpp:454] ip2 <- ip1
I1127 11:13:05.819619  6721 net.cpp:411] ip2 -> ip2
I1127 11:13:05.819772  6721 net.cpp:150] Setting up ip2
I1127 11:13:05.819785  6721 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:13:05.819792  6721 net.cpp:165] Memory required for data: 8078800
I1127 11:13:05.819803  6721 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:13:05.819813  6721 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:13:05.819820  6721 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:13:05.819831  6721 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:13:05.819842  6721 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:13:05.819888  6721 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:13:05.819901  6721 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:13:05.819911  6721 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:13:05.819918  6721 net.cpp:165] Memory required for data: 8086800
I1127 11:13:05.819926  6721 layer_factory.hpp:76] Creating layer accuracy
I1127 11:13:05.819936  6721 net.cpp:106] Creating Layer accuracy
I1127 11:13:05.819942  6721 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:13:05.819950  6721 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:13:05.819962  6721 net.cpp:411] accuracy -> accuracy
I1127 11:13:05.819975  6721 net.cpp:150] Setting up accuracy
I1127 11:13:05.819984  6721 net.cpp:157] Top shape: (1)
I1127 11:13:05.819991  6721 net.cpp:165] Memory required for data: 8086804
I1127 11:13:05.819998  6721 layer_factory.hpp:76] Creating layer loss
I1127 11:13:05.820013  6721 net.cpp:106] Creating Layer loss
I1127 11:13:05.820020  6721 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:13:05.820029  6721 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:13:05.820040  6721 net.cpp:411] loss -> loss
I1127 11:13:05.820055  6721 layer_factory.hpp:76] Creating layer loss
I1127 11:13:05.820160  6721 net.cpp:150] Setting up loss
I1127 11:13:05.820173  6721 net.cpp:157] Top shape: (1)
I1127 11:13:05.820179  6721 net.cpp:160]     with loss weight 1
I1127 11:13:05.820194  6721 net.cpp:165] Memory required for data: 8086808
I1127 11:13:05.820201  6721 net.cpp:226] loss needs backward computation.
I1127 11:13:05.820211  6721 net.cpp:228] accuracy does not need backward computation.
I1127 11:13:05.820220  6721 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:13:05.820227  6721 net.cpp:226] ip2 needs backward computation.
I1127 11:13:05.820235  6721 net.cpp:226] relu1 needs backward computation.
I1127 11:13:05.820241  6721 net.cpp:226] ip1 needs backward computation.
I1127 11:13:05.820248  6721 net.cpp:226] pool2 needs backward computation.
I1127 11:13:05.820255  6721 net.cpp:226] conv2 needs backward computation.
I1127 11:13:05.820262  6721 net.cpp:226] pool1 needs backward computation.
I1127 11:13:05.820269  6721 net.cpp:226] conv1 needs backward computation.
I1127 11:13:05.820276  6721 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:13:05.820284  6721 net.cpp:228] mnist does not need backward computation.
I1127 11:13:05.820291  6721 net.cpp:270] This network produces output accuracy
I1127 11:13:05.820297  6721 net.cpp:270] This network produces output loss
I1127 11:13:05.820315  6721 net.cpp:283] Network initialization done.
I1127 11:13:05.820365  6721 solver.cpp:59] Solver scaffolding done.
I1127 11:13:05.820678  6721 caffe.cpp:212] Starting Optimization
I1127 11:13:05.820688  6721 solver.cpp:287] Solving LeNet
I1127 11:13:05.820694  6721 solver.cpp:288] Learning Rate Policy: inv
I1127 11:13:05.821141  6721 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:13:08.246068  6721 solver.cpp:408]     Test net output #0: accuracy = 0.0986
I1127 11:13:08.246194  6721 solver.cpp:408]     Test net output #1: loss = 2.35354 (* 1 = 2.35354 loss)
I1127 11:13:08.257638  6721 solver.cpp:236] Iteration 0, loss = 2.32508
I1127 11:13:08.257771  6721 solver.cpp:252]     Train net output #0: loss = 2.32508 (* 1 = 2.32508 loss)
I1127 11:13:08.257813  6721 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:13:20.781008  6721 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:13:22.736801  6721 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:13:22.847203  6721 solver.cpp:408]     Test net output #0: accuracy = 0.9741
I1127 11:13:22.847369  6721 solver.cpp:408]     Test net output #1: loss = 0.0832433 (* 1 = 0.0832433 loss)
I1127 11:13:22.859994  6721 solver.cpp:236] Iteration 500, loss = 0.130763
I1127 11:13:22.860146  6721 solver.cpp:252]     Train net output #0: loss = 0.130763 (* 1 = 0.130763 loss)
I1127 11:13:22.860203  6721 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:13:36.280105  6721 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:13:36.297080  6721 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:13:36.309794  6721 solver.cpp:320] Iteration 1000, loss = 0.0762978
I1127 11:13:36.309902  6721 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:13:38.163025  6721 solver.cpp:408]     Test net output #0: accuracy = 0.9823
I1127 11:13:38.163064  6721 solver.cpp:408]     Test net output #1: loss = 0.0564406 (* 1 = 0.0564406 loss)
I1127 11:13:38.163070  6721 solver.cpp:325] Optimization Done.
I1127 11:13:38.163074  6721 caffe.cpp:215] Optimization Done.
I1127 11:13:38.227766  6749 caffe.cpp:184] Using GPUs 0
I1127 11:13:38.646255  6749 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:13:38.646378  6749 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:13:38.646632  6749 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:13:38.646647  6749 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:13:38.646730  6749 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:13:38.646786  6749 layer_factory.hpp:76] Creating layer mnist
I1127 11:13:38.647094  6749 net.cpp:106] Creating Layer mnist
I1127 11:13:38.647116  6749 net.cpp:411] mnist -> data
I1127 11:13:38.647141  6749 net.cpp:411] mnist -> label
I1127 11:13:38.647905  6753 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:13:38.682307  6749 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:13:38.688705  6749 net.cpp:150] Setting up mnist
I1127 11:13:38.688725  6749 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:13:38.688732  6749 net.cpp:157] Top shape: 64 (64)
I1127 11:13:38.688736  6749 net.cpp:165] Memory required for data: 200960
I1127 11:13:38.688745  6749 layer_factory.hpp:76] Creating layer conv1
I1127 11:13:38.688765  6749 net.cpp:106] Creating Layer conv1
I1127 11:13:38.688771  6749 net.cpp:454] conv1 <- data
I1127 11:13:38.688781  6749 net.cpp:411] conv1 -> conv1
I1127 11:13:38.689496  6749 net.cpp:150] Setting up conv1
I1127 11:13:38.689507  6749 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:13:38.689512  6749 net.cpp:165] Memory required for data: 3150080
I1127 11:13:38.689524  6749 layer_factory.hpp:76] Creating layer pool1
I1127 11:13:38.689535  6749 net.cpp:106] Creating Layer pool1
I1127 11:13:38.689540  6749 net.cpp:454] pool1 <- conv1
I1127 11:13:38.689545  6749 net.cpp:411] pool1 -> pool1
I1127 11:13:38.689594  6749 net.cpp:150] Setting up pool1
I1127 11:13:38.689601  6749 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:13:38.689605  6749 net.cpp:165] Memory required for data: 3887360
I1127 11:13:38.689613  6749 layer_factory.hpp:76] Creating layer conv2
I1127 11:13:38.689622  6749 net.cpp:106] Creating Layer conv2
I1127 11:13:38.689627  6749 net.cpp:454] conv2 <- pool1
I1127 11:13:38.689635  6749 net.cpp:411] conv2 -> conv2
I1127 11:13:38.689882  6749 net.cpp:150] Setting up conv2
I1127 11:13:38.689890  6749 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:13:38.689895  6749 net.cpp:165] Memory required for data: 4706560
I1127 11:13:38.689903  6749 layer_factory.hpp:76] Creating layer pool2
I1127 11:13:38.689910  6749 net.cpp:106] Creating Layer pool2
I1127 11:13:38.689915  6749 net.cpp:454] pool2 <- conv2
I1127 11:13:38.689923  6749 net.cpp:411] pool2 -> pool2
I1127 11:13:38.689951  6749 net.cpp:150] Setting up pool2
I1127 11:13:38.689959  6749 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:13:38.689962  6749 net.cpp:165] Memory required for data: 4911360
I1127 11:13:38.689966  6749 layer_factory.hpp:76] Creating layer ip1
I1127 11:13:38.689976  6749 net.cpp:106] Creating Layer ip1
I1127 11:13:38.689981  6749 net.cpp:454] ip1 <- pool2
I1127 11:13:38.689987  6749 net.cpp:411] ip1 -> ip1
I1127 11:13:38.692100  6749 net.cpp:150] Setting up ip1
I1127 11:13:38.692111  6749 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:13:38.692116  6749 net.cpp:165] Memory required for data: 5039360
I1127 11:13:38.692126  6749 layer_factory.hpp:76] Creating layer relu1
I1127 11:13:38.692132  6749 net.cpp:106] Creating Layer relu1
I1127 11:13:38.692137  6749 net.cpp:454] relu1 <- ip1
I1127 11:13:38.692143  6749 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:13:38.692152  6749 net.cpp:150] Setting up relu1
I1127 11:13:38.692157  6749 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:13:38.692162  6749 net.cpp:165] Memory required for data: 5167360
I1127 11:13:38.692165  6749 layer_factory.hpp:76] Creating layer ip2
I1127 11:13:38.692173  6749 net.cpp:106] Creating Layer ip2
I1127 11:13:38.692176  6749 net.cpp:454] ip2 <- ip1
I1127 11:13:38.692183  6749 net.cpp:411] ip2 -> ip2
I1127 11:13:38.692575  6749 net.cpp:150] Setting up ip2
I1127 11:13:38.692585  6749 net.cpp:157] Top shape: 64 10 (640)
I1127 11:13:38.692589  6749 net.cpp:165] Memory required for data: 5169920
I1127 11:13:38.692596  6749 layer_factory.hpp:76] Creating layer loss
I1127 11:13:38.692605  6749 net.cpp:106] Creating Layer loss
I1127 11:13:38.692610  6749 net.cpp:454] loss <- ip2
I1127 11:13:38.692615  6749 net.cpp:454] loss <- label
I1127 11:13:38.692622  6749 net.cpp:411] loss -> loss
I1127 11:13:38.692633  6749 layer_factory.hpp:76] Creating layer loss
I1127 11:13:38.692698  6749 net.cpp:150] Setting up loss
I1127 11:13:38.692706  6749 net.cpp:157] Top shape: (1)
I1127 11:13:38.692710  6749 net.cpp:160]     with loss weight 1
I1127 11:13:38.692726  6749 net.cpp:165] Memory required for data: 5169924
I1127 11:13:38.692730  6749 net.cpp:226] loss needs backward computation.
I1127 11:13:38.692735  6749 net.cpp:226] ip2 needs backward computation.
I1127 11:13:38.692739  6749 net.cpp:226] relu1 needs backward computation.
I1127 11:13:38.692744  6749 net.cpp:226] ip1 needs backward computation.
I1127 11:13:38.692747  6749 net.cpp:226] pool2 needs backward computation.
I1127 11:13:38.692752  6749 net.cpp:226] conv2 needs backward computation.
I1127 11:13:38.692756  6749 net.cpp:226] pool1 needs backward computation.
I1127 11:13:38.692760  6749 net.cpp:226] conv1 needs backward computation.
I1127 11:13:38.692764  6749 net.cpp:228] mnist does not need backward computation.
I1127 11:13:38.692769  6749 net.cpp:270] This network produces output loss
I1127 11:13:38.692777  6749 net.cpp:283] Network initialization done.
I1127 11:13:38.693011  6749 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:13:38.693032  6749 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:13:38.693142  6749 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:13:38.693202  6749 layer_factory.hpp:76] Creating layer mnist
I1127 11:13:38.693284  6749 net.cpp:106] Creating Layer mnist
I1127 11:13:38.693292  6749 net.cpp:411] mnist -> data
I1127 11:13:38.693300  6749 net.cpp:411] mnist -> label
I1127 11:13:38.694020  6755 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:13:38.694094  6749 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:13:38.697711  6749 net.cpp:150] Setting up mnist
I1127 11:13:38.697724  6749 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:13:38.697731  6749 net.cpp:157] Top shape: 100 (100)
I1127 11:13:38.697734  6749 net.cpp:165] Memory required for data: 314000
I1127 11:13:38.697738  6749 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:13:38.697746  6749 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:13:38.697751  6749 net.cpp:454] label_mnist_1_split <- label
I1127 11:13:38.697757  6749 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:13:38.697765  6749 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:13:38.697794  6749 net.cpp:150] Setting up label_mnist_1_split
I1127 11:13:38.697801  6749 net.cpp:157] Top shape: 100 (100)
I1127 11:13:38.697806  6749 net.cpp:157] Top shape: 100 (100)
I1127 11:13:38.697811  6749 net.cpp:165] Memory required for data: 314800
I1127 11:13:38.697815  6749 layer_factory.hpp:76] Creating layer conv1
I1127 11:13:38.697825  6749 net.cpp:106] Creating Layer conv1
I1127 11:13:38.697829  6749 net.cpp:454] conv1 <- data
I1127 11:13:38.697836  6749 net.cpp:411] conv1 -> conv1
I1127 11:13:38.697975  6749 net.cpp:150] Setting up conv1
I1127 11:13:38.697984  6749 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:13:38.697988  6749 net.cpp:165] Memory required for data: 4922800
I1127 11:13:38.697998  6749 layer_factory.hpp:76] Creating layer pool1
I1127 11:13:38.698004  6749 net.cpp:106] Creating Layer pool1
I1127 11:13:38.698009  6749 net.cpp:454] pool1 <- conv1
I1127 11:13:38.698021  6749 net.cpp:411] pool1 -> pool1
I1127 11:13:38.698050  6749 net.cpp:150] Setting up pool1
I1127 11:13:38.698060  6749 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:13:38.698063  6749 net.cpp:165] Memory required for data: 6074800
I1127 11:13:38.698067  6749 layer_factory.hpp:76] Creating layer conv2
I1127 11:13:38.698076  6749 net.cpp:106] Creating Layer conv2
I1127 11:13:38.698081  6749 net.cpp:454] conv2 <- pool1
I1127 11:13:38.698088  6749 net.cpp:411] conv2 -> conv2
I1127 11:13:38.698346  6749 net.cpp:150] Setting up conv2
I1127 11:13:38.698355  6749 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:13:38.698360  6749 net.cpp:165] Memory required for data: 7354800
I1127 11:13:38.698369  6749 layer_factory.hpp:76] Creating layer pool2
I1127 11:13:38.698374  6749 net.cpp:106] Creating Layer pool2
I1127 11:13:38.698379  6749 net.cpp:454] pool2 <- conv2
I1127 11:13:38.698385  6749 net.cpp:411] pool2 -> pool2
I1127 11:13:38.698417  6749 net.cpp:150] Setting up pool2
I1127 11:13:38.698426  6749 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:13:38.698429  6749 net.cpp:165] Memory required for data: 7674800
I1127 11:13:38.698433  6749 layer_factory.hpp:76] Creating layer ip1
I1127 11:13:38.698441  6749 net.cpp:106] Creating Layer ip1
I1127 11:13:38.698446  6749 net.cpp:454] ip1 <- pool2
I1127 11:13:38.698451  6749 net.cpp:411] ip1 -> ip1
I1127 11:13:38.701118  6749 net.cpp:150] Setting up ip1
I1127 11:13:38.701134  6749 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:13:38.701139  6749 net.cpp:165] Memory required for data: 7874800
I1127 11:13:38.701148  6749 layer_factory.hpp:76] Creating layer relu1
I1127 11:13:38.701156  6749 net.cpp:106] Creating Layer relu1
I1127 11:13:38.701161  6749 net.cpp:454] relu1 <- ip1
I1127 11:13:38.701167  6749 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:13:38.701174  6749 net.cpp:150] Setting up relu1
I1127 11:13:38.701179  6749 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:13:38.701184  6749 net.cpp:165] Memory required for data: 8074800
I1127 11:13:38.701189  6749 layer_factory.hpp:76] Creating layer ip2
I1127 11:13:38.701197  6749 net.cpp:106] Creating Layer ip2
I1127 11:13:38.701201  6749 net.cpp:454] ip2 <- ip1
I1127 11:13:38.701210  6749 net.cpp:411] ip2 -> ip2
I1127 11:13:38.701300  6749 net.cpp:150] Setting up ip2
I1127 11:13:38.701308  6749 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:13:38.701313  6749 net.cpp:165] Memory required for data: 8078800
I1127 11:13:38.701319  6749 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:13:38.701325  6749 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:13:38.701330  6749 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:13:38.701335  6749 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:13:38.701342  6749 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:13:38.701369  6749 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:13:38.701375  6749 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:13:38.701380  6749 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:13:38.701385  6749 net.cpp:165] Memory required for data: 8086800
I1127 11:13:38.701390  6749 layer_factory.hpp:76] Creating layer accuracy
I1127 11:13:38.701395  6749 net.cpp:106] Creating Layer accuracy
I1127 11:13:38.701400  6749 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:13:38.701406  6749 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:13:38.701411  6749 net.cpp:411] accuracy -> accuracy
I1127 11:13:38.701421  6749 net.cpp:150] Setting up accuracy
I1127 11:13:38.701426  6749 net.cpp:157] Top shape: (1)
I1127 11:13:38.701431  6749 net.cpp:165] Memory required for data: 8086804
I1127 11:13:38.701434  6749 layer_factory.hpp:76] Creating layer loss
I1127 11:13:38.701441  6749 net.cpp:106] Creating Layer loss
I1127 11:13:38.701444  6749 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:13:38.701449  6749 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:13:38.701457  6749 net.cpp:411] loss -> loss
I1127 11:13:38.701464  6749 layer_factory.hpp:76] Creating layer loss
I1127 11:13:38.701530  6749 net.cpp:150] Setting up loss
I1127 11:13:38.701539  6749 net.cpp:157] Top shape: (1)
I1127 11:13:38.701544  6749 net.cpp:160]     with loss weight 1
I1127 11:13:38.701555  6749 net.cpp:165] Memory required for data: 8086808
I1127 11:13:38.701560  6749 net.cpp:226] loss needs backward computation.
I1127 11:13:38.701566  6749 net.cpp:228] accuracy does not need backward computation.
I1127 11:13:38.701571  6749 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:13:38.701575  6749 net.cpp:226] ip2 needs backward computation.
I1127 11:13:38.701580  6749 net.cpp:226] relu1 needs backward computation.
I1127 11:13:38.701582  6749 net.cpp:226] ip1 needs backward computation.
I1127 11:13:38.701586  6749 net.cpp:226] pool2 needs backward computation.
I1127 11:13:38.701591  6749 net.cpp:226] conv2 needs backward computation.
I1127 11:13:38.701596  6749 net.cpp:226] pool1 needs backward computation.
I1127 11:13:38.701599  6749 net.cpp:226] conv1 needs backward computation.
I1127 11:13:38.701603  6749 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:13:38.701608  6749 net.cpp:228] mnist does not need backward computation.
I1127 11:13:38.701612  6749 net.cpp:270] This network produces output accuracy
I1127 11:13:38.701617  6749 net.cpp:270] This network produces output loss
I1127 11:13:38.701625  6749 net.cpp:283] Network initialization done.
I1127 11:13:38.701659  6749 solver.cpp:59] Solver scaffolding done.
I1127 11:13:38.701845  6749 caffe.cpp:212] Starting Optimization
I1127 11:13:38.701853  6749 solver.cpp:287] Solving LeNet
I1127 11:13:38.701856  6749 solver.cpp:288] Learning Rate Policy: inv
I1127 11:13:38.702183  6749 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:13:40.593778  6749 solver.cpp:408]     Test net output #0: accuracy = 0.136
I1127 11:13:40.593881  6749 solver.cpp:408]     Test net output #1: loss = 2.36823 (* 1 = 2.36823 loss)
I1127 11:13:40.609653  6749 solver.cpp:236] Iteration 0, loss = 2.36861
I1127 11:13:40.609752  6749 solver.cpp:252]     Train net output #0: loss = 2.36861 (* 1 = 2.36861 loss)
I1127 11:13:40.609789  6749 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:13:52.099709  6749 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:13:55.232158  6749 solver.cpp:408]     Test net output #0: accuracy = 0.9747
I1127 11:13:55.232240  6749 solver.cpp:408]     Test net output #1: loss = 0.0840735 (* 1 = 0.0840735 loss)
I1127 11:13:55.244638  6749 solver.cpp:236] Iteration 500, loss = 0.0984051
I1127 11:13:55.244704  6749 solver.cpp:252]     Train net output #0: loss = 0.0984051 (* 1 = 0.0984051 loss)
I1127 11:13:55.244720  6749 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:14:07.002768  6749 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:14:07.016125  6749 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:14:07.027060  6749 solver.cpp:320] Iteration 1000, loss = 0.0748252
I1127 11:14:07.027138  6749 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:14:08.432348  6749 solver.cpp:408]     Test net output #0: accuracy = 0.9819
I1127 11:14:08.432595  6749 solver.cpp:408]     Test net output #1: loss = 0.0600191 (* 1 = 0.0600191 loss)
I1127 11:14:08.432606  6749 solver.cpp:325] Optimization Done.
I1127 11:14:08.432610  6749 caffe.cpp:215] Optimization Done.
I1127 11:14:08.544360  6821 caffe.cpp:184] Using GPUs 0
I1127 11:14:08.862439  6821 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:14:08.862555  6821 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:14:08.862812  6821 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:14:08.862826  6821 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:14:08.862926  6821 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:14:08.862998  6821 layer_factory.hpp:76] Creating layer mnist
I1127 11:14:08.863382  6821 net.cpp:106] Creating Layer mnist
I1127 11:14:08.863405  6821 net.cpp:411] mnist -> data
I1127 11:14:08.863446  6821 net.cpp:411] mnist -> label
I1127 11:14:08.864637  6826 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:14:08.873930  6821 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:14:08.875054  6821 net.cpp:150] Setting up mnist
I1127 11:14:08.875102  6821 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:14:08.875114  6821 net.cpp:157] Top shape: 64 (64)
I1127 11:14:08.875123  6821 net.cpp:165] Memory required for data: 200960
I1127 11:14:08.875140  6821 layer_factory.hpp:76] Creating layer conv1
I1127 11:14:08.875175  6821 net.cpp:106] Creating Layer conv1
I1127 11:14:08.875185  6821 net.cpp:454] conv1 <- data
I1127 11:14:08.875202  6821 net.cpp:411] conv1 -> conv1
I1127 11:14:08.876179  6821 net.cpp:150] Setting up conv1
I1127 11:14:08.876222  6821 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:14:08.876231  6821 net.cpp:165] Memory required for data: 3150080
I1127 11:14:08.876260  6821 layer_factory.hpp:76] Creating layer pool1
I1127 11:14:08.876281  6821 net.cpp:106] Creating Layer pool1
I1127 11:14:08.876291  6821 net.cpp:454] pool1 <- conv1
I1127 11:14:08.876305  6821 net.cpp:411] pool1 -> pool1
I1127 11:14:08.876394  6821 net.cpp:150] Setting up pool1
I1127 11:14:08.876410  6821 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:14:08.876420  6821 net.cpp:165] Memory required for data: 3887360
I1127 11:14:08.876428  6821 layer_factory.hpp:76] Creating layer conv2
I1127 11:14:08.876446  6821 net.cpp:106] Creating Layer conv2
I1127 11:14:08.876456  6821 net.cpp:454] conv2 <- pool1
I1127 11:14:08.876467  6821 net.cpp:411] conv2 -> conv2
I1127 11:14:08.876934  6821 net.cpp:150] Setting up conv2
I1127 11:14:08.876950  6821 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:14:08.876957  6821 net.cpp:165] Memory required for data: 4706560
I1127 11:14:08.876976  6821 layer_factory.hpp:76] Creating layer pool2
I1127 11:14:08.876996  6821 net.cpp:106] Creating Layer pool2
I1127 11:14:08.877004  6821 net.cpp:454] pool2 <- conv2
I1127 11:14:08.877014  6821 net.cpp:411] pool2 -> pool2
I1127 11:14:08.877058  6821 net.cpp:150] Setting up pool2
I1127 11:14:08.877069  6821 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:14:08.877075  6821 net.cpp:165] Memory required for data: 4911360
I1127 11:14:08.877087  6821 layer_factory.hpp:76] Creating layer ip1
I1127 11:14:08.877101  6821 net.cpp:106] Creating Layer ip1
I1127 11:14:08.877109  6821 net.cpp:454] ip1 <- pool2
I1127 11:14:08.877120  6821 net.cpp:411] ip1 -> ip1
I1127 11:14:08.880944  6821 net.cpp:150] Setting up ip1
I1127 11:14:08.881003  6821 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:14:08.881014  6821 net.cpp:165] Memory required for data: 5039360
I1127 11:14:08.881042  6821 layer_factory.hpp:76] Creating layer relu1
I1127 11:14:08.881067  6821 net.cpp:106] Creating Layer relu1
I1127 11:14:08.881080  6821 net.cpp:454] relu1 <- ip1
I1127 11:14:08.881098  6821 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:14:08.881132  6821 net.cpp:150] Setting up relu1
I1127 11:14:08.881145  6821 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:14:08.881155  6821 net.cpp:165] Memory required for data: 5167360
I1127 11:14:08.881162  6821 layer_factory.hpp:76] Creating layer ip2
I1127 11:14:08.881180  6821 net.cpp:106] Creating Layer ip2
I1127 11:14:08.881189  6821 net.cpp:454] ip2 <- ip1
I1127 11:14:08.881204  6821 net.cpp:411] ip2 -> ip2
I1127 11:14:08.882381  6821 net.cpp:150] Setting up ip2
I1127 11:14:08.882414  6821 net.cpp:157] Top shape: 64 10 (640)
I1127 11:14:08.882422  6821 net.cpp:165] Memory required for data: 5169920
I1127 11:14:08.882436  6821 layer_factory.hpp:76] Creating layer loss
I1127 11:14:08.882454  6821 net.cpp:106] Creating Layer loss
I1127 11:14:08.882462  6821 net.cpp:454] loss <- ip2
I1127 11:14:08.882472  6821 net.cpp:454] loss <- label
I1127 11:14:08.882488  6821 net.cpp:411] loss -> loss
I1127 11:14:08.882515  6821 layer_factory.hpp:76] Creating layer loss
I1127 11:14:08.882633  6821 net.cpp:150] Setting up loss
I1127 11:14:08.882647  6821 net.cpp:157] Top shape: (1)
I1127 11:14:08.882657  6821 net.cpp:160]     with loss weight 1
I1127 11:14:08.882691  6821 net.cpp:165] Memory required for data: 5169924
I1127 11:14:08.882700  6821 net.cpp:226] loss needs backward computation.
I1127 11:14:08.882709  6821 net.cpp:226] ip2 needs backward computation.
I1127 11:14:08.882716  6821 net.cpp:226] relu1 needs backward computation.
I1127 11:14:08.882725  6821 net.cpp:226] ip1 needs backward computation.
I1127 11:14:08.882733  6821 net.cpp:226] pool2 needs backward computation.
I1127 11:14:08.882750  6821 net.cpp:226] conv2 needs backward computation.
I1127 11:14:08.882766  6821 net.cpp:226] pool1 needs backward computation.
I1127 11:14:08.882776  6821 net.cpp:226] conv1 needs backward computation.
I1127 11:14:08.882786  6821 net.cpp:228] mnist does not need backward computation.
I1127 11:14:08.882793  6821 net.cpp:270] This network produces output loss
I1127 11:14:08.882808  6821 net.cpp:283] Network initialization done.
I1127 11:14:08.883167  6821 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:14:08.883200  6821 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:14:08.883373  6821 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:14:08.883463  6821 layer_factory.hpp:76] Creating layer mnist
I1127 11:14:08.883600  6821 net.cpp:106] Creating Layer mnist
I1127 11:14:08.883615  6821 net.cpp:411] mnist -> data
I1127 11:14:08.883627  6821 net.cpp:411] mnist -> label
I1127 11:14:08.884418  6828 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:14:08.884547  6821 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:14:08.885673  6821 net.cpp:150] Setting up mnist
I1127 11:14:08.885699  6821 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:14:08.885712  6821 net.cpp:157] Top shape: 100 (100)
I1127 11:14:08.885722  6821 net.cpp:165] Memory required for data: 314000
I1127 11:14:08.885735  6821 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:14:08.885751  6821 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:14:08.885761  6821 net.cpp:454] label_mnist_1_split <- label
I1127 11:14:08.885772  6821 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:14:08.885788  6821 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:14:08.885843  6821 net.cpp:150] Setting up label_mnist_1_split
I1127 11:14:08.885856  6821 net.cpp:157] Top shape: 100 (100)
I1127 11:14:08.885867  6821 net.cpp:157] Top shape: 100 (100)
I1127 11:14:08.885876  6821 net.cpp:165] Memory required for data: 314800
I1127 11:14:08.885885  6821 layer_factory.hpp:76] Creating layer conv1
I1127 11:14:08.885905  6821 net.cpp:106] Creating Layer conv1
I1127 11:14:08.885915  6821 net.cpp:454] conv1 <- data
I1127 11:14:08.885931  6821 net.cpp:411] conv1 -> conv1
I1127 11:14:08.886198  6821 net.cpp:150] Setting up conv1
I1127 11:14:08.886214  6821 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:14:08.886224  6821 net.cpp:165] Memory required for data: 4922800
I1127 11:14:08.886243  6821 layer_factory.hpp:76] Creating layer pool1
I1127 11:14:08.886258  6821 net.cpp:106] Creating Layer pool1
I1127 11:14:08.886266  6821 net.cpp:454] pool1 <- conv1
I1127 11:14:08.886291  6821 net.cpp:411] pool1 -> pool1
I1127 11:14:08.886353  6821 net.cpp:150] Setting up pool1
I1127 11:14:08.886369  6821 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:14:08.886379  6821 net.cpp:165] Memory required for data: 6074800
I1127 11:14:08.886386  6821 layer_factory.hpp:76] Creating layer conv2
I1127 11:14:08.886404  6821 net.cpp:106] Creating Layer conv2
I1127 11:14:08.886415  6821 net.cpp:454] conv2 <- pool1
I1127 11:14:08.886430  6821 net.cpp:411] conv2 -> conv2
I1127 11:14:08.886809  6821 net.cpp:150] Setting up conv2
I1127 11:14:08.886819  6821 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:14:08.886824  6821 net.cpp:165] Memory required for data: 7354800
I1127 11:14:08.886837  6821 layer_factory.hpp:76] Creating layer pool2
I1127 11:14:08.886845  6821 net.cpp:106] Creating Layer pool2
I1127 11:14:08.886850  6821 net.cpp:454] pool2 <- conv2
I1127 11:14:08.886857  6821 net.cpp:411] pool2 -> pool2
I1127 11:14:08.886883  6821 net.cpp:150] Setting up pool2
I1127 11:14:08.886889  6821 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:14:08.886893  6821 net.cpp:165] Memory required for data: 7674800
I1127 11:14:08.886898  6821 layer_factory.hpp:76] Creating layer ip1
I1127 11:14:08.886907  6821 net.cpp:106] Creating Layer ip1
I1127 11:14:08.886912  6821 net.cpp:454] ip1 <- pool2
I1127 11:14:08.886917  6821 net.cpp:411] ip1 -> ip1
I1127 11:14:08.889225  6821 net.cpp:150] Setting up ip1
I1127 11:14:08.889272  6821 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:14:08.889277  6821 net.cpp:165] Memory required for data: 7874800
I1127 11:14:08.889291  6821 layer_factory.hpp:76] Creating layer relu1
I1127 11:14:08.889305  6821 net.cpp:106] Creating Layer relu1
I1127 11:14:08.889312  6821 net.cpp:454] relu1 <- ip1
I1127 11:14:08.889319  6821 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:14:08.889330  6821 net.cpp:150] Setting up relu1
I1127 11:14:08.889335  6821 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:14:08.889339  6821 net.cpp:165] Memory required for data: 8074800
I1127 11:14:08.889343  6821 layer_factory.hpp:76] Creating layer ip2
I1127 11:14:08.889356  6821 net.cpp:106] Creating Layer ip2
I1127 11:14:08.889360  6821 net.cpp:454] ip2 <- ip1
I1127 11:14:08.889376  6821 net.cpp:411] ip2 -> ip2
I1127 11:14:08.889487  6821 net.cpp:150] Setting up ip2
I1127 11:14:08.889495  6821 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:14:08.889499  6821 net.cpp:165] Memory required for data: 8078800
I1127 11:14:08.889505  6821 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:14:08.889513  6821 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:14:08.889518  6821 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:14:08.889525  6821 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:14:08.889533  6821 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:14:08.889559  6821 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:14:08.889567  6821 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:14:08.889572  6821 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:14:08.889576  6821 net.cpp:165] Memory required for data: 8086800
I1127 11:14:08.889580  6821 layer_factory.hpp:76] Creating layer accuracy
I1127 11:14:08.889588  6821 net.cpp:106] Creating Layer accuracy
I1127 11:14:08.889593  6821 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:14:08.889598  6821 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:14:08.889605  6821 net.cpp:411] accuracy -> accuracy
I1127 11:14:08.889614  6821 net.cpp:150] Setting up accuracy
I1127 11:14:08.889619  6821 net.cpp:157] Top shape: (1)
I1127 11:14:08.889624  6821 net.cpp:165] Memory required for data: 8086804
I1127 11:14:08.889628  6821 layer_factory.hpp:76] Creating layer loss
I1127 11:14:08.889637  6821 net.cpp:106] Creating Layer loss
I1127 11:14:08.889642  6821 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:14:08.889647  6821 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:14:08.889652  6821 net.cpp:411] loss -> loss
I1127 11:14:08.889660  6821 layer_factory.hpp:76] Creating layer loss
I1127 11:14:08.889742  6821 net.cpp:150] Setting up loss
I1127 11:14:08.889750  6821 net.cpp:157] Top shape: (1)
I1127 11:14:08.889755  6821 net.cpp:160]     with loss weight 1
I1127 11:14:08.889770  6821 net.cpp:165] Memory required for data: 8086808
I1127 11:14:08.889773  6821 net.cpp:226] loss needs backward computation.
I1127 11:14:08.889782  6821 net.cpp:228] accuracy does not need backward computation.
I1127 11:14:08.889787  6821 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:14:08.889791  6821 net.cpp:226] ip2 needs backward computation.
I1127 11:14:08.889796  6821 net.cpp:226] relu1 needs backward computation.
I1127 11:14:08.889799  6821 net.cpp:226] ip1 needs backward computation.
I1127 11:14:08.889808  6821 net.cpp:226] pool2 needs backward computation.
I1127 11:14:08.889812  6821 net.cpp:226] conv2 needs backward computation.
I1127 11:14:08.889817  6821 net.cpp:226] pool1 needs backward computation.
I1127 11:14:08.889822  6821 net.cpp:226] conv1 needs backward computation.
I1127 11:14:08.889827  6821 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:14:08.889832  6821 net.cpp:228] mnist does not need backward computation.
I1127 11:14:08.889834  6821 net.cpp:270] This network produces output accuracy
I1127 11:14:08.889839  6821 net.cpp:270] This network produces output loss
I1127 11:14:08.889850  6821 net.cpp:283] Network initialization done.
I1127 11:14:08.889900  6821 solver.cpp:59] Solver scaffolding done.
I1127 11:14:08.890094  6821 caffe.cpp:212] Starting Optimization
I1127 11:14:08.890100  6821 solver.cpp:287] Solving LeNet
I1127 11:14:08.890105  6821 solver.cpp:288] Learning Rate Policy: inv
I1127 11:14:08.890584  6821 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:14:10.296473  6821 solver.cpp:408]     Test net output #0: accuracy = 0.0574
I1127 11:14:10.296546  6821 solver.cpp:408]     Test net output #1: loss = 2.39439 (* 1 = 2.39439 loss)
I1127 11:14:10.306795  6821 solver.cpp:236] Iteration 0, loss = 2.40336
I1127 11:14:10.306849  6821 solver.cpp:252]     Train net output #0: loss = 2.40336 (* 1 = 2.40336 loss)
I1127 11:14:10.306879  6821 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:14:19.565991  6821 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:14:20.973948  6821 solver.cpp:408]     Test net output #0: accuracy = 0.9757
I1127 11:14:20.973996  6821 solver.cpp:408]     Test net output #1: loss = 0.082382 (* 1 = 0.082382 loss)
I1127 11:14:20.983101  6821 solver.cpp:236] Iteration 500, loss = 0.0887822
I1127 11:14:20.983167  6821 solver.cpp:252]     Train net output #0: loss = 0.0887822 (* 1 = 0.0887822 loss)
I1127 11:14:20.983180  6821 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:14:30.382164  6821 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:14:30.394696  6821 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:14:30.405220  6821 solver.cpp:320] Iteration 1000, loss = 0.110273
I1127 11:14:30.405306  6821 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:14:31.799101  6821 solver.cpp:408]     Test net output #0: accuracy = 0.9811
I1127 11:14:31.799288  6821 solver.cpp:408]     Test net output #1: loss = 0.057195 (* 1 = 0.057195 loss)
I1127 11:14:31.799312  6821 solver.cpp:325] Optimization Done.
I1127 11:14:31.799330  6821 caffe.cpp:215] Optimization Done.
I1127 11:14:31.914909  6928 caffe.cpp:184] Using GPUs 0
I1127 11:14:32.166213  6928 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:14:32.166517  6928 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:14:32.166952  6928 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:14:32.166993  6928 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:14:32.167151  6928 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:14:32.167280  6928 layer_factory.hpp:76] Creating layer mnist
I1127 11:14:32.167800  6928 net.cpp:106] Creating Layer mnist
I1127 11:14:32.167837  6928 net.cpp:411] mnist -> data
I1127 11:14:32.167889  6928 net.cpp:411] mnist -> label
I1127 11:14:32.169277  6932 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:14:32.176404  6928 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:14:32.177572  6928 net.cpp:150] Setting up mnist
I1127 11:14:32.177635  6928 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:14:32.177644  6928 net.cpp:157] Top shape: 64 (64)
I1127 11:14:32.177647  6928 net.cpp:165] Memory required for data: 200960
I1127 11:14:32.177661  6928 layer_factory.hpp:76] Creating layer conv1
I1127 11:14:32.177682  6928 net.cpp:106] Creating Layer conv1
I1127 11:14:32.177690  6928 net.cpp:454] conv1 <- data
I1127 11:14:32.177703  6928 net.cpp:411] conv1 -> conv1
I1127 11:14:32.179069  6928 net.cpp:150] Setting up conv1
I1127 11:14:32.179124  6928 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:14:32.179129  6928 net.cpp:165] Memory required for data: 3150080
I1127 11:14:32.179147  6928 layer_factory.hpp:76] Creating layer pool1
I1127 11:14:32.179162  6928 net.cpp:106] Creating Layer pool1
I1127 11:14:32.179167  6928 net.cpp:454] pool1 <- conv1
I1127 11:14:32.179175  6928 net.cpp:411] pool1 -> pool1
I1127 11:14:32.179236  6928 net.cpp:150] Setting up pool1
I1127 11:14:32.179244  6928 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:14:32.179249  6928 net.cpp:165] Memory required for data: 3887360
I1127 11:14:32.179253  6928 layer_factory.hpp:76] Creating layer conv2
I1127 11:14:32.179265  6928 net.cpp:106] Creating Layer conv2
I1127 11:14:32.179270  6928 net.cpp:454] conv2 <- pool1
I1127 11:14:32.179280  6928 net.cpp:411] conv2 -> conv2
I1127 11:14:32.179576  6928 net.cpp:150] Setting up conv2
I1127 11:14:32.179597  6928 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:14:32.179602  6928 net.cpp:165] Memory required for data: 4706560
I1127 11:14:32.179615  6928 layer_factory.hpp:76] Creating layer pool2
I1127 11:14:32.179627  6928 net.cpp:106] Creating Layer pool2
I1127 11:14:32.179633  6928 net.cpp:454] pool2 <- conv2
I1127 11:14:32.179639  6928 net.cpp:411] pool2 -> pool2
I1127 11:14:32.179678  6928 net.cpp:150] Setting up pool2
I1127 11:14:32.179687  6928 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:14:32.179690  6928 net.cpp:165] Memory required for data: 4911360
I1127 11:14:32.179695  6928 layer_factory.hpp:76] Creating layer ip1
I1127 11:14:32.179705  6928 net.cpp:106] Creating Layer ip1
I1127 11:14:32.179710  6928 net.cpp:454] ip1 <- pool2
I1127 11:14:32.179725  6928 net.cpp:411] ip1 -> ip1
I1127 11:14:32.182363  6928 net.cpp:150] Setting up ip1
I1127 11:14:32.182423  6928 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:14:32.182430  6928 net.cpp:165] Memory required for data: 5039360
I1127 11:14:32.182451  6928 layer_factory.hpp:76] Creating layer relu1
I1127 11:14:32.182464  6928 net.cpp:106] Creating Layer relu1
I1127 11:14:32.182471  6928 net.cpp:454] relu1 <- ip1
I1127 11:14:32.182477  6928 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:14:32.182492  6928 net.cpp:150] Setting up relu1
I1127 11:14:32.182497  6928 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:14:32.182502  6928 net.cpp:165] Memory required for data: 5167360
I1127 11:14:32.182505  6928 layer_factory.hpp:76] Creating layer ip2
I1127 11:14:32.182514  6928 net.cpp:106] Creating Layer ip2
I1127 11:14:32.182519  6928 net.cpp:454] ip2 <- ip1
I1127 11:14:32.182528  6928 net.cpp:411] ip2 -> ip2
I1127 11:14:32.183120  6928 net.cpp:150] Setting up ip2
I1127 11:14:32.183146  6928 net.cpp:157] Top shape: 64 10 (640)
I1127 11:14:32.183151  6928 net.cpp:165] Memory required for data: 5169920
I1127 11:14:32.183158  6928 layer_factory.hpp:76] Creating layer loss
I1127 11:14:32.183168  6928 net.cpp:106] Creating Layer loss
I1127 11:14:32.183173  6928 net.cpp:454] loss <- ip2
I1127 11:14:32.183179  6928 net.cpp:454] loss <- label
I1127 11:14:32.183187  6928 net.cpp:411] loss -> loss
I1127 11:14:32.183200  6928 layer_factory.hpp:76] Creating layer loss
I1127 11:14:32.183275  6928 net.cpp:150] Setting up loss
I1127 11:14:32.183284  6928 net.cpp:157] Top shape: (1)
I1127 11:14:32.183287  6928 net.cpp:160]     with loss weight 1
I1127 11:14:32.183305  6928 net.cpp:165] Memory required for data: 5169924
I1127 11:14:32.183310  6928 net.cpp:226] loss needs backward computation.
I1127 11:14:32.183315  6928 net.cpp:226] ip2 needs backward computation.
I1127 11:14:32.183318  6928 net.cpp:226] relu1 needs backward computation.
I1127 11:14:32.183322  6928 net.cpp:226] ip1 needs backward computation.
I1127 11:14:32.183327  6928 net.cpp:226] pool2 needs backward computation.
I1127 11:14:32.183331  6928 net.cpp:226] conv2 needs backward computation.
I1127 11:14:32.183336  6928 net.cpp:226] pool1 needs backward computation.
I1127 11:14:32.183341  6928 net.cpp:226] conv1 needs backward computation.
I1127 11:14:32.183346  6928 net.cpp:228] mnist does not need backward computation.
I1127 11:14:32.183349  6928 net.cpp:270] This network produces output loss
I1127 11:14:32.183361  6928 net.cpp:283] Network initialization done.
I1127 11:14:32.183600  6928 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:14:32.183622  6928 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:14:32.183735  6928 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:14:32.183799  6928 layer_factory.hpp:76] Creating layer mnist
I1127 11:14:32.183895  6928 net.cpp:106] Creating Layer mnist
I1127 11:14:32.183903  6928 net.cpp:411] mnist -> data
I1127 11:14:32.183912  6928 net.cpp:411] mnist -> label
I1127 11:14:32.184653  6934 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:14:32.184770  6928 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:14:32.185756  6928 net.cpp:150] Setting up mnist
I1127 11:14:32.185803  6928 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:14:32.185811  6928 net.cpp:157] Top shape: 100 (100)
I1127 11:14:32.185814  6928 net.cpp:165] Memory required for data: 314000
I1127 11:14:32.185824  6928 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:14:32.185840  6928 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:14:32.185847  6928 net.cpp:454] label_mnist_1_split <- label
I1127 11:14:32.185854  6928 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:14:32.185866  6928 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:14:32.185906  6928 net.cpp:150] Setting up label_mnist_1_split
I1127 11:14:32.185914  6928 net.cpp:157] Top shape: 100 (100)
I1127 11:14:32.185920  6928 net.cpp:157] Top shape: 100 (100)
I1127 11:14:32.185924  6928 net.cpp:165] Memory required for data: 314800
I1127 11:14:32.185928  6928 layer_factory.hpp:76] Creating layer conv1
I1127 11:14:32.185940  6928 net.cpp:106] Creating Layer conv1
I1127 11:14:32.185945  6928 net.cpp:454] conv1 <- data
I1127 11:14:32.185953  6928 net.cpp:411] conv1 -> conv1
I1127 11:14:32.186199  6928 net.cpp:150] Setting up conv1
I1127 11:14:32.186219  6928 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:14:32.186224  6928 net.cpp:165] Memory required for data: 4922800
I1127 11:14:32.186233  6928 layer_factory.hpp:76] Creating layer pool1
I1127 11:14:32.186242  6928 net.cpp:106] Creating Layer pool1
I1127 11:14:32.186246  6928 net.cpp:454] pool1 <- conv1
I1127 11:14:32.186282  6928 net.cpp:411] pool1 -> pool1
I1127 11:14:32.186317  6928 net.cpp:150] Setting up pool1
I1127 11:14:32.186336  6928 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:14:32.186342  6928 net.cpp:165] Memory required for data: 6074800
I1127 11:14:32.186355  6928 layer_factory.hpp:76] Creating layer conv2
I1127 11:14:32.186363  6928 net.cpp:106] Creating Layer conv2
I1127 11:14:32.186368  6928 net.cpp:454] conv2 <- pool1
I1127 11:14:32.186377  6928 net.cpp:411] conv2 -> conv2
I1127 11:14:32.186784  6928 net.cpp:150] Setting up conv2
I1127 11:14:32.186805  6928 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:14:32.186812  6928 net.cpp:165] Memory required for data: 7354800
I1127 11:14:32.186825  6928 layer_factory.hpp:76] Creating layer pool2
I1127 11:14:32.186835  6928 net.cpp:106] Creating Layer pool2
I1127 11:14:32.186839  6928 net.cpp:454] pool2 <- conv2
I1127 11:14:32.186846  6928 net.cpp:411] pool2 -> pool2
I1127 11:14:32.186879  6928 net.cpp:150] Setting up pool2
I1127 11:14:32.186887  6928 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:14:32.186892  6928 net.cpp:165] Memory required for data: 7674800
I1127 11:14:32.186895  6928 layer_factory.hpp:76] Creating layer ip1
I1127 11:14:32.186913  6928 net.cpp:106] Creating Layer ip1
I1127 11:14:32.186918  6928 net.cpp:454] ip1 <- pool2
I1127 11:14:32.186925  6928 net.cpp:411] ip1 -> ip1
I1127 11:14:32.189208  6928 net.cpp:150] Setting up ip1
I1127 11:14:32.189252  6928 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:14:32.189257  6928 net.cpp:165] Memory required for data: 7874800
I1127 11:14:32.189275  6928 layer_factory.hpp:76] Creating layer relu1
I1127 11:14:32.189285  6928 net.cpp:106] Creating Layer relu1
I1127 11:14:32.189291  6928 net.cpp:454] relu1 <- ip1
I1127 11:14:32.189301  6928 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:14:32.189313  6928 net.cpp:150] Setting up relu1
I1127 11:14:32.189318  6928 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:14:32.189322  6928 net.cpp:165] Memory required for data: 8074800
I1127 11:14:32.189327  6928 layer_factory.hpp:76] Creating layer ip2
I1127 11:14:32.189338  6928 net.cpp:106] Creating Layer ip2
I1127 11:14:32.189343  6928 net.cpp:454] ip2 <- ip1
I1127 11:14:32.189352  6928 net.cpp:411] ip2 -> ip2
I1127 11:14:32.189460  6928 net.cpp:150] Setting up ip2
I1127 11:14:32.189470  6928 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:14:32.189473  6928 net.cpp:165] Memory required for data: 8078800
I1127 11:14:32.189481  6928 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:14:32.189488  6928 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:14:32.189492  6928 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:14:32.189498  6928 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:14:32.189505  6928 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:14:32.189537  6928 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:14:32.189544  6928 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:14:32.189549  6928 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:14:32.189553  6928 net.cpp:165] Memory required for data: 8086800
I1127 11:14:32.189558  6928 layer_factory.hpp:76] Creating layer accuracy
I1127 11:14:32.189568  6928 net.cpp:106] Creating Layer accuracy
I1127 11:14:32.189573  6928 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:14:32.189577  6928 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:14:32.189584  6928 net.cpp:411] accuracy -> accuracy
I1127 11:14:32.189594  6928 net.cpp:150] Setting up accuracy
I1127 11:14:32.189599  6928 net.cpp:157] Top shape: (1)
I1127 11:14:32.189604  6928 net.cpp:165] Memory required for data: 8086804
I1127 11:14:32.189609  6928 layer_factory.hpp:76] Creating layer loss
I1127 11:14:32.189615  6928 net.cpp:106] Creating Layer loss
I1127 11:14:32.189620  6928 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:14:32.189625  6928 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:14:32.189632  6928 net.cpp:411] loss -> loss
I1127 11:14:32.189642  6928 layer_factory.hpp:76] Creating layer loss
I1127 11:14:32.189728  6928 net.cpp:150] Setting up loss
I1127 11:14:32.189735  6928 net.cpp:157] Top shape: (1)
I1127 11:14:32.189739  6928 net.cpp:160]     with loss weight 1
I1127 11:14:32.189755  6928 net.cpp:165] Memory required for data: 8086808
I1127 11:14:32.189759  6928 net.cpp:226] loss needs backward computation.
I1127 11:14:32.189767  6928 net.cpp:228] accuracy does not need backward computation.
I1127 11:14:32.189772  6928 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:14:32.189779  6928 net.cpp:226] ip2 needs backward computation.
I1127 11:14:32.189785  6928 net.cpp:226] relu1 needs backward computation.
I1127 11:14:32.189788  6928 net.cpp:226] ip1 needs backward computation.
I1127 11:14:32.189792  6928 net.cpp:226] pool2 needs backward computation.
I1127 11:14:32.189797  6928 net.cpp:226] conv2 needs backward computation.
I1127 11:14:32.189801  6928 net.cpp:226] pool1 needs backward computation.
I1127 11:14:32.189806  6928 net.cpp:226] conv1 needs backward computation.
I1127 11:14:32.189810  6928 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:14:32.189815  6928 net.cpp:228] mnist does not need backward computation.
I1127 11:14:32.189820  6928 net.cpp:270] This network produces output accuracy
I1127 11:14:32.189828  6928 net.cpp:270] This network produces output loss
I1127 11:14:32.189841  6928 net.cpp:283] Network initialization done.
I1127 11:14:32.189895  6928 solver.cpp:59] Solver scaffolding done.
I1127 11:14:32.190093  6928 caffe.cpp:212] Starting Optimization
I1127 11:14:32.190099  6928 solver.cpp:287] Solving LeNet
I1127 11:14:32.190104  6928 solver.cpp:288] Learning Rate Policy: inv
I1127 11:14:32.190572  6928 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:14:33.586509  6928 solver.cpp:408]     Test net output #0: accuracy = 0.1652
I1127 11:14:33.586626  6928 solver.cpp:408]     Test net output #1: loss = 2.32046 (* 1 = 2.32046 loss)
I1127 11:14:33.598120  6928 solver.cpp:236] Iteration 0, loss = 2.27249
I1127 11:14:33.598222  6928 solver.cpp:252]     Train net output #0: loss = 2.27249 (* 1 = 2.27249 loss)
I1127 11:14:33.598261  6928 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:14:42.979408  6928 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:14:44.456204  6928 solver.cpp:408]     Test net output #0: accuracy = 0.9764
I1127 11:14:44.456251  6928 solver.cpp:408]     Test net output #1: loss = 0.0818815 (* 1 = 0.0818815 loss)
I1127 11:14:44.467875  6928 solver.cpp:236] Iteration 500, loss = 0.128479
I1127 11:14:44.467957  6928 solver.cpp:252]     Train net output #0: loss = 0.128478 (* 1 = 0.128478 loss)
I1127 11:14:44.467983  6928 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:14:54.437459  6928 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:14:54.457180  6928 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:14:54.483532  6928 solver.cpp:320] Iteration 1000, loss = 0.0721739
I1127 11:14:54.483553  6928 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:14:56.850903  6928 solver.cpp:408]     Test net output #0: accuracy = 0.981
I1127 11:14:56.850957  6928 solver.cpp:408]     Test net output #1: loss = 0.0578429 (* 1 = 0.0578429 loss)
I1127 11:14:56.850965  6928 solver.cpp:325] Optimization Done.
I1127 11:14:56.850970  6928 caffe.cpp:215] Optimization Done.
I1127 11:14:56.964589  7004 caffe.cpp:184] Using GPUs 0
I1127 11:14:57.266477  7004 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:14:57.266819  7004 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:14:57.267261  7004 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:14:57.267297  7004 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:14:57.267505  7004 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:14:57.267663  7004 layer_factory.hpp:76] Creating layer mnist
I1127 11:14:57.268437  7004 net.cpp:106] Creating Layer mnist
I1127 11:14:57.268481  7004 net.cpp:411] mnist -> data
I1127 11:14:57.268527  7004 net.cpp:411] mnist -> label
I1127 11:14:57.269646  7007 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:14:57.286913  7004 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:14:57.326287  7004 net.cpp:150] Setting up mnist
I1127 11:14:57.326387  7004 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:14:57.326411  7004 net.cpp:157] Top shape: 64 (64)
I1127 11:14:57.326422  7004 net.cpp:165] Memory required for data: 200960
I1127 11:14:57.326441  7004 layer_factory.hpp:76] Creating layer conv1
I1127 11:14:57.326473  7004 net.cpp:106] Creating Layer conv1
I1127 11:14:57.326490  7004 net.cpp:454] conv1 <- data
I1127 11:14:57.326516  7004 net.cpp:411] conv1 -> conv1
I1127 11:14:57.327873  7004 net.cpp:150] Setting up conv1
I1127 11:14:57.327930  7004 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:14:57.327942  7004 net.cpp:165] Memory required for data: 3150080
I1127 11:14:57.327976  7004 layer_factory.hpp:76] Creating layer pool1
I1127 11:14:57.327998  7004 net.cpp:106] Creating Layer pool1
I1127 11:14:57.328011  7004 net.cpp:454] pool1 <- conv1
I1127 11:14:57.328024  7004 net.cpp:411] pool1 -> pool1
I1127 11:14:57.328105  7004 net.cpp:150] Setting up pool1
I1127 11:14:57.328121  7004 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:14:57.328135  7004 net.cpp:165] Memory required for data: 3887360
I1127 11:14:57.328143  7004 layer_factory.hpp:76] Creating layer conv2
I1127 11:14:57.328164  7004 net.cpp:106] Creating Layer conv2
I1127 11:14:57.328176  7004 net.cpp:454] conv2 <- pool1
I1127 11:14:57.328212  7004 net.cpp:411] conv2 -> conv2
I1127 11:14:57.328845  7004 net.cpp:150] Setting up conv2
I1127 11:14:57.328883  7004 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:14:57.328891  7004 net.cpp:165] Memory required for data: 4706560
I1127 11:14:57.328912  7004 layer_factory.hpp:76] Creating layer pool2
I1127 11:14:57.328933  7004 net.cpp:106] Creating Layer pool2
I1127 11:14:57.328943  7004 net.cpp:454] pool2 <- conv2
I1127 11:14:57.328954  7004 net.cpp:411] pool2 -> pool2
I1127 11:14:57.329030  7004 net.cpp:150] Setting up pool2
I1127 11:14:57.329043  7004 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:14:57.329051  7004 net.cpp:165] Memory required for data: 4911360
I1127 11:14:57.329061  7004 layer_factory.hpp:76] Creating layer ip1
I1127 11:14:57.329077  7004 net.cpp:106] Creating Layer ip1
I1127 11:14:57.329084  7004 net.cpp:454] ip1 <- pool2
I1127 11:14:57.329102  7004 net.cpp:411] ip1 -> ip1
I1127 11:14:57.333722  7004 net.cpp:150] Setting up ip1
I1127 11:14:57.333818  7004 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:14:57.333838  7004 net.cpp:165] Memory required for data: 5039360
I1127 11:14:57.333878  7004 layer_factory.hpp:76] Creating layer relu1
I1127 11:14:57.333920  7004 net.cpp:106] Creating Layer relu1
I1127 11:14:57.333936  7004 net.cpp:454] relu1 <- ip1
I1127 11:14:57.333956  7004 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:14:57.334000  7004 net.cpp:150] Setting up relu1
I1127 11:14:57.334053  7004 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:14:57.334069  7004 net.cpp:165] Memory required for data: 5167360
I1127 11:14:57.334085  7004 layer_factory.hpp:76] Creating layer ip2
I1127 11:14:57.334133  7004 net.cpp:106] Creating Layer ip2
I1127 11:14:57.334174  7004 net.cpp:454] ip2 <- ip1
I1127 11:14:57.334206  7004 net.cpp:411] ip2 -> ip2
I1127 11:14:57.335748  7004 net.cpp:150] Setting up ip2
I1127 11:14:57.335796  7004 net.cpp:157] Top shape: 64 10 (640)
I1127 11:14:57.335809  7004 net.cpp:165] Memory required for data: 5169920
I1127 11:14:57.335829  7004 layer_factory.hpp:76] Creating layer loss
I1127 11:14:57.335855  7004 net.cpp:106] Creating Layer loss
I1127 11:14:57.335866  7004 net.cpp:454] loss <- ip2
I1127 11:14:57.335878  7004 net.cpp:454] loss <- label
I1127 11:14:57.335901  7004 net.cpp:411] loss -> loss
I1127 11:14:57.335947  7004 layer_factory.hpp:76] Creating layer loss
I1127 11:14:57.336206  7004 net.cpp:150] Setting up loss
I1127 11:14:57.336233  7004 net.cpp:157] Top shape: (1)
I1127 11:14:57.336241  7004 net.cpp:160]     with loss weight 1
I1127 11:14:57.336283  7004 net.cpp:165] Memory required for data: 5169924
I1127 11:14:57.336294  7004 net.cpp:226] loss needs backward computation.
I1127 11:14:57.336305  7004 net.cpp:226] ip2 needs backward computation.
I1127 11:14:57.336314  7004 net.cpp:226] relu1 needs backward computation.
I1127 11:14:57.336324  7004 net.cpp:226] ip1 needs backward computation.
I1127 11:14:57.336331  7004 net.cpp:226] pool2 needs backward computation.
I1127 11:14:57.336340  7004 net.cpp:226] conv2 needs backward computation.
I1127 11:14:57.336349  7004 net.cpp:226] pool1 needs backward computation.
I1127 11:14:57.336355  7004 net.cpp:226] conv1 needs backward computation.
I1127 11:14:57.336364  7004 net.cpp:228] mnist does not need backward computation.
I1127 11:14:57.336371  7004 net.cpp:270] This network produces output loss
I1127 11:14:57.336391  7004 net.cpp:283] Network initialization done.
I1127 11:14:57.336995  7004 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:14:57.337070  7004 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:14:57.337342  7004 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:14:57.337535  7004 layer_factory.hpp:76] Creating layer mnist
I1127 11:14:57.337759  7004 net.cpp:106] Creating Layer mnist
I1127 11:14:57.337780  7004 net.cpp:411] mnist -> data
I1127 11:14:57.337805  7004 net.cpp:411] mnist -> label
I1127 11:14:57.339115  7009 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:14:57.339576  7004 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:14:57.341264  7004 net.cpp:150] Setting up mnist
I1127 11:14:57.341341  7004 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:14:57.341353  7004 net.cpp:157] Top shape: 100 (100)
I1127 11:14:57.341361  7004 net.cpp:165] Memory required for data: 314000
I1127 11:14:57.341377  7004 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:14:57.341397  7004 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:14:57.341405  7004 net.cpp:454] label_mnist_1_split <- label
I1127 11:14:57.341421  7004 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:14:57.341444  7004 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:14:57.341513  7004 net.cpp:150] Setting up label_mnist_1_split
I1127 11:14:57.341528  7004 net.cpp:157] Top shape: 100 (100)
I1127 11:14:57.341537  7004 net.cpp:157] Top shape: 100 (100)
I1127 11:14:57.341544  7004 net.cpp:165] Memory required for data: 314800
I1127 11:14:57.341552  7004 layer_factory.hpp:76] Creating layer conv1
I1127 11:14:57.341572  7004 net.cpp:106] Creating Layer conv1
I1127 11:14:57.341581  7004 net.cpp:454] conv1 <- data
I1127 11:14:57.341593  7004 net.cpp:411] conv1 -> conv1
I1127 11:14:57.341912  7004 net.cpp:150] Setting up conv1
I1127 11:14:57.341943  7004 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:14:57.341953  7004 net.cpp:165] Memory required for data: 4922800
I1127 11:14:57.341977  7004 layer_factory.hpp:76] Creating layer pool1
I1127 11:14:57.341998  7004 net.cpp:106] Creating Layer pool1
I1127 11:14:57.342010  7004 net.cpp:454] pool1 <- conv1
I1127 11:14:57.342061  7004 net.cpp:411] pool1 -> pool1
I1127 11:14:57.342113  7004 net.cpp:150] Setting up pool1
I1127 11:14:57.342126  7004 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:14:57.342133  7004 net.cpp:165] Memory required for data: 6074800
I1127 11:14:57.342155  7004 layer_factory.hpp:76] Creating layer conv2
I1127 11:14:57.342176  7004 net.cpp:106] Creating Layer conv2
I1127 11:14:57.342185  7004 net.cpp:454] conv2 <- pool1
I1127 11:14:57.342196  7004 net.cpp:411] conv2 -> conv2
I1127 11:14:57.343559  7004 net.cpp:150] Setting up conv2
I1127 11:14:57.343595  7004 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:14:57.343601  7004 net.cpp:165] Memory required for data: 7354800
I1127 11:14:57.343621  7004 layer_factory.hpp:76] Creating layer pool2
I1127 11:14:57.343638  7004 net.cpp:106] Creating Layer pool2
I1127 11:14:57.343647  7004 net.cpp:454] pool2 <- conv2
I1127 11:14:57.343659  7004 net.cpp:411] pool2 -> pool2
I1127 11:14:57.343710  7004 net.cpp:150] Setting up pool2
I1127 11:14:57.343722  7004 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:14:57.343729  7004 net.cpp:165] Memory required for data: 7674800
I1127 11:14:57.343736  7004 layer_factory.hpp:76] Creating layer ip1
I1127 11:14:57.343755  7004 net.cpp:106] Creating Layer ip1
I1127 11:14:57.343762  7004 net.cpp:454] ip1 <- pool2
I1127 11:14:57.343772  7004 net.cpp:411] ip1 -> ip1
I1127 11:14:57.348523  7004 net.cpp:150] Setting up ip1
I1127 11:14:57.348579  7004 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:14:57.348589  7004 net.cpp:165] Memory required for data: 7874800
I1127 11:14:57.348613  7004 layer_factory.hpp:76] Creating layer relu1
I1127 11:14:57.348631  7004 net.cpp:106] Creating Layer relu1
I1127 11:14:57.348641  7004 net.cpp:454] relu1 <- ip1
I1127 11:14:57.348656  7004 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:14:57.348685  7004 net.cpp:150] Setting up relu1
I1127 11:14:57.348695  7004 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:14:57.348702  7004 net.cpp:165] Memory required for data: 8074800
I1127 11:14:57.348711  7004 layer_factory.hpp:76] Creating layer ip2
I1127 11:14:57.348726  7004 net.cpp:106] Creating Layer ip2
I1127 11:14:57.348736  7004 net.cpp:454] ip2 <- ip1
I1127 11:14:57.348752  7004 net.cpp:411] ip2 -> ip2
I1127 11:14:57.348927  7004 net.cpp:150] Setting up ip2
I1127 11:14:57.348942  7004 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:14:57.348948  7004 net.cpp:165] Memory required for data: 8078800
I1127 11:14:57.348960  7004 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:14:57.348973  7004 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:14:57.348980  7004 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:14:57.348992  7004 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:14:57.349004  7004 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:14:57.349045  7004 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:14:57.349056  7004 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:14:57.349066  7004 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:14:57.349072  7004 net.cpp:165] Memory required for data: 8086800
I1127 11:14:57.349079  7004 layer_factory.hpp:76] Creating layer accuracy
I1127 11:14:57.349095  7004 net.cpp:106] Creating Layer accuracy
I1127 11:14:57.349103  7004 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:14:57.349112  7004 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:14:57.349123  7004 net.cpp:411] accuracy -> accuracy
I1127 11:14:57.349143  7004 net.cpp:150] Setting up accuracy
I1127 11:14:57.349153  7004 net.cpp:157] Top shape: (1)
I1127 11:14:57.349159  7004 net.cpp:165] Memory required for data: 8086804
I1127 11:14:57.349166  7004 layer_factory.hpp:76] Creating layer loss
I1127 11:14:57.349179  7004 net.cpp:106] Creating Layer loss
I1127 11:14:57.349185  7004 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:14:57.349194  7004 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:14:57.349202  7004 net.cpp:411] loss -> loss
I1127 11:14:57.349218  7004 layer_factory.hpp:76] Creating layer loss
I1127 11:14:57.349341  7004 net.cpp:150] Setting up loss
I1127 11:14:57.349354  7004 net.cpp:157] Top shape: (1)
I1127 11:14:57.349361  7004 net.cpp:160]     with loss weight 1
I1127 11:14:57.349381  7004 net.cpp:165] Memory required for data: 8086808
I1127 11:14:57.349390  7004 net.cpp:226] loss needs backward computation.
I1127 11:14:57.349402  7004 net.cpp:228] accuracy does not need backward computation.
I1127 11:14:57.349411  7004 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:14:57.349416  7004 net.cpp:226] ip2 needs backward computation.
I1127 11:14:57.349423  7004 net.cpp:226] relu1 needs backward computation.
I1127 11:14:57.349431  7004 net.cpp:226] ip1 needs backward computation.
I1127 11:14:57.349437  7004 net.cpp:226] pool2 needs backward computation.
I1127 11:14:57.349445  7004 net.cpp:226] conv2 needs backward computation.
I1127 11:14:57.349453  7004 net.cpp:226] pool1 needs backward computation.
I1127 11:14:57.349462  7004 net.cpp:226] conv1 needs backward computation.
I1127 11:14:57.349469  7004 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:14:57.349478  7004 net.cpp:228] mnist does not need backward computation.
I1127 11:14:57.349484  7004 net.cpp:270] This network produces output accuracy
I1127 11:14:57.349493  7004 net.cpp:270] This network produces output loss
I1127 11:14:57.349511  7004 net.cpp:283] Network initialization done.
I1127 11:14:57.349594  7004 solver.cpp:59] Solver scaffolding done.
I1127 11:14:57.349903  7004 caffe.cpp:212] Starting Optimization
I1127 11:14:57.349921  7004 solver.cpp:287] Solving LeNet
I1127 11:14:57.349928  7004 solver.cpp:288] Learning Rate Policy: inv
I1127 11:14:57.350836  7004 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:14:58.515597  7004 solver.cpp:408]     Test net output #0: accuracy = 0.0839
I1127 11:14:58.515663  7004 solver.cpp:408]     Test net output #1: loss = 2.34195 (* 1 = 2.34195 loss)
I1127 11:14:58.547667  7004 solver.cpp:236] Iteration 0, loss = 2.31371
I1127 11:14:58.547744  7004 solver.cpp:252]     Train net output #0: loss = 2.31371 (* 1 = 2.31371 loss)
I1127 11:14:58.547780  7004 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:15:12.499402  7004 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:15:14.907436  7004 solver.cpp:408]     Test net output #0: accuracy = 0.9724
I1127 11:15:14.907527  7004 solver.cpp:408]     Test net output #1: loss = 0.0841659 (* 1 = 0.0841659 loss)
I1127 11:15:14.917332  7004 solver.cpp:236] Iteration 500, loss = 0.0769934
I1127 11:15:14.917376  7004 solver.cpp:252]     Train net output #0: loss = 0.0769934 (* 1 = 0.0769934 loss)
I1127 11:15:14.917388  7004 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:15:26.392729  7004 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:15:26.412622  7004 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:15:26.438865  7004 solver.cpp:320] Iteration 1000, loss = 0.102062
I1127 11:15:26.438899  7004 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:15:29.457479  7004 solver.cpp:408]     Test net output #0: accuracy = 0.9821
I1127 11:15:29.457608  7004 solver.cpp:408]     Test net output #1: loss = 0.0595242 (* 1 = 0.0595242 loss)
I1127 11:15:29.457617  7004 solver.cpp:325] Optimization Done.
I1127 11:15:29.457623  7004 caffe.cpp:215] Optimization Done.
I1127 11:15:29.569577  7031 caffe.cpp:184] Using GPUs 0
I1127 11:15:29.966370  7031 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:15:29.966750  7031 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:15:29.967314  7031 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:15:29.967371  7031 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:15:29.967593  7031 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:15:29.967741  7031 layer_factory.hpp:76] Creating layer mnist
I1127 11:15:29.968438  7031 net.cpp:106] Creating Layer mnist
I1127 11:15:29.968479  7031 net.cpp:411] mnist -> data
I1127 11:15:29.968530  7031 net.cpp:411] mnist -> label
I1127 11:15:29.969894  7035 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:15:29.991842  7031 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:15:29.993819  7031 net.cpp:150] Setting up mnist
I1127 11:15:29.993948  7031 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:15:29.993966  7031 net.cpp:157] Top shape: 64 (64)
I1127 11:15:29.993978  7031 net.cpp:165] Memory required for data: 200960
I1127 11:15:29.994005  7031 layer_factory.hpp:76] Creating layer conv1
I1127 11:15:29.994071  7031 net.cpp:106] Creating Layer conv1
I1127 11:15:29.994093  7031 net.cpp:454] conv1 <- data
I1127 11:15:29.994133  7031 net.cpp:411] conv1 -> conv1
I1127 11:15:29.999711  7031 net.cpp:150] Setting up conv1
I1127 11:15:29.999812  7031 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:15:29.999827  7031 net.cpp:165] Memory required for data: 3150080
I1127 11:15:29.999876  7031 layer_factory.hpp:76] Creating layer pool1
I1127 11:15:29.999920  7031 net.cpp:106] Creating Layer pool1
I1127 11:15:29.999938  7031 net.cpp:454] pool1 <- conv1
I1127 11:15:29.999956  7031 net.cpp:411] pool1 -> pool1
I1127 11:15:30.000155  7031 net.cpp:150] Setting up pool1
I1127 11:15:30.000174  7031 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:15:30.000183  7031 net.cpp:165] Memory required for data: 3887360
I1127 11:15:30.000191  7031 layer_factory.hpp:76] Creating layer conv2
I1127 11:15:30.000218  7031 net.cpp:106] Creating Layer conv2
I1127 11:15:30.000231  7031 net.cpp:454] conv2 <- pool1
I1127 11:15:30.000244  7031 net.cpp:411] conv2 -> conv2
I1127 11:15:30.000782  7031 net.cpp:150] Setting up conv2
I1127 11:15:30.000838  7031 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:15:30.000847  7031 net.cpp:165] Memory required for data: 4706560
I1127 11:15:30.000874  7031 layer_factory.hpp:76] Creating layer pool2
I1127 11:15:30.000900  7031 net.cpp:106] Creating Layer pool2
I1127 11:15:30.000911  7031 net.cpp:454] pool2 <- conv2
I1127 11:15:30.000924  7031 net.cpp:411] pool2 -> pool2
I1127 11:15:30.000993  7031 net.cpp:150] Setting up pool2
I1127 11:15:30.001005  7031 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:15:30.001013  7031 net.cpp:165] Memory required for data: 4911360
I1127 11:15:30.001019  7031 layer_factory.hpp:76] Creating layer ip1
I1127 11:15:30.001039  7031 net.cpp:106] Creating Layer ip1
I1127 11:15:30.001047  7031 net.cpp:454] ip1 <- pool2
I1127 11:15:30.001060  7031 net.cpp:411] ip1 -> ip1
I1127 11:15:30.006186  7031 net.cpp:150] Setting up ip1
I1127 11:15:30.006280  7031 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:15:30.006291  7031 net.cpp:165] Memory required for data: 5039360
I1127 11:15:30.006321  7031 layer_factory.hpp:76] Creating layer relu1
I1127 11:15:30.006355  7031 net.cpp:106] Creating Layer relu1
I1127 11:15:30.006368  7031 net.cpp:454] relu1 <- ip1
I1127 11:15:30.006384  7031 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:15:30.006415  7031 net.cpp:150] Setting up relu1
I1127 11:15:30.006428  7031 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:15:30.006436  7031 net.cpp:165] Memory required for data: 5167360
I1127 11:15:30.006444  7031 layer_factory.hpp:76] Creating layer ip2
I1127 11:15:30.006461  7031 net.cpp:106] Creating Layer ip2
I1127 11:15:30.006476  7031 net.cpp:454] ip2 <- ip1
I1127 11:15:30.006491  7031 net.cpp:411] ip2 -> ip2
I1127 11:15:30.007638  7031 net.cpp:150] Setting up ip2
I1127 11:15:30.007675  7031 net.cpp:157] Top shape: 64 10 (640)
I1127 11:15:30.007684  7031 net.cpp:165] Memory required for data: 5169920
I1127 11:15:30.007719  7031 layer_factory.hpp:76] Creating layer loss
I1127 11:15:30.007753  7031 net.cpp:106] Creating Layer loss
I1127 11:15:30.007767  7031 net.cpp:454] loss <- ip2
I1127 11:15:30.007782  7031 net.cpp:454] loss <- label
I1127 11:15:30.007805  7031 net.cpp:411] loss -> loss
I1127 11:15:30.007846  7031 layer_factory.hpp:76] Creating layer loss
I1127 11:15:30.008059  7031 net.cpp:150] Setting up loss
I1127 11:15:30.008080  7031 net.cpp:157] Top shape: (1)
I1127 11:15:30.008086  7031 net.cpp:160]     with loss weight 1
I1127 11:15:30.008127  7031 net.cpp:165] Memory required for data: 5169924
I1127 11:15:30.008137  7031 net.cpp:226] loss needs backward computation.
I1127 11:15:30.008147  7031 net.cpp:226] ip2 needs backward computation.
I1127 11:15:30.008154  7031 net.cpp:226] relu1 needs backward computation.
I1127 11:15:30.008162  7031 net.cpp:226] ip1 needs backward computation.
I1127 11:15:30.008172  7031 net.cpp:226] pool2 needs backward computation.
I1127 11:15:30.008178  7031 net.cpp:226] conv2 needs backward computation.
I1127 11:15:30.008186  7031 net.cpp:226] pool1 needs backward computation.
I1127 11:15:30.008194  7031 net.cpp:226] conv1 needs backward computation.
I1127 11:15:30.008203  7031 net.cpp:228] mnist does not need backward computation.
I1127 11:15:30.008209  7031 net.cpp:270] This network produces output loss
I1127 11:15:30.008227  7031 net.cpp:283] Network initialization done.
I1127 11:15:30.008684  7031 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:15:30.008726  7031 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:15:30.008911  7031 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:15:30.009035  7031 layer_factory.hpp:76] Creating layer mnist
I1127 11:15:30.009225  7031 net.cpp:106] Creating Layer mnist
I1127 11:15:30.009241  7031 net.cpp:411] mnist -> data
I1127 11:15:30.009259  7031 net.cpp:411] mnist -> label
I1127 11:15:30.011713  7037 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:15:30.012109  7031 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:15:30.014664  7031 net.cpp:150] Setting up mnist
I1127 11:15:30.014763  7031 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:15:30.014780  7031 net.cpp:157] Top shape: 100 (100)
I1127 11:15:30.014788  7031 net.cpp:165] Memory required for data: 314000
I1127 11:15:30.014804  7031 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:15:30.014838  7031 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:15:30.014853  7031 net.cpp:454] label_mnist_1_split <- label
I1127 11:15:30.014870  7031 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:15:30.014897  7031 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:15:30.015147  7031 net.cpp:150] Setting up label_mnist_1_split
I1127 11:15:30.015180  7031 net.cpp:157] Top shape: 100 (100)
I1127 11:15:30.015197  7031 net.cpp:157] Top shape: 100 (100)
I1127 11:15:30.015204  7031 net.cpp:165] Memory required for data: 314800
I1127 11:15:30.015213  7031 layer_factory.hpp:76] Creating layer conv1
I1127 11:15:30.015249  7031 net.cpp:106] Creating Layer conv1
I1127 11:15:30.015262  7031 net.cpp:454] conv1 <- data
I1127 11:15:30.015280  7031 net.cpp:411] conv1 -> conv1
I1127 11:15:30.015683  7031 net.cpp:150] Setting up conv1
I1127 11:15:30.015712  7031 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:15:30.015722  7031 net.cpp:165] Memory required for data: 4922800
I1127 11:15:30.015745  7031 layer_factory.hpp:76] Creating layer pool1
I1127 11:15:30.015768  7031 net.cpp:106] Creating Layer pool1
I1127 11:15:30.015776  7031 net.cpp:454] pool1 <- conv1
I1127 11:15:30.015825  7031 net.cpp:411] pool1 -> pool1
I1127 11:15:30.015892  7031 net.cpp:150] Setting up pool1
I1127 11:15:30.015908  7031 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:15:30.015916  7031 net.cpp:165] Memory required for data: 6074800
I1127 11:15:30.015924  7031 layer_factory.hpp:76] Creating layer conv2
I1127 11:15:30.015947  7031 net.cpp:106] Creating Layer conv2
I1127 11:15:30.015957  7031 net.cpp:454] conv2 <- pool1
I1127 11:15:30.015970  7031 net.cpp:411] conv2 -> conv2
I1127 11:15:30.016479  7031 net.cpp:150] Setting up conv2
I1127 11:15:30.016530  7031 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:15:30.016540  7031 net.cpp:165] Memory required for data: 7354800
I1127 11:15:30.016566  7031 layer_factory.hpp:76] Creating layer pool2
I1127 11:15:30.016593  7031 net.cpp:106] Creating Layer pool2
I1127 11:15:30.016605  7031 net.cpp:454] pool2 <- conv2
I1127 11:15:30.016623  7031 net.cpp:411] pool2 -> pool2
I1127 11:15:30.016701  7031 net.cpp:150] Setting up pool2
I1127 11:15:30.016721  7031 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:15:30.016729  7031 net.cpp:165] Memory required for data: 7674800
I1127 11:15:30.016736  7031 layer_factory.hpp:76] Creating layer ip1
I1127 11:15:30.016754  7031 net.cpp:106] Creating Layer ip1
I1127 11:15:30.016764  7031 net.cpp:454] ip1 <- pool2
I1127 11:15:30.016779  7031 net.cpp:411] ip1 -> ip1
I1127 11:15:30.021956  7031 net.cpp:150] Setting up ip1
I1127 11:15:30.022073  7031 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:15:30.022089  7031 net.cpp:165] Memory required for data: 7874800
I1127 11:15:30.022127  7031 layer_factory.hpp:76] Creating layer relu1
I1127 11:15:30.026986  7031 net.cpp:106] Creating Layer relu1
I1127 11:15:30.027233  7031 net.cpp:454] relu1 <- ip1
I1127 11:15:30.027328  7031 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:15:30.027436  7031 net.cpp:150] Setting up relu1
I1127 11:15:30.027508  7031 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:15:30.027542  7031 net.cpp:165] Memory required for data: 8074800
I1127 11:15:30.027601  7031 layer_factory.hpp:76] Creating layer ip2
I1127 11:15:30.027649  7031 net.cpp:106] Creating Layer ip2
I1127 11:15:30.027660  7031 net.cpp:454] ip2 <- ip1
I1127 11:15:30.027673  7031 net.cpp:411] ip2 -> ip2
I1127 11:15:30.027990  7031 net.cpp:150] Setting up ip2
I1127 11:15:30.028012  7031 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:15:30.028028  7031 net.cpp:165] Memory required for data: 8078800
I1127 11:15:30.028043  7031 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:15:30.028056  7031 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:15:30.028065  7031 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:15:30.028075  7031 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:15:30.028089  7031 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:15:30.028133  7031 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:15:30.028146  7031 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:15:30.028154  7031 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:15:30.028162  7031 net.cpp:165] Memory required for data: 8086800
I1127 11:15:30.028169  7031 layer_factory.hpp:76] Creating layer accuracy
I1127 11:15:30.028185  7031 net.cpp:106] Creating Layer accuracy
I1127 11:15:30.028192  7031 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:15:30.028201  7031 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:15:30.028211  7031 net.cpp:411] accuracy -> accuracy
I1127 11:15:30.028226  7031 net.cpp:150] Setting up accuracy
I1127 11:15:30.028236  7031 net.cpp:157] Top shape: (1)
I1127 11:15:30.028244  7031 net.cpp:165] Memory required for data: 8086804
I1127 11:15:30.028251  7031 layer_factory.hpp:76] Creating layer loss
I1127 11:15:30.028265  7031 net.cpp:106] Creating Layer loss
I1127 11:15:30.028273  7031 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:15:30.028282  7031 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:15:30.028292  7031 net.cpp:411] loss -> loss
I1127 11:15:30.028312  7031 layer_factory.hpp:76] Creating layer loss
I1127 11:15:30.028475  7031 net.cpp:150] Setting up loss
I1127 11:15:30.028496  7031 net.cpp:157] Top shape: (1)
I1127 11:15:30.028503  7031 net.cpp:160]     with loss weight 1
I1127 11:15:30.028527  7031 net.cpp:165] Memory required for data: 8086808
I1127 11:15:30.028537  7031 net.cpp:226] loss needs backward computation.
I1127 11:15:30.028550  7031 net.cpp:228] accuracy does not need backward computation.
I1127 11:15:30.028558  7031 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:15:30.028566  7031 net.cpp:226] ip2 needs backward computation.
I1127 11:15:30.028573  7031 net.cpp:226] relu1 needs backward computation.
I1127 11:15:30.028581  7031 net.cpp:226] ip1 needs backward computation.
I1127 11:15:30.028589  7031 net.cpp:226] pool2 needs backward computation.
I1127 11:15:30.028597  7031 net.cpp:226] conv2 needs backward computation.
I1127 11:15:30.028606  7031 net.cpp:226] pool1 needs backward computation.
I1127 11:15:30.028614  7031 net.cpp:226] conv1 needs backward computation.
I1127 11:15:30.028622  7031 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:15:30.028631  7031 net.cpp:228] mnist does not need backward computation.
I1127 11:15:30.028638  7031 net.cpp:270] This network produces output accuracy
I1127 11:15:30.028645  7031 net.cpp:270] This network produces output loss
I1127 11:15:30.028662  7031 net.cpp:283] Network initialization done.
I1127 11:15:30.028736  7031 solver.cpp:59] Solver scaffolding done.
I1127 11:15:30.029072  7031 caffe.cpp:212] Starting Optimization
I1127 11:15:30.029084  7031 solver.cpp:287] Solving LeNet
I1127 11:15:30.029093  7031 solver.cpp:288] Learning Rate Policy: inv
I1127 11:15:30.029973  7031 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:15:31.197681  7031 solver.cpp:408]     Test net output #0: accuracy = 0.1201
I1127 11:15:31.197780  7031 solver.cpp:408]     Test net output #1: loss = 2.34656 (* 1 = 2.34656 loss)
I1127 11:15:31.209959  7031 solver.cpp:236] Iteration 0, loss = 2.33785
I1127 11:15:31.210057  7031 solver.cpp:252]     Train net output #0: loss = 2.33785 (* 1 = 2.33785 loss)
I1127 11:15:31.210099  7031 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:15:44.640007  7031 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:15:46.466915  7031 solver.cpp:408]     Test net output #0: accuracy = 0.9747
I1127 11:15:46.466976  7031 solver.cpp:408]     Test net output #1: loss = 0.0814748 (* 1 = 0.0814748 loss)
I1127 11:15:46.498236  7031 solver.cpp:236] Iteration 500, loss = 0.0856645
I1127 11:15:46.498307  7031 solver.cpp:252]     Train net output #0: loss = 0.0856645 (* 1 = 0.0856645 loss)
I1127 11:15:46.498322  7031 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:15:59.767293  7031 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:15:59.787127  7031 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:15:59.813659  7031 solver.cpp:320] Iteration 1000, loss = 0.0830486
I1127 11:15:59.813679  7031 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:16:02.207878  7031 solver.cpp:408]     Test net output #0: accuracy = 0.9818
I1127 11:16:02.208019  7031 solver.cpp:408]     Test net output #1: loss = 0.0552621 (* 1 = 0.0552621 loss)
I1127 11:16:02.208029  7031 solver.cpp:325] Optimization Done.
I1127 11:16:02.208036  7031 caffe.cpp:215] Optimization Done.
I1127 11:16:02.395645  7059 caffe.cpp:184] Using GPUs 0
I1127 11:16:02.736215  7059 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:16:02.736364  7059 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:16:02.736762  7059 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:16:02.736788  7059 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:16:02.736929  7059 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:16:02.737059  7059 layer_factory.hpp:76] Creating layer mnist
I1127 11:16:02.737859  7059 net.cpp:106] Creating Layer mnist
I1127 11:16:02.737907  7059 net.cpp:411] mnist -> data
I1127 11:16:02.737972  7059 net.cpp:411] mnist -> label
I1127 11:16:02.739395  7063 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:16:02.750483  7059 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:16:02.751814  7059 net.cpp:150] Setting up mnist
I1127 11:16:02.751880  7059 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:16:02.751890  7059 net.cpp:157] Top shape: 64 (64)
I1127 11:16:02.751898  7059 net.cpp:165] Memory required for data: 200960
I1127 11:16:02.751914  7059 layer_factory.hpp:76] Creating layer conv1
I1127 11:16:02.751952  7059 net.cpp:106] Creating Layer conv1
I1127 11:16:02.751967  7059 net.cpp:454] conv1 <- data
I1127 11:16:02.751994  7059 net.cpp:411] conv1 -> conv1
I1127 11:16:02.752902  7059 net.cpp:150] Setting up conv1
I1127 11:16:02.752935  7059 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:16:02.752943  7059 net.cpp:165] Memory required for data: 3150080
I1127 11:16:02.752962  7059 layer_factory.hpp:76] Creating layer pool1
I1127 11:16:02.752976  7059 net.cpp:106] Creating Layer pool1
I1127 11:16:02.752985  7059 net.cpp:454] pool1 <- conv1
I1127 11:16:02.752993  7059 net.cpp:411] pool1 -> pool1
I1127 11:16:02.753105  7059 net.cpp:150] Setting up pool1
I1127 11:16:02.753118  7059 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:16:02.753123  7059 net.cpp:165] Memory required for data: 3887360
I1127 11:16:02.753130  7059 layer_factory.hpp:76] Creating layer conv2
I1127 11:16:02.753144  7059 net.cpp:106] Creating Layer conv2
I1127 11:16:02.753152  7059 net.cpp:454] conv2 <- pool1
I1127 11:16:02.753162  7059 net.cpp:411] conv2 -> conv2
I1127 11:16:02.753589  7059 net.cpp:150] Setting up conv2
I1127 11:16:02.753602  7059 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:16:02.753609  7059 net.cpp:165] Memory required for data: 4706560
I1127 11:16:02.753621  7059 layer_factory.hpp:76] Creating layer pool2
I1127 11:16:02.753633  7059 net.cpp:106] Creating Layer pool2
I1127 11:16:02.753640  7059 net.cpp:454] pool2 <- conv2
I1127 11:16:02.753649  7059 net.cpp:411] pool2 -> pool2
I1127 11:16:02.753686  7059 net.cpp:150] Setting up pool2
I1127 11:16:02.753696  7059 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:16:02.753706  7059 net.cpp:165] Memory required for data: 4911360
I1127 11:16:02.753713  7059 layer_factory.hpp:76] Creating layer ip1
I1127 11:16:02.753725  7059 net.cpp:106] Creating Layer ip1
I1127 11:16:02.753731  7059 net.cpp:454] ip1 <- pool2
I1127 11:16:02.753742  7059 net.cpp:411] ip1 -> ip1
I1127 11:16:02.757598  7059 net.cpp:150] Setting up ip1
I1127 11:16:02.757685  7059 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:16:02.757701  7059 net.cpp:165] Memory required for data: 5039360
I1127 11:16:02.757741  7059 layer_factory.hpp:76] Creating layer relu1
I1127 11:16:02.757766  7059 net.cpp:106] Creating Layer relu1
I1127 11:16:02.757781  7059 net.cpp:454] relu1 <- ip1
I1127 11:16:02.757797  7059 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:16:02.757828  7059 net.cpp:150] Setting up relu1
I1127 11:16:02.757843  7059 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:16:02.757853  7059 net.cpp:165] Memory required for data: 5167360
I1127 11:16:02.757864  7059 layer_factory.hpp:76] Creating layer ip2
I1127 11:16:02.757891  7059 net.cpp:106] Creating Layer ip2
I1127 11:16:02.757900  7059 net.cpp:454] ip2 <- ip1
I1127 11:16:02.757910  7059 net.cpp:411] ip2 -> ip2
I1127 11:16:02.758780  7059 net.cpp:150] Setting up ip2
I1127 11:16:02.758852  7059 net.cpp:157] Top shape: 64 10 (640)
I1127 11:16:02.758863  7059 net.cpp:165] Memory required for data: 5169920
I1127 11:16:02.758884  7059 layer_factory.hpp:76] Creating layer loss
I1127 11:16:02.758903  7059 net.cpp:106] Creating Layer loss
I1127 11:16:02.758913  7059 net.cpp:454] loss <- ip2
I1127 11:16:02.758924  7059 net.cpp:454] loss <- label
I1127 11:16:02.758937  7059 net.cpp:411] loss -> loss
I1127 11:16:02.758958  7059 layer_factory.hpp:76] Creating layer loss
I1127 11:16:02.759058  7059 net.cpp:150] Setting up loss
I1127 11:16:02.759070  7059 net.cpp:157] Top shape: (1)
I1127 11:16:02.759078  7059 net.cpp:160]     with loss weight 1
I1127 11:16:02.759101  7059 net.cpp:165] Memory required for data: 5169924
I1127 11:16:02.759119  7059 net.cpp:226] loss needs backward computation.
I1127 11:16:02.759127  7059 net.cpp:226] ip2 needs backward computation.
I1127 11:16:02.759135  7059 net.cpp:226] relu1 needs backward computation.
I1127 11:16:02.759142  7059 net.cpp:226] ip1 needs backward computation.
I1127 11:16:02.759150  7059 net.cpp:226] pool2 needs backward computation.
I1127 11:16:02.759156  7059 net.cpp:226] conv2 needs backward computation.
I1127 11:16:02.759163  7059 net.cpp:226] pool1 needs backward computation.
I1127 11:16:02.759171  7059 net.cpp:226] conv1 needs backward computation.
I1127 11:16:02.759179  7059 net.cpp:228] mnist does not need backward computation.
I1127 11:16:02.759186  7059 net.cpp:270] This network produces output loss
I1127 11:16:02.759199  7059 net.cpp:283] Network initialization done.
I1127 11:16:02.759579  7059 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:16:02.759615  7059 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:16:02.759784  7059 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:16:02.759877  7059 layer_factory.hpp:76] Creating layer mnist
I1127 11:16:02.760033  7059 net.cpp:106] Creating Layer mnist
I1127 11:16:02.760066  7059 net.cpp:411] mnist -> data
I1127 11:16:02.760084  7059 net.cpp:411] mnist -> label
I1127 11:16:02.760895  7065 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:16:02.761098  7059 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:16:02.762440  7059 net.cpp:150] Setting up mnist
I1127 11:16:02.762490  7059 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:16:02.762497  7059 net.cpp:157] Top shape: 100 (100)
I1127 11:16:02.762501  7059 net.cpp:165] Memory required for data: 314000
I1127 11:16:02.762509  7059 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:16:02.762526  7059 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:16:02.762540  7059 net.cpp:454] label_mnist_1_split <- label
I1127 11:16:02.762548  7059 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:16:02.762559  7059 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:16:02.762599  7059 net.cpp:150] Setting up label_mnist_1_split
I1127 11:16:02.762608  7059 net.cpp:157] Top shape: 100 (100)
I1127 11:16:02.762612  7059 net.cpp:157] Top shape: 100 (100)
I1127 11:16:02.762616  7059 net.cpp:165] Memory required for data: 314800
I1127 11:16:02.762620  7059 layer_factory.hpp:76] Creating layer conv1
I1127 11:16:02.762634  7059 net.cpp:106] Creating Layer conv1
I1127 11:16:02.762639  7059 net.cpp:454] conv1 <- data
I1127 11:16:02.762647  7059 net.cpp:411] conv1 -> conv1
I1127 11:16:02.762804  7059 net.cpp:150] Setting up conv1
I1127 11:16:02.762812  7059 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:16:02.762816  7059 net.cpp:165] Memory required for data: 4922800
I1127 11:16:02.762826  7059 layer_factory.hpp:76] Creating layer pool1
I1127 11:16:02.762837  7059 net.cpp:106] Creating Layer pool1
I1127 11:16:02.762841  7059 net.cpp:454] pool1 <- conv1
I1127 11:16:02.762861  7059 net.cpp:411] pool1 -> pool1
I1127 11:16:02.762893  7059 net.cpp:150] Setting up pool1
I1127 11:16:02.762900  7059 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:16:02.762905  7059 net.cpp:165] Memory required for data: 6074800
I1127 11:16:02.762909  7059 layer_factory.hpp:76] Creating layer conv2
I1127 11:16:02.762919  7059 net.cpp:106] Creating Layer conv2
I1127 11:16:02.762924  7059 net.cpp:454] conv2 <- pool1
I1127 11:16:02.762933  7059 net.cpp:411] conv2 -> conv2
I1127 11:16:02.763200  7059 net.cpp:150] Setting up conv2
I1127 11:16:02.763208  7059 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:16:02.763212  7059 net.cpp:165] Memory required for data: 7354800
I1127 11:16:02.763221  7059 layer_factory.hpp:76] Creating layer pool2
I1127 11:16:02.763229  7059 net.cpp:106] Creating Layer pool2
I1127 11:16:02.763234  7059 net.cpp:454] pool2 <- conv2
I1127 11:16:02.763239  7059 net.cpp:411] pool2 -> pool2
I1127 11:16:02.763267  7059 net.cpp:150] Setting up pool2
I1127 11:16:02.763273  7059 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:16:02.763278  7059 net.cpp:165] Memory required for data: 7674800
I1127 11:16:02.763281  7059 layer_factory.hpp:76] Creating layer ip1
I1127 11:16:02.763289  7059 net.cpp:106] Creating Layer ip1
I1127 11:16:02.763294  7059 net.cpp:454] ip1 <- pool2
I1127 11:16:02.763301  7059 net.cpp:411] ip1 -> ip1
I1127 11:16:02.765504  7059 net.cpp:150] Setting up ip1
I1127 11:16:02.765532  7059 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:16:02.765537  7059 net.cpp:165] Memory required for data: 7874800
I1127 11:16:02.765548  7059 layer_factory.hpp:76] Creating layer relu1
I1127 11:16:02.765561  7059 net.cpp:106] Creating Layer relu1
I1127 11:16:02.765568  7059 net.cpp:454] relu1 <- ip1
I1127 11:16:02.765574  7059 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:16:02.765583  7059 net.cpp:150] Setting up relu1
I1127 11:16:02.765588  7059 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:16:02.765594  7059 net.cpp:165] Memory required for data: 8074800
I1127 11:16:02.765597  7059 layer_factory.hpp:76] Creating layer ip2
I1127 11:16:02.765609  7059 net.cpp:106] Creating Layer ip2
I1127 11:16:02.765614  7059 net.cpp:454] ip2 <- ip1
I1127 11:16:02.765619  7059 net.cpp:411] ip2 -> ip2
I1127 11:16:02.765725  7059 net.cpp:150] Setting up ip2
I1127 11:16:02.765738  7059 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:16:02.765743  7059 net.cpp:165] Memory required for data: 8078800
I1127 11:16:02.765749  7059 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:16:02.765758  7059 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:16:02.765761  7059 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:16:02.765769  7059 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:16:02.765775  7059 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:16:02.765802  7059 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:16:02.765810  7059 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:16:02.765821  7059 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:16:02.765825  7059 net.cpp:165] Memory required for data: 8086800
I1127 11:16:02.765830  7059 layer_factory.hpp:76] Creating layer accuracy
I1127 11:16:02.765838  7059 net.cpp:106] Creating Layer accuracy
I1127 11:16:02.765843  7059 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:16:02.765848  7059 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:16:02.765856  7059 net.cpp:411] accuracy -> accuracy
I1127 11:16:02.765866  7059 net.cpp:150] Setting up accuracy
I1127 11:16:02.765871  7059 net.cpp:157] Top shape: (1)
I1127 11:16:02.765875  7059 net.cpp:165] Memory required for data: 8086804
I1127 11:16:02.765879  7059 layer_factory.hpp:76] Creating layer loss
I1127 11:16:02.765885  7059 net.cpp:106] Creating Layer loss
I1127 11:16:02.765889  7059 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:16:02.765894  7059 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:16:02.765900  7059 net.cpp:411] loss -> loss
I1127 11:16:02.765908  7059 layer_factory.hpp:76] Creating layer loss
I1127 11:16:02.765980  7059 net.cpp:150] Setting up loss
I1127 11:16:02.765987  7059 net.cpp:157] Top shape: (1)
I1127 11:16:02.765991  7059 net.cpp:160]     with loss weight 1
I1127 11:16:02.766002  7059 net.cpp:165] Memory required for data: 8086808
I1127 11:16:02.766006  7059 net.cpp:226] loss needs backward computation.
I1127 11:16:02.766016  7059 net.cpp:228] accuracy does not need backward computation.
I1127 11:16:02.766021  7059 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:16:02.766026  7059 net.cpp:226] ip2 needs backward computation.
I1127 11:16:02.766029  7059 net.cpp:226] relu1 needs backward computation.
I1127 11:16:02.766033  7059 net.cpp:226] ip1 needs backward computation.
I1127 11:16:02.766038  7059 net.cpp:226] pool2 needs backward computation.
I1127 11:16:02.766042  7059 net.cpp:226] conv2 needs backward computation.
I1127 11:16:02.766047  7059 net.cpp:226] pool1 needs backward computation.
I1127 11:16:02.766052  7059 net.cpp:226] conv1 needs backward computation.
I1127 11:16:02.766057  7059 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:16:02.766062  7059 net.cpp:228] mnist does not need backward computation.
I1127 11:16:02.766067  7059 net.cpp:270] This network produces output accuracy
I1127 11:16:02.766072  7059 net.cpp:270] This network produces output loss
I1127 11:16:02.766084  7059 net.cpp:283] Network initialization done.
I1127 11:16:02.766137  7059 solver.cpp:59] Solver scaffolding done.
I1127 11:16:02.766340  7059 caffe.cpp:212] Starting Optimization
I1127 11:16:02.766348  7059 solver.cpp:287] Solving LeNet
I1127 11:16:02.766352  7059 solver.cpp:288] Learning Rate Policy: inv
I1127 11:16:02.766841  7059 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:16:04.575382  7059 solver.cpp:408]     Test net output #0: accuracy = 0.0953
I1127 11:16:04.575417  7059 solver.cpp:408]     Test net output #1: loss = 2.32856 (* 1 = 2.32856 loss)
I1127 11:16:04.606384  7059 solver.cpp:236] Iteration 0, loss = 2.33221
I1127 11:16:04.606400  7059 solver.cpp:252]     Train net output #0: loss = 2.33221 (* 1 = 2.33221 loss)
I1127 11:16:04.606411  7059 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:16:17.196279  7059 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:16:18.404861  7059 solver.cpp:408]     Test net output #0: accuracy = 0.9732
I1127 11:16:18.405021  7059 solver.cpp:408]     Test net output #1: loss = 0.086507 (* 1 = 0.086507 loss)
I1127 11:16:18.418180  7059 solver.cpp:236] Iteration 500, loss = 0.095056
I1127 11:16:18.418267  7059 solver.cpp:252]     Train net output #0: loss = 0.0950558 (* 1 = 0.0950558 loss)
I1127 11:16:18.418284  7059 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:16:31.768985  7059 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:16:31.783291  7059 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:16:31.793092  7059 solver.cpp:320] Iteration 1000, loss = 0.0707632
I1127 11:16:31.793139  7059 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:16:33.738536  7059 solver.cpp:408]     Test net output #0: accuracy = 0.9827
I1127 11:16:33.738692  7059 solver.cpp:408]     Test net output #1: loss = 0.0544803 (* 1 = 0.0544803 loss)
I1127 11:16:33.738701  7059 solver.cpp:325] Optimization Done.
I1127 11:16:33.738708  7059 caffe.cpp:215] Optimization Done.
I1127 11:16:33.829741  7086 caffe.cpp:184] Using GPUs 0
I1127 11:16:34.332423  7086 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:16:34.332698  7086 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:16:34.333101  7086 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:16:34.333138  7086 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:16:34.333281  7086 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:16:34.333389  7086 layer_factory.hpp:76] Creating layer mnist
I1127 11:16:34.333921  7086 net.cpp:106] Creating Layer mnist
I1127 11:16:34.333962  7086 net.cpp:411] mnist -> data
I1127 11:16:34.334024  7086 net.cpp:411] mnist -> label
I1127 11:16:34.335257  7089 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:16:34.376317  7086 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:16:34.385035  7086 net.cpp:150] Setting up mnist
I1127 11:16:34.385174  7086 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:16:34.385191  7086 net.cpp:157] Top shape: 64 (64)
I1127 11:16:34.385200  7086 net.cpp:165] Memory required for data: 200960
I1127 11:16:34.385226  7086 layer_factory.hpp:76] Creating layer conv1
I1127 11:16:34.385273  7086 net.cpp:106] Creating Layer conv1
I1127 11:16:34.385316  7086 net.cpp:454] conv1 <- data
I1127 11:16:34.385354  7086 net.cpp:411] conv1 -> conv1
I1127 11:16:34.390482  7086 net.cpp:150] Setting up conv1
I1127 11:16:34.390561  7086 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:16:34.390573  7086 net.cpp:165] Memory required for data: 3150080
I1127 11:16:34.390617  7086 layer_factory.hpp:76] Creating layer pool1
I1127 11:16:34.390661  7086 net.cpp:106] Creating Layer pool1
I1127 11:16:34.390679  7086 net.cpp:454] pool1 <- conv1
I1127 11:16:34.390699  7086 net.cpp:411] pool1 -> pool1
I1127 11:16:34.390871  7086 net.cpp:150] Setting up pool1
I1127 11:16:34.390887  7086 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:16:34.390895  7086 net.cpp:165] Memory required for data: 3887360
I1127 11:16:34.390903  7086 layer_factory.hpp:76] Creating layer conv2
I1127 11:16:34.390928  7086 net.cpp:106] Creating Layer conv2
I1127 11:16:34.390938  7086 net.cpp:454] conv2 <- pool1
I1127 11:16:34.390954  7086 net.cpp:411] conv2 -> conv2
I1127 11:16:34.391476  7086 net.cpp:150] Setting up conv2
I1127 11:16:34.391540  7086 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:16:34.391552  7086 net.cpp:165] Memory required for data: 4706560
I1127 11:16:34.391585  7086 layer_factory.hpp:76] Creating layer pool2
I1127 11:16:34.391625  7086 net.cpp:106] Creating Layer pool2
I1127 11:16:34.391641  7086 net.cpp:454] pool2 <- conv2
I1127 11:16:34.391657  7086 net.cpp:411] pool2 -> pool2
I1127 11:16:34.391733  7086 net.cpp:150] Setting up pool2
I1127 11:16:34.391753  7086 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:16:34.391764  7086 net.cpp:165] Memory required for data: 4911360
I1127 11:16:34.391777  7086 layer_factory.hpp:76] Creating layer ip1
I1127 11:16:34.391803  7086 net.cpp:106] Creating Layer ip1
I1127 11:16:34.391815  7086 net.cpp:454] ip1 <- pool2
I1127 11:16:34.391837  7086 net.cpp:411] ip1 -> ip1
I1127 11:16:34.396850  7086 net.cpp:150] Setting up ip1
I1127 11:16:34.396975  7086 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:16:34.396987  7086 net.cpp:165] Memory required for data: 5039360
I1127 11:16:34.397037  7086 layer_factory.hpp:76] Creating layer relu1
I1127 11:16:34.397097  7086 net.cpp:106] Creating Layer relu1
I1127 11:16:34.397114  7086 net.cpp:454] relu1 <- ip1
I1127 11:16:34.397130  7086 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:16:34.397160  7086 net.cpp:150] Setting up relu1
I1127 11:16:34.397171  7086 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:16:34.397177  7086 net.cpp:165] Memory required for data: 5167360
I1127 11:16:34.397184  7086 layer_factory.hpp:76] Creating layer ip2
I1127 11:16:34.397205  7086 net.cpp:106] Creating Layer ip2
I1127 11:16:34.397212  7086 net.cpp:454] ip2 <- ip1
I1127 11:16:34.397222  7086 net.cpp:411] ip2 -> ip2
I1127 11:16:34.398501  7086 net.cpp:150] Setting up ip2
I1127 11:16:34.398596  7086 net.cpp:157] Top shape: 64 10 (640)
I1127 11:16:34.398605  7086 net.cpp:165] Memory required for data: 5169920
I1127 11:16:34.398625  7086 layer_factory.hpp:76] Creating layer loss
I1127 11:16:34.398654  7086 net.cpp:106] Creating Layer loss
I1127 11:16:34.398668  7086 net.cpp:454] loss <- ip2
I1127 11:16:34.398680  7086 net.cpp:454] loss <- label
I1127 11:16:34.398697  7086 net.cpp:411] loss -> loss
I1127 11:16:34.398727  7086 layer_factory.hpp:76] Creating layer loss
I1127 11:16:34.398994  7086 net.cpp:150] Setting up loss
I1127 11:16:34.399034  7086 net.cpp:157] Top shape: (1)
I1127 11:16:34.399051  7086 net.cpp:160]     with loss weight 1
I1127 11:16:34.399140  7086 net.cpp:165] Memory required for data: 5169924
I1127 11:16:34.399163  7086 net.cpp:226] loss needs backward computation.
I1127 11:16:34.399184  7086 net.cpp:226] ip2 needs backward computation.
I1127 11:16:34.399204  7086 net.cpp:226] relu1 needs backward computation.
I1127 11:16:34.399215  7086 net.cpp:226] ip1 needs backward computation.
I1127 11:16:34.399231  7086 net.cpp:226] pool2 needs backward computation.
I1127 11:16:34.399247  7086 net.cpp:226] conv2 needs backward computation.
I1127 11:16:34.399267  7086 net.cpp:226] pool1 needs backward computation.
I1127 11:16:34.399322  7086 net.cpp:226] conv1 needs backward computation.
I1127 11:16:34.399338  7086 net.cpp:228] mnist does not need backward computation.
I1127 11:16:34.399354  7086 net.cpp:270] This network produces output loss
I1127 11:16:34.399389  7086 net.cpp:283] Network initialization done.
I1127 11:16:34.400070  7086 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:16:34.400198  7086 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:16:34.400527  7086 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:16:34.400677  7086 layer_factory.hpp:76] Creating layer mnist
I1127 11:16:34.400925  7086 net.cpp:106] Creating Layer mnist
I1127 11:16:34.400945  7086 net.cpp:411] mnist -> data
I1127 11:16:34.400976  7086 net.cpp:411] mnist -> label
I1127 11:16:34.402629  7091 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:16:34.403028  7086 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:16:34.407543  7086 net.cpp:150] Setting up mnist
I1127 11:16:34.407661  7086 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:16:34.407677  7086 net.cpp:157] Top shape: 100 (100)
I1127 11:16:34.407686  7086 net.cpp:165] Memory required for data: 314000
I1127 11:16:34.407703  7086 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:16:34.407738  7086 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:16:34.407752  7086 net.cpp:454] label_mnist_1_split <- label
I1127 11:16:34.407773  7086 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:16:34.407804  7086 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:16:34.407907  7086 net.cpp:150] Setting up label_mnist_1_split
I1127 11:16:34.407922  7086 net.cpp:157] Top shape: 100 (100)
I1127 11:16:34.407932  7086 net.cpp:157] Top shape: 100 (100)
I1127 11:16:34.407938  7086 net.cpp:165] Memory required for data: 314800
I1127 11:16:34.407945  7086 layer_factory.hpp:76] Creating layer conv1
I1127 11:16:34.407991  7086 net.cpp:106] Creating Layer conv1
I1127 11:16:34.408001  7086 net.cpp:454] conv1 <- data
I1127 11:16:34.408015  7086 net.cpp:411] conv1 -> conv1
I1127 11:16:34.408349  7086 net.cpp:150] Setting up conv1
I1127 11:16:34.408373  7086 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:16:34.408382  7086 net.cpp:165] Memory required for data: 4922800
I1127 11:16:34.408403  7086 layer_factory.hpp:76] Creating layer pool1
I1127 11:16:34.408427  7086 net.cpp:106] Creating Layer pool1
I1127 11:16:34.408437  7086 net.cpp:454] pool1 <- conv1
I1127 11:16:34.408481  7086 net.cpp:411] pool1 -> pool1
I1127 11:16:34.408556  7086 net.cpp:150] Setting up pool1
I1127 11:16:34.408576  7086 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:16:34.408589  7086 net.cpp:165] Memory required for data: 6074800
I1127 11:16:34.408602  7086 layer_factory.hpp:76] Creating layer conv2
I1127 11:16:34.408639  7086 net.cpp:106] Creating Layer conv2
I1127 11:16:34.408653  7086 net.cpp:454] conv2 <- pool1
I1127 11:16:34.408665  7086 net.cpp:411] conv2 -> conv2
I1127 11:16:34.409370  7086 net.cpp:150] Setting up conv2
I1127 11:16:34.409430  7086 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:16:34.409440  7086 net.cpp:165] Memory required for data: 7354800
I1127 11:16:34.409463  7086 layer_factory.hpp:76] Creating layer pool2
I1127 11:16:34.409489  7086 net.cpp:106] Creating Layer pool2
I1127 11:16:34.409500  7086 net.cpp:454] pool2 <- conv2
I1127 11:16:34.409513  7086 net.cpp:411] pool2 -> pool2
I1127 11:16:34.409569  7086 net.cpp:150] Setting up pool2
I1127 11:16:34.409582  7086 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:16:34.409591  7086 net.cpp:165] Memory required for data: 7674800
I1127 11:16:34.409600  7086 layer_factory.hpp:76] Creating layer ip1
I1127 11:16:34.409615  7086 net.cpp:106] Creating Layer ip1
I1127 11:16:34.409626  7086 net.cpp:454] ip1 <- pool2
I1127 11:16:34.409658  7086 net.cpp:411] ip1 -> ip1
I1127 11:16:34.414420  7086 net.cpp:150] Setting up ip1
I1127 11:16:34.414481  7086 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:16:34.414489  7086 net.cpp:165] Memory required for data: 7874800
I1127 11:16:34.414515  7086 layer_factory.hpp:76] Creating layer relu1
I1127 11:16:34.414537  7086 net.cpp:106] Creating Layer relu1
I1127 11:16:34.414547  7086 net.cpp:454] relu1 <- ip1
I1127 11:16:34.414559  7086 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:16:34.414577  7086 net.cpp:150] Setting up relu1
I1127 11:16:34.414587  7086 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:16:34.414592  7086 net.cpp:165] Memory required for data: 8074800
I1127 11:16:34.414599  7086 layer_factory.hpp:76] Creating layer ip2
I1127 11:16:34.414618  7086 net.cpp:106] Creating Layer ip2
I1127 11:16:34.414625  7086 net.cpp:454] ip2 <- ip1
I1127 11:16:34.414635  7086 net.cpp:411] ip2 -> ip2
I1127 11:16:34.414824  7086 net.cpp:150] Setting up ip2
I1127 11:16:34.414841  7086 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:16:34.414847  7086 net.cpp:165] Memory required for data: 8078800
I1127 11:16:34.414858  7086 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:16:34.414870  7086 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:16:34.414877  7086 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:16:34.414890  7086 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:16:34.414901  7086 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:16:34.414953  7086 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:16:34.414964  7086 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:16:34.414973  7086 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:16:34.414979  7086 net.cpp:165] Memory required for data: 8086800
I1127 11:16:34.414988  7086 layer_factory.hpp:76] Creating layer accuracy
I1127 11:16:34.415006  7086 net.cpp:106] Creating Layer accuracy
I1127 11:16:34.415017  7086 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:16:34.415027  7086 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:16:34.415042  7086 net.cpp:411] accuracy -> accuracy
I1127 11:16:34.415062  7086 net.cpp:150] Setting up accuracy
I1127 11:16:34.415098  7086 net.cpp:157] Top shape: (1)
I1127 11:16:34.415112  7086 net.cpp:165] Memory required for data: 8086804
I1127 11:16:34.415123  7086 layer_factory.hpp:76] Creating layer loss
I1127 11:16:34.415145  7086 net.cpp:106] Creating Layer loss
I1127 11:16:34.415158  7086 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:16:34.415170  7086 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:16:34.415186  7086 net.cpp:411] loss -> loss
I1127 11:16:34.415207  7086 layer_factory.hpp:76] Creating layer loss
I1127 11:16:34.415421  7086 net.cpp:150] Setting up loss
I1127 11:16:34.415442  7086 net.cpp:157] Top shape: (1)
I1127 11:16:34.415452  7086 net.cpp:160]     with loss weight 1
I1127 11:16:34.415488  7086 net.cpp:165] Memory required for data: 8086808
I1127 11:16:34.415501  7086 net.cpp:226] loss needs backward computation.
I1127 11:16:34.415524  7086 net.cpp:228] accuracy does not need backward computation.
I1127 11:16:34.415534  7086 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:16:34.415541  7086 net.cpp:226] ip2 needs backward computation.
I1127 11:16:34.415549  7086 net.cpp:226] relu1 needs backward computation.
I1127 11:16:34.415557  7086 net.cpp:226] ip1 needs backward computation.
I1127 11:16:34.415565  7086 net.cpp:226] pool2 needs backward computation.
I1127 11:16:34.415575  7086 net.cpp:226] conv2 needs backward computation.
I1127 11:16:34.415591  7086 net.cpp:226] pool1 needs backward computation.
I1127 11:16:34.415611  7086 net.cpp:226] conv1 needs backward computation.
I1127 11:16:34.415635  7086 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:16:34.415658  7086 net.cpp:228] mnist does not need backward computation.
I1127 11:16:34.415668  7086 net.cpp:270] This network produces output accuracy
I1127 11:16:34.415676  7086 net.cpp:270] This network produces output loss
I1127 11:16:34.415698  7086 net.cpp:283] Network initialization done.
I1127 11:16:34.415794  7086 solver.cpp:59] Solver scaffolding done.
I1127 11:16:34.416123  7086 caffe.cpp:212] Starting Optimization
I1127 11:16:34.416139  7086 solver.cpp:287] Solving LeNet
I1127 11:16:34.416146  7086 solver.cpp:288] Learning Rate Policy: inv
I1127 11:16:34.416887  7086 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:16:35.980384  7086 solver.cpp:408]     Test net output #0: accuracy = 0.1172
I1127 11:16:35.980517  7086 solver.cpp:408]     Test net output #1: loss = 2.46809 (* 1 = 2.46809 loss)
I1127 11:16:35.996666  7086 solver.cpp:236] Iteration 0, loss = 2.44478
I1127 11:16:35.996767  7086 solver.cpp:252]     Train net output #0: loss = 2.44478 (* 1 = 2.44478 loss)
I1127 11:16:35.996830  7086 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:16:48.924197  7086 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:16:50.661345  7086 solver.cpp:408]     Test net output #0: accuracy = 0.9697
I1127 11:16:50.661515  7086 solver.cpp:408]     Test net output #1: loss = 0.0930066 (* 1 = 0.0930066 loss)
I1127 11:16:50.674209  7086 solver.cpp:236] Iteration 500, loss = 0.141267
I1127 11:16:50.674331  7086 solver.cpp:252]     Train net output #0: loss = 0.141267 (* 1 = 0.141267 loss)
I1127 11:16:50.674353  7086 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:17:04.142664  7086 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:17:04.155694  7086 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:17:04.166296  7086 solver.cpp:320] Iteration 1000, loss = 0.100662
I1127 11:17:04.166337  7086 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:17:05.240420  7086 solver.cpp:408]     Test net output #0: accuracy = 0.9798
I1127 11:17:05.240499  7086 solver.cpp:408]     Test net output #1: loss = 0.0612679 (* 1 = 0.0612679 loss)
I1127 11:17:05.240509  7086 solver.cpp:325] Optimization Done.
I1127 11:17:05.240514  7086 caffe.cpp:215] Optimization Done.
I1127 11:17:05.364280  7116 caffe.cpp:184] Using GPUs 0
I1127 11:17:05.860168  7116 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:17:05.860306  7116 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:17:05.860707  7116 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:17:05.860728  7116 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:17:05.860865  7116 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:17:05.860934  7116 layer_factory.hpp:76] Creating layer mnist
I1127 11:17:05.861397  7116 net.cpp:106] Creating Layer mnist
I1127 11:17:05.861412  7116 net.cpp:411] mnist -> data
I1127 11:17:05.861443  7116 net.cpp:411] mnist -> label
I1127 11:17:05.862139  7119 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:17:05.897724  7116 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:17:05.905004  7116 net.cpp:150] Setting up mnist
I1127 11:17:05.905027  7116 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:17:05.905038  7116 net.cpp:157] Top shape: 64 (64)
I1127 11:17:05.905045  7116 net.cpp:165] Memory required for data: 200960
I1127 11:17:05.905057  7116 layer_factory.hpp:76] Creating layer conv1
I1127 11:17:05.905078  7116 net.cpp:106] Creating Layer conv1
I1127 11:17:05.905089  7116 net.cpp:454] conv1 <- data
I1127 11:17:05.905104  7116 net.cpp:411] conv1 -> conv1
I1127 11:17:05.905843  7116 net.cpp:150] Setting up conv1
I1127 11:17:05.905858  7116 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:17:05.905864  7116 net.cpp:165] Memory required for data: 3150080
I1127 11:17:05.905882  7116 layer_factory.hpp:76] Creating layer pool1
I1127 11:17:05.905894  7116 net.cpp:106] Creating Layer pool1
I1127 11:17:05.905901  7116 net.cpp:454] pool1 <- conv1
I1127 11:17:05.905913  7116 net.cpp:411] pool1 -> pool1
I1127 11:17:05.906303  7116 net.cpp:150] Setting up pool1
I1127 11:17:05.906316  7116 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:17:05.906322  7116 net.cpp:165] Memory required for data: 3887360
I1127 11:17:05.906329  7116 layer_factory.hpp:76] Creating layer conv2
I1127 11:17:05.906343  7116 net.cpp:106] Creating Layer conv2
I1127 11:17:05.906352  7116 net.cpp:454] conv2 <- pool1
I1127 11:17:05.906361  7116 net.cpp:411] conv2 -> conv2
I1127 11:17:05.906757  7116 net.cpp:150] Setting up conv2
I1127 11:17:05.906770  7116 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:17:05.906776  7116 net.cpp:165] Memory required for data: 4706560
I1127 11:17:05.906790  7116 layer_factory.hpp:76] Creating layer pool2
I1127 11:17:05.906800  7116 net.cpp:106] Creating Layer pool2
I1127 11:17:05.906807  7116 net.cpp:454] pool2 <- conv2
I1127 11:17:05.906816  7116 net.cpp:411] pool2 -> pool2
I1127 11:17:05.906877  7116 net.cpp:150] Setting up pool2
I1127 11:17:05.906889  7116 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:17:05.906896  7116 net.cpp:165] Memory required for data: 4911360
I1127 11:17:05.906903  7116 layer_factory.hpp:76] Creating layer ip1
I1127 11:17:05.906914  7116 net.cpp:106] Creating Layer ip1
I1127 11:17:05.906921  7116 net.cpp:454] ip1 <- pool2
I1127 11:17:05.906931  7116 net.cpp:411] ip1 -> ip1
I1127 11:17:05.910503  7116 net.cpp:150] Setting up ip1
I1127 11:17:05.910519  7116 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:17:05.910526  7116 net.cpp:165] Memory required for data: 5039360
I1127 11:17:05.910539  7116 layer_factory.hpp:76] Creating layer relu1
I1127 11:17:05.910550  7116 net.cpp:106] Creating Layer relu1
I1127 11:17:05.910557  7116 net.cpp:454] relu1 <- ip1
I1127 11:17:05.910567  7116 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:17:05.910578  7116 net.cpp:150] Setting up relu1
I1127 11:17:05.910585  7116 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:17:05.910593  7116 net.cpp:165] Memory required for data: 5167360
I1127 11:17:05.910598  7116 layer_factory.hpp:76] Creating layer ip2
I1127 11:17:05.910609  7116 net.cpp:106] Creating Layer ip2
I1127 11:17:05.910615  7116 net.cpp:454] ip2 <- ip1
I1127 11:17:05.910625  7116 net.cpp:411] ip2 -> ip2
I1127 11:17:05.911171  7116 net.cpp:150] Setting up ip2
I1127 11:17:05.911185  7116 net.cpp:157] Top shape: 64 10 (640)
I1127 11:17:05.911191  7116 net.cpp:165] Memory required for data: 5169920
I1127 11:17:05.911202  7116 layer_factory.hpp:76] Creating layer loss
I1127 11:17:05.911213  7116 net.cpp:106] Creating Layer loss
I1127 11:17:05.911221  7116 net.cpp:454] loss <- ip2
I1127 11:17:05.911228  7116 net.cpp:454] loss <- label
I1127 11:17:05.911240  7116 net.cpp:411] loss -> loss
I1127 11:17:05.911255  7116 layer_factory.hpp:76] Creating layer loss
I1127 11:17:05.911350  7116 net.cpp:150] Setting up loss
I1127 11:17:05.911360  7116 net.cpp:157] Top shape: (1)
I1127 11:17:05.911367  7116 net.cpp:160]     with loss weight 1
I1127 11:17:05.911386  7116 net.cpp:165] Memory required for data: 5169924
I1127 11:17:05.911393  7116 net.cpp:226] loss needs backward computation.
I1127 11:17:05.911401  7116 net.cpp:226] ip2 needs backward computation.
I1127 11:17:05.911407  7116 net.cpp:226] relu1 needs backward computation.
I1127 11:17:05.911413  7116 net.cpp:226] ip1 needs backward computation.
I1127 11:17:05.911419  7116 net.cpp:226] pool2 needs backward computation.
I1127 11:17:05.911427  7116 net.cpp:226] conv2 needs backward computation.
I1127 11:17:05.911432  7116 net.cpp:226] pool1 needs backward computation.
I1127 11:17:05.911442  7116 net.cpp:226] conv1 needs backward computation.
I1127 11:17:05.911448  7116 net.cpp:228] mnist does not need backward computation.
I1127 11:17:05.911454  7116 net.cpp:270] This network produces output loss
I1127 11:17:05.911468  7116 net.cpp:283] Network initialization done.
I1127 11:17:05.911808  7116 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:17:05.911837  7116 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:17:05.911998  7116 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:17:05.912082  7116 layer_factory.hpp:76] Creating layer mnist
I1127 11:17:06.004839  7116 net.cpp:106] Creating Layer mnist
I1127 11:17:06.004874  7116 net.cpp:411] mnist -> data
I1127 11:17:06.004892  7116 net.cpp:411] mnist -> label
I1127 11:17:06.005676  7121 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:17:06.005837  7116 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:17:06.009526  7116 net.cpp:150] Setting up mnist
I1127 11:17:06.009549  7116 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:17:06.009557  7116 net.cpp:157] Top shape: 100 (100)
I1127 11:17:06.009560  7116 net.cpp:165] Memory required for data: 314000
I1127 11:17:06.009567  7116 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:17:06.009579  7116 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:17:06.009585  7116 net.cpp:454] label_mnist_1_split <- label
I1127 11:17:06.009593  7116 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:17:06.009603  7116 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:17:06.009654  7116 net.cpp:150] Setting up label_mnist_1_split
I1127 11:17:06.009662  7116 net.cpp:157] Top shape: 100 (100)
I1127 11:17:06.009668  7116 net.cpp:157] Top shape: 100 (100)
I1127 11:17:06.009672  7116 net.cpp:165] Memory required for data: 314800
I1127 11:17:06.009677  7116 layer_factory.hpp:76] Creating layer conv1
I1127 11:17:06.009690  7116 net.cpp:106] Creating Layer conv1
I1127 11:17:06.009696  7116 net.cpp:454] conv1 <- data
I1127 11:17:06.009703  7116 net.cpp:411] conv1 -> conv1
I1127 11:17:06.009865  7116 net.cpp:150] Setting up conv1
I1127 11:17:06.009874  7116 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:17:06.009879  7116 net.cpp:165] Memory required for data: 4922800
I1127 11:17:06.009888  7116 layer_factory.hpp:76] Creating layer pool1
I1127 11:17:06.009897  7116 net.cpp:106] Creating Layer pool1
I1127 11:17:06.009902  7116 net.cpp:454] pool1 <- conv1
I1127 11:17:06.009924  7116 net.cpp:411] pool1 -> pool1
I1127 11:17:06.009955  7116 net.cpp:150] Setting up pool1
I1127 11:17:06.009963  7116 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:17:06.009966  7116 net.cpp:165] Memory required for data: 6074800
I1127 11:17:06.009971  7116 layer_factory.hpp:76] Creating layer conv2
I1127 11:17:06.009980  7116 net.cpp:106] Creating Layer conv2
I1127 11:17:06.009985  7116 net.cpp:454] conv2 <- pool1
I1127 11:17:06.009992  7116 net.cpp:411] conv2 -> conv2
I1127 11:17:06.010265  7116 net.cpp:150] Setting up conv2
I1127 11:17:06.010277  7116 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:17:06.010282  7116 net.cpp:165] Memory required for data: 7354800
I1127 11:17:06.010290  7116 layer_factory.hpp:76] Creating layer pool2
I1127 11:17:06.010299  7116 net.cpp:106] Creating Layer pool2
I1127 11:17:06.010303  7116 net.cpp:454] pool2 <- conv2
I1127 11:17:06.010309  7116 net.cpp:411] pool2 -> pool2
I1127 11:17:06.010421  7116 net.cpp:150] Setting up pool2
I1127 11:17:06.010437  7116 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:17:06.010444  7116 net.cpp:165] Memory required for data: 7674800
I1127 11:17:06.010448  7116 layer_factory.hpp:76] Creating layer ip1
I1127 11:17:06.010458  7116 net.cpp:106] Creating Layer ip1
I1127 11:17:06.010463  7116 net.cpp:454] ip1 <- pool2
I1127 11:17:06.010471  7116 net.cpp:411] ip1 -> ip1
I1127 11:17:06.012949  7116 net.cpp:150] Setting up ip1
I1127 11:17:06.012967  7116 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:17:06.012972  7116 net.cpp:165] Memory required for data: 7874800
I1127 11:17:06.012984  7116 layer_factory.hpp:76] Creating layer relu1
I1127 11:17:06.012995  7116 net.cpp:106] Creating Layer relu1
I1127 11:17:06.013000  7116 net.cpp:454] relu1 <- ip1
I1127 11:17:06.013007  7116 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:17:06.013015  7116 net.cpp:150] Setting up relu1
I1127 11:17:06.013021  7116 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:17:06.013026  7116 net.cpp:165] Memory required for data: 8074800
I1127 11:17:06.013031  7116 layer_factory.hpp:76] Creating layer ip2
I1127 11:17:06.013041  7116 net.cpp:106] Creating Layer ip2
I1127 11:17:06.013046  7116 net.cpp:454] ip2 <- ip1
I1127 11:17:06.013053  7116 net.cpp:411] ip2 -> ip2
I1127 11:17:06.013164  7116 net.cpp:150] Setting up ip2
I1127 11:17:06.013171  7116 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:17:06.013175  7116 net.cpp:165] Memory required for data: 8078800
I1127 11:17:06.013182  7116 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:17:06.013190  7116 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:17:06.013195  7116 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:17:06.013201  7116 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:17:06.013208  7116 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:17:06.013242  7116 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:17:06.013250  7116 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:17:06.013257  7116 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:17:06.013260  7116 net.cpp:165] Memory required for data: 8086800
I1127 11:17:06.013265  7116 layer_factory.hpp:76] Creating layer accuracy
I1127 11:17:06.013274  7116 net.cpp:106] Creating Layer accuracy
I1127 11:17:06.013279  7116 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:17:06.013285  7116 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:17:06.013293  7116 net.cpp:411] accuracy -> accuracy
I1127 11:17:06.013303  7116 net.cpp:150] Setting up accuracy
I1127 11:17:06.013311  7116 net.cpp:157] Top shape: (1)
I1127 11:17:06.013316  7116 net.cpp:165] Memory required for data: 8086804
I1127 11:17:06.013320  7116 layer_factory.hpp:76] Creating layer loss
I1127 11:17:06.013331  7116 net.cpp:106] Creating Layer loss
I1127 11:17:06.013337  7116 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:17:06.013344  7116 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:17:06.013350  7116 net.cpp:411] loss -> loss
I1127 11:17:06.013360  7116 layer_factory.hpp:76] Creating layer loss
I1127 11:17:06.013453  7116 net.cpp:150] Setting up loss
I1127 11:17:06.013461  7116 net.cpp:157] Top shape: (1)
I1127 11:17:06.013465  7116 net.cpp:160]     with loss weight 1
I1127 11:17:06.013476  7116 net.cpp:165] Memory required for data: 8086808
I1127 11:17:06.013481  7116 net.cpp:226] loss needs backward computation.
I1127 11:17:06.013489  7116 net.cpp:228] accuracy does not need backward computation.
I1127 11:17:06.013494  7116 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:17:06.013499  7116 net.cpp:226] ip2 needs backward computation.
I1127 11:17:06.013504  7116 net.cpp:226] relu1 needs backward computation.
I1127 11:17:06.013507  7116 net.cpp:226] ip1 needs backward computation.
I1127 11:17:06.013512  7116 net.cpp:226] pool2 needs backward computation.
I1127 11:17:06.013516  7116 net.cpp:226] conv2 needs backward computation.
I1127 11:17:06.013521  7116 net.cpp:226] pool1 needs backward computation.
I1127 11:17:06.013525  7116 net.cpp:226] conv1 needs backward computation.
I1127 11:17:06.013530  7116 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:17:06.013535  7116 net.cpp:228] mnist does not need backward computation.
I1127 11:17:06.013540  7116 net.cpp:270] This network produces output accuracy
I1127 11:17:06.013545  7116 net.cpp:270] This network produces output loss
I1127 11:17:06.013556  7116 net.cpp:283] Network initialization done.
I1127 11:17:06.013619  7116 solver.cpp:59] Solver scaffolding done.
I1127 11:17:06.013842  7116 caffe.cpp:212] Starting Optimization
I1127 11:17:06.013851  7116 solver.cpp:287] Solving LeNet
I1127 11:17:06.013855  7116 solver.cpp:288] Learning Rate Policy: inv
I1127 11:17:06.014294  7116 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:17:08.087270  7116 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:17:08.422909  7116 solver.cpp:408]     Test net output #0: accuracy = 0.0889
I1127 11:17:08.422996  7116 solver.cpp:408]     Test net output #1: loss = 2.3265 (* 1 = 2.3265 loss)
I1127 11:17:08.438017  7116 solver.cpp:236] Iteration 0, loss = 2.31033
I1127 11:17:08.438192  7116 solver.cpp:252]     Train net output #0: loss = 2.31033 (* 1 = 2.31033 loss)
I1127 11:17:08.438256  7116 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:17:21.756163  7116 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:17:23.016912  7116 solver.cpp:408]     Test net output #0: accuracy = 0.9726
I1127 11:17:23.016978  7116 solver.cpp:408]     Test net output #1: loss = 0.0861716 (* 1 = 0.0861716 loss)
I1127 11:17:23.026610  7116 solver.cpp:236] Iteration 500, loss = 0.107484
I1127 11:17:23.026747  7116 solver.cpp:252]     Train net output #0: loss = 0.107484 (* 1 = 0.107484 loss)
I1127 11:17:23.026769  7116 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:17:35.094506  7116 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:17:35.114976  7116 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:17:35.142690  7116 solver.cpp:320] Iteration 1000, loss = 0.0945269
I1127 11:17:35.142725  7116 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:17:37.604724  7116 solver.cpp:408]     Test net output #0: accuracy = 0.9804
I1127 11:17:37.605247  7116 solver.cpp:408]     Test net output #1: loss = 0.0600792 (* 1 = 0.0600792 loss)
I1127 11:17:37.605263  7116 solver.cpp:325] Optimization Done.
I1127 11:17:37.605269  7116 caffe.cpp:215] Optimization Done.
I1127 11:17:37.756672  7143 caffe.cpp:184] Using GPUs 0
I1127 11:17:38.110342  7143 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:17:38.110517  7143 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:17:38.110873  7143 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:17:38.110905  7143 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:17:38.111021  7143 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:17:38.111106  7143 layer_factory.hpp:76] Creating layer mnist
I1127 11:17:38.111536  7143 net.cpp:106] Creating Layer mnist
I1127 11:17:38.111558  7143 net.cpp:411] mnist -> data
I1127 11:17:38.111598  7143 net.cpp:411] mnist -> label
I1127 11:17:38.112975  7146 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:17:38.120647  7143 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:17:38.121547  7143 net.cpp:150] Setting up mnist
I1127 11:17:38.121572  7143 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:17:38.121579  7143 net.cpp:157] Top shape: 64 (64)
I1127 11:17:38.121583  7143 net.cpp:165] Memory required for data: 200960
I1127 11:17:38.121595  7143 layer_factory.hpp:76] Creating layer conv1
I1127 11:17:38.121623  7143 net.cpp:106] Creating Layer conv1
I1127 11:17:38.121630  7143 net.cpp:454] conv1 <- data
I1127 11:17:38.121644  7143 net.cpp:411] conv1 -> conv1
I1127 11:17:38.126211  7143 net.cpp:150] Setting up conv1
I1127 11:17:38.126296  7143 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:17:38.126302  7143 net.cpp:165] Memory required for data: 3150080
I1127 11:17:38.126334  7143 layer_factory.hpp:76] Creating layer pool1
I1127 11:17:38.126351  7143 net.cpp:106] Creating Layer pool1
I1127 11:17:38.126358  7143 net.cpp:454] pool1 <- conv1
I1127 11:17:38.126365  7143 net.cpp:411] pool1 -> pool1
I1127 11:17:38.126441  7143 net.cpp:150] Setting up pool1
I1127 11:17:38.126449  7143 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:17:38.126453  7143 net.cpp:165] Memory required for data: 3887360
I1127 11:17:38.126458  7143 layer_factory.hpp:76] Creating layer conv2
I1127 11:17:38.126469  7143 net.cpp:106] Creating Layer conv2
I1127 11:17:38.126474  7143 net.cpp:454] conv2 <- pool1
I1127 11:17:38.126480  7143 net.cpp:411] conv2 -> conv2
I1127 11:17:38.126787  7143 net.cpp:150] Setting up conv2
I1127 11:17:38.126807  7143 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:17:38.126813  7143 net.cpp:165] Memory required for data: 4706560
I1127 11:17:38.126826  7143 layer_factory.hpp:76] Creating layer pool2
I1127 11:17:38.126837  7143 net.cpp:106] Creating Layer pool2
I1127 11:17:38.126845  7143 net.cpp:454] pool2 <- conv2
I1127 11:17:38.126852  7143 net.cpp:411] pool2 -> pool2
I1127 11:17:38.126891  7143 net.cpp:150] Setting up pool2
I1127 11:17:38.126904  7143 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:17:38.126909  7143 net.cpp:165] Memory required for data: 4911360
I1127 11:17:38.126916  7143 layer_factory.hpp:76] Creating layer ip1
I1127 11:17:38.126929  7143 net.cpp:106] Creating Layer ip1
I1127 11:17:38.126935  7143 net.cpp:454] ip1 <- pool2
I1127 11:17:38.126942  7143 net.cpp:411] ip1 -> ip1
I1127 11:17:38.130172  7143 net.cpp:150] Setting up ip1
I1127 11:17:38.130273  7143 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:17:38.130288  7143 net.cpp:165] Memory required for data: 5039360
I1127 11:17:38.130324  7143 layer_factory.hpp:76] Creating layer relu1
I1127 11:17:38.130359  7143 net.cpp:106] Creating Layer relu1
I1127 11:17:38.130375  7143 net.cpp:454] relu1 <- ip1
I1127 11:17:38.130390  7143 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:17:38.130417  7143 net.cpp:150] Setting up relu1
I1127 11:17:38.130431  7143 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:17:38.130439  7143 net.cpp:165] Memory required for data: 5167360
I1127 11:17:38.130446  7143 layer_factory.hpp:76] Creating layer ip2
I1127 11:17:38.130467  7143 net.cpp:106] Creating Layer ip2
I1127 11:17:38.130477  7143 net.cpp:454] ip2 <- ip1
I1127 11:17:38.130489  7143 net.cpp:411] ip2 -> ip2
I1127 11:17:38.131451  7143 net.cpp:150] Setting up ip2
I1127 11:17:38.131502  7143 net.cpp:157] Top shape: 64 10 (640)
I1127 11:17:38.131506  7143 net.cpp:165] Memory required for data: 5169920
I1127 11:17:38.131520  7143 layer_factory.hpp:76] Creating layer loss
I1127 11:17:38.131542  7143 net.cpp:106] Creating Layer loss
I1127 11:17:38.131549  7143 net.cpp:454] loss <- ip2
I1127 11:17:38.131557  7143 net.cpp:454] loss <- label
I1127 11:17:38.131572  7143 net.cpp:411] loss -> loss
I1127 11:17:38.131600  7143 layer_factory.hpp:76] Creating layer loss
I1127 11:17:38.131714  7143 net.cpp:150] Setting up loss
I1127 11:17:38.131724  7143 net.cpp:157] Top shape: (1)
I1127 11:17:38.131729  7143 net.cpp:160]     with loss weight 1
I1127 11:17:38.131762  7143 net.cpp:165] Memory required for data: 5169924
I1127 11:17:38.131770  7143 net.cpp:226] loss needs backward computation.
I1127 11:17:38.131775  7143 net.cpp:226] ip2 needs backward computation.
I1127 11:17:38.131781  7143 net.cpp:226] relu1 needs backward computation.
I1127 11:17:38.131788  7143 net.cpp:226] ip1 needs backward computation.
I1127 11:17:38.131793  7143 net.cpp:226] pool2 needs backward computation.
I1127 11:17:38.131799  7143 net.cpp:226] conv2 needs backward computation.
I1127 11:17:38.131805  7143 net.cpp:226] pool1 needs backward computation.
I1127 11:17:38.131810  7143 net.cpp:226] conv1 needs backward computation.
I1127 11:17:38.131815  7143 net.cpp:228] mnist does not need backward computation.
I1127 11:17:38.131820  7143 net.cpp:270] This network produces output loss
I1127 11:17:38.131832  7143 net.cpp:283] Network initialization done.
I1127 11:17:38.132161  7143 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:17:38.132205  7143 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:17:38.132377  7143 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:17:38.132493  7143 layer_factory.hpp:76] Creating layer mnist
I1127 11:17:38.132659  7143 net.cpp:106] Creating Layer mnist
I1127 11:17:38.132674  7143 net.cpp:411] mnist -> data
I1127 11:17:38.132691  7143 net.cpp:411] mnist -> label
I1127 11:17:38.133957  7148 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:17:38.134209  7143 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:17:38.135301  7143 net.cpp:150] Setting up mnist
I1127 11:17:38.135339  7143 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:17:38.135345  7143 net.cpp:157] Top shape: 100 (100)
I1127 11:17:38.135350  7143 net.cpp:165] Memory required for data: 314000
I1127 11:17:38.135359  7143 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:17:38.135373  7143 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:17:38.135380  7143 net.cpp:454] label_mnist_1_split <- label
I1127 11:17:38.135388  7143 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:17:38.135399  7143 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:17:38.135439  7143 net.cpp:150] Setting up label_mnist_1_split
I1127 11:17:38.135448  7143 net.cpp:157] Top shape: 100 (100)
I1127 11:17:38.135454  7143 net.cpp:157] Top shape: 100 (100)
I1127 11:17:38.135458  7143 net.cpp:165] Memory required for data: 314800
I1127 11:17:38.135463  7143 layer_factory.hpp:76] Creating layer conv1
I1127 11:17:38.135475  7143 net.cpp:106] Creating Layer conv1
I1127 11:17:38.135480  7143 net.cpp:454] conv1 <- data
I1127 11:17:38.135491  7143 net.cpp:411] conv1 -> conv1
I1127 11:17:38.135660  7143 net.cpp:150] Setting up conv1
I1127 11:17:38.135671  7143 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:17:38.135676  7143 net.cpp:165] Memory required for data: 4922800
I1127 11:17:38.135686  7143 layer_factory.hpp:76] Creating layer pool1
I1127 11:17:38.135700  7143 net.cpp:106] Creating Layer pool1
I1127 11:17:38.135705  7143 net.cpp:454] pool1 <- conv1
I1127 11:17:38.135735  7143 net.cpp:411] pool1 -> pool1
I1127 11:17:38.135766  7143 net.cpp:150] Setting up pool1
I1127 11:17:38.135773  7143 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:17:38.135777  7143 net.cpp:165] Memory required for data: 6074800
I1127 11:17:38.135782  7143 layer_factory.hpp:76] Creating layer conv2
I1127 11:17:38.135792  7143 net.cpp:106] Creating Layer conv2
I1127 11:17:38.135797  7143 net.cpp:454] conv2 <- pool1
I1127 11:17:38.135808  7143 net.cpp:411] conv2 -> conv2
I1127 11:17:38.136086  7143 net.cpp:150] Setting up conv2
I1127 11:17:38.136097  7143 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:17:38.136101  7143 net.cpp:165] Memory required for data: 7354800
I1127 11:17:38.136111  7143 layer_factory.hpp:76] Creating layer pool2
I1127 11:17:38.136119  7143 net.cpp:106] Creating Layer pool2
I1127 11:17:38.136124  7143 net.cpp:454] pool2 <- conv2
I1127 11:17:38.136131  7143 net.cpp:411] pool2 -> pool2
I1127 11:17:38.136157  7143 net.cpp:150] Setting up pool2
I1127 11:17:38.136164  7143 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:17:38.136169  7143 net.cpp:165] Memory required for data: 7674800
I1127 11:17:38.136173  7143 layer_factory.hpp:76] Creating layer ip1
I1127 11:17:38.136183  7143 net.cpp:106] Creating Layer ip1
I1127 11:17:38.136188  7143 net.cpp:454] ip1 <- pool2
I1127 11:17:38.136193  7143 net.cpp:411] ip1 -> ip1
I1127 11:17:38.140027  7143 net.cpp:150] Setting up ip1
I1127 11:17:38.140076  7143 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:17:38.140084  7143 net.cpp:165] Memory required for data: 7874800
I1127 11:17:38.140103  7143 layer_factory.hpp:76] Creating layer relu1
I1127 11:17:38.140120  7143 net.cpp:106] Creating Layer relu1
I1127 11:17:38.140128  7143 net.cpp:454] relu1 <- ip1
I1127 11:17:38.140139  7143 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:17:38.140154  7143 net.cpp:150] Setting up relu1
I1127 11:17:38.140163  7143 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:17:38.140171  7143 net.cpp:165] Memory required for data: 8074800
I1127 11:17:38.140177  7143 layer_factory.hpp:76] Creating layer ip2
I1127 11:17:38.140195  7143 net.cpp:106] Creating Layer ip2
I1127 11:17:38.140202  7143 net.cpp:454] ip2 <- ip1
I1127 11:17:38.140211  7143 net.cpp:411] ip2 -> ip2
I1127 11:17:38.140367  7143 net.cpp:150] Setting up ip2
I1127 11:17:38.140379  7143 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:17:38.140385  7143 net.cpp:165] Memory required for data: 8078800
I1127 11:17:38.140396  7143 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:17:38.140406  7143 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:17:38.140413  7143 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:17:38.140424  7143 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:17:38.140434  7143 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:17:38.140473  7143 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:17:38.140483  7143 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:17:38.140491  7143 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:17:38.140497  7143 net.cpp:165] Memory required for data: 8086800
I1127 11:17:38.140504  7143 layer_factory.hpp:76] Creating layer accuracy
I1127 11:17:38.140517  7143 net.cpp:106] Creating Layer accuracy
I1127 11:17:38.140524  7143 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:17:38.140532  7143 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:17:38.140543  7143 net.cpp:411] accuracy -> accuracy
I1127 11:17:38.140557  7143 net.cpp:150] Setting up accuracy
I1127 11:17:38.140564  7143 net.cpp:157] Top shape: (1)
I1127 11:17:38.140571  7143 net.cpp:165] Memory required for data: 8086804
I1127 11:17:38.140578  7143 layer_factory.hpp:76] Creating layer loss
I1127 11:17:38.140586  7143 net.cpp:106] Creating Layer loss
I1127 11:17:38.140594  7143 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:17:38.140601  7143 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:17:38.140610  7143 net.cpp:411] loss -> loss
I1127 11:17:38.140624  7143 layer_factory.hpp:76] Creating layer loss
I1127 11:17:38.140733  7143 net.cpp:150] Setting up loss
I1127 11:17:38.140743  7143 net.cpp:157] Top shape: (1)
I1127 11:17:38.140750  7143 net.cpp:160]     with loss weight 1
I1127 11:17:38.140765  7143 net.cpp:165] Memory required for data: 8086808
I1127 11:17:38.140772  7143 net.cpp:226] loss needs backward computation.
I1127 11:17:38.140784  7143 net.cpp:228] accuracy does not need backward computation.
I1127 11:17:38.140791  7143 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:17:38.140804  7143 net.cpp:226] ip2 needs backward computation.
I1127 11:17:38.140810  7143 net.cpp:226] relu1 needs backward computation.
I1127 11:17:38.140817  7143 net.cpp:226] ip1 needs backward computation.
I1127 11:17:38.140825  7143 net.cpp:226] pool2 needs backward computation.
I1127 11:17:38.140832  7143 net.cpp:226] conv2 needs backward computation.
I1127 11:17:38.140840  7143 net.cpp:226] pool1 needs backward computation.
I1127 11:17:38.140846  7143 net.cpp:226] conv1 needs backward computation.
I1127 11:17:38.140853  7143 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:17:38.140861  7143 net.cpp:228] mnist does not need backward computation.
I1127 11:17:38.140867  7143 net.cpp:270] This network produces output accuracy
I1127 11:17:38.140873  7143 net.cpp:270] This network produces output loss
I1127 11:17:38.140890  7143 net.cpp:283] Network initialization done.
I1127 11:17:38.140962  7143 solver.cpp:59] Solver scaffolding done.
I1127 11:17:38.141286  7143 caffe.cpp:212] Starting Optimization
I1127 11:17:38.141300  7143 solver.cpp:287] Solving LeNet
I1127 11:17:38.141307  7143 solver.cpp:288] Learning Rate Policy: inv
I1127 11:17:38.142109  7143 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:17:40.879232  7143 solver.cpp:408]     Test net output #0: accuracy = 0.0825
I1127 11:17:40.879276  7143 solver.cpp:408]     Test net output #1: loss = 2.3864 (* 1 = 2.3864 loss)
I1127 11:17:40.910879  7143 solver.cpp:236] Iteration 0, loss = 2.42958
I1127 11:17:40.910912  7143 solver.cpp:252]     Train net output #0: loss = 2.42958 (* 1 = 2.42958 loss)
I1127 11:17:40.910929  7143 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:17:54.312907  7143 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:17:55.776803  7143 solver.cpp:408]     Test net output #0: accuracy = 0.9746
I1127 11:17:55.776877  7143 solver.cpp:408]     Test net output #1: loss = 0.0810838 (* 1 = 0.0810838 loss)
I1127 11:17:55.788684  7143 solver.cpp:236] Iteration 500, loss = 0.0902213
I1127 11:17:55.788771  7143 solver.cpp:252]     Train net output #0: loss = 0.0902214 (* 1 = 0.0902214 loss)
I1127 11:17:55.788789  7143 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:18:09.095006  7143 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:18:09.115258  7143 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:18:09.142765  7143 solver.cpp:320] Iteration 1000, loss = 0.103181
I1127 11:18:09.142786  7143 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:18:10.382329  7143 solver.cpp:408]     Test net output #0: accuracy = 0.9829
I1127 11:18:10.382462  7143 solver.cpp:408]     Test net output #1: loss = 0.0557372 (* 1 = 0.0557372 loss)
I1127 11:18:10.382477  7143 solver.cpp:325] Optimization Done.
I1127 11:18:10.382484  7143 caffe.cpp:215] Optimization Done.
I1127 11:18:10.515382  7171 caffe.cpp:184] Using GPUs 0
I1127 11:18:10.906939  7171 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:18:10.907250  7171 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:18:10.907902  7171 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:18:10.907982  7171 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:18:10.908254  7171 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:18:10.908437  7171 layer_factory.hpp:76] Creating layer mnist
I1127 11:18:10.914551  7171 net.cpp:106] Creating Layer mnist
I1127 11:18:10.914613  7171 net.cpp:411] mnist -> data
I1127 11:18:10.914660  7171 net.cpp:411] mnist -> label
I1127 11:18:10.916034  7174 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:18:10.934815  7171 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:18:10.937644  7171 net.cpp:150] Setting up mnist
I1127 11:18:10.937829  7171 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:18:10.937858  7171 net.cpp:157] Top shape: 64 (64)
I1127 11:18:10.937875  7171 net.cpp:165] Memory required for data: 200960
I1127 11:18:10.937916  7171 layer_factory.hpp:76] Creating layer conv1
I1127 11:18:10.937999  7171 net.cpp:106] Creating Layer conv1
I1127 11:18:10.938019  7171 net.cpp:454] conv1 <- data
I1127 11:18:10.938050  7171 net.cpp:411] conv1 -> conv1
I1127 11:18:10.939939  7171 net.cpp:150] Setting up conv1
I1127 11:18:10.940079  7171 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:18:10.940096  7171 net.cpp:165] Memory required for data: 3150080
I1127 11:18:10.940146  7171 layer_factory.hpp:76] Creating layer pool1
I1127 11:18:10.940188  7171 net.cpp:106] Creating Layer pool1
I1127 11:18:10.940201  7171 net.cpp:454] pool1 <- conv1
I1127 11:18:10.940227  7171 net.cpp:411] pool1 -> pool1
I1127 11:18:10.940356  7171 net.cpp:150] Setting up pool1
I1127 11:18:10.940371  7171 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:18:10.940378  7171 net.cpp:165] Memory required for data: 3887360
I1127 11:18:10.940387  7171 layer_factory.hpp:76] Creating layer conv2
I1127 11:18:10.940410  7171 net.cpp:106] Creating Layer conv2
I1127 11:18:10.940421  7171 net.cpp:454] conv2 <- pool1
I1127 11:18:10.940438  7171 net.cpp:411] conv2 -> conv2
I1127 11:18:10.941014  7171 net.cpp:150] Setting up conv2
I1127 11:18:10.941061  7171 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:18:10.941071  7171 net.cpp:165] Memory required for data: 4706560
I1127 11:18:10.941097  7171 layer_factory.hpp:76] Creating layer pool2
I1127 11:18:10.941128  7171 net.cpp:106] Creating Layer pool2
I1127 11:18:10.941140  7171 net.cpp:454] pool2 <- conv2
I1127 11:18:10.941156  7171 net.cpp:411] pool2 -> pool2
I1127 11:18:10.941221  7171 net.cpp:150] Setting up pool2
I1127 11:18:10.941234  7171 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:18:10.941244  7171 net.cpp:165] Memory required for data: 4911360
I1127 11:18:10.941270  7171 layer_factory.hpp:76] Creating layer ip1
I1127 11:18:10.941289  7171 net.cpp:106] Creating Layer ip1
I1127 11:18:10.941299  7171 net.cpp:454] ip1 <- pool2
I1127 11:18:10.941314  7171 net.cpp:411] ip1 -> ip1
I1127 11:18:10.946133  7171 net.cpp:150] Setting up ip1
I1127 11:18:10.950278  7171 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:18:10.950290  7171 net.cpp:165] Memory required for data: 5039360
I1127 11:18:10.950316  7171 layer_factory.hpp:76] Creating layer relu1
I1127 11:18:10.950337  7171 net.cpp:106] Creating Layer relu1
I1127 11:18:10.950348  7171 net.cpp:454] relu1 <- ip1
I1127 11:18:10.950361  7171 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:18:10.950387  7171 net.cpp:150] Setting up relu1
I1127 11:18:10.950397  7171 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:18:10.950403  7171 net.cpp:165] Memory required for data: 5167360
I1127 11:18:10.950409  7171 layer_factory.hpp:76] Creating layer ip2
I1127 11:18:10.950434  7171 net.cpp:106] Creating Layer ip2
I1127 11:18:10.950443  7171 net.cpp:454] ip2 <- ip1
I1127 11:18:10.950455  7171 net.cpp:411] ip2 -> ip2
I1127 11:18:10.951961  7171 net.cpp:150] Setting up ip2
I1127 11:18:10.952064  7171 net.cpp:157] Top shape: 64 10 (640)
I1127 11:18:10.952076  7171 net.cpp:165] Memory required for data: 5169920
I1127 11:18:10.952100  7171 layer_factory.hpp:76] Creating layer loss
I1127 11:18:10.952136  7171 net.cpp:106] Creating Layer loss
I1127 11:18:10.952150  7171 net.cpp:454] loss <- ip2
I1127 11:18:10.952167  7171 net.cpp:454] loss <- label
I1127 11:18:10.952189  7171 net.cpp:411] loss -> loss
I1127 11:18:10.952235  7171 layer_factory.hpp:76] Creating layer loss
I1127 11:18:10.952473  7171 net.cpp:150] Setting up loss
I1127 11:18:10.952497  7171 net.cpp:157] Top shape: (1)
I1127 11:18:10.952505  7171 net.cpp:160]     with loss weight 1
I1127 11:18:10.952555  7171 net.cpp:165] Memory required for data: 5169924
I1127 11:18:10.952566  7171 net.cpp:226] loss needs backward computation.
I1127 11:18:10.952576  7171 net.cpp:226] ip2 needs backward computation.
I1127 11:18:10.952587  7171 net.cpp:226] relu1 needs backward computation.
I1127 11:18:10.952599  7171 net.cpp:226] ip1 needs backward computation.
I1127 11:18:10.952615  7171 net.cpp:226] pool2 needs backward computation.
I1127 11:18:10.952630  7171 net.cpp:226] conv2 needs backward computation.
I1127 11:18:10.952646  7171 net.cpp:226] pool1 needs backward computation.
I1127 11:18:10.952661  7171 net.cpp:226] conv1 needs backward computation.
I1127 11:18:10.952678  7171 net.cpp:228] mnist does not need backward computation.
I1127 11:18:10.952692  7171 net.cpp:270] This network produces output loss
I1127 11:18:10.952720  7171 net.cpp:283] Network initialization done.
I1127 11:18:10.953369  7171 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:18:10.953456  7171 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:18:10.953709  7171 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:18:10.953899  7171 layer_factory.hpp:76] Creating layer mnist
I1127 11:18:10.954201  7171 net.cpp:106] Creating Layer mnist
I1127 11:18:10.954226  7171 net.cpp:411] mnist -> data
I1127 11:18:10.954260  7171 net.cpp:411] mnist -> label
I1127 11:18:10.959576  7176 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:18:10.959879  7171 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:18:10.963021  7171 net.cpp:150] Setting up mnist
I1127 11:18:10.963138  7171 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:18:10.963155  7171 net.cpp:157] Top shape: 100 (100)
I1127 11:18:10.963166  7171 net.cpp:165] Memory required for data: 314000
I1127 11:18:10.963184  7171 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:18:10.963225  7171 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:18:10.963240  7171 net.cpp:454] label_mnist_1_split <- label
I1127 11:18:10.963260  7171 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:18:10.963291  7171 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:18:10.963409  7171 net.cpp:150] Setting up label_mnist_1_split
I1127 11:18:10.963425  7171 net.cpp:157] Top shape: 100 (100)
I1127 11:18:10.963436  7171 net.cpp:157] Top shape: 100 (100)
I1127 11:18:10.963444  7171 net.cpp:165] Memory required for data: 314800
I1127 11:18:10.963454  7171 layer_factory.hpp:76] Creating layer conv1
I1127 11:18:10.963485  7171 net.cpp:106] Creating Layer conv1
I1127 11:18:10.963495  7171 net.cpp:454] conv1 <- data
I1127 11:18:10.963508  7171 net.cpp:411] conv1 -> conv1
I1127 11:18:10.963832  7171 net.cpp:150] Setting up conv1
I1127 11:18:10.963850  7171 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:18:10.963860  7171 net.cpp:165] Memory required for data: 4922800
I1127 11:18:10.963881  7171 layer_factory.hpp:76] Creating layer pool1
I1127 11:18:10.963897  7171 net.cpp:106] Creating Layer pool1
I1127 11:18:10.963907  7171 net.cpp:454] pool1 <- conv1
I1127 11:18:10.963961  7171 net.cpp:411] pool1 -> pool1
I1127 11:18:10.964059  7171 net.cpp:150] Setting up pool1
I1127 11:18:10.964082  7171 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:18:10.964098  7171 net.cpp:165] Memory required for data: 6074800
I1127 11:18:10.964112  7171 layer_factory.hpp:76] Creating layer conv2
I1127 11:18:10.964140  7171 net.cpp:106] Creating Layer conv2
I1127 11:18:10.964157  7171 net.cpp:454] conv2 <- pool1
I1127 11:18:10.964176  7171 net.cpp:411] conv2 -> conv2
I1127 11:18:10.964745  7171 net.cpp:150] Setting up conv2
I1127 11:18:10.964792  7171 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:18:10.964802  7171 net.cpp:165] Memory required for data: 7354800
I1127 11:18:10.964826  7171 layer_factory.hpp:76] Creating layer pool2
I1127 11:18:10.964854  7171 net.cpp:106] Creating Layer pool2
I1127 11:18:10.964864  7171 net.cpp:454] pool2 <- conv2
I1127 11:18:10.964876  7171 net.cpp:411] pool2 -> pool2
I1127 11:18:10.964956  7171 net.cpp:150] Setting up pool2
I1127 11:18:10.964995  7171 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:18:10.965006  7171 net.cpp:165] Memory required for data: 7674800
I1127 11:18:10.965015  7171 layer_factory.hpp:76] Creating layer ip1
I1127 11:18:10.965034  7171 net.cpp:106] Creating Layer ip1
I1127 11:18:10.965044  7171 net.cpp:454] ip1 <- pool2
I1127 11:18:10.965060  7171 net.cpp:411] ip1 -> ip1
I1127 11:18:10.978262  7171 net.cpp:150] Setting up ip1
I1127 11:18:10.978380  7171 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:18:10.978394  7171 net.cpp:165] Memory required for data: 7874800
I1127 11:18:10.978436  7171 layer_factory.hpp:76] Creating layer relu1
I1127 11:18:10.978474  7171 net.cpp:106] Creating Layer relu1
I1127 11:18:10.978493  7171 net.cpp:454] relu1 <- ip1
I1127 11:18:10.978516  7171 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:18:10.978552  7171 net.cpp:150] Setting up relu1
I1127 11:18:10.978564  7171 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:18:10.978572  7171 net.cpp:165] Memory required for data: 8074800
I1127 11:18:10.978580  7171 layer_factory.hpp:76] Creating layer ip2
I1127 11:18:10.978610  7171 net.cpp:106] Creating Layer ip2
I1127 11:18:10.978620  7171 net.cpp:454] ip2 <- ip1
I1127 11:18:10.978631  7171 net.cpp:411] ip2 -> ip2
I1127 11:18:10.978868  7171 net.cpp:150] Setting up ip2
I1127 11:18:10.978886  7171 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:18:10.978893  7171 net.cpp:165] Memory required for data: 8078800
I1127 11:18:10.978906  7171 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:18:10.978919  7171 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:18:10.978926  7171 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:18:10.978936  7171 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:18:10.978946  7171 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:18:10.978994  7171 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:18:10.979007  7171 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:18:10.979015  7171 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:18:10.979022  7171 net.cpp:165] Memory required for data: 8086800
I1127 11:18:10.979029  7171 layer_factory.hpp:76] Creating layer accuracy
I1127 11:18:10.979043  7171 net.cpp:106] Creating Layer accuracy
I1127 11:18:10.979050  7171 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:18:10.979058  7171 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:18:10.979070  7171 net.cpp:411] accuracy -> accuracy
I1127 11:18:10.979085  7171 net.cpp:150] Setting up accuracy
I1127 11:18:10.979095  7171 net.cpp:157] Top shape: (1)
I1127 11:18:10.979101  7171 net.cpp:165] Memory required for data: 8086804
I1127 11:18:10.979110  7171 layer_factory.hpp:76] Creating layer loss
I1127 11:18:10.979121  7171 net.cpp:106] Creating Layer loss
I1127 11:18:10.979130  7171 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:18:10.979138  7171 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:18:10.979151  7171 net.cpp:411] loss -> loss
I1127 11:18:10.979168  7171 layer_factory.hpp:76] Creating layer loss
I1127 11:18:10.979348  7171 net.cpp:150] Setting up loss
I1127 11:18:10.979368  7171 net.cpp:157] Top shape: (1)
I1127 11:18:10.979375  7171 net.cpp:160]     with loss weight 1
I1127 11:18:10.979405  7171 net.cpp:165] Memory required for data: 8086808
I1127 11:18:10.979413  7171 net.cpp:226] loss needs backward computation.
I1127 11:18:10.979428  7171 net.cpp:228] accuracy does not need backward computation.
I1127 11:18:10.979436  7171 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:18:10.979444  7171 net.cpp:226] ip2 needs backward computation.
I1127 11:18:10.979451  7171 net.cpp:226] relu1 needs backward computation.
I1127 11:18:10.979459  7171 net.cpp:226] ip1 needs backward computation.
I1127 11:18:10.979467  7171 net.cpp:226] pool2 needs backward computation.
I1127 11:18:10.979475  7171 net.cpp:226] conv2 needs backward computation.
I1127 11:18:10.979483  7171 net.cpp:226] pool1 needs backward computation.
I1127 11:18:10.979491  7171 net.cpp:226] conv1 needs backward computation.
I1127 11:18:10.979503  7171 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:18:10.979524  7171 net.cpp:228] mnist does not need backward computation.
I1127 11:18:10.979534  7171 net.cpp:270] This network produces output accuracy
I1127 11:18:10.979543  7171 net.cpp:270] This network produces output loss
I1127 11:18:10.979562  7171 net.cpp:283] Network initialization done.
I1127 11:18:10.979719  7171 solver.cpp:59] Solver scaffolding done.
I1127 11:18:10.980152  7171 caffe.cpp:212] Starting Optimization
I1127 11:18:10.980175  7171 solver.cpp:287] Solving LeNet
I1127 11:18:10.980182  7171 solver.cpp:288] Learning Rate Policy: inv
I1127 11:18:10.981145  7171 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:18:10.986352  7171 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:18:13.308383  7171 solver.cpp:408]     Test net output #0: accuracy = 0.0712
I1127 11:18:13.308418  7171 solver.cpp:408]     Test net output #1: loss = 2.41624 (* 1 = 2.41624 loss)
I1127 11:18:13.340540  7171 solver.cpp:236] Iteration 0, loss = 2.37821
I1127 11:18:13.340559  7171 solver.cpp:252]     Train net output #0: loss = 2.37821 (* 1 = 2.37821 loss)
I1127 11:18:13.340571  7171 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:18:26.794770  7171 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:18:28.651115  7171 solver.cpp:408]     Test net output #0: accuracy = 0.9703
I1127 11:18:28.651227  7171 solver.cpp:408]     Test net output #1: loss = 0.0874911 (* 1 = 0.0874911 loss)
I1127 11:18:28.662102  7171 solver.cpp:236] Iteration 500, loss = 0.076257
I1127 11:18:28.662196  7171 solver.cpp:252]     Train net output #0: loss = 0.076257 (* 1 = 0.076257 loss)
I1127 11:18:28.662214  7171 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:18:42.128304  7171 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:18:42.144924  7171 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:18:42.158131  7171 solver.cpp:320] Iteration 1000, loss = 0.0728299
I1127 11:18:42.158234  7171 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:18:43.276288  7171 solver.cpp:408]     Test net output #0: accuracy = 0.9803
I1127 11:18:43.276396  7171 solver.cpp:408]     Test net output #1: loss = 0.0621367 (* 1 = 0.0621367 loss)
I1127 11:18:43.276404  7171 solver.cpp:325] Optimization Done.
I1127 11:18:43.276412  7171 caffe.cpp:215] Optimization Done.
I1127 11:18:43.405133  7199 caffe.cpp:184] Using GPUs 0
I1127 11:18:43.747804  7199 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:18:43.748054  7199 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:18:43.748488  7199 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:18:43.748534  7199 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:18:43.748670  7199 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:18:43.748812  7199 layer_factory.hpp:76] Creating layer mnist
I1127 11:18:43.749493  7199 net.cpp:106] Creating Layer mnist
I1127 11:18:43.749547  7199 net.cpp:411] mnist -> data
I1127 11:18:43.749600  7199 net.cpp:411] mnist -> label
I1127 11:18:43.751008  7202 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:18:43.761744  7199 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:18:43.762631  7199 net.cpp:150] Setting up mnist
I1127 11:18:43.762673  7199 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:18:43.762681  7199 net.cpp:157] Top shape: 64 (64)
I1127 11:18:43.762686  7199 net.cpp:165] Memory required for data: 200960
I1127 11:18:43.762696  7199 layer_factory.hpp:76] Creating layer conv1
I1127 11:18:43.762719  7199 net.cpp:106] Creating Layer conv1
I1127 11:18:43.762727  7199 net.cpp:454] conv1 <- data
I1127 11:18:43.762743  7199 net.cpp:411] conv1 -> conv1
I1127 11:18:43.763752  7199 net.cpp:150] Setting up conv1
I1127 11:18:43.763808  7199 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:18:43.763813  7199 net.cpp:165] Memory required for data: 3150080
I1127 11:18:43.763833  7199 layer_factory.hpp:76] Creating layer pool1
I1127 11:18:43.763852  7199 net.cpp:106] Creating Layer pool1
I1127 11:18:43.763859  7199 net.cpp:454] pool1 <- conv1
I1127 11:18:43.763869  7199 net.cpp:411] pool1 -> pool1
I1127 11:18:43.763938  7199 net.cpp:150] Setting up pool1
I1127 11:18:43.763947  7199 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:18:43.763952  7199 net.cpp:165] Memory required for data: 3887360
I1127 11:18:43.763960  7199 layer_factory.hpp:76] Creating layer conv2
I1127 11:18:43.763981  7199 net.cpp:106] Creating Layer conv2
I1127 11:18:43.763988  7199 net.cpp:454] conv2 <- pool1
I1127 11:18:43.763996  7199 net.cpp:411] conv2 -> conv2
I1127 11:18:43.764549  7199 net.cpp:150] Setting up conv2
I1127 11:18:43.764600  7199 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:18:43.764610  7199 net.cpp:165] Memory required for data: 4706560
I1127 11:18:43.764636  7199 layer_factory.hpp:76] Creating layer pool2
I1127 11:18:43.764660  7199 net.cpp:106] Creating Layer pool2
I1127 11:18:43.764672  7199 net.cpp:454] pool2 <- conv2
I1127 11:18:43.764690  7199 net.cpp:411] pool2 -> pool2
I1127 11:18:43.764755  7199 net.cpp:150] Setting up pool2
I1127 11:18:43.764767  7199 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:18:43.764775  7199 net.cpp:165] Memory required for data: 4911360
I1127 11:18:43.764782  7199 layer_factory.hpp:76] Creating layer ip1
I1127 11:18:43.764801  7199 net.cpp:106] Creating Layer ip1
I1127 11:18:43.764809  7199 net.cpp:454] ip1 <- pool2
I1127 11:18:43.764822  7199 net.cpp:411] ip1 -> ip1
I1127 11:18:43.769490  7199 net.cpp:150] Setting up ip1
I1127 11:18:43.769558  7199 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:18:43.769567  7199 net.cpp:165] Memory required for data: 5039360
I1127 11:18:43.769598  7199 layer_factory.hpp:76] Creating layer relu1
I1127 11:18:43.769616  7199 net.cpp:106] Creating Layer relu1
I1127 11:18:43.769628  7199 net.cpp:454] relu1 <- ip1
I1127 11:18:43.769641  7199 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:18:43.769670  7199 net.cpp:150] Setting up relu1
I1127 11:18:43.769683  7199 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:18:43.769693  7199 net.cpp:165] Memory required for data: 5167360
I1127 11:18:43.769704  7199 layer_factory.hpp:76] Creating layer ip2
I1127 11:18:43.769722  7199 net.cpp:106] Creating Layer ip2
I1127 11:18:43.769729  7199 net.cpp:454] ip2 <- ip1
I1127 11:18:43.769744  7199 net.cpp:411] ip2 -> ip2
I1127 11:18:43.771339  7199 net.cpp:150] Setting up ip2
I1127 11:18:43.771431  7199 net.cpp:157] Top shape: 64 10 (640)
I1127 11:18:43.771440  7199 net.cpp:165] Memory required for data: 5169920
I1127 11:18:43.771471  7199 layer_factory.hpp:76] Creating layer loss
I1127 11:18:43.771512  7199 net.cpp:106] Creating Layer loss
I1127 11:18:43.771528  7199 net.cpp:454] loss <- ip2
I1127 11:18:43.771544  7199 net.cpp:454] loss <- label
I1127 11:18:43.771565  7199 net.cpp:411] loss -> loss
I1127 11:18:43.771603  7199 layer_factory.hpp:76] Creating layer loss
I1127 11:18:43.771739  7199 net.cpp:150] Setting up loss
I1127 11:18:43.771754  7199 net.cpp:157] Top shape: (1)
I1127 11:18:43.771759  7199 net.cpp:160]     with loss weight 1
I1127 11:18:43.771787  7199 net.cpp:165] Memory required for data: 5169924
I1127 11:18:43.771797  7199 net.cpp:226] loss needs backward computation.
I1127 11:18:43.771805  7199 net.cpp:226] ip2 needs backward computation.
I1127 11:18:43.771808  7199 net.cpp:226] relu1 needs backward computation.
I1127 11:18:43.771813  7199 net.cpp:226] ip1 needs backward computation.
I1127 11:18:43.771817  7199 net.cpp:226] pool2 needs backward computation.
I1127 11:18:43.771822  7199 net.cpp:226] conv2 needs backward computation.
I1127 11:18:43.771829  7199 net.cpp:226] pool1 needs backward computation.
I1127 11:18:43.771837  7199 net.cpp:226] conv1 needs backward computation.
I1127 11:18:43.771847  7199 net.cpp:228] mnist does not need backward computation.
I1127 11:18:43.771853  7199 net.cpp:270] This network produces output loss
I1127 11:18:43.771872  7199 net.cpp:283] Network initialization done.
I1127 11:18:43.772271  7199 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:18:43.772322  7199 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:18:43.772512  7199 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:18:43.772646  7199 layer_factory.hpp:76] Creating layer mnist
I1127 11:18:43.772858  7199 net.cpp:106] Creating Layer mnist
I1127 11:18:43.772874  7199 net.cpp:411] mnist -> data
I1127 11:18:43.772898  7199 net.cpp:411] mnist -> label
I1127 11:18:43.774379  7204 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:18:43.774725  7199 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:18:43.777719  7199 net.cpp:150] Setting up mnist
I1127 11:18:43.777822  7199 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:18:43.777837  7199 net.cpp:157] Top shape: 100 (100)
I1127 11:18:43.777845  7199 net.cpp:165] Memory required for data: 314000
I1127 11:18:43.777863  7199 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:18:43.777900  7199 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:18:43.777915  7199 net.cpp:454] label_mnist_1_split <- label
I1127 11:18:43.777931  7199 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:18:43.777963  7199 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:18:43.778089  7199 net.cpp:150] Setting up label_mnist_1_split
I1127 11:18:43.778111  7199 net.cpp:157] Top shape: 100 (100)
I1127 11:18:43.778126  7199 net.cpp:157] Top shape: 100 (100)
I1127 11:18:43.778136  7199 net.cpp:165] Memory required for data: 314800
I1127 11:18:43.778172  7199 layer_factory.hpp:76] Creating layer conv1
I1127 11:18:43.778205  7199 net.cpp:106] Creating Layer conv1
I1127 11:18:43.778215  7199 net.cpp:454] conv1 <- data
I1127 11:18:43.778228  7199 net.cpp:411] conv1 -> conv1
I1127 11:18:43.778591  7199 net.cpp:150] Setting up conv1
I1127 11:18:43.778627  7199 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:18:43.778635  7199 net.cpp:165] Memory required for data: 4922800
I1127 11:18:43.778661  7199 layer_factory.hpp:76] Creating layer pool1
I1127 11:18:43.778683  7199 net.cpp:106] Creating Layer pool1
I1127 11:18:43.778692  7199 net.cpp:454] pool1 <- conv1
I1127 11:18:43.778749  7199 net.cpp:411] pool1 -> pool1
I1127 11:18:43.778820  7199 net.cpp:150] Setting up pool1
I1127 11:18:43.778833  7199 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:18:43.778841  7199 net.cpp:165] Memory required for data: 6074800
I1127 11:18:43.778848  7199 layer_factory.hpp:76] Creating layer conv2
I1127 11:18:43.778874  7199 net.cpp:106] Creating Layer conv2
I1127 11:18:43.778883  7199 net.cpp:454] conv2 <- pool1
I1127 11:18:43.778897  7199 net.cpp:411] conv2 -> conv2
I1127 11:18:43.779613  7199 net.cpp:150] Setting up conv2
I1127 11:18:43.779664  7199 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:18:43.779671  7199 net.cpp:165] Memory required for data: 7354800
I1127 11:18:43.779698  7199 layer_factory.hpp:76] Creating layer pool2
I1127 11:18:43.779723  7199 net.cpp:106] Creating Layer pool2
I1127 11:18:43.779733  7199 net.cpp:454] pool2 <- conv2
I1127 11:18:43.779749  7199 net.cpp:411] pool2 -> pool2
I1127 11:18:43.779819  7199 net.cpp:150] Setting up pool2
I1127 11:18:43.779831  7199 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:18:43.779839  7199 net.cpp:165] Memory required for data: 7674800
I1127 11:18:43.779846  7199 layer_factory.hpp:76] Creating layer ip1
I1127 11:18:43.779865  7199 net.cpp:106] Creating Layer ip1
I1127 11:18:43.779875  7199 net.cpp:454] ip1 <- pool2
I1127 11:18:43.779887  7199 net.cpp:411] ip1 -> ip1
I1127 11:18:43.784623  7199 net.cpp:150] Setting up ip1
I1127 11:18:43.784689  7199 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:18:43.784706  7199 net.cpp:165] Memory required for data: 7874800
I1127 11:18:43.784726  7199 layer_factory.hpp:76] Creating layer relu1
I1127 11:18:43.784740  7199 net.cpp:106] Creating Layer relu1
I1127 11:18:43.784749  7199 net.cpp:454] relu1 <- ip1
I1127 11:18:43.784756  7199 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:18:43.784771  7199 net.cpp:150] Setting up relu1
I1127 11:18:43.784777  7199 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:18:43.784781  7199 net.cpp:165] Memory required for data: 8074800
I1127 11:18:43.784785  7199 layer_factory.hpp:76] Creating layer ip2
I1127 11:18:43.784801  7199 net.cpp:106] Creating Layer ip2
I1127 11:18:43.784806  7199 net.cpp:454] ip2 <- ip1
I1127 11:18:43.784813  7199 net.cpp:411] ip2 -> ip2
I1127 11:18:43.784932  7199 net.cpp:150] Setting up ip2
I1127 11:18:43.784941  7199 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:18:43.784945  7199 net.cpp:165] Memory required for data: 8078800
I1127 11:18:43.784952  7199 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:18:43.784960  7199 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:18:43.784965  7199 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:18:43.784971  7199 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:18:43.784978  7199 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:18:43.785009  7199 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:18:43.785017  7199 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:18:43.785022  7199 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:18:43.785027  7199 net.cpp:165] Memory required for data: 8086800
I1127 11:18:43.785030  7199 layer_factory.hpp:76] Creating layer accuracy
I1127 11:18:43.785042  7199 net.cpp:106] Creating Layer accuracy
I1127 11:18:43.785045  7199 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:18:43.785051  7199 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:18:43.785058  7199 net.cpp:411] accuracy -> accuracy
I1127 11:18:43.785068  7199 net.cpp:150] Setting up accuracy
I1127 11:18:43.785073  7199 net.cpp:157] Top shape: (1)
I1127 11:18:43.785078  7199 net.cpp:165] Memory required for data: 8086804
I1127 11:18:43.785081  7199 layer_factory.hpp:76] Creating layer loss
I1127 11:18:43.785091  7199 net.cpp:106] Creating Layer loss
I1127 11:18:43.785097  7199 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:18:43.785102  7199 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:18:43.785107  7199 net.cpp:411] loss -> loss
I1127 11:18:43.785120  7199 layer_factory.hpp:76] Creating layer loss
I1127 11:18:43.785208  7199 net.cpp:150] Setting up loss
I1127 11:18:43.785218  7199 net.cpp:157] Top shape: (1)
I1127 11:18:43.785222  7199 net.cpp:160]     with loss weight 1
I1127 11:18:43.785238  7199 net.cpp:165] Memory required for data: 8086808
I1127 11:18:43.785243  7199 net.cpp:226] loss needs backward computation.
I1127 11:18:43.785254  7199 net.cpp:228] accuracy does not need backward computation.
I1127 11:18:43.785259  7199 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:18:43.785262  7199 net.cpp:226] ip2 needs backward computation.
I1127 11:18:43.785266  7199 net.cpp:226] relu1 needs backward computation.
I1127 11:18:43.785270  7199 net.cpp:226] ip1 needs backward computation.
I1127 11:18:43.785275  7199 net.cpp:226] pool2 needs backward computation.
I1127 11:18:43.785279  7199 net.cpp:226] conv2 needs backward computation.
I1127 11:18:43.785284  7199 net.cpp:226] pool1 needs backward computation.
I1127 11:18:43.785289  7199 net.cpp:226] conv1 needs backward computation.
I1127 11:18:43.785293  7199 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:18:43.785298  7199 net.cpp:228] mnist does not need backward computation.
I1127 11:18:43.785302  7199 net.cpp:270] This network produces output accuracy
I1127 11:18:43.785306  7199 net.cpp:270] This network produces output loss
I1127 11:18:43.785320  7199 net.cpp:283] Network initialization done.
I1127 11:18:43.785392  7199 solver.cpp:59] Solver scaffolding done.
I1127 11:18:43.785608  7199 caffe.cpp:212] Starting Optimization
I1127 11:18:43.785624  7199 solver.cpp:287] Solving LeNet
I1127 11:18:43.785629  7199 solver.cpp:288] Learning Rate Policy: inv
I1127 11:18:43.786217  7199 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:18:43.861963  7199 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:18:45.742622  7199 solver.cpp:408]     Test net output #0: accuracy = 0.0864
I1127 11:18:45.742668  7199 solver.cpp:408]     Test net output #1: loss = 2.38008 (* 1 = 2.38008 loss)
I1127 11:18:45.773784  7199 solver.cpp:236] Iteration 0, loss = 2.40035
I1127 11:18:45.773807  7199 solver.cpp:252]     Train net output #0: loss = 2.40035 (* 1 = 2.40035 loss)
I1127 11:18:45.773823  7199 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:18:58.404747  7199 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:19:00.702548  7199 solver.cpp:408]     Test net output #0: accuracy = 0.9755
I1127 11:19:00.702590  7199 solver.cpp:408]     Test net output #1: loss = 0.0810339 (* 1 = 0.0810339 loss)
I1127 11:19:00.731861  7199 solver.cpp:236] Iteration 500, loss = 0.0879966
I1127 11:19:00.731902  7199 solver.cpp:252]     Train net output #0: loss = 0.0879966 (* 1 = 0.0879966 loss)
I1127 11:19:00.731912  7199 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:19:14.959764  7199 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:19:14.986822  7199 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:19:14.999802  7199 solver.cpp:320] Iteration 1000, loss = 0.0751937
I1127 11:19:14.999891  7199 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:19:16.098139  7199 solver.cpp:408]     Test net output #0: accuracy = 0.983
I1127 11:19:16.098255  7199 solver.cpp:408]     Test net output #1: loss = 0.0520595 (* 1 = 0.0520595 loss)
I1127 11:19:16.098268  7199 solver.cpp:325] Optimization Done.
I1127 11:19:16.098273  7199 caffe.cpp:215] Optimization Done.
I1127 11:19:16.196074  7230 caffe.cpp:184] Using GPUs 0
I1127 11:19:16.479492  7230 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:19:16.479746  7230 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:19:16.480173  7230 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:19:16.480209  7230 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:19:16.480335  7230 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:19:16.480439  7230 layer_factory.hpp:76] Creating layer mnist
I1127 11:19:16.518256  7230 net.cpp:106] Creating Layer mnist
I1127 11:19:16.518306  7230 net.cpp:411] mnist -> data
I1127 11:19:16.518342  7230 net.cpp:411] mnist -> label
I1127 11:19:16.519387  7233 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:19:16.531209  7230 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:19:16.532901  7230 net.cpp:150] Setting up mnist
I1127 11:19:16.532984  7230 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:19:16.532997  7230 net.cpp:157] Top shape: 64 (64)
I1127 11:19:16.533005  7230 net.cpp:165] Memory required for data: 200960
I1127 11:19:16.533020  7230 layer_factory.hpp:76] Creating layer conv1
I1127 11:19:16.533064  7230 net.cpp:106] Creating Layer conv1
I1127 11:19:16.533077  7230 net.cpp:454] conv1 <- data
I1127 11:19:16.533093  7230 net.cpp:411] conv1 -> conv1
I1127 11:19:16.534137  7230 net.cpp:150] Setting up conv1
I1127 11:19:16.534184  7230 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:19:16.534191  7230 net.cpp:165] Memory required for data: 3150080
I1127 11:19:16.534214  7230 layer_factory.hpp:76] Creating layer pool1
I1127 11:19:16.534235  7230 net.cpp:106] Creating Layer pool1
I1127 11:19:16.534242  7230 net.cpp:454] pool1 <- conv1
I1127 11:19:16.534251  7230 net.cpp:411] pool1 -> pool1
I1127 11:19:16.534379  7230 net.cpp:150] Setting up pool1
I1127 11:19:16.534390  7230 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:19:16.534400  7230 net.cpp:165] Memory required for data: 3887360
I1127 11:19:16.534407  7230 layer_factory.hpp:76] Creating layer conv2
I1127 11:19:16.534422  7230 net.cpp:106] Creating Layer conv2
I1127 11:19:16.534430  7230 net.cpp:454] conv2 <- pool1
I1127 11:19:16.534440  7230 net.cpp:411] conv2 -> conv2
I1127 11:19:16.534893  7230 net.cpp:150] Setting up conv2
I1127 11:19:16.534931  7230 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:19:16.534947  7230 net.cpp:165] Memory required for data: 4706560
I1127 11:19:16.534970  7230 layer_factory.hpp:76] Creating layer pool2
I1127 11:19:16.534983  7230 net.cpp:106] Creating Layer pool2
I1127 11:19:16.534991  7230 net.cpp:454] pool2 <- conv2
I1127 11:19:16.534999  7230 net.cpp:411] pool2 -> pool2
I1127 11:19:16.535039  7230 net.cpp:150] Setting up pool2
I1127 11:19:16.535049  7230 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:19:16.535055  7230 net.cpp:165] Memory required for data: 4911360
I1127 11:19:16.535063  7230 layer_factory.hpp:76] Creating layer ip1
I1127 11:19:16.535078  7230 net.cpp:106] Creating Layer ip1
I1127 11:19:16.535084  7230 net.cpp:454] ip1 <- pool2
I1127 11:19:16.535092  7230 net.cpp:411] ip1 -> ip1
I1127 11:19:16.539441  7230 net.cpp:150] Setting up ip1
I1127 11:19:16.539541  7230 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:19:16.539557  7230 net.cpp:165] Memory required for data: 5039360
I1127 11:19:16.539593  7230 layer_factory.hpp:76] Creating layer relu1
I1127 11:19:16.539621  7230 net.cpp:106] Creating Layer relu1
I1127 11:19:16.539638  7230 net.cpp:454] relu1 <- ip1
I1127 11:19:16.539660  7230 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:19:16.539687  7230 net.cpp:150] Setting up relu1
I1127 11:19:16.539702  7230 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:19:16.539711  7230 net.cpp:165] Memory required for data: 5167360
I1127 11:19:16.539721  7230 layer_factory.hpp:76] Creating layer ip2
I1127 11:19:16.539752  7230 net.cpp:106] Creating Layer ip2
I1127 11:19:16.539759  7230 net.cpp:454] ip2 <- ip1
I1127 11:19:16.539774  7230 net.cpp:411] ip2 -> ip2
I1127 11:19:16.540993  7230 net.cpp:150] Setting up ip2
I1127 11:19:16.541064  7230 net.cpp:157] Top shape: 64 10 (640)
I1127 11:19:16.541076  7230 net.cpp:165] Memory required for data: 5169920
I1127 11:19:16.541101  7230 layer_factory.hpp:76] Creating layer loss
I1127 11:19:16.541134  7230 net.cpp:106] Creating Layer loss
I1127 11:19:16.541147  7230 net.cpp:454] loss <- ip2
I1127 11:19:16.541164  7230 net.cpp:454] loss <- label
I1127 11:19:16.541189  7230 net.cpp:411] loss -> loss
I1127 11:19:16.541235  7230 layer_factory.hpp:76] Creating layer loss
I1127 11:19:16.541385  7230 net.cpp:150] Setting up loss
I1127 11:19:16.541401  7230 net.cpp:157] Top shape: (1)
I1127 11:19:16.541411  7230 net.cpp:160]     with loss weight 1
I1127 11:19:16.541458  7230 net.cpp:165] Memory required for data: 5169924
I1127 11:19:16.541471  7230 net.cpp:226] loss needs backward computation.
I1127 11:19:16.541482  7230 net.cpp:226] ip2 needs backward computation.
I1127 11:19:16.541494  7230 net.cpp:226] relu1 needs backward computation.
I1127 11:19:16.541504  7230 net.cpp:226] ip1 needs backward computation.
I1127 11:19:16.541515  7230 net.cpp:226] pool2 needs backward computation.
I1127 11:19:16.541524  7230 net.cpp:226] conv2 needs backward computation.
I1127 11:19:16.541535  7230 net.cpp:226] pool1 needs backward computation.
I1127 11:19:16.541548  7230 net.cpp:226] conv1 needs backward computation.
I1127 11:19:16.541559  7230 net.cpp:228] mnist does not need backward computation.
I1127 11:19:16.541570  7230 net.cpp:270] This network produces output loss
I1127 11:19:16.541592  7230 net.cpp:283] Network initialization done.
I1127 11:19:16.542067  7230 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:19:16.542156  7230 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:19:16.542433  7230 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:19:16.542575  7230 layer_factory.hpp:76] Creating layer mnist
I1127 11:19:16.542837  7230 net.cpp:106] Creating Layer mnist
I1127 11:19:16.542858  7230 net.cpp:411] mnist -> data
I1127 11:19:16.542889  7230 net.cpp:411] mnist -> label
I1127 11:19:16.547157  7235 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:19:16.547417  7230 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:19:16.549445  7230 net.cpp:150] Setting up mnist
I1127 11:19:16.549536  7230 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:19:16.549549  7230 net.cpp:157] Top shape: 100 (100)
I1127 11:19:16.549556  7230 net.cpp:165] Memory required for data: 314000
I1127 11:19:16.549577  7230 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:19:16.549614  7230 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:19:16.549624  7230 net.cpp:454] label_mnist_1_split <- label
I1127 11:19:16.549636  7230 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:19:16.549651  7230 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:19:16.549713  7230 net.cpp:150] Setting up label_mnist_1_split
I1127 11:19:16.549724  7230 net.cpp:157] Top shape: 100 (100)
I1127 11:19:16.549732  7230 net.cpp:157] Top shape: 100 (100)
I1127 11:19:16.549738  7230 net.cpp:165] Memory required for data: 314800
I1127 11:19:16.549744  7230 layer_factory.hpp:76] Creating layer conv1
I1127 11:19:16.549763  7230 net.cpp:106] Creating Layer conv1
I1127 11:19:16.549770  7230 net.cpp:454] conv1 <- data
I1127 11:19:16.549782  7230 net.cpp:411] conv1 -> conv1
I1127 11:19:16.550010  7230 net.cpp:150] Setting up conv1
I1127 11:19:16.550024  7230 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:19:16.550029  7230 net.cpp:165] Memory required for data: 4922800
I1127 11:19:16.550045  7230 layer_factory.hpp:76] Creating layer pool1
I1127 11:19:16.550055  7230 net.cpp:106] Creating Layer pool1
I1127 11:19:16.550062  7230 net.cpp:454] pool1 <- conv1
I1127 11:19:16.550088  7230 net.cpp:411] pool1 -> pool1
I1127 11:19:16.550132  7230 net.cpp:150] Setting up pool1
I1127 11:19:16.550151  7230 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:19:16.550159  7230 net.cpp:165] Memory required for data: 6074800
I1127 11:19:16.550166  7230 layer_factory.hpp:76] Creating layer conv2
I1127 11:19:16.550181  7230 net.cpp:106] Creating Layer conv2
I1127 11:19:16.550189  7230 net.cpp:454] conv2 <- pool1
I1127 11:19:16.550199  7230 net.cpp:411] conv2 -> conv2
I1127 11:19:16.551832  7230 net.cpp:150] Setting up conv2
I1127 11:19:16.551867  7230 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:19:16.551874  7230 net.cpp:165] Memory required for data: 7354800
I1127 11:19:16.551892  7230 layer_factory.hpp:76] Creating layer pool2
I1127 11:19:16.551908  7230 net.cpp:106] Creating Layer pool2
I1127 11:19:16.551918  7230 net.cpp:454] pool2 <- conv2
I1127 11:19:16.551928  7230 net.cpp:411] pool2 -> pool2
I1127 11:19:16.551975  7230 net.cpp:150] Setting up pool2
I1127 11:19:16.551985  7230 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:19:16.551995  7230 net.cpp:165] Memory required for data: 7674800
I1127 11:19:16.552002  7230 layer_factory.hpp:76] Creating layer ip1
I1127 11:19:16.552017  7230 net.cpp:106] Creating Layer ip1
I1127 11:19:16.552024  7230 net.cpp:454] ip1 <- pool2
I1127 11:19:16.552371  7230 net.cpp:411] ip1 -> ip1
I1127 11:19:16.556615  7230 net.cpp:150] Setting up ip1
I1127 11:19:16.556671  7230 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:19:16.556681  7230 net.cpp:165] Memory required for data: 7874800
I1127 11:19:16.556704  7230 layer_factory.hpp:76] Creating layer relu1
I1127 11:19:16.556721  7230 net.cpp:106] Creating Layer relu1
I1127 11:19:16.556733  7230 net.cpp:454] relu1 <- ip1
I1127 11:19:16.556746  7230 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:19:16.556762  7230 net.cpp:150] Setting up relu1
I1127 11:19:16.556773  7230 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:19:16.556792  7230 net.cpp:165] Memory required for data: 8074800
I1127 11:19:16.556799  7230 layer_factory.hpp:76] Creating layer ip2
I1127 11:19:16.556823  7230 net.cpp:106] Creating Layer ip2
I1127 11:19:16.556831  7230 net.cpp:454] ip2 <- ip1
I1127 11:19:16.556844  7230 net.cpp:411] ip2 -> ip2
I1127 11:19:16.557025  7230 net.cpp:150] Setting up ip2
I1127 11:19:16.557039  7230 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:19:16.557045  7230 net.cpp:165] Memory required for data: 8078800
I1127 11:19:16.557059  7230 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:19:16.557072  7230 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:19:16.557082  7230 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:19:16.557093  7230 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:19:16.557107  7230 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:19:16.557163  7230 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:19:16.557178  7230 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:19:16.557188  7230 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:19:16.557196  7230 net.cpp:165] Memory required for data: 8086800
I1127 11:19:16.557205  7230 layer_factory.hpp:76] Creating layer accuracy
I1127 11:19:16.557220  7230 net.cpp:106] Creating Layer accuracy
I1127 11:19:16.557229  7230 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:19:16.557238  7230 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:19:16.557253  7230 net.cpp:411] accuracy -> accuracy
I1127 11:19:16.557272  7230 net.cpp:150] Setting up accuracy
I1127 11:19:16.557286  7230 net.cpp:157] Top shape: (1)
I1127 11:19:16.557296  7230 net.cpp:165] Memory required for data: 8086804
I1127 11:19:16.557306  7230 layer_factory.hpp:76] Creating layer loss
I1127 11:19:16.557315  7230 net.cpp:106] Creating Layer loss
I1127 11:19:16.557323  7230 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:19:16.557332  7230 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:19:16.557343  7230 net.cpp:411] loss -> loss
I1127 11:19:16.557355  7230 layer_factory.hpp:76] Creating layer loss
I1127 11:19:16.557476  7230 net.cpp:150] Setting up loss
I1127 11:19:16.557489  7230 net.cpp:157] Top shape: (1)
I1127 11:19:16.557497  7230 net.cpp:160]     with loss weight 1
I1127 11:19:16.557513  7230 net.cpp:165] Memory required for data: 8086808
I1127 11:19:16.557520  7230 net.cpp:226] loss needs backward computation.
I1127 11:19:16.557533  7230 net.cpp:228] accuracy does not need backward computation.
I1127 11:19:16.557540  7230 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:19:16.557548  7230 net.cpp:226] ip2 needs backward computation.
I1127 11:19:16.557554  7230 net.cpp:226] relu1 needs backward computation.
I1127 11:19:16.557561  7230 net.cpp:226] ip1 needs backward computation.
I1127 11:19:16.557569  7230 net.cpp:226] pool2 needs backward computation.
I1127 11:19:16.557575  7230 net.cpp:226] conv2 needs backward computation.
I1127 11:19:16.557584  7230 net.cpp:226] pool1 needs backward computation.
I1127 11:19:16.557590  7230 net.cpp:226] conv1 needs backward computation.
I1127 11:19:16.557597  7230 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:19:16.557606  7230 net.cpp:228] mnist does not need backward computation.
I1127 11:19:16.557612  7230 net.cpp:270] This network produces output accuracy
I1127 11:19:16.557620  7230 net.cpp:270] This network produces output loss
I1127 11:19:16.557633  7230 net.cpp:283] Network initialization done.
I1127 11:19:16.557710  7230 solver.cpp:59] Solver scaffolding done.
I1127 11:19:16.558007  7230 caffe.cpp:212] Starting Optimization
I1127 11:19:16.558017  7230 solver.cpp:287] Solving LeNet
I1127 11:19:16.558023  7230 solver.cpp:288] Learning Rate Policy: inv
I1127 11:19:16.558622  7230 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:19:19.698390  7230 solver.cpp:408]     Test net output #0: accuracy = 0.0951
I1127 11:19:19.698498  7230 solver.cpp:408]     Test net output #1: loss = 2.31028 (* 1 = 2.31028 loss)
I1127 11:19:19.713827  7230 solver.cpp:236] Iteration 0, loss = 2.28896
I1127 11:19:19.714011  7230 solver.cpp:252]     Train net output #0: loss = 2.28896 (* 1 = 2.28896 loss)
I1127 11:19:19.714073  7230 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:19:31.259676  7230 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:19:33.931435  7230 solver.cpp:408]     Test net output #0: accuracy = 0.9708
I1127 11:19:33.931474  7230 solver.cpp:408]     Test net output #1: loss = 0.0886924 (* 1 = 0.0886924 loss)
I1127 11:19:33.960664  7230 solver.cpp:236] Iteration 500, loss = 0.103895
I1127 11:19:33.960680  7230 solver.cpp:252]     Train net output #0: loss = 0.103895 (* 1 = 0.103895 loss)
I1127 11:19:33.960690  7230 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:19:47.807870  7230 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:19:47.828542  7230 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:19:47.850072  7230 solver.cpp:320] Iteration 1000, loss = 0.0983829
I1127 11:19:47.850112  7230 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:19:48.922257  7230 solver.cpp:408]     Test net output #0: accuracy = 0.9801
I1127 11:19:48.922346  7230 solver.cpp:408]     Test net output #1: loss = 0.0619877 (* 1 = 0.0619877 loss)
I1127 11:19:48.922355  7230 solver.cpp:325] Optimization Done.
I1127 11:19:48.922361  7230 caffe.cpp:215] Optimization Done.
I1127 11:19:49.046906  7259 caffe.cpp:184] Using GPUs 0
I1127 11:19:49.337533  7259 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:19:49.337657  7259 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:19:49.337918  7259 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:19:49.337935  7259 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:19:49.338023  7259 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:19:49.338085  7259 layer_factory.hpp:76] Creating layer mnist
I1127 11:19:49.338415  7259 net.cpp:106] Creating Layer mnist
I1127 11:19:49.338428  7259 net.cpp:411] mnist -> data
I1127 11:19:49.338449  7259 net.cpp:411] mnist -> label
I1127 11:19:49.339207  7265 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:19:49.372982  7259 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:19:49.379175  7259 net.cpp:150] Setting up mnist
I1127 11:19:49.379194  7259 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:19:49.379202  7259 net.cpp:157] Top shape: 64 (64)
I1127 11:19:49.379207  7259 net.cpp:165] Memory required for data: 200960
I1127 11:19:49.379215  7259 layer_factory.hpp:76] Creating layer conv1
I1127 11:19:49.379230  7259 net.cpp:106] Creating Layer conv1
I1127 11:19:49.379236  7259 net.cpp:454] conv1 <- data
I1127 11:19:49.379248  7259 net.cpp:411] conv1 -> conv1
I1127 11:19:49.379884  7259 net.cpp:150] Setting up conv1
I1127 11:19:49.379895  7259 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:19:49.379899  7259 net.cpp:165] Memory required for data: 3150080
I1127 11:19:49.379911  7259 layer_factory.hpp:76] Creating layer pool1
I1127 11:19:49.379920  7259 net.cpp:106] Creating Layer pool1
I1127 11:19:49.379925  7259 net.cpp:454] pool1 <- conv1
I1127 11:19:49.379930  7259 net.cpp:411] pool1 -> pool1
I1127 11:19:49.379976  7259 net.cpp:150] Setting up pool1
I1127 11:19:49.379984  7259 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:19:49.379988  7259 net.cpp:165] Memory required for data: 3887360
I1127 11:19:49.379992  7259 layer_factory.hpp:76] Creating layer conv2
I1127 11:19:49.380002  7259 net.cpp:106] Creating Layer conv2
I1127 11:19:49.380007  7259 net.cpp:454] conv2 <- pool1
I1127 11:19:49.380013  7259 net.cpp:411] conv2 -> conv2
I1127 11:19:49.380261  7259 net.cpp:150] Setting up conv2
I1127 11:19:49.380270  7259 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:19:49.380275  7259 net.cpp:165] Memory required for data: 4706560
I1127 11:19:49.380282  7259 layer_factory.hpp:76] Creating layer pool2
I1127 11:19:49.380290  7259 net.cpp:106] Creating Layer pool2
I1127 11:19:49.380293  7259 net.cpp:454] pool2 <- conv2
I1127 11:19:49.380300  7259 net.cpp:411] pool2 -> pool2
I1127 11:19:49.380405  7259 net.cpp:150] Setting up pool2
I1127 11:19:49.380417  7259 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:19:49.380424  7259 net.cpp:165] Memory required for data: 4911360
I1127 11:19:49.380430  7259 layer_factory.hpp:76] Creating layer ip1
I1127 11:19:49.380440  7259 net.cpp:106] Creating Layer ip1
I1127 11:19:49.380448  7259 net.cpp:454] ip1 <- pool2
I1127 11:19:49.380456  7259 net.cpp:411] ip1 -> ip1
I1127 11:19:49.382585  7259 net.cpp:150] Setting up ip1
I1127 11:19:49.382597  7259 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:19:49.382601  7259 net.cpp:165] Memory required for data: 5039360
I1127 11:19:49.382611  7259 layer_factory.hpp:76] Creating layer relu1
I1127 11:19:49.382617  7259 net.cpp:106] Creating Layer relu1
I1127 11:19:49.382622  7259 net.cpp:454] relu1 <- ip1
I1127 11:19:49.382629  7259 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:19:49.382637  7259 net.cpp:150] Setting up relu1
I1127 11:19:49.382643  7259 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:19:49.382647  7259 net.cpp:165] Memory required for data: 5167360
I1127 11:19:49.382652  7259 layer_factory.hpp:76] Creating layer ip2
I1127 11:19:49.382658  7259 net.cpp:106] Creating Layer ip2
I1127 11:19:49.382663  7259 net.cpp:454] ip2 <- ip1
I1127 11:19:49.382669  7259 net.cpp:411] ip2 -> ip2
I1127 11:19:49.383088  7259 net.cpp:150] Setting up ip2
I1127 11:19:49.383098  7259 net.cpp:157] Top shape: 64 10 (640)
I1127 11:19:49.383102  7259 net.cpp:165] Memory required for data: 5169920
I1127 11:19:49.383110  7259 layer_factory.hpp:76] Creating layer loss
I1127 11:19:49.383117  7259 net.cpp:106] Creating Layer loss
I1127 11:19:49.383122  7259 net.cpp:454] loss <- ip2
I1127 11:19:49.383131  7259 net.cpp:454] loss <- label
I1127 11:19:49.383138  7259 net.cpp:411] loss -> loss
I1127 11:19:49.383150  7259 layer_factory.hpp:76] Creating layer loss
I1127 11:19:49.383215  7259 net.cpp:150] Setting up loss
I1127 11:19:49.383224  7259 net.cpp:157] Top shape: (1)
I1127 11:19:49.383227  7259 net.cpp:160]     with loss weight 1
I1127 11:19:49.383242  7259 net.cpp:165] Memory required for data: 5169924
I1127 11:19:49.383246  7259 net.cpp:226] loss needs backward computation.
I1127 11:19:49.383251  7259 net.cpp:226] ip2 needs backward computation.
I1127 11:19:49.383255  7259 net.cpp:226] relu1 needs backward computation.
I1127 11:19:49.383260  7259 net.cpp:226] ip1 needs backward computation.
I1127 11:19:49.383263  7259 net.cpp:226] pool2 needs backward computation.
I1127 11:19:49.383268  7259 net.cpp:226] conv2 needs backward computation.
I1127 11:19:49.383272  7259 net.cpp:226] pool1 needs backward computation.
I1127 11:19:49.383276  7259 net.cpp:226] conv1 needs backward computation.
I1127 11:19:49.383280  7259 net.cpp:228] mnist does not need backward computation.
I1127 11:19:49.383285  7259 net.cpp:270] This network produces output loss
I1127 11:19:49.383297  7259 net.cpp:283] Network initialization done.
I1127 11:19:49.383527  7259 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:19:49.383549  7259 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:19:49.383656  7259 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:19:49.383713  7259 layer_factory.hpp:76] Creating layer mnist
I1127 11:19:49.457588  7259 net.cpp:106] Creating Layer mnist
I1127 11:19:49.457629  7259 net.cpp:411] mnist -> data
I1127 11:19:49.457645  7259 net.cpp:411] mnist -> label
I1127 11:19:49.458497  7267 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:19:49.458634  7259 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:19:49.462366  7259 net.cpp:150] Setting up mnist
I1127 11:19:49.462389  7259 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:19:49.462395  7259 net.cpp:157] Top shape: 100 (100)
I1127 11:19:49.462399  7259 net.cpp:165] Memory required for data: 314000
I1127 11:19:49.462405  7259 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:19:49.462414  7259 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:19:49.462419  7259 net.cpp:454] label_mnist_1_split <- label
I1127 11:19:49.462426  7259 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:19:49.462435  7259 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:19:49.462499  7259 net.cpp:150] Setting up label_mnist_1_split
I1127 11:19:49.462512  7259 net.cpp:157] Top shape: 100 (100)
I1127 11:19:49.462520  7259 net.cpp:157] Top shape: 100 (100)
I1127 11:19:49.462527  7259 net.cpp:165] Memory required for data: 314800
I1127 11:19:49.462533  7259 layer_factory.hpp:76] Creating layer conv1
I1127 11:19:49.462546  7259 net.cpp:106] Creating Layer conv1
I1127 11:19:49.462551  7259 net.cpp:454] conv1 <- data
I1127 11:19:49.462559  7259 net.cpp:411] conv1 -> conv1
I1127 11:19:49.462716  7259 net.cpp:150] Setting up conv1
I1127 11:19:49.462725  7259 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:19:49.462729  7259 net.cpp:165] Memory required for data: 4922800
I1127 11:19:49.462739  7259 layer_factory.hpp:76] Creating layer pool1
I1127 11:19:49.462749  7259 net.cpp:106] Creating Layer pool1
I1127 11:19:49.462754  7259 net.cpp:454] pool1 <- conv1
I1127 11:19:49.462769  7259 net.cpp:411] pool1 -> pool1
I1127 11:19:49.462797  7259 net.cpp:150] Setting up pool1
I1127 11:19:49.462805  7259 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:19:49.462808  7259 net.cpp:165] Memory required for data: 6074800
I1127 11:19:49.462812  7259 layer_factory.hpp:76] Creating layer conv2
I1127 11:19:49.462822  7259 net.cpp:106] Creating Layer conv2
I1127 11:19:49.462827  7259 net.cpp:454] conv2 <- pool1
I1127 11:19:49.462834  7259 net.cpp:411] conv2 -> conv2
I1127 11:19:49.463086  7259 net.cpp:150] Setting up conv2
I1127 11:19:49.463094  7259 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:19:49.463099  7259 net.cpp:165] Memory required for data: 7354800
I1127 11:19:49.463107  7259 layer_factory.hpp:76] Creating layer pool2
I1127 11:19:49.463116  7259 net.cpp:106] Creating Layer pool2
I1127 11:19:49.463121  7259 net.cpp:454] pool2 <- conv2
I1127 11:19:49.463126  7259 net.cpp:411] pool2 -> pool2
I1127 11:19:49.463582  7259 net.cpp:150] Setting up pool2
I1127 11:19:49.463592  7259 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:19:49.463595  7259 net.cpp:165] Memory required for data: 7674800
I1127 11:19:49.463599  7259 layer_factory.hpp:76] Creating layer ip1
I1127 11:19:49.463626  7259 net.cpp:106] Creating Layer ip1
I1127 11:19:49.463634  7259 net.cpp:454] ip1 <- pool2
I1127 11:19:49.463644  7259 net.cpp:411] ip1 -> ip1
I1127 11:19:49.465875  7259 net.cpp:150] Setting up ip1
I1127 11:19:49.465888  7259 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:19:49.465891  7259 net.cpp:165] Memory required for data: 7874800
I1127 11:19:49.465900  7259 layer_factory.hpp:76] Creating layer relu1
I1127 11:19:49.465909  7259 net.cpp:106] Creating Layer relu1
I1127 11:19:49.465914  7259 net.cpp:454] relu1 <- ip1
I1127 11:19:49.465920  7259 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:19:49.465927  7259 net.cpp:150] Setting up relu1
I1127 11:19:49.465932  7259 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:19:49.465936  7259 net.cpp:165] Memory required for data: 8074800
I1127 11:19:49.465940  7259 layer_factory.hpp:76] Creating layer ip2
I1127 11:19:49.465950  7259 net.cpp:106] Creating Layer ip2
I1127 11:19:49.465955  7259 net.cpp:454] ip2 <- ip1
I1127 11:19:49.465960  7259 net.cpp:411] ip2 -> ip2
I1127 11:19:49.466058  7259 net.cpp:150] Setting up ip2
I1127 11:19:49.466066  7259 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:19:49.466070  7259 net.cpp:165] Memory required for data: 8078800
I1127 11:19:49.466078  7259 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:19:49.466087  7259 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:19:49.466092  7259 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:19:49.466099  7259 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:19:49.466106  7259 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:19:49.466133  7259 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:19:49.466146  7259 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:19:49.466155  7259 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:19:49.466163  7259 net.cpp:165] Memory required for data: 8086800
I1127 11:19:49.466167  7259 layer_factory.hpp:76] Creating layer accuracy
I1127 11:19:49.466174  7259 net.cpp:106] Creating Layer accuracy
I1127 11:19:49.466179  7259 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:19:49.466184  7259 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:19:49.466192  7259 net.cpp:411] accuracy -> accuracy
I1127 11:19:49.466202  7259 net.cpp:150] Setting up accuracy
I1127 11:19:49.466207  7259 net.cpp:157] Top shape: (1)
I1127 11:19:49.466212  7259 net.cpp:165] Memory required for data: 8086804
I1127 11:19:49.466215  7259 layer_factory.hpp:76] Creating layer loss
I1127 11:19:49.466222  7259 net.cpp:106] Creating Layer loss
I1127 11:19:49.466226  7259 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:19:49.466230  7259 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:19:49.466236  7259 net.cpp:411] loss -> loss
I1127 11:19:49.466244  7259 layer_factory.hpp:76] Creating layer loss
I1127 11:19:49.466316  7259 net.cpp:150] Setting up loss
I1127 11:19:49.466325  7259 net.cpp:157] Top shape: (1)
I1127 11:19:49.466328  7259 net.cpp:160]     with loss weight 1
I1127 11:19:49.466339  7259 net.cpp:165] Memory required for data: 8086808
I1127 11:19:49.466343  7259 net.cpp:226] loss needs backward computation.
I1127 11:19:49.466351  7259 net.cpp:228] accuracy does not need backward computation.
I1127 11:19:49.466356  7259 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:19:49.466359  7259 net.cpp:226] ip2 needs backward computation.
I1127 11:19:49.466363  7259 net.cpp:226] relu1 needs backward computation.
I1127 11:19:49.466367  7259 net.cpp:226] ip1 needs backward computation.
I1127 11:19:49.466372  7259 net.cpp:226] pool2 needs backward computation.
I1127 11:19:49.466375  7259 net.cpp:226] conv2 needs backward computation.
I1127 11:19:49.466380  7259 net.cpp:226] pool1 needs backward computation.
I1127 11:19:49.466384  7259 net.cpp:226] conv1 needs backward computation.
I1127 11:19:49.466388  7259 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:19:49.466393  7259 net.cpp:228] mnist does not need backward computation.
I1127 11:19:49.466398  7259 net.cpp:270] This network produces output accuracy
I1127 11:19:49.466401  7259 net.cpp:270] This network produces output loss
I1127 11:19:49.466413  7259 net.cpp:283] Network initialization done.
I1127 11:19:49.466454  7259 solver.cpp:59] Solver scaffolding done.
I1127 11:19:49.466641  7259 caffe.cpp:212] Starting Optimization
I1127 11:19:49.466648  7259 solver.cpp:287] Solving LeNet
I1127 11:19:49.466652  7259 solver.cpp:288] Learning Rate Policy: inv
I1127 11:19:49.466995  7259 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:19:52.373982  7259 solver.cpp:408]     Test net output #0: accuracy = 0.1282
I1127 11:19:52.374078  7259 solver.cpp:408]     Test net output #1: loss = 2.38727 (* 1 = 2.38727 loss)
I1127 11:19:52.391252  7259 solver.cpp:236] Iteration 0, loss = 2.3097
I1127 11:19:52.391343  7259 solver.cpp:252]     Train net output #0: loss = 2.3097 (* 1 = 2.3097 loss)
I1127 11:19:52.391367  7259 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:20:03.920456  7259 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:20:06.375885  7259 solver.cpp:408]     Test net output #0: accuracy = 0.9734
I1127 11:20:06.375926  7259 solver.cpp:408]     Test net output #1: loss = 0.0836247 (* 1 = 0.0836247 loss)
I1127 11:20:06.405114  7259 solver.cpp:236] Iteration 500, loss = 0.0623113
I1127 11:20:06.405131  7259 solver.cpp:252]     Train net output #0: loss = 0.0623112 (* 1 = 0.0623112 loss)
I1127 11:20:06.405148  7259 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:20:18.473206  7259 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:20:18.486943  7259 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:20:18.497810  7259 solver.cpp:320] Iteration 1000, loss = 0.0793155
I1127 11:20:18.497903  7259 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:20:21.629058  7259 solver.cpp:408]     Test net output #0: accuracy = 0.9835
I1127 11:20:21.629277  7259 solver.cpp:408]     Test net output #1: loss = 0.0536289 (* 1 = 0.0536289 loss)
I1127 11:20:21.629292  7259 solver.cpp:325] Optimization Done.
I1127 11:20:21.629298  7259 caffe.cpp:215] Optimization Done.
I1127 11:20:21.726579  7289 caffe.cpp:184] Using GPUs 0
I1127 11:20:22.073407  7289 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:20:22.073619  7289 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:20:22.074334  7289 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:20:22.074385  7289 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:20:22.074565  7289 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:20:22.074656  7289 layer_factory.hpp:76] Creating layer mnist
I1127 11:20:22.075304  7289 net.cpp:106] Creating Layer mnist
I1127 11:20:22.075328  7289 net.cpp:411] mnist -> data
I1127 11:20:22.075372  7289 net.cpp:411] mnist -> label
I1127 11:20:22.076478  7292 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:20:22.090363  7289 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:20:22.091938  7289 net.cpp:150] Setting up mnist
I1127 11:20:22.092016  7289 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:20:22.092036  7289 net.cpp:157] Top shape: 64 (64)
I1127 11:20:22.092043  7289 net.cpp:165] Memory required for data: 200960
I1127 11:20:22.092062  7289 layer_factory.hpp:76] Creating layer conv1
I1127 11:20:22.092092  7289 net.cpp:106] Creating Layer conv1
I1127 11:20:22.092108  7289 net.cpp:454] conv1 <- data
I1127 11:20:22.092134  7289 net.cpp:411] conv1 -> conv1
I1127 11:20:22.093691  7289 net.cpp:150] Setting up conv1
I1127 11:20:22.093789  7289 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:20:22.093802  7289 net.cpp:165] Memory required for data: 3150080
I1127 11:20:22.093844  7289 layer_factory.hpp:76] Creating layer pool1
I1127 11:20:22.093879  7289 net.cpp:106] Creating Layer pool1
I1127 11:20:22.093894  7289 net.cpp:454] pool1 <- conv1
I1127 11:20:22.093910  7289 net.cpp:411] pool1 -> pool1
I1127 11:20:22.094069  7289 net.cpp:150] Setting up pool1
I1127 11:20:22.094089  7289 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:20:22.094097  7289 net.cpp:165] Memory required for data: 3887360
I1127 11:20:22.094108  7289 layer_factory.hpp:76] Creating layer conv2
I1127 11:20:22.094140  7289 net.cpp:106] Creating Layer conv2
I1127 11:20:22.094172  7289 net.cpp:454] conv2 <- pool1
I1127 11:20:22.094211  7289 net.cpp:411] conv2 -> conv2
I1127 11:20:22.094975  7289 net.cpp:150] Setting up conv2
I1127 11:20:22.095027  7289 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:20:22.095034  7289 net.cpp:165] Memory required for data: 4706560
I1127 11:20:22.095055  7289 layer_factory.hpp:76] Creating layer pool2
I1127 11:20:22.095079  7289 net.cpp:106] Creating Layer pool2
I1127 11:20:22.095088  7289 net.cpp:454] pool2 <- conv2
I1127 11:20:22.095100  7289 net.cpp:411] pool2 -> pool2
I1127 11:20:22.095149  7289 net.cpp:150] Setting up pool2
I1127 11:20:22.095160  7289 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:20:22.095165  7289 net.cpp:165] Memory required for data: 4911360
I1127 11:20:22.095172  7289 layer_factory.hpp:76] Creating layer ip1
I1127 11:20:22.095187  7289 net.cpp:106] Creating Layer ip1
I1127 11:20:22.095194  7289 net.cpp:454] ip1 <- pool2
I1127 11:20:22.095203  7289 net.cpp:411] ip1 -> ip1
I1127 11:20:22.100828  7289 net.cpp:150] Setting up ip1
I1127 11:20:22.100955  7289 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:20:22.100965  7289 net.cpp:165] Memory required for data: 5039360
I1127 11:20:22.101003  7289 layer_factory.hpp:76] Creating layer relu1
I1127 11:20:22.101035  7289 net.cpp:106] Creating Layer relu1
I1127 11:20:22.101047  7289 net.cpp:454] relu1 <- ip1
I1127 11:20:22.101063  7289 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:20:22.101100  7289 net.cpp:150] Setting up relu1
I1127 11:20:22.101115  7289 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:20:22.101125  7289 net.cpp:165] Memory required for data: 5167360
I1127 11:20:22.101133  7289 layer_factory.hpp:76] Creating layer ip2
I1127 11:20:22.101160  7289 net.cpp:106] Creating Layer ip2
I1127 11:20:22.101168  7289 net.cpp:454] ip2 <- ip1
I1127 11:20:22.101181  7289 net.cpp:411] ip2 -> ip2
I1127 11:20:22.102843  7289 net.cpp:150] Setting up ip2
I1127 11:20:22.102943  7289 net.cpp:157] Top shape: 64 10 (640)
I1127 11:20:22.102953  7289 net.cpp:165] Memory required for data: 5169920
I1127 11:20:22.102977  7289 layer_factory.hpp:76] Creating layer loss
I1127 11:20:22.103006  7289 net.cpp:106] Creating Layer loss
I1127 11:20:22.103020  7289 net.cpp:454] loss <- ip2
I1127 11:20:22.103036  7289 net.cpp:454] loss <- label
I1127 11:20:22.103059  7289 net.cpp:411] loss -> loss
I1127 11:20:22.103096  7289 layer_factory.hpp:76] Creating layer loss
I1127 11:20:22.103243  7289 net.cpp:150] Setting up loss
I1127 11:20:22.103260  7289 net.cpp:157] Top shape: (1)
I1127 11:20:22.103269  7289 net.cpp:160]     with loss weight 1
I1127 11:20:22.103314  7289 net.cpp:165] Memory required for data: 5169924
I1127 11:20:22.103323  7289 net.cpp:226] loss needs backward computation.
I1127 11:20:22.103332  7289 net.cpp:226] ip2 needs backward computation.
I1127 11:20:22.103366  7289 net.cpp:226] relu1 needs backward computation.
I1127 11:20:22.103375  7289 net.cpp:226] ip1 needs backward computation.
I1127 11:20:22.103384  7289 net.cpp:226] pool2 needs backward computation.
I1127 11:20:22.103391  7289 net.cpp:226] conv2 needs backward computation.
I1127 11:20:22.103399  7289 net.cpp:226] pool1 needs backward computation.
I1127 11:20:22.103406  7289 net.cpp:226] conv1 needs backward computation.
I1127 11:20:22.103415  7289 net.cpp:228] mnist does not need backward computation.
I1127 11:20:22.103423  7289 net.cpp:270] This network produces output loss
I1127 11:20:22.103443  7289 net.cpp:283] Network initialization done.
I1127 11:20:22.103989  7289 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:20:22.104096  7289 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:20:22.104393  7289 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:20:22.104564  7289 layer_factory.hpp:76] Creating layer mnist
I1127 11:20:22.104835  7289 net.cpp:106] Creating Layer mnist
I1127 11:20:22.104852  7289 net.cpp:411] mnist -> data
I1127 11:20:22.104883  7289 net.cpp:411] mnist -> label
I1127 11:20:22.106323  7294 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:20:22.106592  7289 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:20:22.108160  7289 net.cpp:150] Setting up mnist
I1127 11:20:22.108247  7289 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:20:22.108263  7289 net.cpp:157] Top shape: 100 (100)
I1127 11:20:22.108270  7289 net.cpp:165] Memory required for data: 314000
I1127 11:20:22.108283  7289 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:20:22.108310  7289 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:20:22.108325  7289 net.cpp:454] label_mnist_1_split <- label
I1127 11:20:22.108341  7289 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:20:22.108362  7289 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:20:22.108471  7289 net.cpp:150] Setting up label_mnist_1_split
I1127 11:20:22.108486  7289 net.cpp:157] Top shape: 100 (100)
I1127 11:20:22.108494  7289 net.cpp:157] Top shape: 100 (100)
I1127 11:20:22.108501  7289 net.cpp:165] Memory required for data: 314800
I1127 11:20:22.108508  7289 layer_factory.hpp:76] Creating layer conv1
I1127 11:20:22.108530  7289 net.cpp:106] Creating Layer conv1
I1127 11:20:22.108544  7289 net.cpp:454] conv1 <- data
I1127 11:20:22.108563  7289 net.cpp:411] conv1 -> conv1
I1127 11:20:22.108882  7289 net.cpp:150] Setting up conv1
I1127 11:20:22.108906  7289 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:20:22.108918  7289 net.cpp:165] Memory required for data: 4922800
I1127 11:20:22.108947  7289 layer_factory.hpp:76] Creating layer pool1
I1127 11:20:22.108969  7289 net.cpp:106] Creating Layer pool1
I1127 11:20:22.108983  7289 net.cpp:454] pool1 <- conv1
I1127 11:20:22.109046  7289 net.cpp:411] pool1 -> pool1
I1127 11:20:22.109094  7289 net.cpp:150] Setting up pool1
I1127 11:20:22.109107  7289 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:20:22.109112  7289 net.cpp:165] Memory required for data: 6074800
I1127 11:20:22.109118  7289 layer_factory.hpp:76] Creating layer conv2
I1127 11:20:22.109135  7289 net.cpp:106] Creating Layer conv2
I1127 11:20:22.109143  7289 net.cpp:454] conv2 <- pool1
I1127 11:20:22.109153  7289 net.cpp:411] conv2 -> conv2
I1127 11:20:22.109596  7289 net.cpp:150] Setting up conv2
I1127 11:20:22.109621  7289 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:20:22.109629  7289 net.cpp:165] Memory required for data: 7354800
I1127 11:20:22.109650  7289 layer_factory.hpp:76] Creating layer pool2
I1127 11:20:22.109670  7289 net.cpp:106] Creating Layer pool2
I1127 11:20:22.109681  7289 net.cpp:454] pool2 <- conv2
I1127 11:20:22.109697  7289 net.cpp:411] pool2 -> pool2
I1127 11:20:22.109756  7289 net.cpp:150] Setting up pool2
I1127 11:20:22.109772  7289 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:20:22.109781  7289 net.cpp:165] Memory required for data: 7674800
I1127 11:20:22.109791  7289 layer_factory.hpp:76] Creating layer ip1
I1127 11:20:22.109812  7289 net.cpp:106] Creating Layer ip1
I1127 11:20:22.109822  7289 net.cpp:454] ip1 <- pool2
I1127 11:20:22.109836  7289 net.cpp:411] ip1 -> ip1
I1127 11:20:22.118986  7289 net.cpp:150] Setting up ip1
I1127 11:20:22.119060  7289 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:20:22.119070  7289 net.cpp:165] Memory required for data: 7874800
I1127 11:20:22.119096  7289 layer_factory.hpp:76] Creating layer relu1
I1127 11:20:22.119114  7289 net.cpp:106] Creating Layer relu1
I1127 11:20:22.119123  7289 net.cpp:454] relu1 <- ip1
I1127 11:20:22.119140  7289 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:20:22.119155  7289 net.cpp:150] Setting up relu1
I1127 11:20:22.119164  7289 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:20:22.119170  7289 net.cpp:165] Memory required for data: 8074800
I1127 11:20:22.119177  7289 layer_factory.hpp:76] Creating layer ip2
I1127 11:20:22.119197  7289 net.cpp:106] Creating Layer ip2
I1127 11:20:22.119204  7289 net.cpp:454] ip2 <- ip1
I1127 11:20:22.119216  7289 net.cpp:411] ip2 -> ip2
I1127 11:20:22.119447  7289 net.cpp:150] Setting up ip2
I1127 11:20:22.119474  7289 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:20:22.119482  7289 net.cpp:165] Memory required for data: 8078800
I1127 11:20:22.119494  7289 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:20:22.119509  7289 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:20:22.119518  7289 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:20:22.119529  7289 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:20:22.119539  7289 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:20:22.119597  7289 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:20:22.119609  7289 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:20:22.119617  7289 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:20:22.119623  7289 net.cpp:165] Memory required for data: 8086800
I1127 11:20:22.119649  7289 layer_factory.hpp:76] Creating layer accuracy
I1127 11:20:22.119667  7289 net.cpp:106] Creating Layer accuracy
I1127 11:20:22.119673  7289 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:20:22.119683  7289 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:20:22.119691  7289 net.cpp:411] accuracy -> accuracy
I1127 11:20:22.119707  7289 net.cpp:150] Setting up accuracy
I1127 11:20:22.119716  7289 net.cpp:157] Top shape: (1)
I1127 11:20:22.119722  7289 net.cpp:165] Memory required for data: 8086804
I1127 11:20:22.119729  7289 layer_factory.hpp:76] Creating layer loss
I1127 11:20:22.119743  7289 net.cpp:106] Creating Layer loss
I1127 11:20:22.119750  7289 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:20:22.119760  7289 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:20:22.119770  7289 net.cpp:411] loss -> loss
I1127 11:20:22.119786  7289 layer_factory.hpp:76] Creating layer loss
I1127 11:20:22.119973  7289 net.cpp:150] Setting up loss
I1127 11:20:22.120005  7289 net.cpp:157] Top shape: (1)
I1127 11:20:22.120020  7289 net.cpp:160]     with loss weight 1
I1127 11:20:22.120059  7289 net.cpp:165] Memory required for data: 8086808
I1127 11:20:22.120074  7289 net.cpp:226] loss needs backward computation.
I1127 11:20:22.120090  7289 net.cpp:228] accuracy does not need backward computation.
I1127 11:20:22.120100  7289 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:20:22.120107  7289 net.cpp:226] ip2 needs backward computation.
I1127 11:20:22.120115  7289 net.cpp:226] relu1 needs backward computation.
I1127 11:20:22.120123  7289 net.cpp:226] ip1 needs backward computation.
I1127 11:20:22.120131  7289 net.cpp:226] pool2 needs backward computation.
I1127 11:20:22.120138  7289 net.cpp:226] conv2 needs backward computation.
I1127 11:20:22.120146  7289 net.cpp:226] pool1 needs backward computation.
I1127 11:20:22.120154  7289 net.cpp:226] conv1 needs backward computation.
I1127 11:20:22.120163  7289 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:20:22.120172  7289 net.cpp:228] mnist does not need backward computation.
I1127 11:20:22.120178  7289 net.cpp:270] This network produces output accuracy
I1127 11:20:22.120187  7289 net.cpp:270] This network produces output loss
I1127 11:20:22.120206  7289 net.cpp:283] Network initialization done.
I1127 11:20:22.120348  7289 solver.cpp:59] Solver scaffolding done.
I1127 11:20:22.120767  7289 caffe.cpp:212] Starting Optimization
I1127 11:20:22.120795  7289 solver.cpp:287] Solving LeNet
I1127 11:20:22.120802  7289 solver.cpp:288] Learning Rate Policy: inv
I1127 11:20:22.121842  7289 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:20:22.122963  7289 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:20:23.892082  7289 solver.cpp:408]     Test net output #0: accuracy = 0.1031
I1127 11:20:23.892148  7289 solver.cpp:408]     Test net output #1: loss = 2.41245 (* 1 = 2.41245 loss)
I1127 11:20:23.924187  7289 solver.cpp:236] Iteration 0, loss = 2.47842
I1127 11:20:23.924264  7289 solver.cpp:252]     Train net output #0: loss = 2.47842 (* 1 = 2.47842 loss)
I1127 11:20:23.924291  7289 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:20:37.601613  7289 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:20:39.782402  7289 solver.cpp:408]     Test net output #0: accuracy = 0.9752
I1127 11:20:39.782528  7289 solver.cpp:408]     Test net output #1: loss = 0.0818099 (* 1 = 0.0818099 loss)
I1127 11:20:39.796434  7289 solver.cpp:236] Iteration 500, loss = 0.0989331
I1127 11:20:39.796634  7289 solver.cpp:252]     Train net output #0: loss = 0.0989331 (* 1 = 0.0989331 loss)
I1127 11:20:39.796674  7289 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:20:51.252569  7289 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:20:51.267204  7289 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:20:51.278267  7289 solver.cpp:320] Iteration 1000, loss = 0.0673988
I1127 11:20:51.278409  7289 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:20:54.177237  7289 solver.cpp:408]     Test net output #0: accuracy = 0.9822
I1127 11:20:54.177300  7289 solver.cpp:408]     Test net output #1: loss = 0.0595063 (* 1 = 0.0595063 loss)
I1127 11:20:54.177307  7289 solver.cpp:325] Optimization Done.
I1127 11:20:54.177311  7289 caffe.cpp:215] Optimization Done.
I1127 11:20:54.242879  7318 caffe.cpp:184] Using GPUs 0
I1127 11:20:54.623517  7318 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:20:54.623888  7318 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:20:54.624656  7318 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:20:54.624714  7318 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:20:54.624918  7318 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:20:54.625038  7318 layer_factory.hpp:76] Creating layer mnist
I1127 11:20:54.625785  7318 net.cpp:106] Creating Layer mnist
I1127 11:20:54.625843  7318 net.cpp:411] mnist -> data
I1127 11:20:54.625901  7318 net.cpp:411] mnist -> label
I1127 11:20:54.627089  7321 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:20:54.640528  7318 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:20:54.641882  7318 net.cpp:150] Setting up mnist
I1127 11:20:54.641926  7318 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:20:54.641937  7318 net.cpp:157] Top shape: 64 (64)
I1127 11:20:54.641943  7318 net.cpp:165] Memory required for data: 200960
I1127 11:20:54.641957  7318 layer_factory.hpp:76] Creating layer conv1
I1127 11:20:54.641975  7318 net.cpp:106] Creating Layer conv1
I1127 11:20:54.641986  7318 net.cpp:454] conv1 <- data
I1127 11:20:54.642001  7318 net.cpp:411] conv1 -> conv1
I1127 11:20:54.642778  7318 net.cpp:150] Setting up conv1
I1127 11:20:54.642802  7318 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:20:54.642807  7318 net.cpp:165] Memory required for data: 3150080
I1127 11:20:54.642823  7318 layer_factory.hpp:76] Creating layer pool1
I1127 11:20:54.642835  7318 net.cpp:106] Creating Layer pool1
I1127 11:20:54.642840  7318 net.cpp:454] pool1 <- conv1
I1127 11:20:54.642848  7318 net.cpp:411] pool1 -> pool1
I1127 11:20:54.642987  7318 net.cpp:150] Setting up pool1
I1127 11:20:54.643003  7318 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:20:54.643010  7318 net.cpp:165] Memory required for data: 3887360
I1127 11:20:54.643018  7318 layer_factory.hpp:76] Creating layer conv2
I1127 11:20:54.643031  7318 net.cpp:106] Creating Layer conv2
I1127 11:20:54.643038  7318 net.cpp:454] conv2 <- pool1
I1127 11:20:54.643056  7318 net.cpp:411] conv2 -> conv2
I1127 11:20:54.643492  7318 net.cpp:150] Setting up conv2
I1127 11:20:54.643509  7318 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:20:54.643518  7318 net.cpp:165] Memory required for data: 4706560
I1127 11:20:54.643534  7318 layer_factory.hpp:76] Creating layer pool2
I1127 11:20:54.643556  7318 net.cpp:106] Creating Layer pool2
I1127 11:20:54.643565  7318 net.cpp:454] pool2 <- conv2
I1127 11:20:54.643576  7318 net.cpp:411] pool2 -> pool2
I1127 11:20:54.643621  7318 net.cpp:150] Setting up pool2
I1127 11:20:54.643635  7318 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:20:54.643642  7318 net.cpp:165] Memory required for data: 4911360
I1127 11:20:54.643651  7318 layer_factory.hpp:76] Creating layer ip1
I1127 11:20:54.643668  7318 net.cpp:106] Creating Layer ip1
I1127 11:20:54.643678  7318 net.cpp:454] ip1 <- pool2
I1127 11:20:54.643689  7318 net.cpp:411] ip1 -> ip1
I1127 11:20:54.647472  7318 net.cpp:150] Setting up ip1
I1127 11:20:54.647518  7318 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:20:54.647524  7318 net.cpp:165] Memory required for data: 5039360
I1127 11:20:54.647547  7318 layer_factory.hpp:76] Creating layer relu1
I1127 11:20:54.647562  7318 net.cpp:106] Creating Layer relu1
I1127 11:20:54.647567  7318 net.cpp:454] relu1 <- ip1
I1127 11:20:54.647578  7318 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:20:54.647599  7318 net.cpp:150] Setting up relu1
I1127 11:20:54.647606  7318 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:20:54.647610  7318 net.cpp:165] Memory required for data: 5167360
I1127 11:20:54.647614  7318 layer_factory.hpp:76] Creating layer ip2
I1127 11:20:54.647630  7318 net.cpp:106] Creating Layer ip2
I1127 11:20:54.647635  7318 net.cpp:454] ip2 <- ip1
I1127 11:20:54.647642  7318 net.cpp:411] ip2 -> ip2
I1127 11:20:54.648468  7318 net.cpp:150] Setting up ip2
I1127 11:20:54.648519  7318 net.cpp:157] Top shape: 64 10 (640)
I1127 11:20:54.648524  7318 net.cpp:165] Memory required for data: 5169920
I1127 11:20:54.648537  7318 layer_factory.hpp:76] Creating layer loss
I1127 11:20:54.648555  7318 net.cpp:106] Creating Layer loss
I1127 11:20:54.648561  7318 net.cpp:454] loss <- ip2
I1127 11:20:54.648567  7318 net.cpp:454] loss <- label
I1127 11:20:54.648582  7318 net.cpp:411] loss -> loss
I1127 11:20:54.648603  7318 layer_factory.hpp:76] Creating layer loss
I1127 11:20:54.648685  7318 net.cpp:150] Setting up loss
I1127 11:20:54.648694  7318 net.cpp:157] Top shape: (1)
I1127 11:20:54.648699  7318 net.cpp:160]     with loss weight 1
I1127 11:20:54.648735  7318 net.cpp:165] Memory required for data: 5169924
I1127 11:20:54.648741  7318 net.cpp:226] loss needs backward computation.
I1127 11:20:54.648746  7318 net.cpp:226] ip2 needs backward computation.
I1127 11:20:54.648751  7318 net.cpp:226] relu1 needs backward computation.
I1127 11:20:54.648756  7318 net.cpp:226] ip1 needs backward computation.
I1127 11:20:54.648761  7318 net.cpp:226] pool2 needs backward computation.
I1127 11:20:54.648766  7318 net.cpp:226] conv2 needs backward computation.
I1127 11:20:54.648772  7318 net.cpp:226] pool1 needs backward computation.
I1127 11:20:54.648777  7318 net.cpp:226] conv1 needs backward computation.
I1127 11:20:54.648795  7318 net.cpp:228] mnist does not need backward computation.
I1127 11:20:54.648800  7318 net.cpp:270] This network produces output loss
I1127 11:20:54.648810  7318 net.cpp:283] Network initialization done.
I1127 11:20:54.649147  7318 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:20:54.649193  7318 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:20:54.649370  7318 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:20:54.649474  7318 layer_factory.hpp:76] Creating layer mnist
I1127 11:20:54.649703  7318 net.cpp:106] Creating Layer mnist
I1127 11:20:54.649746  7318 net.cpp:411] mnist -> data
I1127 11:20:54.649775  7318 net.cpp:411] mnist -> label
I1127 11:20:54.650818  7323 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:20:54.651098  7318 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:20:54.652755  7318 net.cpp:150] Setting up mnist
I1127 11:20:54.652837  7318 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:20:54.652848  7318 net.cpp:157] Top shape: 100 (100)
I1127 11:20:54.652856  7318 net.cpp:165] Memory required for data: 314000
I1127 11:20:54.652873  7318 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:20:54.652911  7318 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:20:54.652926  7318 net.cpp:454] label_mnist_1_split <- label
I1127 11:20:54.652946  7318 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:20:54.652973  7318 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:20:54.653056  7318 net.cpp:150] Setting up label_mnist_1_split
I1127 11:20:54.653074  7318 net.cpp:157] Top shape: 100 (100)
I1127 11:20:54.653086  7318 net.cpp:157] Top shape: 100 (100)
I1127 11:20:54.653097  7318 net.cpp:165] Memory required for data: 314800
I1127 11:20:54.653107  7318 layer_factory.hpp:76] Creating layer conv1
I1127 11:20:54.653139  7318 net.cpp:106] Creating Layer conv1
I1127 11:20:54.653178  7318 net.cpp:454] conv1 <- data
I1127 11:20:54.653199  7318 net.cpp:411] conv1 -> conv1
I1127 11:20:54.653520  7318 net.cpp:150] Setting up conv1
I1127 11:20:54.653550  7318 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:20:54.653558  7318 net.cpp:165] Memory required for data: 4922800
I1127 11:20:54.653578  7318 layer_factory.hpp:76] Creating layer pool1
I1127 11:20:54.653592  7318 net.cpp:106] Creating Layer pool1
I1127 11:20:54.653599  7318 net.cpp:454] pool1 <- conv1
I1127 11:20:54.653631  7318 net.cpp:411] pool1 -> pool1
I1127 11:20:54.653686  7318 net.cpp:150] Setting up pool1
I1127 11:20:54.653697  7318 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:20:54.653704  7318 net.cpp:165] Memory required for data: 6074800
I1127 11:20:54.653712  7318 layer_factory.hpp:76] Creating layer conv2
I1127 11:20:54.653730  7318 net.cpp:106] Creating Layer conv2
I1127 11:20:54.653738  7318 net.cpp:454] conv2 <- pool1
I1127 11:20:54.653748  7318 net.cpp:411] conv2 -> conv2
I1127 11:20:54.654212  7318 net.cpp:150] Setting up conv2
I1127 11:20:54.654240  7318 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:20:54.654249  7318 net.cpp:165] Memory required for data: 7354800
I1127 11:20:54.654266  7318 layer_factory.hpp:76] Creating layer pool2
I1127 11:20:54.654283  7318 net.cpp:106] Creating Layer pool2
I1127 11:20:54.654291  7318 net.cpp:454] pool2 <- conv2
I1127 11:20:54.654304  7318 net.cpp:411] pool2 -> pool2
I1127 11:20:54.654351  7318 net.cpp:150] Setting up pool2
I1127 11:20:54.654363  7318 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:20:54.654371  7318 net.cpp:165] Memory required for data: 7674800
I1127 11:20:54.654378  7318 layer_factory.hpp:76] Creating layer ip1
I1127 11:20:54.654393  7318 net.cpp:106] Creating Layer ip1
I1127 11:20:54.654402  7318 net.cpp:454] ip1 <- pool2
I1127 11:20:54.654413  7318 net.cpp:411] ip1 -> ip1
I1127 11:20:54.658366  7318 net.cpp:150] Setting up ip1
I1127 11:20:54.658438  7318 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:20:54.658449  7318 net.cpp:165] Memory required for data: 7874800
I1127 11:20:54.658474  7318 layer_factory.hpp:76] Creating layer relu1
I1127 11:20:54.658493  7318 net.cpp:106] Creating Layer relu1
I1127 11:20:54.658504  7318 net.cpp:454] relu1 <- ip1
I1127 11:20:54.658521  7318 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:20:54.658538  7318 net.cpp:150] Setting up relu1
I1127 11:20:54.658547  7318 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:20:54.658555  7318 net.cpp:165] Memory required for data: 8074800
I1127 11:20:54.658561  7318 layer_factory.hpp:76] Creating layer ip2
I1127 11:20:54.658573  7318 net.cpp:106] Creating Layer ip2
I1127 11:20:54.658577  7318 net.cpp:454] ip2 <- ip1
I1127 11:20:54.658586  7318 net.cpp:411] ip2 -> ip2
I1127 11:20:54.658718  7318 net.cpp:150] Setting up ip2
I1127 11:20:54.658728  7318 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:20:54.658732  7318 net.cpp:165] Memory required for data: 8078800
I1127 11:20:54.658740  7318 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:20:54.658748  7318 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:20:54.658752  7318 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:20:54.658758  7318 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:20:54.658766  7318 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:20:54.658797  7318 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:20:54.658803  7318 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:20:54.658808  7318 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:20:54.658813  7318 net.cpp:165] Memory required for data: 8086800
I1127 11:20:54.658818  7318 layer_factory.hpp:76] Creating layer accuracy
I1127 11:20:54.658826  7318 net.cpp:106] Creating Layer accuracy
I1127 11:20:54.658831  7318 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:20:54.658836  7318 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:20:54.658843  7318 net.cpp:411] accuracy -> accuracy
I1127 11:20:54.658851  7318 net.cpp:150] Setting up accuracy
I1127 11:20:54.658857  7318 net.cpp:157] Top shape: (1)
I1127 11:20:54.658869  7318 net.cpp:165] Memory required for data: 8086804
I1127 11:20:54.658874  7318 layer_factory.hpp:76] Creating layer loss
I1127 11:20:54.658880  7318 net.cpp:106] Creating Layer loss
I1127 11:20:54.658885  7318 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:20:54.658890  7318 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:20:54.658896  7318 net.cpp:411] loss -> loss
I1127 11:20:54.658905  7318 layer_factory.hpp:76] Creating layer loss
I1127 11:20:54.658982  7318 net.cpp:150] Setting up loss
I1127 11:20:54.658990  7318 net.cpp:157] Top shape: (1)
I1127 11:20:54.658994  7318 net.cpp:160]     with loss weight 1
I1127 11:20:54.659009  7318 net.cpp:165] Memory required for data: 8086808
I1127 11:20:54.659014  7318 net.cpp:226] loss needs backward computation.
I1127 11:20:54.659024  7318 net.cpp:228] accuracy does not need backward computation.
I1127 11:20:54.659029  7318 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:20:54.659034  7318 net.cpp:226] ip2 needs backward computation.
I1127 11:20:54.659037  7318 net.cpp:226] relu1 needs backward computation.
I1127 11:20:54.659042  7318 net.cpp:226] ip1 needs backward computation.
I1127 11:20:54.659046  7318 net.cpp:226] pool2 needs backward computation.
I1127 11:20:54.659051  7318 net.cpp:226] conv2 needs backward computation.
I1127 11:20:54.659056  7318 net.cpp:226] pool1 needs backward computation.
I1127 11:20:54.659060  7318 net.cpp:226] conv1 needs backward computation.
I1127 11:20:54.659065  7318 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:20:54.659070  7318 net.cpp:228] mnist does not need backward computation.
I1127 11:20:54.659075  7318 net.cpp:270] This network produces output accuracy
I1127 11:20:54.659080  7318 net.cpp:270] This network produces output loss
I1127 11:20:54.659093  7318 net.cpp:283] Network initialization done.
I1127 11:20:54.659155  7318 solver.cpp:59] Solver scaffolding done.
I1127 11:20:54.659360  7318 caffe.cpp:212] Starting Optimization
I1127 11:20:54.659373  7318 solver.cpp:287] Solving LeNet
I1127 11:20:54.659376  7318 solver.cpp:288] Learning Rate Policy: inv
I1127 11:20:54.659911  7318 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:20:55.758663  7318 solver.cpp:408]     Test net output #0: accuracy = 0.1361
I1127 11:20:55.758785  7318 solver.cpp:408]     Test net output #1: loss = 2.32417 (* 1 = 2.32417 loss)
I1127 11:20:55.773913  7318 solver.cpp:236] Iteration 0, loss = 2.29683
I1127 11:20:55.774022  7318 solver.cpp:252]     Train net output #0: loss = 2.29683 (* 1 = 2.29683 loss)
I1127 11:20:55.774062  7318 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:21:09.179895  7318 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:21:12.026708  7318 solver.cpp:408]     Test net output #0: accuracy = 0.9757
I1127 11:21:12.026753  7318 solver.cpp:408]     Test net output #1: loss = 0.0774412 (* 1 = 0.0774412 loss)
I1127 11:21:12.058118  7318 solver.cpp:236] Iteration 500, loss = 0.117493
I1127 11:21:12.058162  7318 solver.cpp:252]     Train net output #0: loss = 0.117493 (* 1 = 0.117493 loss)
I1127 11:21:12.058174  7318 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:21:24.934633  7318 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:21:24.954890  7318 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:21:24.981848  7318 solver.cpp:320] Iteration 1000, loss = 0.0808799
I1127 11:21:24.981869  7318 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:21:26.883358  7318 solver.cpp:408]     Test net output #0: accuracy = 0.9821
I1127 11:21:26.883579  7318 solver.cpp:408]     Test net output #1: loss = 0.0533428 (* 1 = 0.0533428 loss)
I1127 11:21:26.883596  7318 solver.cpp:325] Optimization Done.
I1127 11:21:26.883606  7318 caffe.cpp:215] Optimization Done.
I1127 11:21:27.038382  7346 caffe.cpp:184] Using GPUs 0
I1127 11:21:27.461992  7346 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:21:27.462554  7346 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:21:27.463244  7346 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:21:27.463305  7346 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:21:27.463529  7346 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:21:27.463666  7346 layer_factory.hpp:76] Creating layer mnist
I1127 11:21:27.464360  7346 net.cpp:106] Creating Layer mnist
I1127 11:21:27.464421  7346 net.cpp:411] mnist -> data
I1127 11:21:27.464500  7346 net.cpp:411] mnist -> label
I1127 11:21:27.466156  7350 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:21:27.484400  7346 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:21:27.486933  7346 net.cpp:150] Setting up mnist
I1127 11:21:27.487115  7346 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:21:27.487134  7346 net.cpp:157] Top shape: 64 (64)
I1127 11:21:27.487143  7346 net.cpp:165] Memory required for data: 200960
I1127 11:21:27.487169  7346 layer_factory.hpp:76] Creating layer conv1
I1127 11:21:27.487229  7346 net.cpp:106] Creating Layer conv1
I1127 11:21:27.487246  7346 net.cpp:454] conv1 <- data
I1127 11:21:27.487278  7346 net.cpp:411] conv1 -> conv1
I1127 11:21:27.489246  7346 net.cpp:150] Setting up conv1
I1127 11:21:27.489347  7346 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:21:27.489358  7346 net.cpp:165] Memory required for data: 3150080
I1127 11:21:27.489398  7346 layer_factory.hpp:76] Creating layer pool1
I1127 11:21:27.489433  7346 net.cpp:106] Creating Layer pool1
I1127 11:21:27.489445  7346 net.cpp:454] pool1 <- conv1
I1127 11:21:27.489465  7346 net.cpp:411] pool1 -> pool1
I1127 11:21:27.489624  7346 net.cpp:150] Setting up pool1
I1127 11:21:27.489639  7346 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:21:27.489657  7346 net.cpp:165] Memory required for data: 3887360
I1127 11:21:27.489667  7346 layer_factory.hpp:76] Creating layer conv2
I1127 11:21:27.489697  7346 net.cpp:106] Creating Layer conv2
I1127 11:21:27.489708  7346 net.cpp:454] conv2 <- pool1
I1127 11:21:27.489722  7346 net.cpp:411] conv2 -> conv2
I1127 11:21:27.490298  7346 net.cpp:150] Setting up conv2
I1127 11:21:27.490356  7346 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:21:27.490373  7346 net.cpp:165] Memory required for data: 4706560
I1127 11:21:27.490411  7346 layer_factory.hpp:76] Creating layer pool2
I1127 11:21:27.490452  7346 net.cpp:106] Creating Layer pool2
I1127 11:21:27.490474  7346 net.cpp:454] pool2 <- conv2
I1127 11:21:27.490492  7346 net.cpp:411] pool2 -> pool2
I1127 11:21:27.490588  7346 net.cpp:150] Setting up pool2
I1127 11:21:27.490605  7346 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:21:27.490615  7346 net.cpp:165] Memory required for data: 4911360
I1127 11:21:27.490624  7346 layer_factory.hpp:76] Creating layer ip1
I1127 11:21:27.490656  7346 net.cpp:106] Creating Layer ip1
I1127 11:21:27.490670  7346 net.cpp:454] ip1 <- pool2
I1127 11:21:27.490682  7346 net.cpp:411] ip1 -> ip1
I1127 11:21:27.496577  7346 net.cpp:150] Setting up ip1
I1127 11:21:27.496666  7346 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:21:27.496678  7346 net.cpp:165] Memory required for data: 5039360
I1127 11:21:27.496698  7346 layer_factory.hpp:76] Creating layer relu1
I1127 11:21:27.496713  7346 net.cpp:106] Creating Layer relu1
I1127 11:21:27.496721  7346 net.cpp:454] relu1 <- ip1
I1127 11:21:27.496731  7346 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:21:27.496748  7346 net.cpp:150] Setting up relu1
I1127 11:21:27.496757  7346 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:21:27.496763  7346 net.cpp:165] Memory required for data: 5167360
I1127 11:21:27.496769  7346 layer_factory.hpp:76] Creating layer ip2
I1127 11:21:27.496783  7346 net.cpp:106] Creating Layer ip2
I1127 11:21:27.496790  7346 net.cpp:454] ip2 <- ip1
I1127 11:21:27.496799  7346 net.cpp:411] ip2 -> ip2
I1127 11:21:27.497557  7346 net.cpp:150] Setting up ip2
I1127 11:21:27.497584  7346 net.cpp:157] Top shape: 64 10 (640)
I1127 11:21:27.497591  7346 net.cpp:165] Memory required for data: 5169920
I1127 11:21:27.497604  7346 layer_factory.hpp:76] Creating layer loss
I1127 11:21:27.497625  7346 net.cpp:106] Creating Layer loss
I1127 11:21:27.497632  7346 net.cpp:454] loss <- ip2
I1127 11:21:27.497642  7346 net.cpp:454] loss <- label
I1127 11:21:27.497658  7346 net.cpp:411] loss -> loss
I1127 11:21:27.497679  7346 layer_factory.hpp:76] Creating layer loss
I1127 11:21:27.497792  7346 net.cpp:150] Setting up loss
I1127 11:21:27.497804  7346 net.cpp:157] Top shape: (1)
I1127 11:21:27.497810  7346 net.cpp:160]     with loss weight 1
I1127 11:21:27.497843  7346 net.cpp:165] Memory required for data: 5169924
I1127 11:21:27.497850  7346 net.cpp:226] loss needs backward computation.
I1127 11:21:27.497858  7346 net.cpp:226] ip2 needs backward computation.
I1127 11:21:27.497867  7346 net.cpp:226] relu1 needs backward computation.
I1127 11:21:27.497874  7346 net.cpp:226] ip1 needs backward computation.
I1127 11:21:27.497880  7346 net.cpp:226] pool2 needs backward computation.
I1127 11:21:27.497887  7346 net.cpp:226] conv2 needs backward computation.
I1127 11:21:27.497895  7346 net.cpp:226] pool1 needs backward computation.
I1127 11:21:27.497901  7346 net.cpp:226] conv1 needs backward computation.
I1127 11:21:27.497908  7346 net.cpp:228] mnist does not need backward computation.
I1127 11:21:27.497915  7346 net.cpp:270] This network produces output loss
I1127 11:21:27.497928  7346 net.cpp:283] Network initialization done.
I1127 11:21:27.498339  7346 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:21:27.498401  7346 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:21:27.498653  7346 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:21:27.498785  7346 layer_factory.hpp:76] Creating layer mnist
I1127 11:21:27.498922  7346 net.cpp:106] Creating Layer mnist
I1127 11:21:27.498936  7346 net.cpp:411] mnist -> data
I1127 11:21:27.498950  7346 net.cpp:411] mnist -> label
I1127 11:21:27.499943  7352 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:21:27.500216  7346 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:21:27.504847  7346 net.cpp:150] Setting up mnist
I1127 11:21:27.504923  7346 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:21:27.504951  7346 net.cpp:157] Top shape: 100 (100)
I1127 11:21:27.504967  7346 net.cpp:165] Memory required for data: 314000
I1127 11:21:27.504987  7346 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:21:27.505017  7346 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:21:27.505038  7346 net.cpp:454] label_mnist_1_split <- label
I1127 11:21:27.505059  7346 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:21:27.505080  7346 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:21:27.505187  7346 net.cpp:150] Setting up label_mnist_1_split
I1127 11:21:27.505208  7346 net.cpp:157] Top shape: 100 (100)
I1127 11:21:27.505218  7346 net.cpp:157] Top shape: 100 (100)
I1127 11:21:27.505225  7346 net.cpp:165] Memory required for data: 314800
I1127 11:21:27.505234  7346 layer_factory.hpp:76] Creating layer conv1
I1127 11:21:27.505264  7346 net.cpp:106] Creating Layer conv1
I1127 11:21:27.505278  7346 net.cpp:454] conv1 <- data
I1127 11:21:27.505295  7346 net.cpp:411] conv1 -> conv1
I1127 11:21:27.505760  7346 net.cpp:150] Setting up conv1
I1127 11:21:27.505781  7346 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:21:27.505787  7346 net.cpp:165] Memory required for data: 4922800
I1127 11:21:27.505800  7346 layer_factory.hpp:76] Creating layer pool1
I1127 11:21:27.505812  7346 net.cpp:106] Creating Layer pool1
I1127 11:21:27.505820  7346 net.cpp:454] pool1 <- conv1
I1127 11:21:27.505848  7346 net.cpp:411] pool1 -> pool1
I1127 11:21:27.505903  7346 net.cpp:150] Setting up pool1
I1127 11:21:27.505914  7346 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:21:27.505918  7346 net.cpp:165] Memory required for data: 6074800
I1127 11:21:27.505924  7346 layer_factory.hpp:76] Creating layer conv2
I1127 11:21:27.505939  7346 net.cpp:106] Creating Layer conv2
I1127 11:21:27.505945  7346 net.cpp:454] conv2 <- pool1
I1127 11:21:27.505956  7346 net.cpp:411] conv2 -> conv2
I1127 11:21:27.506361  7346 net.cpp:150] Setting up conv2
I1127 11:21:27.506410  7346 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:21:27.506415  7346 net.cpp:165] Memory required for data: 7354800
I1127 11:21:27.506430  7346 layer_factory.hpp:76] Creating layer pool2
I1127 11:21:27.506445  7346 net.cpp:106] Creating Layer pool2
I1127 11:21:27.506453  7346 net.cpp:454] pool2 <- conv2
I1127 11:21:27.506461  7346 net.cpp:411] pool2 -> pool2
I1127 11:21:27.506510  7346 net.cpp:150] Setting up pool2
I1127 11:21:27.506520  7346 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:21:27.506523  7346 net.cpp:165] Memory required for data: 7674800
I1127 11:21:27.506530  7346 layer_factory.hpp:76] Creating layer ip1
I1127 11:21:27.506542  7346 net.cpp:106] Creating Layer ip1
I1127 11:21:27.506548  7346 net.cpp:454] ip1 <- pool2
I1127 11:21:27.506556  7346 net.cpp:411] ip1 -> ip1
I1127 11:21:27.509423  7346 net.cpp:150] Setting up ip1
I1127 11:21:27.509500  7346 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:21:27.509505  7346 net.cpp:165] Memory required for data: 7874800
I1127 11:21:27.509531  7346 layer_factory.hpp:76] Creating layer relu1
I1127 11:21:27.509553  7346 net.cpp:106] Creating Layer relu1
I1127 11:21:27.509562  7346 net.cpp:454] relu1 <- ip1
I1127 11:21:27.509573  7346 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:21:27.509593  7346 net.cpp:150] Setting up relu1
I1127 11:21:27.509599  7346 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:21:27.509603  7346 net.cpp:165] Memory required for data: 8074800
I1127 11:21:27.509608  7346 layer_factory.hpp:76] Creating layer ip2
I1127 11:21:27.509624  7346 net.cpp:106] Creating Layer ip2
I1127 11:21:27.509629  7346 net.cpp:454] ip2 <- ip1
I1127 11:21:27.509637  7346 net.cpp:411] ip2 -> ip2
I1127 11:21:27.509757  7346 net.cpp:150] Setting up ip2
I1127 11:21:27.509765  7346 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:21:27.509769  7346 net.cpp:165] Memory required for data: 8078800
I1127 11:21:27.509776  7346 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:21:27.509783  7346 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:21:27.509788  7346 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:21:27.509793  7346 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:21:27.509799  7346 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:21:27.509827  7346 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:21:27.509835  7346 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:21:27.509840  7346 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:21:27.509845  7346 net.cpp:165] Memory required for data: 8086800
I1127 11:21:27.509848  7346 layer_factory.hpp:76] Creating layer accuracy
I1127 11:21:27.509855  7346 net.cpp:106] Creating Layer accuracy
I1127 11:21:27.509860  7346 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:21:27.509865  7346 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:21:27.509872  7346 net.cpp:411] accuracy -> accuracy
I1127 11:21:27.509881  7346 net.cpp:150] Setting up accuracy
I1127 11:21:27.509887  7346 net.cpp:157] Top shape: (1)
I1127 11:21:27.509891  7346 net.cpp:165] Memory required for data: 8086804
I1127 11:21:27.509896  7346 layer_factory.hpp:76] Creating layer loss
I1127 11:21:27.509902  7346 net.cpp:106] Creating Layer loss
I1127 11:21:27.509908  7346 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:21:27.509913  7346 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:21:27.509919  7346 net.cpp:411] loss -> loss
I1127 11:21:27.509928  7346 layer_factory.hpp:76] Creating layer loss
I1127 11:21:27.510015  7346 net.cpp:150] Setting up loss
I1127 11:21:27.510025  7346 net.cpp:157] Top shape: (1)
I1127 11:21:27.510033  7346 net.cpp:160]     with loss weight 1
I1127 11:21:27.510051  7346 net.cpp:165] Memory required for data: 8086808
I1127 11:21:27.510056  7346 net.cpp:226] loss needs backward computation.
I1127 11:21:27.510066  7346 net.cpp:228] accuracy does not need backward computation.
I1127 11:21:27.510071  7346 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:21:27.510076  7346 net.cpp:226] ip2 needs backward computation.
I1127 11:21:27.510079  7346 net.cpp:226] relu1 needs backward computation.
I1127 11:21:27.510083  7346 net.cpp:226] ip1 needs backward computation.
I1127 11:21:27.510088  7346 net.cpp:226] pool2 needs backward computation.
I1127 11:21:27.510093  7346 net.cpp:226] conv2 needs backward computation.
I1127 11:21:27.510097  7346 net.cpp:226] pool1 needs backward computation.
I1127 11:21:27.510102  7346 net.cpp:226] conv1 needs backward computation.
I1127 11:21:27.510107  7346 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:21:27.510113  7346 net.cpp:228] mnist does not need backward computation.
I1127 11:21:27.510116  7346 net.cpp:270] This network produces output accuracy
I1127 11:21:27.510121  7346 net.cpp:270] This network produces output loss
I1127 11:21:27.510134  7346 net.cpp:283] Network initialization done.
I1127 11:21:27.510241  7346 solver.cpp:59] Solver scaffolding done.
I1127 11:21:27.510635  7346 caffe.cpp:212] Starting Optimization
I1127 11:21:27.510668  7346 solver.cpp:287] Solving LeNet
I1127 11:21:27.510676  7346 solver.cpp:288] Learning Rate Policy: inv
I1127 11:21:27.511636  7346 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:21:27.512791  7346 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:21:28.681633  7346 solver.cpp:408]     Test net output #0: accuracy = 0.1229
I1127 11:21:28.681721  7346 solver.cpp:408]     Test net output #1: loss = 2.31034 (* 1 = 2.31034 loss)
I1127 11:21:28.710325  7346 solver.cpp:236] Iteration 0, loss = 2.32804
I1127 11:21:28.710369  7346 solver.cpp:252]     Train net output #0: loss = 2.32804 (* 1 = 2.32804 loss)
I1127 11:21:28.710381  7346 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:21:41.717061  7346 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:21:43.281882  7346 solver.cpp:408]     Test net output #0: accuracy = 0.9717
I1127 11:21:43.281946  7346 solver.cpp:408]     Test net output #1: loss = 0.0856924 (* 1 = 0.0856924 loss)
I1127 11:21:43.293965  7346 solver.cpp:236] Iteration 500, loss = 0.0797169
I1127 11:21:43.294050  7346 solver.cpp:252]     Train net output #0: loss = 0.079717 (* 1 = 0.079717 loss)
I1127 11:21:43.294066  7346 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:21:56.709233  7346 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:21:56.726563  7346 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:21:56.738865  7346 solver.cpp:320] Iteration 1000, loss = 0.0803732
I1127 11:21:56.738981  7346 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:21:59.040575  7346 solver.cpp:408]     Test net output #0: accuracy = 0.982
I1127 11:21:59.042402  7346 solver.cpp:408]     Test net output #1: loss = 0.0553776 (* 1 = 0.0553776 loss)
I1127 11:21:59.042443  7346 solver.cpp:325] Optimization Done.
I1127 11:21:59.042455  7346 caffe.cpp:215] Optimization Done.
I1127 11:21:59.184722  7374 caffe.cpp:184] Using GPUs 0
I1127 11:21:59.682534  7374 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:21:59.682718  7374 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:21:59.683138  7374 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:21:59.683172  7374 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:21:59.683316  7374 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:21:59.683410  7374 layer_factory.hpp:76] Creating layer mnist
I1127 11:21:59.683992  7374 net.cpp:106] Creating Layer mnist
I1127 11:21:59.684025  7374 net.cpp:411] mnist -> data
I1127 11:21:59.684078  7374 net.cpp:411] mnist -> label
I1127 11:21:59.685315  7377 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:21:59.722693  7374 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:21:59.729779  7374 net.cpp:150] Setting up mnist
I1127 11:21:59.729871  7374 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:21:59.729881  7374 net.cpp:157] Top shape: 64 (64)
I1127 11:21:59.729884  7374 net.cpp:165] Memory required for data: 200960
I1127 11:21:59.729902  7374 layer_factory.hpp:76] Creating layer conv1
I1127 11:21:59.729928  7374 net.cpp:106] Creating Layer conv1
I1127 11:21:59.729935  7374 net.cpp:454] conv1 <- data
I1127 11:21:59.729946  7374 net.cpp:411] conv1 -> conv1
I1127 11:21:59.730657  7374 net.cpp:150] Setting up conv1
I1127 11:21:59.730684  7374 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:21:59.730689  7374 net.cpp:165] Memory required for data: 3150080
I1127 11:21:59.730702  7374 layer_factory.hpp:76] Creating layer pool1
I1127 11:21:59.730713  7374 net.cpp:106] Creating Layer pool1
I1127 11:21:59.730718  7374 net.cpp:454] pool1 <- conv1
I1127 11:21:59.730726  7374 net.cpp:411] pool1 -> pool1
I1127 11:21:59.730775  7374 net.cpp:150] Setting up pool1
I1127 11:21:59.730783  7374 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:21:59.730787  7374 net.cpp:165] Memory required for data: 3887360
I1127 11:21:59.730792  7374 layer_factory.hpp:76] Creating layer conv2
I1127 11:21:59.730803  7374 net.cpp:106] Creating Layer conv2
I1127 11:21:59.730808  7374 net.cpp:454] conv2 <- pool1
I1127 11:21:59.730815  7374 net.cpp:411] conv2 -> conv2
I1127 11:21:59.731148  7374 net.cpp:150] Setting up conv2
I1127 11:21:59.731159  7374 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:21:59.731169  7374 net.cpp:165] Memory required for data: 4706560
I1127 11:21:59.731178  7374 layer_factory.hpp:76] Creating layer pool2
I1127 11:21:59.731186  7374 net.cpp:106] Creating Layer pool2
I1127 11:21:59.731190  7374 net.cpp:454] pool2 <- conv2
I1127 11:21:59.731196  7374 net.cpp:411] pool2 -> pool2
I1127 11:21:59.731259  7374 net.cpp:150] Setting up pool2
I1127 11:21:59.731268  7374 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:21:59.731273  7374 net.cpp:165] Memory required for data: 4911360
I1127 11:21:59.731277  7374 layer_factory.hpp:76] Creating layer ip1
I1127 11:21:59.731287  7374 net.cpp:106] Creating Layer ip1
I1127 11:21:59.731292  7374 net.cpp:454] ip1 <- pool2
I1127 11:21:59.731300  7374 net.cpp:411] ip1 -> ip1
I1127 11:21:59.733893  7374 net.cpp:150] Setting up ip1
I1127 11:21:59.733952  7374 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:21:59.733957  7374 net.cpp:165] Memory required for data: 5039360
I1127 11:21:59.733976  7374 layer_factory.hpp:76] Creating layer relu1
I1127 11:21:59.733995  7374 net.cpp:106] Creating Layer relu1
I1127 11:21:59.734004  7374 net.cpp:454] relu1 <- ip1
I1127 11:21:59.734012  7374 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:21:59.734028  7374 net.cpp:150] Setting up relu1
I1127 11:21:59.734035  7374 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:21:59.734040  7374 net.cpp:165] Memory required for data: 5167360
I1127 11:21:59.734043  7374 layer_factory.hpp:76] Creating layer ip2
I1127 11:21:59.734053  7374 net.cpp:106] Creating Layer ip2
I1127 11:21:59.734057  7374 net.cpp:454] ip2 <- ip1
I1127 11:21:59.734066  7374 net.cpp:411] ip2 -> ip2
I1127 11:21:59.734844  7374 net.cpp:150] Setting up ip2
I1127 11:21:59.734884  7374 net.cpp:157] Top shape: 64 10 (640)
I1127 11:21:59.734889  7374 net.cpp:165] Memory required for data: 5169920
I1127 11:21:59.734900  7374 layer_factory.hpp:76] Creating layer loss
I1127 11:21:59.734915  7374 net.cpp:106] Creating Layer loss
I1127 11:21:59.734921  7374 net.cpp:454] loss <- ip2
I1127 11:21:59.734928  7374 net.cpp:454] loss <- label
I1127 11:21:59.734938  7374 net.cpp:411] loss -> loss
I1127 11:21:59.734956  7374 layer_factory.hpp:76] Creating layer loss
I1127 11:21:59.735034  7374 net.cpp:150] Setting up loss
I1127 11:21:59.735043  7374 net.cpp:157] Top shape: (1)
I1127 11:21:59.735046  7374 net.cpp:160]     with loss weight 1
I1127 11:21:59.735076  7374 net.cpp:165] Memory required for data: 5169924
I1127 11:21:59.735081  7374 net.cpp:226] loss needs backward computation.
I1127 11:21:59.735086  7374 net.cpp:226] ip2 needs backward computation.
I1127 11:21:59.735091  7374 net.cpp:226] relu1 needs backward computation.
I1127 11:21:59.735096  7374 net.cpp:226] ip1 needs backward computation.
I1127 11:21:59.735101  7374 net.cpp:226] pool2 needs backward computation.
I1127 11:21:59.735105  7374 net.cpp:226] conv2 needs backward computation.
I1127 11:21:59.735110  7374 net.cpp:226] pool1 needs backward computation.
I1127 11:21:59.735113  7374 net.cpp:226] conv1 needs backward computation.
I1127 11:21:59.735118  7374 net.cpp:228] mnist does not need backward computation.
I1127 11:21:59.735123  7374 net.cpp:270] This network produces output loss
I1127 11:21:59.735136  7374 net.cpp:283] Network initialization done.
I1127 11:21:59.735440  7374 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:21:59.735493  7374 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:21:59.735626  7374 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:21:59.735702  7374 layer_factory.hpp:76] Creating layer mnist
I1127 11:21:59.735821  7374 net.cpp:106] Creating Layer mnist
I1127 11:21:59.735836  7374 net.cpp:411] mnist -> data
I1127 11:21:59.735854  7374 net.cpp:411] mnist -> label
I1127 11:21:59.736685  7379 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:21:59.737069  7374 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:21:59.740974  7374 net.cpp:150] Setting up mnist
I1127 11:21:59.741055  7374 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:21:59.741070  7374 net.cpp:157] Top shape: 100 (100)
I1127 11:21:59.741077  7374 net.cpp:165] Memory required for data: 314000
I1127 11:21:59.741092  7374 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:21:59.741130  7374 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:21:59.741147  7374 net.cpp:454] label_mnist_1_split <- label
I1127 11:21:59.741166  7374 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:21:59.741194  7374 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:21:59.741308  7374 net.cpp:150] Setting up label_mnist_1_split
I1127 11:21:59.741333  7374 net.cpp:157] Top shape: 100 (100)
I1127 11:21:59.741343  7374 net.cpp:157] Top shape: 100 (100)
I1127 11:21:59.741350  7374 net.cpp:165] Memory required for data: 314800
I1127 11:21:59.741358  7374 layer_factory.hpp:76] Creating layer conv1
I1127 11:21:59.741389  7374 net.cpp:106] Creating Layer conv1
I1127 11:21:59.741400  7374 net.cpp:454] conv1 <- data
I1127 11:21:59.741415  7374 net.cpp:411] conv1 -> conv1
I1127 11:21:59.741724  7374 net.cpp:150] Setting up conv1
I1127 11:21:59.741746  7374 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:21:59.741755  7374 net.cpp:165] Memory required for data: 4922800
I1127 11:21:59.741775  7374 layer_factory.hpp:76] Creating layer pool1
I1127 11:21:59.741791  7374 net.cpp:106] Creating Layer pool1
I1127 11:21:59.741798  7374 net.cpp:454] pool1 <- conv1
I1127 11:21:59.741829  7374 net.cpp:411] pool1 -> pool1
I1127 11:21:59.741880  7374 net.cpp:150] Setting up pool1
I1127 11:21:59.741894  7374 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:21:59.741899  7374 net.cpp:165] Memory required for data: 6074800
I1127 11:21:59.741906  7374 layer_factory.hpp:76] Creating layer conv2
I1127 11:21:59.741920  7374 net.cpp:106] Creating Layer conv2
I1127 11:21:59.741927  7374 net.cpp:454] conv2 <- pool1
I1127 11:21:59.741940  7374 net.cpp:411] conv2 -> conv2
I1127 11:21:59.742499  7374 net.cpp:150] Setting up conv2
I1127 11:21:59.742568  7374 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:21:59.742575  7374 net.cpp:165] Memory required for data: 7354800
I1127 11:21:59.742596  7374 layer_factory.hpp:76] Creating layer pool2
I1127 11:21:59.742614  7374 net.cpp:106] Creating Layer pool2
I1127 11:21:59.742622  7374 net.cpp:454] pool2 <- conv2
I1127 11:21:59.742633  7374 net.cpp:411] pool2 -> pool2
I1127 11:21:59.742683  7374 net.cpp:150] Setting up pool2
I1127 11:21:59.742697  7374 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:21:59.742702  7374 net.cpp:165] Memory required for data: 7674800
I1127 11:21:59.742708  7374 layer_factory.hpp:76] Creating layer ip1
I1127 11:21:59.742720  7374 net.cpp:106] Creating Layer ip1
I1127 11:21:59.742727  7374 net.cpp:454] ip1 <- pool2
I1127 11:21:59.742735  7374 net.cpp:411] ip1 -> ip1
I1127 11:21:59.747237  7374 net.cpp:150] Setting up ip1
I1127 11:21:59.747323  7374 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:21:59.747337  7374 net.cpp:165] Memory required for data: 7874800
I1127 11:21:59.747372  7374 layer_factory.hpp:76] Creating layer relu1
I1127 11:21:59.747395  7374 net.cpp:106] Creating Layer relu1
I1127 11:21:59.747406  7374 net.cpp:454] relu1 <- ip1
I1127 11:21:59.747416  7374 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:21:59.747432  7374 net.cpp:150] Setting up relu1
I1127 11:21:59.747442  7374 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:21:59.747448  7374 net.cpp:165] Memory required for data: 8074800
I1127 11:21:59.747455  7374 layer_factory.hpp:76] Creating layer ip2
I1127 11:21:59.747479  7374 net.cpp:106] Creating Layer ip2
I1127 11:21:59.747488  7374 net.cpp:454] ip2 <- ip1
I1127 11:21:59.747499  7374 net.cpp:411] ip2 -> ip2
I1127 11:21:59.747678  7374 net.cpp:150] Setting up ip2
I1127 11:21:59.747695  7374 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:21:59.747704  7374 net.cpp:165] Memory required for data: 8078800
I1127 11:21:59.747717  7374 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:21:59.747732  7374 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:21:59.747740  7374 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:21:59.747750  7374 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:21:59.747762  7374 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:21:59.747829  7374 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:21:59.747843  7374 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:21:59.747853  7374 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:21:59.747859  7374 net.cpp:165] Memory required for data: 8086800
I1127 11:21:59.747867  7374 layer_factory.hpp:76] Creating layer accuracy
I1127 11:21:59.747882  7374 net.cpp:106] Creating Layer accuracy
I1127 11:21:59.747889  7374 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:21:59.747897  7374 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:21:59.747910  7374 net.cpp:411] accuracy -> accuracy
I1127 11:21:59.747926  7374 net.cpp:150] Setting up accuracy
I1127 11:21:59.747936  7374 net.cpp:157] Top shape: (1)
I1127 11:21:59.747942  7374 net.cpp:165] Memory required for data: 8086804
I1127 11:21:59.747949  7374 layer_factory.hpp:76] Creating layer loss
I1127 11:21:59.747959  7374 net.cpp:106] Creating Layer loss
I1127 11:21:59.747967  7374 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:21:59.747973  7374 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:21:59.747984  7374 net.cpp:411] loss -> loss
I1127 11:21:59.747999  7374 layer_factory.hpp:76] Creating layer loss
I1127 11:21:59.748157  7374 net.cpp:150] Setting up loss
I1127 11:21:59.748174  7374 net.cpp:157] Top shape: (1)
I1127 11:21:59.748181  7374 net.cpp:160]     with loss weight 1
I1127 11:21:59.748203  7374 net.cpp:165] Memory required for data: 8086808
I1127 11:21:59.748211  7374 net.cpp:226] loss needs backward computation.
I1127 11:21:59.748225  7374 net.cpp:228] accuracy does not need backward computation.
I1127 11:21:59.748234  7374 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:21:59.748240  7374 net.cpp:226] ip2 needs backward computation.
I1127 11:21:59.748247  7374 net.cpp:226] relu1 needs backward computation.
I1127 11:21:59.748263  7374 net.cpp:226] ip1 needs backward computation.
I1127 11:21:59.748271  7374 net.cpp:226] pool2 needs backward computation.
I1127 11:21:59.748280  7374 net.cpp:226] conv2 needs backward computation.
I1127 11:21:59.748287  7374 net.cpp:226] pool1 needs backward computation.
I1127 11:21:59.748294  7374 net.cpp:226] conv1 needs backward computation.
I1127 11:21:59.748302  7374 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:21:59.748317  7374 net.cpp:228] mnist does not need backward computation.
I1127 11:21:59.748323  7374 net.cpp:270] This network produces output accuracy
I1127 11:21:59.748332  7374 net.cpp:270] This network produces output loss
I1127 11:21:59.748347  7374 net.cpp:283] Network initialization done.
I1127 11:21:59.748453  7374 solver.cpp:59] Solver scaffolding done.
I1127 11:21:59.748769  7374 caffe.cpp:212] Starting Optimization
I1127 11:21:59.748783  7374 solver.cpp:287] Solving LeNet
I1127 11:21:59.748790  7374 solver.cpp:288] Learning Rate Policy: inv
I1127 11:21:59.749436  7374 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:21:59.750876  7374 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:22:00.973809  7374 solver.cpp:408]     Test net output #0: accuracy = 0.0804
I1127 11:22:00.973937  7374 solver.cpp:408]     Test net output #1: loss = 2.35311 (* 1 = 2.35311 loss)
I1127 11:22:00.984959  7374 solver.cpp:236] Iteration 0, loss = 2.36757
I1127 11:22:00.985025  7374 solver.cpp:252]     Train net output #0: loss = 2.36757 (* 1 = 2.36757 loss)
I1127 11:22:00.985054  7374 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:22:13.086807  7374 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:22:15.645750  7374 solver.cpp:408]     Test net output #0: accuracy = 0.9726
I1127 11:22:15.645902  7374 solver.cpp:408]     Test net output #1: loss = 0.0853142 (* 1 = 0.0853142 loss)
I1127 11:22:15.658790  7374 solver.cpp:236] Iteration 500, loss = 0.128929
I1127 11:22:15.658965  7374 solver.cpp:252]     Train net output #0: loss = 0.128929 (* 1 = 0.128929 loss)
I1127 11:22:15.658998  7374 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:22:27.154603  7374 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:22:27.178968  7374 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:22:27.194458  7374 solver.cpp:320] Iteration 1000, loss = 0.0996634
I1127 11:22:27.194613  7374 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:22:30.311738  7374 solver.cpp:408]     Test net output #0: accuracy = 0.9818
I1127 11:22:30.311877  7374 solver.cpp:408]     Test net output #1: loss = 0.0595565 (* 1 = 0.0595565 loss)
I1127 11:22:30.311885  7374 solver.cpp:325] Optimization Done.
I1127 11:22:30.311890  7374 caffe.cpp:215] Optimization Done.
I1127 11:22:30.448240  7400 caffe.cpp:184] Using GPUs 0
I1127 11:22:30.703980  7400 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:22:30.704118  7400 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:22:30.704394  7400 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:22:30.704409  7400 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:22:30.704500  7400 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:22:30.704563  7400 layer_factory.hpp:76] Creating layer mnist
I1127 11:22:30.704957  7400 net.cpp:106] Creating Layer mnist
I1127 11:22:30.704977  7400 net.cpp:411] mnist -> data
I1127 11:22:30.705000  7400 net.cpp:411] mnist -> label
I1127 11:22:30.706161  7403 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:22:30.717306  7400 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:22:30.719351  7400 net.cpp:150] Setting up mnist
I1127 11:22:30.719480  7400 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:22:30.719521  7400 net.cpp:157] Top shape: 64 (64)
I1127 11:22:30.719543  7400 net.cpp:165] Memory required for data: 200960
I1127 11:22:30.719573  7400 layer_factory.hpp:76] Creating layer conv1
I1127 11:22:30.719632  7400 net.cpp:106] Creating Layer conv1
I1127 11:22:30.719658  7400 net.cpp:454] conv1 <- data
I1127 11:22:30.719696  7400 net.cpp:411] conv1 -> conv1
I1127 11:22:30.721289  7400 net.cpp:150] Setting up conv1
I1127 11:22:30.721362  7400 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:22:30.721374  7400 net.cpp:165] Memory required for data: 3150080
I1127 11:22:30.721405  7400 layer_factory.hpp:76] Creating layer pool1
I1127 11:22:30.721429  7400 net.cpp:106] Creating Layer pool1
I1127 11:22:30.721437  7400 net.cpp:454] pool1 <- conv1
I1127 11:22:30.721448  7400 net.cpp:411] pool1 -> pool1
I1127 11:22:30.721524  7400 net.cpp:150] Setting up pool1
I1127 11:22:30.721534  7400 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:22:30.721539  7400 net.cpp:165] Memory required for data: 3887360
I1127 11:22:30.721545  7400 layer_factory.hpp:76] Creating layer conv2
I1127 11:22:30.721561  7400 net.cpp:106] Creating Layer conv2
I1127 11:22:30.721570  7400 net.cpp:454] conv2 <- pool1
I1127 11:22:30.721591  7400 net.cpp:411] conv2 -> conv2
I1127 11:22:30.722086  7400 net.cpp:150] Setting up conv2
I1127 11:22:30.722136  7400 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:22:30.722167  7400 net.cpp:165] Memory required for data: 4706560
I1127 11:22:30.722193  7400 layer_factory.hpp:76] Creating layer pool2
I1127 11:22:30.722218  7400 net.cpp:106] Creating Layer pool2
I1127 11:22:30.722229  7400 net.cpp:454] pool2 <- conv2
I1127 11:22:30.722241  7400 net.cpp:411] pool2 -> pool2
I1127 11:22:30.722301  7400 net.cpp:150] Setting up pool2
I1127 11:22:30.722314  7400 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:22:30.722322  7400 net.cpp:165] Memory required for data: 4911360
I1127 11:22:30.722350  7400 layer_factory.hpp:76] Creating layer ip1
I1127 11:22:30.722369  7400 net.cpp:106] Creating Layer ip1
I1127 11:22:30.722379  7400 net.cpp:454] ip1 <- pool2
I1127 11:22:30.722391  7400 net.cpp:411] ip1 -> ip1
I1127 11:22:30.727023  7400 net.cpp:150] Setting up ip1
I1127 11:22:30.727113  7400 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:22:30.727123  7400 net.cpp:165] Memory required for data: 5039360
I1127 11:22:30.727147  7400 layer_factory.hpp:76] Creating layer relu1
I1127 11:22:30.727170  7400 net.cpp:106] Creating Layer relu1
I1127 11:22:30.727182  7400 net.cpp:454] relu1 <- ip1
I1127 11:22:30.727197  7400 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:22:30.727217  7400 net.cpp:150] Setting up relu1
I1127 11:22:30.727228  7400 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:22:30.727236  7400 net.cpp:165] Memory required for data: 5167360
I1127 11:22:30.727243  7400 layer_factory.hpp:76] Creating layer ip2
I1127 11:22:30.727254  7400 net.cpp:106] Creating Layer ip2
I1127 11:22:30.727258  7400 net.cpp:454] ip2 <- ip1
I1127 11:22:30.727267  7400 net.cpp:411] ip2 -> ip2
I1127 11:22:30.728114  7400 net.cpp:150] Setting up ip2
I1127 11:22:30.728159  7400 net.cpp:157] Top shape: 64 10 (640)
I1127 11:22:30.728165  7400 net.cpp:165] Memory required for data: 5169920
I1127 11:22:30.728175  7400 layer_factory.hpp:76] Creating layer loss
I1127 11:22:30.728191  7400 net.cpp:106] Creating Layer loss
I1127 11:22:30.728201  7400 net.cpp:454] loss <- ip2
I1127 11:22:30.728212  7400 net.cpp:454] loss <- label
I1127 11:22:30.728224  7400 net.cpp:411] loss -> loss
I1127 11:22:30.728240  7400 layer_factory.hpp:76] Creating layer loss
I1127 11:22:30.728330  7400 net.cpp:150] Setting up loss
I1127 11:22:30.728341  7400 net.cpp:157] Top shape: (1)
I1127 11:22:30.728349  7400 net.cpp:160]     with loss weight 1
I1127 11:22:30.728369  7400 net.cpp:165] Memory required for data: 5169924
I1127 11:22:30.728375  7400 net.cpp:226] loss needs backward computation.
I1127 11:22:30.728380  7400 net.cpp:226] ip2 needs backward computation.
I1127 11:22:30.728384  7400 net.cpp:226] relu1 needs backward computation.
I1127 11:22:30.728392  7400 net.cpp:226] ip1 needs backward computation.
I1127 11:22:30.728400  7400 net.cpp:226] pool2 needs backward computation.
I1127 11:22:30.728407  7400 net.cpp:226] conv2 needs backward computation.
I1127 11:22:30.728412  7400 net.cpp:226] pool1 needs backward computation.
I1127 11:22:30.728417  7400 net.cpp:226] conv1 needs backward computation.
I1127 11:22:30.728421  7400 net.cpp:228] mnist does not need backward computation.
I1127 11:22:30.728425  7400 net.cpp:270] This network produces output loss
I1127 11:22:30.728440  7400 net.cpp:283] Network initialization done.
I1127 11:22:30.728744  7400 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:22:30.728772  7400 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:22:30.728914  7400 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:22:30.728996  7400 layer_factory.hpp:76] Creating layer mnist
I1127 11:22:30.729137  7400 net.cpp:106] Creating Layer mnist
I1127 11:22:30.729152  7400 net.cpp:411] mnist -> data
I1127 11:22:30.729166  7400 net.cpp:411] mnist -> label
I1127 11:22:30.730226  7405 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:22:30.730376  7400 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:22:30.732106  7400 net.cpp:150] Setting up mnist
I1127 11:22:30.732157  7400 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:22:30.732170  7400 net.cpp:157] Top shape: 100 (100)
I1127 11:22:30.732177  7400 net.cpp:165] Memory required for data: 314000
I1127 11:22:30.732188  7400 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:22:30.732213  7400 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:22:30.732226  7400 net.cpp:454] label_mnist_1_split <- label
I1127 11:22:30.732237  7400 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:22:30.732256  7400 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:22:30.732309  7400 net.cpp:150] Setting up label_mnist_1_split
I1127 11:22:30.732323  7400 net.cpp:157] Top shape: 100 (100)
I1127 11:22:30.732332  7400 net.cpp:157] Top shape: 100 (100)
I1127 11:22:30.732339  7400 net.cpp:165] Memory required for data: 314800
I1127 11:22:30.732349  7400 layer_factory.hpp:76] Creating layer conv1
I1127 11:22:30.732375  7400 net.cpp:106] Creating Layer conv1
I1127 11:22:30.732383  7400 net.cpp:454] conv1 <- data
I1127 11:22:30.732395  7400 net.cpp:411] conv1 -> conv1
I1127 11:22:30.732673  7400 net.cpp:150] Setting up conv1
I1127 11:22:30.732699  7400 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:22:30.732709  7400 net.cpp:165] Memory required for data: 4922800
I1127 11:22:30.732725  7400 layer_factory.hpp:76] Creating layer pool1
I1127 11:22:30.732744  7400 net.cpp:106] Creating Layer pool1
I1127 11:22:30.732753  7400 net.cpp:454] pool1 <- conv1
I1127 11:22:30.732795  7400 net.cpp:411] pool1 -> pool1
I1127 11:22:30.732846  7400 net.cpp:150] Setting up pool1
I1127 11:22:30.732857  7400 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:22:30.732863  7400 net.cpp:165] Memory required for data: 6074800
I1127 11:22:30.732870  7400 layer_factory.hpp:76] Creating layer conv2
I1127 11:22:30.732887  7400 net.cpp:106] Creating Layer conv2
I1127 11:22:30.732893  7400 net.cpp:454] conv2 <- pool1
I1127 11:22:30.732904  7400 net.cpp:411] conv2 -> conv2
I1127 11:22:30.733479  7400 net.cpp:150] Setting up conv2
I1127 11:22:30.733511  7400 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:22:30.733520  7400 net.cpp:165] Memory required for data: 7354800
I1127 11:22:30.733541  7400 layer_factory.hpp:76] Creating layer pool2
I1127 11:22:30.733558  7400 net.cpp:106] Creating Layer pool2
I1127 11:22:30.733567  7400 net.cpp:454] pool2 <- conv2
I1127 11:22:30.733579  7400 net.cpp:411] pool2 -> pool2
I1127 11:22:30.733639  7400 net.cpp:150] Setting up pool2
I1127 11:22:30.733650  7400 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:22:30.733675  7400 net.cpp:165] Memory required for data: 7674800
I1127 11:22:30.733685  7400 layer_factory.hpp:76] Creating layer ip1
I1127 11:22:30.733707  7400 net.cpp:106] Creating Layer ip1
I1127 11:22:30.733717  7400 net.cpp:454] ip1 <- pool2
I1127 11:22:30.733731  7400 net.cpp:411] ip1 -> ip1
I1127 11:22:30.738415  7400 net.cpp:150] Setting up ip1
I1127 11:22:30.738494  7400 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:22:30.738503  7400 net.cpp:165] Memory required for data: 7874800
I1127 11:22:30.738523  7400 layer_factory.hpp:76] Creating layer relu1
I1127 11:22:30.738536  7400 net.cpp:106] Creating Layer relu1
I1127 11:22:30.738543  7400 net.cpp:454] relu1 <- ip1
I1127 11:22:30.738549  7400 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:22:30.738559  7400 net.cpp:150] Setting up relu1
I1127 11:22:30.738564  7400 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:22:30.738569  7400 net.cpp:165] Memory required for data: 8074800
I1127 11:22:30.738572  7400 layer_factory.hpp:76] Creating layer ip2
I1127 11:22:30.738584  7400 net.cpp:106] Creating Layer ip2
I1127 11:22:30.738589  7400 net.cpp:454] ip2 <- ip1
I1127 11:22:30.738595  7400 net.cpp:411] ip2 -> ip2
I1127 11:22:30.738733  7400 net.cpp:150] Setting up ip2
I1127 11:22:30.738744  7400 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:22:30.738749  7400 net.cpp:165] Memory required for data: 8078800
I1127 11:22:30.738755  7400 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:22:30.738767  7400 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:22:30.738773  7400 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:22:30.738780  7400 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:22:30.738787  7400 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:22:30.738823  7400 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:22:30.738832  7400 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:22:30.738837  7400 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:22:30.738842  7400 net.cpp:165] Memory required for data: 8086800
I1127 11:22:30.738847  7400 layer_factory.hpp:76] Creating layer accuracy
I1127 11:22:30.738858  7400 net.cpp:106] Creating Layer accuracy
I1127 11:22:30.738867  7400 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:22:30.738873  7400 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:22:30.738879  7400 net.cpp:411] accuracy -> accuracy
I1127 11:22:30.738889  7400 net.cpp:150] Setting up accuracy
I1127 11:22:30.738895  7400 net.cpp:157] Top shape: (1)
I1127 11:22:30.738899  7400 net.cpp:165] Memory required for data: 8086804
I1127 11:22:30.738903  7400 layer_factory.hpp:76] Creating layer loss
I1127 11:22:30.738911  7400 net.cpp:106] Creating Layer loss
I1127 11:22:30.738915  7400 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:22:30.738920  7400 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:22:30.738927  7400 net.cpp:411] loss -> loss
I1127 11:22:30.738936  7400 layer_factory.hpp:76] Creating layer loss
I1127 11:22:30.739017  7400 net.cpp:150] Setting up loss
I1127 11:22:30.739023  7400 net.cpp:157] Top shape: (1)
I1127 11:22:30.739027  7400 net.cpp:160]     with loss weight 1
I1127 11:22:30.739038  7400 net.cpp:165] Memory required for data: 8086808
I1127 11:22:30.739042  7400 net.cpp:226] loss needs backward computation.
I1127 11:22:30.739050  7400 net.cpp:228] accuracy does not need backward computation.
I1127 11:22:30.739055  7400 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:22:30.739059  7400 net.cpp:226] ip2 needs backward computation.
I1127 11:22:30.739063  7400 net.cpp:226] relu1 needs backward computation.
I1127 11:22:30.739068  7400 net.cpp:226] ip1 needs backward computation.
I1127 11:22:30.739073  7400 net.cpp:226] pool2 needs backward computation.
I1127 11:22:30.739076  7400 net.cpp:226] conv2 needs backward computation.
I1127 11:22:30.739080  7400 net.cpp:226] pool1 needs backward computation.
I1127 11:22:30.739086  7400 net.cpp:226] conv1 needs backward computation.
I1127 11:22:30.739091  7400 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:22:30.739101  7400 net.cpp:228] mnist does not need backward computation.
I1127 11:22:30.739106  7400 net.cpp:270] This network produces output accuracy
I1127 11:22:30.739110  7400 net.cpp:270] This network produces output loss
I1127 11:22:30.739122  7400 net.cpp:283] Network initialization done.
I1127 11:22:30.739187  7400 solver.cpp:59] Solver scaffolding done.
I1127 11:22:30.739406  7400 caffe.cpp:212] Starting Optimization
I1127 11:22:30.739421  7400 solver.cpp:287] Solving LeNet
I1127 11:22:30.739426  7400 solver.cpp:288] Learning Rate Policy: inv
I1127 11:22:30.740231  7400 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:22:32.524930  7400 solver.cpp:408]     Test net output #0: accuracy = 0.1139
I1127 11:22:32.525027  7400 solver.cpp:408]     Test net output #1: loss = 2.36789 (* 1 = 2.36789 loss)
I1127 11:22:32.556680  7400 solver.cpp:236] Iteration 0, loss = 2.36055
I1127 11:22:32.556742  7400 solver.cpp:252]     Train net output #0: loss = 2.36055 (* 1 = 2.36055 loss)
I1127 11:22:32.556761  7400 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:22:39.622294  7400 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:22:45.602468  7400 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:22:46.996836  7400 solver.cpp:408]     Test net output #0: accuracy = 0.9717
I1127 11:22:46.996894  7400 solver.cpp:408]     Test net output #1: loss = 0.0903439 (* 1 = 0.0903439 loss)
I1127 11:22:47.007444  7400 solver.cpp:236] Iteration 500, loss = 0.0784461
I1127 11:22:47.007529  7400 solver.cpp:252]     Train net output #0: loss = 0.0784461 (* 1 = 0.0784461 loss)
I1127 11:22:47.007545  7400 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:22:56.369974  7400 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:22:56.382414  7400 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:22:56.391894  7400 solver.cpp:320] Iteration 1000, loss = 0.0830922
I1127 11:22:56.391945  7400 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:22:57.802574  7400 solver.cpp:408]     Test net output #0: accuracy = 0.9814
I1127 11:22:57.802652  7400 solver.cpp:408]     Test net output #1: loss = 0.0567956 (* 1 = 0.0567956 loss)
I1127 11:22:57.802660  7400 solver.cpp:325] Optimization Done.
I1127 11:22:57.802665  7400 caffe.cpp:215] Optimization Done.
I1127 11:22:57.926030  7453 caffe.cpp:184] Using GPUs 0
I1127 11:22:58.301782  7453 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:22:58.301997  7453 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:22:58.302536  7453 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:22:58.302574  7453 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:22:58.302749  7453 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:22:58.302897  7453 layer_factory.hpp:76] Creating layer mnist
I1127 11:22:58.303737  7453 net.cpp:106] Creating Layer mnist
I1127 11:22:58.303812  7453 net.cpp:411] mnist -> data
I1127 11:22:58.303916  7453 net.cpp:411] mnist -> label
I1127 11:22:58.305371  7457 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:22:58.319236  7453 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:22:58.320617  7453 net.cpp:150] Setting up mnist
I1127 11:22:58.320672  7453 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:22:58.320685  7453 net.cpp:157] Top shape: 64 (64)
I1127 11:22:58.320693  7453 net.cpp:165] Memory required for data: 200960
I1127 11:22:58.320713  7453 layer_factory.hpp:76] Creating layer conv1
I1127 11:22:58.320755  7453 net.cpp:106] Creating Layer conv1
I1127 11:22:58.320772  7453 net.cpp:454] conv1 <- data
I1127 11:22:58.320797  7453 net.cpp:411] conv1 -> conv1
I1127 11:22:58.322259  7453 net.cpp:150] Setting up conv1
I1127 11:22:58.322324  7453 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:22:58.322330  7453 net.cpp:165] Memory required for data: 3150080
I1127 11:22:58.322353  7453 layer_factory.hpp:76] Creating layer pool1
I1127 11:22:58.322373  7453 net.cpp:106] Creating Layer pool1
I1127 11:22:58.322381  7453 net.cpp:454] pool1 <- conv1
I1127 11:22:58.322391  7453 net.cpp:411] pool1 -> pool1
I1127 11:22:58.322496  7453 net.cpp:150] Setting up pool1
I1127 11:22:58.322512  7453 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:22:58.322516  7453 net.cpp:165] Memory required for data: 3887360
I1127 11:22:58.322521  7453 layer_factory.hpp:76] Creating layer conv2
I1127 11:22:58.322535  7453 net.cpp:106] Creating Layer conv2
I1127 11:22:58.322540  7453 net.cpp:454] conv2 <- pool1
I1127 11:22:58.322549  7453 net.cpp:411] conv2 -> conv2
I1127 11:22:58.322944  7453 net.cpp:150] Setting up conv2
I1127 11:22:58.322964  7453 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:22:58.322969  7453 net.cpp:165] Memory required for data: 4706560
I1127 11:22:58.322979  7453 layer_factory.hpp:76] Creating layer pool2
I1127 11:22:58.322990  7453 net.cpp:106] Creating Layer pool2
I1127 11:22:58.322995  7453 net.cpp:454] pool2 <- conv2
I1127 11:22:58.323002  7453 net.cpp:411] pool2 -> pool2
I1127 11:22:58.323031  7453 net.cpp:150] Setting up pool2
I1127 11:22:58.323038  7453 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:22:58.323042  7453 net.cpp:165] Memory required for data: 4911360
I1127 11:22:58.323047  7453 layer_factory.hpp:76] Creating layer ip1
I1127 11:22:58.323060  7453 net.cpp:106] Creating Layer ip1
I1127 11:22:58.323065  7453 net.cpp:454] ip1 <- pool2
I1127 11:22:58.323071  7453 net.cpp:411] ip1 -> ip1
I1127 11:22:58.325778  7453 net.cpp:150] Setting up ip1
I1127 11:22:58.325829  7453 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:22:58.325834  7453 net.cpp:165] Memory required for data: 5039360
I1127 11:22:58.325852  7453 layer_factory.hpp:76] Creating layer relu1
I1127 11:22:58.325881  7453 net.cpp:106] Creating Layer relu1
I1127 11:22:58.325891  7453 net.cpp:454] relu1 <- ip1
I1127 11:22:58.325901  7453 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:22:58.325919  7453 net.cpp:150] Setting up relu1
I1127 11:22:58.325927  7453 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:22:58.325930  7453 net.cpp:165] Memory required for data: 5167360
I1127 11:22:58.325934  7453 layer_factory.hpp:76] Creating layer ip2
I1127 11:22:58.325948  7453 net.cpp:106] Creating Layer ip2
I1127 11:22:58.325953  7453 net.cpp:454] ip2 <- ip1
I1127 11:22:58.325963  7453 net.cpp:411] ip2 -> ip2
I1127 11:22:58.326956  7453 net.cpp:150] Setting up ip2
I1127 11:22:58.327003  7453 net.cpp:157] Top shape: 64 10 (640)
I1127 11:22:58.327008  7453 net.cpp:165] Memory required for data: 5169920
I1127 11:22:58.327019  7453 layer_factory.hpp:76] Creating layer loss
I1127 11:22:58.327035  7453 net.cpp:106] Creating Layer loss
I1127 11:22:58.327041  7453 net.cpp:454] loss <- ip2
I1127 11:22:58.327049  7453 net.cpp:454] loss <- label
I1127 11:22:58.327060  7453 net.cpp:411] loss -> loss
I1127 11:22:58.327080  7453 layer_factory.hpp:76] Creating layer loss
I1127 11:22:58.327167  7453 net.cpp:150] Setting up loss
I1127 11:22:58.327175  7453 net.cpp:157] Top shape: (1)
I1127 11:22:58.327179  7453 net.cpp:160]     with loss weight 1
I1127 11:22:58.327205  7453 net.cpp:165] Memory required for data: 5169924
I1127 11:22:58.327210  7453 net.cpp:226] loss needs backward computation.
I1127 11:22:58.327215  7453 net.cpp:226] ip2 needs backward computation.
I1127 11:22:58.327220  7453 net.cpp:226] relu1 needs backward computation.
I1127 11:22:58.327224  7453 net.cpp:226] ip1 needs backward computation.
I1127 11:22:58.327229  7453 net.cpp:226] pool2 needs backward computation.
I1127 11:22:58.327234  7453 net.cpp:226] conv2 needs backward computation.
I1127 11:22:58.327237  7453 net.cpp:226] pool1 needs backward computation.
I1127 11:22:58.327242  7453 net.cpp:226] conv1 needs backward computation.
I1127 11:22:58.327247  7453 net.cpp:228] mnist does not need backward computation.
I1127 11:22:58.327251  7453 net.cpp:270] This network produces output loss
I1127 11:22:58.327262  7453 net.cpp:283] Network initialization done.
I1127 11:22:58.327572  7453 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:22:58.327631  7453 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:22:58.327806  7453 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:22:58.327898  7453 layer_factory.hpp:76] Creating layer mnist
I1127 11:22:58.330253  7453 net.cpp:106] Creating Layer mnist
I1127 11:22:58.330323  7453 net.cpp:411] mnist -> data
I1127 11:22:58.330344  7453 net.cpp:411] mnist -> label
I1127 11:22:58.331380  7459 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:22:58.331632  7453 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:22:58.333765  7453 net.cpp:150] Setting up mnist
I1127 11:22:58.333883  7453 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:22:58.333907  7453 net.cpp:157] Top shape: 100 (100)
I1127 11:22:58.333925  7453 net.cpp:165] Memory required for data: 314000
I1127 11:22:58.333947  7453 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:22:58.333986  7453 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:22:58.334003  7453 net.cpp:454] label_mnist_1_split <- label
I1127 11:22:58.334028  7453 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:22:58.334064  7453 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:22:58.334188  7453 net.cpp:150] Setting up label_mnist_1_split
I1127 11:22:58.334205  7453 net.cpp:157] Top shape: 100 (100)
I1127 11:22:58.334214  7453 net.cpp:157] Top shape: 100 (100)
I1127 11:22:58.334220  7453 net.cpp:165] Memory required for data: 314800
I1127 11:22:58.334228  7453 layer_factory.hpp:76] Creating layer conv1
I1127 11:22:58.334252  7453 net.cpp:106] Creating Layer conv1
I1127 11:22:58.334261  7453 net.cpp:454] conv1 <- data
I1127 11:22:58.334271  7453 net.cpp:411] conv1 -> conv1
I1127 11:22:58.334537  7453 net.cpp:150] Setting up conv1
I1127 11:22:58.334550  7453 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:22:58.334558  7453 net.cpp:165] Memory required for data: 4922800
I1127 11:22:58.334573  7453 layer_factory.hpp:76] Creating layer pool1
I1127 11:22:58.334588  7453 net.cpp:106] Creating Layer pool1
I1127 11:22:58.334594  7453 net.cpp:454] pool1 <- conv1
I1127 11:22:58.334630  7453 net.cpp:411] pool1 -> pool1
I1127 11:22:58.334687  7453 net.cpp:150] Setting up pool1
I1127 11:22:58.334705  7453 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:22:58.334714  7453 net.cpp:165] Memory required for data: 6074800
I1127 11:22:58.334722  7453 layer_factory.hpp:76] Creating layer conv2
I1127 11:22:58.334735  7453 net.cpp:106] Creating Layer conv2
I1127 11:22:58.334743  7453 net.cpp:454] conv2 <- pool1
I1127 11:22:58.334756  7453 net.cpp:411] conv2 -> conv2
I1127 11:22:58.335544  7453 net.cpp:150] Setting up conv2
I1127 11:22:58.335628  7453 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:22:58.335638  7453 net.cpp:165] Memory required for data: 7354800
I1127 11:22:58.335671  7453 layer_factory.hpp:76] Creating layer pool2
I1127 11:22:58.335710  7453 net.cpp:106] Creating Layer pool2
I1127 11:22:58.335721  7453 net.cpp:454] pool2 <- conv2
I1127 11:22:58.335741  7453 net.cpp:411] pool2 -> pool2
I1127 11:22:58.335934  7453 net.cpp:150] Setting up pool2
I1127 11:22:58.335959  7453 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:22:58.335968  7453 net.cpp:165] Memory required for data: 7674800
I1127 11:22:58.335975  7453 layer_factory.hpp:76] Creating layer ip1
I1127 11:22:58.335994  7453 net.cpp:106] Creating Layer ip1
I1127 11:22:58.336004  7453 net.cpp:454] ip1 <- pool2
I1127 11:22:58.336017  7453 net.cpp:411] ip1 -> ip1
I1127 11:22:58.339424  7453 net.cpp:150] Setting up ip1
I1127 11:22:58.339493  7453 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:22:58.339514  7453 net.cpp:165] Memory required for data: 7874800
I1127 11:22:58.339536  7453 layer_factory.hpp:76] Creating layer relu1
I1127 11:22:58.339558  7453 net.cpp:106] Creating Layer relu1
I1127 11:22:58.339566  7453 net.cpp:454] relu1 <- ip1
I1127 11:22:58.339577  7453 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:22:58.339593  7453 net.cpp:150] Setting up relu1
I1127 11:22:58.339601  7453 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:22:58.339604  7453 net.cpp:165] Memory required for data: 8074800
I1127 11:22:58.339608  7453 layer_factory.hpp:76] Creating layer ip2
I1127 11:22:58.339628  7453 net.cpp:106] Creating Layer ip2
I1127 11:22:58.339633  7453 net.cpp:454] ip2 <- ip1
I1127 11:22:58.339642  7453 net.cpp:411] ip2 -> ip2
I1127 11:22:58.339795  7453 net.cpp:150] Setting up ip2
I1127 11:22:58.339809  7453 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:22:58.339813  7453 net.cpp:165] Memory required for data: 8078800
I1127 11:22:58.339820  7453 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:22:58.339833  7453 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:22:58.339838  7453 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:22:58.339845  7453 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:22:58.339853  7453 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:22:58.339889  7453 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:22:58.339897  7453 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:22:58.339903  7453 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:22:58.339907  7453 net.cpp:165] Memory required for data: 8086800
I1127 11:22:58.339912  7453 layer_factory.hpp:76] Creating layer accuracy
I1127 11:22:58.339923  7453 net.cpp:106] Creating Layer accuracy
I1127 11:22:58.339931  7453 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:22:58.339938  7453 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:22:58.339944  7453 net.cpp:411] accuracy -> accuracy
I1127 11:22:58.339957  7453 net.cpp:150] Setting up accuracy
I1127 11:22:58.339965  7453 net.cpp:157] Top shape: (1)
I1127 11:22:58.339969  7453 net.cpp:165] Memory required for data: 8086804
I1127 11:22:58.339974  7453 layer_factory.hpp:76] Creating layer loss
I1127 11:22:58.339984  7453 net.cpp:106] Creating Layer loss
I1127 11:22:58.339989  7453 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:22:58.339995  7453 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:22:58.340004  7453 net.cpp:411] loss -> loss
I1127 11:22:58.340019  7453 layer_factory.hpp:76] Creating layer loss
I1127 11:22:58.340134  7453 net.cpp:150] Setting up loss
I1127 11:22:58.340148  7453 net.cpp:157] Top shape: (1)
I1127 11:22:58.340153  7453 net.cpp:160]     with loss weight 1
I1127 11:22:58.340178  7453 net.cpp:165] Memory required for data: 8086808
I1127 11:22:58.340183  7453 net.cpp:226] loss needs backward computation.
I1127 11:22:58.340196  7453 net.cpp:228] accuracy does not need backward computation.
I1127 11:22:58.340201  7453 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:22:58.340206  7453 net.cpp:226] ip2 needs backward computation.
I1127 11:22:58.340211  7453 net.cpp:226] relu1 needs backward computation.
I1127 11:22:58.340216  7453 net.cpp:226] ip1 needs backward computation.
I1127 11:22:58.340221  7453 net.cpp:226] pool2 needs backward computation.
I1127 11:22:58.340226  7453 net.cpp:226] conv2 needs backward computation.
I1127 11:22:58.340231  7453 net.cpp:226] pool1 needs backward computation.
I1127 11:22:58.340239  7453 net.cpp:226] conv1 needs backward computation.
I1127 11:22:58.340243  7453 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:22:58.340250  7453 net.cpp:228] mnist does not need backward computation.
I1127 11:22:58.340253  7453 net.cpp:270] This network produces output accuracy
I1127 11:22:58.340258  7453 net.cpp:270] This network produces output loss
I1127 11:22:58.340271  7453 net.cpp:283] Network initialization done.
I1127 11:22:58.340338  7453 solver.cpp:59] Solver scaffolding done.
I1127 11:22:58.340549  7453 caffe.cpp:212] Starting Optimization
I1127 11:22:58.340567  7453 solver.cpp:287] Solving LeNet
I1127 11:22:58.340572  7453 solver.cpp:288] Learning Rate Policy: inv
I1127 11:22:58.341102  7453 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:22:59.735641  7453 solver.cpp:408]     Test net output #0: accuracy = 0.0868
I1127 11:22:59.735682  7453 solver.cpp:408]     Test net output #1: loss = 2.35033 (* 1 = 2.35033 loss)
I1127 11:22:59.745905  7453 solver.cpp:236] Iteration 0, loss = 2.33377
I1127 11:22:59.745987  7453 solver.cpp:252]     Train net output #0: loss = 2.33377 (* 1 = 2.33377 loss)
I1127 11:22:59.746008  7453 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:23:09.159363  7453 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:23:10.604115  7453 solver.cpp:408]     Test net output #0: accuracy = 0.9721
I1127 11:23:10.604156  7453 solver.cpp:408]     Test net output #1: loss = 0.0869547 (* 1 = 0.0869547 loss)
I1127 11:23:10.633421  7453 solver.cpp:236] Iteration 500, loss = 0.0805504
I1127 11:23:10.633438  7453 solver.cpp:252]     Train net output #0: loss = 0.0805504 (* 1 = 0.0805504 loss)
I1127 11:23:10.633447  7453 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:23:20.048106  7453 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:23:20.062001  7453 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:23:20.072427  7453 solver.cpp:320] Iteration 1000, loss = 0.0873967
I1127 11:23:20.072504  7453 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:23:21.564517  7453 solver.cpp:408]     Test net output #0: accuracy = 0.9823
I1127 11:23:21.564613  7453 solver.cpp:408]     Test net output #1: loss = 0.0590665 (* 1 = 0.0590665 loss)
I1127 11:23:21.564623  7453 solver.cpp:325] Optimization Done.
I1127 11:23:21.564630  7453 caffe.cpp:215] Optimization Done.
I1127 11:23:21.643394  7526 caffe.cpp:184] Using GPUs 0
I1127 11:23:22.019460  7526 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:23:22.019570  7526 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:23:22.019840  7526 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:23:22.019856  7526 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:23:22.019948  7526 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:23:22.020012  7526 layer_factory.hpp:76] Creating layer mnist
I1127 11:23:22.020375  7526 net.cpp:106] Creating Layer mnist
I1127 11:23:22.020395  7526 net.cpp:411] mnist -> data
I1127 11:23:22.020419  7526 net.cpp:411] mnist -> label
I1127 11:23:22.021277  7530 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:23:22.028306  7526 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:23:22.029271  7526 net.cpp:150] Setting up mnist
I1127 11:23:22.029336  7526 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:23:22.029345  7526 net.cpp:157] Top shape: 64 (64)
I1127 11:23:22.029348  7526 net.cpp:165] Memory required for data: 200960
I1127 11:23:22.029357  7526 layer_factory.hpp:76] Creating layer conv1
I1127 11:23:22.029377  7526 net.cpp:106] Creating Layer conv1
I1127 11:23:22.029383  7526 net.cpp:454] conv1 <- data
I1127 11:23:22.029394  7526 net.cpp:411] conv1 -> conv1
I1127 11:23:22.029992  7526 net.cpp:150] Setting up conv1
I1127 11:23:22.030004  7526 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:23:22.030007  7526 net.cpp:165] Memory required for data: 3150080
I1127 11:23:22.030019  7526 layer_factory.hpp:76] Creating layer pool1
I1127 11:23:22.030028  7526 net.cpp:106] Creating Layer pool1
I1127 11:23:22.030033  7526 net.cpp:454] pool1 <- conv1
I1127 11:23:22.030040  7526 net.cpp:411] pool1 -> pool1
I1127 11:23:22.030088  7526 net.cpp:150] Setting up pool1
I1127 11:23:22.030095  7526 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:23:22.030099  7526 net.cpp:165] Memory required for data: 3887360
I1127 11:23:22.030104  7526 layer_factory.hpp:76] Creating layer conv2
I1127 11:23:22.030112  7526 net.cpp:106] Creating Layer conv2
I1127 11:23:22.030117  7526 net.cpp:454] conv2 <- pool1
I1127 11:23:22.030123  7526 net.cpp:411] conv2 -> conv2
I1127 11:23:22.030391  7526 net.cpp:150] Setting up conv2
I1127 11:23:22.030401  7526 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:23:22.030406  7526 net.cpp:165] Memory required for data: 4706560
I1127 11:23:22.030414  7526 layer_factory.hpp:76] Creating layer pool2
I1127 11:23:22.030423  7526 net.cpp:106] Creating Layer pool2
I1127 11:23:22.030427  7526 net.cpp:454] pool2 <- conv2
I1127 11:23:22.030433  7526 net.cpp:411] pool2 -> pool2
I1127 11:23:22.030465  7526 net.cpp:150] Setting up pool2
I1127 11:23:22.030474  7526 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:23:22.030478  7526 net.cpp:165] Memory required for data: 4911360
I1127 11:23:22.030483  7526 layer_factory.hpp:76] Creating layer ip1
I1127 11:23:22.030493  7526 net.cpp:106] Creating Layer ip1
I1127 11:23:22.030498  7526 net.cpp:454] ip1 <- pool2
I1127 11:23:22.030505  7526 net.cpp:411] ip1 -> ip1
I1127 11:23:22.033709  7526 net.cpp:150] Setting up ip1
I1127 11:23:22.033747  7526 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:23:22.033752  7526 net.cpp:165] Memory required for data: 5039360
I1127 11:23:22.033774  7526 layer_factory.hpp:76] Creating layer relu1
I1127 11:23:22.033793  7526 net.cpp:106] Creating Layer relu1
I1127 11:23:22.033800  7526 net.cpp:454] relu1 <- ip1
I1127 11:23:22.033809  7526 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:23:22.033829  7526 net.cpp:150] Setting up relu1
I1127 11:23:22.033835  7526 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:23:22.033839  7526 net.cpp:165] Memory required for data: 5167360
I1127 11:23:22.033844  7526 layer_factory.hpp:76] Creating layer ip2
I1127 11:23:22.033857  7526 net.cpp:106] Creating Layer ip2
I1127 11:23:22.033875  7526 net.cpp:454] ip2 <- ip1
I1127 11:23:22.033884  7526 net.cpp:411] ip2 -> ip2
I1127 11:23:22.034581  7526 net.cpp:150] Setting up ip2
I1127 11:23:22.034612  7526 net.cpp:157] Top shape: 64 10 (640)
I1127 11:23:22.034616  7526 net.cpp:165] Memory required for data: 5169920
I1127 11:23:22.034626  7526 layer_factory.hpp:76] Creating layer loss
I1127 11:23:22.034641  7526 net.cpp:106] Creating Layer loss
I1127 11:23:22.034646  7526 net.cpp:454] loss <- ip2
I1127 11:23:22.034654  7526 net.cpp:454] loss <- label
I1127 11:23:22.034665  7526 net.cpp:411] loss -> loss
I1127 11:23:22.034684  7526 layer_factory.hpp:76] Creating layer loss
I1127 11:23:22.034756  7526 net.cpp:150] Setting up loss
I1127 11:23:22.034765  7526 net.cpp:157] Top shape: (1)
I1127 11:23:22.034768  7526 net.cpp:160]     with loss weight 1
I1127 11:23:22.034791  7526 net.cpp:165] Memory required for data: 5169924
I1127 11:23:22.034796  7526 net.cpp:226] loss needs backward computation.
I1127 11:23:22.034801  7526 net.cpp:226] ip2 needs backward computation.
I1127 11:23:22.034806  7526 net.cpp:226] relu1 needs backward computation.
I1127 11:23:22.034809  7526 net.cpp:226] ip1 needs backward computation.
I1127 11:23:22.034813  7526 net.cpp:226] pool2 needs backward computation.
I1127 11:23:22.034818  7526 net.cpp:226] conv2 needs backward computation.
I1127 11:23:22.034822  7526 net.cpp:226] pool1 needs backward computation.
I1127 11:23:22.034826  7526 net.cpp:226] conv1 needs backward computation.
I1127 11:23:22.034832  7526 net.cpp:228] mnist does not need backward computation.
I1127 11:23:22.034837  7526 net.cpp:270] This network produces output loss
I1127 11:23:22.034845  7526 net.cpp:283] Network initialization done.
I1127 11:23:22.035166  7526 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:23:22.035197  7526 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:23:22.035389  7526 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:23:22.035531  7526 layer_factory.hpp:76] Creating layer mnist
I1127 11:23:22.035733  7526 net.cpp:106] Creating Layer mnist
I1127 11:23:22.035748  7526 net.cpp:411] mnist -> data
I1127 11:23:22.035765  7526 net.cpp:411] mnist -> label
I1127 11:23:22.036906  7532 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:23:22.037066  7526 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:23:22.038161  7526 net.cpp:150] Setting up mnist
I1127 11:23:22.038210  7526 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:23:22.038221  7526 net.cpp:157] Top shape: 100 (100)
I1127 11:23:22.038229  7526 net.cpp:165] Memory required for data: 314000
I1127 11:23:22.038241  7526 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:23:22.038262  7526 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:23:22.038275  7526 net.cpp:454] label_mnist_1_split <- label
I1127 11:23:22.038287  7526 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:23:22.038308  7526 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:23:22.038384  7526 net.cpp:150] Setting up label_mnist_1_split
I1127 11:23:22.038408  7526 net.cpp:157] Top shape: 100 (100)
I1127 11:23:22.038419  7526 net.cpp:157] Top shape: 100 (100)
I1127 11:23:22.038427  7526 net.cpp:165] Memory required for data: 314800
I1127 11:23:22.038435  7526 layer_factory.hpp:76] Creating layer conv1
I1127 11:23:22.038463  7526 net.cpp:106] Creating Layer conv1
I1127 11:23:22.038473  7526 net.cpp:454] conv1 <- data
I1127 11:23:22.038487  7526 net.cpp:411] conv1 -> conv1
I1127 11:23:22.038758  7526 net.cpp:150] Setting up conv1
I1127 11:23:22.038777  7526 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:23:22.038784  7526 net.cpp:165] Memory required for data: 4922800
I1127 11:23:22.038803  7526 layer_factory.hpp:76] Creating layer pool1
I1127 11:23:22.038817  7526 net.cpp:106] Creating Layer pool1
I1127 11:23:22.038823  7526 net.cpp:454] pool1 <- conv1
I1127 11:23:22.038864  7526 net.cpp:411] pool1 -> pool1
I1127 11:23:22.038915  7526 net.cpp:150] Setting up pool1
I1127 11:23:22.038929  7526 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:23:22.038938  7526 net.cpp:165] Memory required for data: 6074800
I1127 11:23:22.038950  7526 layer_factory.hpp:76] Creating layer conv2
I1127 11:23:22.038971  7526 net.cpp:106] Creating Layer conv2
I1127 11:23:22.038980  7526 net.cpp:454] conv2 <- pool1
I1127 11:23:22.039000  7526 net.cpp:411] conv2 -> conv2
I1127 11:23:22.039504  7526 net.cpp:150] Setting up conv2
I1127 11:23:22.039526  7526 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:23:22.039533  7526 net.cpp:165] Memory required for data: 7354800
I1127 11:23:22.039548  7526 layer_factory.hpp:76] Creating layer pool2
I1127 11:23:22.039561  7526 net.cpp:106] Creating Layer pool2
I1127 11:23:22.039571  7526 net.cpp:454] pool2 <- conv2
I1127 11:23:22.039579  7526 net.cpp:411] pool2 -> pool2
I1127 11:23:22.039687  7526 net.cpp:150] Setting up pool2
I1127 11:23:22.039700  7526 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:23:22.039706  7526 net.cpp:165] Memory required for data: 7674800
I1127 11:23:22.039715  7526 layer_factory.hpp:76] Creating layer ip1
I1127 11:23:22.039731  7526 net.cpp:106] Creating Layer ip1
I1127 11:23:22.039738  7526 net.cpp:454] ip1 <- pool2
I1127 11:23:22.039752  7526 net.cpp:411] ip1 -> ip1
I1127 11:23:22.043908  7526 net.cpp:150] Setting up ip1
I1127 11:23:22.043984  7526 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:23:22.043994  7526 net.cpp:165] Memory required for data: 7874800
I1127 11:23:22.044024  7526 layer_factory.hpp:76] Creating layer relu1
I1127 11:23:22.044049  7526 net.cpp:106] Creating Layer relu1
I1127 11:23:22.044064  7526 net.cpp:454] relu1 <- ip1
I1127 11:23:22.044080  7526 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:23:22.044106  7526 net.cpp:150] Setting up relu1
I1127 11:23:22.044116  7526 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:23:22.044124  7526 net.cpp:165] Memory required for data: 8074800
I1127 11:23:22.044131  7526 layer_factory.hpp:76] Creating layer ip2
I1127 11:23:22.044172  7526 net.cpp:106] Creating Layer ip2
I1127 11:23:22.044180  7526 net.cpp:454] ip2 <- ip1
I1127 11:23:22.044196  7526 net.cpp:411] ip2 -> ip2
I1127 11:23:22.044421  7526 net.cpp:150] Setting up ip2
I1127 11:23:22.044437  7526 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:23:22.044443  7526 net.cpp:165] Memory required for data: 8078800
I1127 11:23:22.044455  7526 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:23:22.044467  7526 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:23:22.044474  7526 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:23:22.044484  7526 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:23:22.044495  7526 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:23:22.044529  7526 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:23:22.044538  7526 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:23:22.044543  7526 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:23:22.044546  7526 net.cpp:165] Memory required for data: 8086800
I1127 11:23:22.044550  7526 layer_factory.hpp:76] Creating layer accuracy
I1127 11:23:22.044559  7526 net.cpp:106] Creating Layer accuracy
I1127 11:23:22.044564  7526 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:23:22.044569  7526 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:23:22.044574  7526 net.cpp:411] accuracy -> accuracy
I1127 11:23:22.044584  7526 net.cpp:150] Setting up accuracy
I1127 11:23:22.044589  7526 net.cpp:157] Top shape: (1)
I1127 11:23:22.044594  7526 net.cpp:165] Memory required for data: 8086804
I1127 11:23:22.044597  7526 layer_factory.hpp:76] Creating layer loss
I1127 11:23:22.044605  7526 net.cpp:106] Creating Layer loss
I1127 11:23:22.044610  7526 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:23:22.044615  7526 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:23:22.044621  7526 net.cpp:411] loss -> loss
I1127 11:23:22.044630  7526 layer_factory.hpp:76] Creating layer loss
I1127 11:23:22.044708  7526 net.cpp:150] Setting up loss
I1127 11:23:22.044716  7526 net.cpp:157] Top shape: (1)
I1127 11:23:22.044720  7526 net.cpp:160]     with loss weight 1
I1127 11:23:22.044737  7526 net.cpp:165] Memory required for data: 8086808
I1127 11:23:22.044742  7526 net.cpp:226] loss needs backward computation.
I1127 11:23:22.044752  7526 net.cpp:228] accuracy does not need backward computation.
I1127 11:23:22.044757  7526 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:23:22.044762  7526 net.cpp:226] ip2 needs backward computation.
I1127 11:23:22.044766  7526 net.cpp:226] relu1 needs backward computation.
I1127 11:23:22.044770  7526 net.cpp:226] ip1 needs backward computation.
I1127 11:23:22.044775  7526 net.cpp:226] pool2 needs backward computation.
I1127 11:23:22.044780  7526 net.cpp:226] conv2 needs backward computation.
I1127 11:23:22.044785  7526 net.cpp:226] pool1 needs backward computation.
I1127 11:23:22.044790  7526 net.cpp:226] conv1 needs backward computation.
I1127 11:23:22.044795  7526 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:23:22.044798  7526 net.cpp:228] mnist does not need backward computation.
I1127 11:23:22.044802  7526 net.cpp:270] This network produces output accuracy
I1127 11:23:22.044806  7526 net.cpp:270] This network produces output loss
I1127 11:23:22.044817  7526 net.cpp:283] Network initialization done.
I1127 11:23:22.044886  7526 solver.cpp:59] Solver scaffolding done.
I1127 11:23:22.045088  7526 caffe.cpp:212] Starting Optimization
I1127 11:23:22.045096  7526 solver.cpp:287] Solving LeNet
I1127 11:23:22.045100  7526 solver.cpp:288] Learning Rate Policy: inv
I1127 11:23:22.045500  7526 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:23:23.542454  7526 solver.cpp:408]     Test net output #0: accuracy = 0.216
I1127 11:23:23.542537  7526 solver.cpp:408]     Test net output #1: loss = 2.28777 (* 1 = 2.28777 loss)
I1127 11:23:23.557072  7526 solver.cpp:236] Iteration 0, loss = 2.25072
I1127 11:23:23.557184  7526 solver.cpp:252]     Train net output #0: loss = 2.25072 (* 1 = 2.25072 loss)
I1127 11:23:23.557255  7526 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:23:33.907351  7526 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:23:35.530701  7526 solver.cpp:408]     Test net output #0: accuracy = 0.9745
I1127 11:23:35.530829  7526 solver.cpp:408]     Test net output #1: loss = 0.0800538 (* 1 = 0.0800538 loss)
I1127 11:23:35.541035  7526 solver.cpp:236] Iteration 500, loss = 0.0674306
I1127 11:23:35.541107  7526 solver.cpp:252]     Train net output #0: loss = 0.0674306 (* 1 = 0.0674306 loss)
I1127 11:23:35.541124  7526 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:23:48.927858  7526 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:23:48.942214  7526 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:23:48.952986  7526 solver.cpp:320] Iteration 1000, loss = 0.074311
I1127 11:23:48.953105  7526 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:23:50.053185  7526 solver.cpp:408]     Test net output #0: accuracy = 0.9814
I1127 11:23:50.053274  7526 solver.cpp:408]     Test net output #1: loss = 0.0568227 (* 1 = 0.0568227 loss)
I1127 11:23:50.053285  7526 solver.cpp:325] Optimization Done.
I1127 11:23:50.053292  7526 caffe.cpp:215] Optimization Done.
I1127 11:23:50.162538  7604 caffe.cpp:184] Using GPUs 0
I1127 11:23:50.524354  7604 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:23:50.524494  7604 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:23:50.524782  7604 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:23:50.524804  7604 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:23:50.524906  7604 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:23:50.525014  7604 layer_factory.hpp:76] Creating layer mnist
I1127 11:23:50.525442  7604 net.cpp:106] Creating Layer mnist
I1127 11:23:50.525463  7604 net.cpp:411] mnist -> data
I1127 11:23:50.525506  7604 net.cpp:411] mnist -> label
I1127 11:23:50.526628  7609 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:23:50.560019  7604 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:23:50.567368  7604 net.cpp:150] Setting up mnist
I1127 11:23:50.567436  7604 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:23:50.567445  7604 net.cpp:157] Top shape: 64 (64)
I1127 11:23:50.567451  7604 net.cpp:165] Memory required for data: 200960
I1127 11:23:50.567471  7604 layer_factory.hpp:76] Creating layer conv1
I1127 11:23:50.567507  7604 net.cpp:106] Creating Layer conv1
I1127 11:23:50.567517  7604 net.cpp:454] conv1 <- data
I1127 11:23:50.567538  7604 net.cpp:411] conv1 -> conv1
I1127 11:23:50.568552  7604 net.cpp:150] Setting up conv1
I1127 11:23:50.568622  7604 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:23:50.568629  7604 net.cpp:165] Memory required for data: 3150080
I1127 11:23:50.568645  7604 layer_factory.hpp:76] Creating layer pool1
I1127 11:23:50.568660  7604 net.cpp:106] Creating Layer pool1
I1127 11:23:50.568666  7604 net.cpp:454] pool1 <- conv1
I1127 11:23:50.568675  7604 net.cpp:411] pool1 -> pool1
I1127 11:23:50.568728  7604 net.cpp:150] Setting up pool1
I1127 11:23:50.568735  7604 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:23:50.568739  7604 net.cpp:165] Memory required for data: 3887360
I1127 11:23:50.568744  7604 layer_factory.hpp:76] Creating layer conv2
I1127 11:23:50.568756  7604 net.cpp:106] Creating Layer conv2
I1127 11:23:50.568761  7604 net.cpp:454] conv2 <- pool1
I1127 11:23:50.568769  7604 net.cpp:411] conv2 -> conv2
I1127 11:23:50.569226  7604 net.cpp:150] Setting up conv2
I1127 11:23:50.569260  7604 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:23:50.569270  7604 net.cpp:165] Memory required for data: 4706560
I1127 11:23:50.569290  7604 layer_factory.hpp:76] Creating layer pool2
I1127 11:23:50.569310  7604 net.cpp:106] Creating Layer pool2
I1127 11:23:50.569320  7604 net.cpp:454] pool2 <- conv2
I1127 11:23:50.569332  7604 net.cpp:411] pool2 -> pool2
I1127 11:23:50.569383  7604 net.cpp:150] Setting up pool2
I1127 11:23:50.569396  7604 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:23:50.569406  7604 net.cpp:165] Memory required for data: 4911360
I1127 11:23:50.569413  7604 layer_factory.hpp:76] Creating layer ip1
I1127 11:23:50.569428  7604 net.cpp:106] Creating Layer ip1
I1127 11:23:50.569437  7604 net.cpp:454] ip1 <- pool2
I1127 11:23:50.569454  7604 net.cpp:411] ip1 -> ip1
I1127 11:23:50.572335  7604 net.cpp:150] Setting up ip1
I1127 11:23:50.572417  7604 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:23:50.572423  7604 net.cpp:165] Memory required for data: 5039360
I1127 11:23:50.572444  7604 layer_factory.hpp:76] Creating layer relu1
I1127 11:23:50.572463  7604 net.cpp:106] Creating Layer relu1
I1127 11:23:50.572471  7604 net.cpp:454] relu1 <- ip1
I1127 11:23:50.572484  7604 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:23:50.572509  7604 net.cpp:150] Setting up relu1
I1127 11:23:50.572515  7604 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:23:50.572518  7604 net.cpp:165] Memory required for data: 5167360
I1127 11:23:50.572522  7604 layer_factory.hpp:76] Creating layer ip2
I1127 11:23:50.572536  7604 net.cpp:106] Creating Layer ip2
I1127 11:23:50.572540  7604 net.cpp:454] ip2 <- ip1
I1127 11:23:50.572549  7604 net.cpp:411] ip2 -> ip2
I1127 11:23:50.573278  7604 net.cpp:150] Setting up ip2
I1127 11:23:50.573307  7604 net.cpp:157] Top shape: 64 10 (640)
I1127 11:23:50.573312  7604 net.cpp:165] Memory required for data: 5169920
I1127 11:23:50.573323  7604 layer_factory.hpp:76] Creating layer loss
I1127 11:23:50.573344  7604 net.cpp:106] Creating Layer loss
I1127 11:23:50.573350  7604 net.cpp:454] loss <- ip2
I1127 11:23:50.573356  7604 net.cpp:454] loss <- label
I1127 11:23:50.573366  7604 net.cpp:411] loss -> loss
I1127 11:23:50.573398  7604 layer_factory.hpp:76] Creating layer loss
I1127 11:23:50.573487  7604 net.cpp:150] Setting up loss
I1127 11:23:50.573494  7604 net.cpp:157] Top shape: (1)
I1127 11:23:50.573498  7604 net.cpp:160]     with loss weight 1
I1127 11:23:50.573528  7604 net.cpp:165] Memory required for data: 5169924
I1127 11:23:50.573532  7604 net.cpp:226] loss needs backward computation.
I1127 11:23:50.573539  7604 net.cpp:226] ip2 needs backward computation.
I1127 11:23:50.573544  7604 net.cpp:226] relu1 needs backward computation.
I1127 11:23:50.573549  7604 net.cpp:226] ip1 needs backward computation.
I1127 11:23:50.573552  7604 net.cpp:226] pool2 needs backward computation.
I1127 11:23:50.573557  7604 net.cpp:226] conv2 needs backward computation.
I1127 11:23:50.573562  7604 net.cpp:226] pool1 needs backward computation.
I1127 11:23:50.573567  7604 net.cpp:226] conv1 needs backward computation.
I1127 11:23:50.573571  7604 net.cpp:228] mnist does not need backward computation.
I1127 11:23:50.573576  7604 net.cpp:270] This network produces output loss
I1127 11:23:50.573590  7604 net.cpp:283] Network initialization done.
I1127 11:23:50.573935  7604 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:23:50.573963  7604 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:23:50.574084  7604 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:23:50.574156  7604 layer_factory.hpp:76] Creating layer mnist
I1127 11:23:50.622289  7604 net.cpp:106] Creating Layer mnist
I1127 11:23:50.622366  7604 net.cpp:411] mnist -> data
I1127 11:23:50.622398  7604 net.cpp:411] mnist -> label
I1127 11:23:50.623399  7611 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:23:50.623697  7604 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:23:50.628047  7604 net.cpp:150] Setting up mnist
I1127 11:23:50.628152  7604 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:23:50.628185  7604 net.cpp:157] Top shape: 100 (100)
I1127 11:23:50.628192  7604 net.cpp:165] Memory required for data: 314000
I1127 11:23:50.628206  7604 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:23:50.628227  7604 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:23:50.628233  7604 net.cpp:454] label_mnist_1_split <- label
I1127 11:23:50.628245  7604 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:23:50.628259  7604 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:23:50.628407  7604 net.cpp:150] Setting up label_mnist_1_split
I1127 11:23:50.628428  7604 net.cpp:157] Top shape: 100 (100)
I1127 11:23:50.628437  7604 net.cpp:157] Top shape: 100 (100)
I1127 11:23:50.628443  7604 net.cpp:165] Memory required for data: 314800
I1127 11:23:50.628448  7604 layer_factory.hpp:76] Creating layer conv1
I1127 11:23:50.628473  7604 net.cpp:106] Creating Layer conv1
I1127 11:23:50.628479  7604 net.cpp:454] conv1 <- data
I1127 11:23:50.628485  7604 net.cpp:411] conv1 -> conv1
I1127 11:23:50.628670  7604 net.cpp:150] Setting up conv1
I1127 11:23:50.628679  7604 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:23:50.628684  7604 net.cpp:165] Memory required for data: 4922800
I1127 11:23:50.628695  7604 layer_factory.hpp:76] Creating layer pool1
I1127 11:23:50.628705  7604 net.cpp:106] Creating Layer pool1
I1127 11:23:50.628710  7604 net.cpp:454] pool1 <- conv1
I1127 11:23:50.628728  7604 net.cpp:411] pool1 -> pool1
I1127 11:23:50.628761  7604 net.cpp:150] Setting up pool1
I1127 11:23:50.628767  7604 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:23:50.628772  7604 net.cpp:165] Memory required for data: 6074800
I1127 11:23:50.628775  7604 layer_factory.hpp:76] Creating layer conv2
I1127 11:23:50.628785  7604 net.cpp:106] Creating Layer conv2
I1127 11:23:50.628790  7604 net.cpp:454] conv2 <- pool1
I1127 11:23:50.628798  7604 net.cpp:411] conv2 -> conv2
I1127 11:23:50.629078  7604 net.cpp:150] Setting up conv2
I1127 11:23:50.629093  7604 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:23:50.629098  7604 net.cpp:165] Memory required for data: 7354800
I1127 11:23:50.629108  7604 layer_factory.hpp:76] Creating layer pool2
I1127 11:23:50.629117  7604 net.cpp:106] Creating Layer pool2
I1127 11:23:50.629122  7604 net.cpp:454] pool2 <- conv2
I1127 11:23:50.629128  7604 net.cpp:411] pool2 -> pool2
I1127 11:23:50.629166  7604 net.cpp:150] Setting up pool2
I1127 11:23:50.629174  7604 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:23:50.629179  7604 net.cpp:165] Memory required for data: 7674800
I1127 11:23:50.629184  7604 layer_factory.hpp:76] Creating layer ip1
I1127 11:23:50.629192  7604 net.cpp:106] Creating Layer ip1
I1127 11:23:50.629197  7604 net.cpp:454] ip1 <- pool2
I1127 11:23:50.629204  7604 net.cpp:411] ip1 -> ip1
I1127 11:23:50.631830  7604 net.cpp:150] Setting up ip1
I1127 11:23:50.631860  7604 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:23:50.631865  7604 net.cpp:165] Memory required for data: 7874800
I1127 11:23:50.631878  7604 layer_factory.hpp:76] Creating layer relu1
I1127 11:23:50.631891  7604 net.cpp:106] Creating Layer relu1
I1127 11:23:50.631896  7604 net.cpp:454] relu1 <- ip1
I1127 11:23:50.631904  7604 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:23:50.631914  7604 net.cpp:150] Setting up relu1
I1127 11:23:50.631921  7604 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:23:50.631924  7604 net.cpp:165] Memory required for data: 8074800
I1127 11:23:50.631928  7604 layer_factory.hpp:76] Creating layer ip2
I1127 11:23:50.631938  7604 net.cpp:106] Creating Layer ip2
I1127 11:23:50.631942  7604 net.cpp:454] ip2 <- ip1
I1127 11:23:50.631949  7604 net.cpp:411] ip2 -> ip2
I1127 11:23:50.632052  7604 net.cpp:150] Setting up ip2
I1127 11:23:50.632061  7604 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:23:50.632066  7604 net.cpp:165] Memory required for data: 8078800
I1127 11:23:50.632072  7604 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:23:50.632079  7604 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:23:50.632091  7604 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:23:50.632097  7604 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:23:50.632105  7604 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:23:50.632133  7604 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:23:50.632139  7604 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:23:50.632144  7604 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:23:50.632148  7604 net.cpp:165] Memory required for data: 8086800
I1127 11:23:50.632153  7604 layer_factory.hpp:76] Creating layer accuracy
I1127 11:23:50.632163  7604 net.cpp:106] Creating Layer accuracy
I1127 11:23:50.632166  7604 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:23:50.632171  7604 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:23:50.632177  7604 net.cpp:411] accuracy -> accuracy
I1127 11:23:50.632186  7604 net.cpp:150] Setting up accuracy
I1127 11:23:50.632191  7604 net.cpp:157] Top shape: (1)
I1127 11:23:50.632195  7604 net.cpp:165] Memory required for data: 8086804
I1127 11:23:50.632200  7604 layer_factory.hpp:76] Creating layer loss
I1127 11:23:50.632207  7604 net.cpp:106] Creating Layer loss
I1127 11:23:50.632212  7604 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:23:50.632217  7604 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:23:50.632222  7604 net.cpp:411] loss -> loss
I1127 11:23:50.632231  7604 layer_factory.hpp:76] Creating layer loss
I1127 11:23:50.632303  7604 net.cpp:150] Setting up loss
I1127 11:23:50.632310  7604 net.cpp:157] Top shape: (1)
I1127 11:23:50.632315  7604 net.cpp:160]     with loss weight 1
I1127 11:23:50.632325  7604 net.cpp:165] Memory required for data: 8086808
I1127 11:23:50.632330  7604 net.cpp:226] loss needs backward computation.
I1127 11:23:50.632338  7604 net.cpp:228] accuracy does not need backward computation.
I1127 11:23:50.632342  7604 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:23:50.632346  7604 net.cpp:226] ip2 needs backward computation.
I1127 11:23:50.632350  7604 net.cpp:226] relu1 needs backward computation.
I1127 11:23:50.632354  7604 net.cpp:226] ip1 needs backward computation.
I1127 11:23:50.632359  7604 net.cpp:226] pool2 needs backward computation.
I1127 11:23:50.632364  7604 net.cpp:226] conv2 needs backward computation.
I1127 11:23:50.632367  7604 net.cpp:226] pool1 needs backward computation.
I1127 11:23:50.632371  7604 net.cpp:226] conv1 needs backward computation.
I1127 11:23:50.632376  7604 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:23:50.632380  7604 net.cpp:228] mnist does not need backward computation.
I1127 11:23:50.632385  7604 net.cpp:270] This network produces output accuracy
I1127 11:23:50.632388  7604 net.cpp:270] This network produces output loss
I1127 11:23:50.632400  7604 net.cpp:283] Network initialization done.
I1127 11:23:50.632447  7604 solver.cpp:59] Solver scaffolding done.
I1127 11:23:50.632638  7604 caffe.cpp:212] Starting Optimization
I1127 11:23:50.632645  7604 solver.cpp:287] Solving LeNet
I1127 11:23:50.632649  7604 solver.cpp:288] Learning Rate Policy: inv
I1127 11:23:50.633220  7604 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:23:53.422798  7604 solver.cpp:408]     Test net output #0: accuracy = 0.1215
I1127 11:23:53.422909  7604 solver.cpp:408]     Test net output #1: loss = 2.37958 (* 1 = 2.37958 loss)
I1127 11:23:53.434212  7604 solver.cpp:236] Iteration 0, loss = 2.36699
I1127 11:23:53.434326  7604 solver.cpp:252]     Train net output #0: loss = 2.36699 (* 1 = 2.36699 loss)
I1127 11:23:53.434372  7604 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:24:05.295586  7604 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:24:07.926885  7604 solver.cpp:408]     Test net output #0: accuracy = 0.9718
I1127 11:24:07.927019  7604 solver.cpp:408]     Test net output #1: loss = 0.0870901 (* 1 = 0.0870901 loss)
I1127 11:24:07.939095  7604 solver.cpp:236] Iteration 500, loss = 0.0893303
I1127 11:24:07.939240  7604 solver.cpp:252]     Train net output #0: loss = 0.0893303 (* 1 = 0.0893303 loss)
I1127 11:24:07.939276  7604 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:24:20.668601  7604 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:24:20.691611  7604 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:24:20.719804  7604 solver.cpp:320] Iteration 1000, loss = 0.0893152
I1127 11:24:20.719846  7604 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:24:22.443949  7604 solver.cpp:408]     Test net output #0: accuracy = 0.9803
I1127 11:24:22.444124  7604 solver.cpp:408]     Test net output #1: loss = 0.0586295 (* 1 = 0.0586295 loss)
I1127 11:24:22.444144  7604 solver.cpp:325] Optimization Done.
I1127 11:24:22.444155  7604 caffe.cpp:215] Optimization Done.
I1127 11:24:22.544983  7687 caffe.cpp:184] Using GPUs 0
I1127 11:24:22.833544  7687 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:24:22.833761  7687 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:24:22.834174  7687 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:24:22.834200  7687 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:24:22.834321  7687 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:24:22.834411  7687 layer_factory.hpp:76] Creating layer mnist
I1127 11:24:22.878274  7687 net.cpp:106] Creating Layer mnist
I1127 11:24:22.878340  7687 net.cpp:411] mnist -> data
I1127 11:24:22.878382  7687 net.cpp:411] mnist -> label
I1127 11:24:22.879503  7690 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:24:22.885901  7687 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:24:22.886821  7687 net.cpp:150] Setting up mnist
I1127 11:24:22.886863  7687 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:24:22.886878  7687 net.cpp:157] Top shape: 64 (64)
I1127 11:24:22.886883  7687 net.cpp:165] Memory required for data: 200960
I1127 11:24:22.886893  7687 layer_factory.hpp:76] Creating layer conv1
I1127 11:24:22.886909  7687 net.cpp:106] Creating Layer conv1
I1127 11:24:22.886916  7687 net.cpp:454] conv1 <- data
I1127 11:24:22.886929  7687 net.cpp:411] conv1 -> conv1
I1127 11:24:22.887508  7687 net.cpp:150] Setting up conv1
I1127 11:24:22.887523  7687 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:24:22.887528  7687 net.cpp:165] Memory required for data: 3150080
I1127 11:24:22.887540  7687 layer_factory.hpp:76] Creating layer pool1
I1127 11:24:22.887550  7687 net.cpp:106] Creating Layer pool1
I1127 11:24:22.887555  7687 net.cpp:454] pool1 <- conv1
I1127 11:24:22.887562  7687 net.cpp:411] pool1 -> pool1
I1127 11:24:22.887614  7687 net.cpp:150] Setting up pool1
I1127 11:24:22.887624  7687 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:24:22.887627  7687 net.cpp:165] Memory required for data: 3887360
I1127 11:24:22.887632  7687 layer_factory.hpp:76] Creating layer conv2
I1127 11:24:22.887641  7687 net.cpp:106] Creating Layer conv2
I1127 11:24:22.887646  7687 net.cpp:454] conv2 <- pool1
I1127 11:24:22.887653  7687 net.cpp:411] conv2 -> conv2
I1127 11:24:22.887894  7687 net.cpp:150] Setting up conv2
I1127 11:24:22.887903  7687 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:24:22.887907  7687 net.cpp:165] Memory required for data: 4706560
I1127 11:24:22.887917  7687 layer_factory.hpp:76] Creating layer pool2
I1127 11:24:22.887925  7687 net.cpp:106] Creating Layer pool2
I1127 11:24:22.887930  7687 net.cpp:454] pool2 <- conv2
I1127 11:24:22.887935  7687 net.cpp:411] pool2 -> pool2
I1127 11:24:22.888000  7687 net.cpp:150] Setting up pool2
I1127 11:24:22.888008  7687 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:24:22.888012  7687 net.cpp:165] Memory required for data: 4911360
I1127 11:24:22.888017  7687 layer_factory.hpp:76] Creating layer ip1
I1127 11:24:22.888028  7687 net.cpp:106] Creating Layer ip1
I1127 11:24:22.888032  7687 net.cpp:454] ip1 <- pool2
I1127 11:24:22.888041  7687 net.cpp:411] ip1 -> ip1
I1127 11:24:22.890869  7687 net.cpp:150] Setting up ip1
I1127 11:24:22.890911  7687 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:24:22.890916  7687 net.cpp:165] Memory required for data: 5039360
I1127 11:24:22.890929  7687 layer_factory.hpp:76] Creating layer relu1
I1127 11:24:22.890940  7687 net.cpp:106] Creating Layer relu1
I1127 11:24:22.890949  7687 net.cpp:454] relu1 <- ip1
I1127 11:24:22.890959  7687 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:24:22.890974  7687 net.cpp:150] Setting up relu1
I1127 11:24:22.890981  7687 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:24:22.890985  7687 net.cpp:165] Memory required for data: 5167360
I1127 11:24:22.890990  7687 layer_factory.hpp:76] Creating layer ip2
I1127 11:24:22.891000  7687 net.cpp:106] Creating Layer ip2
I1127 11:24:22.891005  7687 net.cpp:454] ip2 <- ip1
I1127 11:24:22.891012  7687 net.cpp:411] ip2 -> ip2
I1127 11:24:22.891774  7687 net.cpp:150] Setting up ip2
I1127 11:24:22.891821  7687 net.cpp:157] Top shape: 64 10 (640)
I1127 11:24:22.891826  7687 net.cpp:165] Memory required for data: 5169920
I1127 11:24:22.891839  7687 layer_factory.hpp:76] Creating layer loss
I1127 11:24:22.891856  7687 net.cpp:106] Creating Layer loss
I1127 11:24:22.891863  7687 net.cpp:454] loss <- ip2
I1127 11:24:22.891870  7687 net.cpp:454] loss <- label
I1127 11:24:22.891881  7687 net.cpp:411] loss -> loss
I1127 11:24:22.891899  7687 layer_factory.hpp:76] Creating layer loss
I1127 11:24:22.891968  7687 net.cpp:150] Setting up loss
I1127 11:24:22.891976  7687 net.cpp:157] Top shape: (1)
I1127 11:24:22.891980  7687 net.cpp:160]     with loss weight 1
I1127 11:24:22.891999  7687 net.cpp:165] Memory required for data: 5169924
I1127 11:24:22.892002  7687 net.cpp:226] loss needs backward computation.
I1127 11:24:22.892007  7687 net.cpp:226] ip2 needs backward computation.
I1127 11:24:22.892012  7687 net.cpp:226] relu1 needs backward computation.
I1127 11:24:22.892024  7687 net.cpp:226] ip1 needs backward computation.
I1127 11:24:22.892029  7687 net.cpp:226] pool2 needs backward computation.
I1127 11:24:22.892032  7687 net.cpp:226] conv2 needs backward computation.
I1127 11:24:22.892036  7687 net.cpp:226] pool1 needs backward computation.
I1127 11:24:22.892041  7687 net.cpp:226] conv1 needs backward computation.
I1127 11:24:22.892045  7687 net.cpp:228] mnist does not need backward computation.
I1127 11:24:22.892050  7687 net.cpp:270] This network produces output loss
I1127 11:24:22.892060  7687 net.cpp:283] Network initialization done.
I1127 11:24:22.892302  7687 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:24:22.892325  7687 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:24:22.892441  7687 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:24:22.892508  7687 layer_factory.hpp:76] Creating layer mnist
I1127 11:24:22.892612  7687 net.cpp:106] Creating Layer mnist
I1127 11:24:22.892622  7687 net.cpp:411] mnist -> data
I1127 11:24:22.892632  7687 net.cpp:411] mnist -> label
I1127 11:24:22.893692  7692 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:24:22.893826  7687 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:24:22.894700  7687 net.cpp:150] Setting up mnist
I1127 11:24:22.894721  7687 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:24:22.894727  7687 net.cpp:157] Top shape: 100 (100)
I1127 11:24:22.894731  7687 net.cpp:165] Memory required for data: 314000
I1127 11:24:22.894738  7687 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:24:22.894750  7687 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:24:22.894755  7687 net.cpp:454] label_mnist_1_split <- label
I1127 11:24:22.894767  7687 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:24:22.894778  7687 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:24:22.894814  7687 net.cpp:150] Setting up label_mnist_1_split
I1127 11:24:22.894829  7687 net.cpp:157] Top shape: 100 (100)
I1127 11:24:22.894834  7687 net.cpp:157] Top shape: 100 (100)
I1127 11:24:22.894840  7687 net.cpp:165] Memory required for data: 314800
I1127 11:24:22.894843  7687 layer_factory.hpp:76] Creating layer conv1
I1127 11:24:22.894855  7687 net.cpp:106] Creating Layer conv1
I1127 11:24:22.894860  7687 net.cpp:454] conv1 <- data
I1127 11:24:22.894866  7687 net.cpp:411] conv1 -> conv1
I1127 11:24:22.895021  7687 net.cpp:150] Setting up conv1
I1127 11:24:22.895030  7687 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:24:22.895035  7687 net.cpp:165] Memory required for data: 4922800
I1127 11:24:22.895045  7687 layer_factory.hpp:76] Creating layer pool1
I1127 11:24:22.895053  7687 net.cpp:106] Creating Layer pool1
I1127 11:24:22.895057  7687 net.cpp:454] pool1 <- conv1
I1127 11:24:22.895071  7687 net.cpp:411] pool1 -> pool1
I1127 11:24:22.895100  7687 net.cpp:150] Setting up pool1
I1127 11:24:22.895107  7687 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:24:22.895112  7687 net.cpp:165] Memory required for data: 6074800
I1127 11:24:22.895117  7687 layer_factory.hpp:76] Creating layer conv2
I1127 11:24:22.895125  7687 net.cpp:106] Creating Layer conv2
I1127 11:24:22.895130  7687 net.cpp:454] conv2 <- pool1
I1127 11:24:22.895138  7687 net.cpp:411] conv2 -> conv2
I1127 11:24:22.895396  7687 net.cpp:150] Setting up conv2
I1127 11:24:22.895406  7687 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:24:22.895409  7687 net.cpp:165] Memory required for data: 7354800
I1127 11:24:22.895418  7687 layer_factory.hpp:76] Creating layer pool2
I1127 11:24:22.895426  7687 net.cpp:106] Creating Layer pool2
I1127 11:24:22.895431  7687 net.cpp:454] pool2 <- conv2
I1127 11:24:22.895437  7687 net.cpp:411] pool2 -> pool2
I1127 11:24:22.895464  7687 net.cpp:150] Setting up pool2
I1127 11:24:22.895472  7687 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:24:22.895475  7687 net.cpp:165] Memory required for data: 7674800
I1127 11:24:22.895480  7687 layer_factory.hpp:76] Creating layer ip1
I1127 11:24:22.895489  7687 net.cpp:106] Creating Layer ip1
I1127 11:24:22.895494  7687 net.cpp:454] ip1 <- pool2
I1127 11:24:22.895499  7687 net.cpp:411] ip1 -> ip1
I1127 11:24:22.897794  7687 net.cpp:150] Setting up ip1
I1127 11:24:22.897835  7687 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:24:22.897841  7687 net.cpp:165] Memory required for data: 7874800
I1127 11:24:22.897856  7687 layer_factory.hpp:76] Creating layer relu1
I1127 11:24:22.897869  7687 net.cpp:106] Creating Layer relu1
I1127 11:24:22.897876  7687 net.cpp:454] relu1 <- ip1
I1127 11:24:22.897884  7687 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:24:22.897899  7687 net.cpp:150] Setting up relu1
I1127 11:24:22.897904  7687 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:24:22.897908  7687 net.cpp:165] Memory required for data: 8074800
I1127 11:24:22.897913  7687 layer_factory.hpp:76] Creating layer ip2
I1127 11:24:22.897929  7687 net.cpp:106] Creating Layer ip2
I1127 11:24:22.897934  7687 net.cpp:454] ip2 <- ip1
I1127 11:24:22.897941  7687 net.cpp:411] ip2 -> ip2
I1127 11:24:22.898080  7687 net.cpp:150] Setting up ip2
I1127 11:24:22.898097  7687 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:24:22.898102  7687 net.cpp:165] Memory required for data: 8078800
I1127 11:24:22.898108  7687 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:24:22.898118  7687 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:24:22.898123  7687 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:24:22.898128  7687 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:24:22.898135  7687 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:24:22.898185  7687 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:24:22.898198  7687 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:24:22.898205  7687 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:24:22.898208  7687 net.cpp:165] Memory required for data: 8086800
I1127 11:24:22.898213  7687 layer_factory.hpp:76] Creating layer accuracy
I1127 11:24:22.898229  7687 net.cpp:106] Creating Layer accuracy
I1127 11:24:22.898236  7687 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:24:22.898241  7687 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:24:22.898247  7687 net.cpp:411] accuracy -> accuracy
I1127 11:24:22.898259  7687 net.cpp:150] Setting up accuracy
I1127 11:24:22.898267  7687 net.cpp:157] Top shape: (1)
I1127 11:24:22.898270  7687 net.cpp:165] Memory required for data: 8086804
I1127 11:24:22.898274  7687 layer_factory.hpp:76] Creating layer loss
I1127 11:24:22.898283  7687 net.cpp:106] Creating Layer loss
I1127 11:24:22.898288  7687 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:24:22.898293  7687 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:24:22.898299  7687 net.cpp:411] loss -> loss
I1127 11:24:22.898309  7687 layer_factory.hpp:76] Creating layer loss
I1127 11:24:22.898393  7687 net.cpp:150] Setting up loss
I1127 11:24:22.898401  7687 net.cpp:157] Top shape: (1)
I1127 11:24:22.898406  7687 net.cpp:160]     with loss weight 1
I1127 11:24:22.898418  7687 net.cpp:165] Memory required for data: 8086808
I1127 11:24:22.898422  7687 net.cpp:226] loss needs backward computation.
I1127 11:24:22.898432  7687 net.cpp:228] accuracy does not need backward computation.
I1127 11:24:22.898437  7687 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:24:22.898440  7687 net.cpp:226] ip2 needs backward computation.
I1127 11:24:22.898445  7687 net.cpp:226] relu1 needs backward computation.
I1127 11:24:22.898449  7687 net.cpp:226] ip1 needs backward computation.
I1127 11:24:22.898454  7687 net.cpp:226] pool2 needs backward computation.
I1127 11:24:22.898458  7687 net.cpp:226] conv2 needs backward computation.
I1127 11:24:22.898463  7687 net.cpp:226] pool1 needs backward computation.
I1127 11:24:22.898468  7687 net.cpp:226] conv1 needs backward computation.
I1127 11:24:22.898475  7687 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:24:22.898480  7687 net.cpp:228] mnist does not need backward computation.
I1127 11:24:22.898484  7687 net.cpp:270] This network produces output accuracy
I1127 11:24:22.898489  7687 net.cpp:270] This network produces output loss
I1127 11:24:22.898500  7687 net.cpp:283] Network initialization done.
I1127 11:24:22.898564  7687 solver.cpp:59] Solver scaffolding done.
I1127 11:24:22.898766  7687 caffe.cpp:212] Starting Optimization
I1127 11:24:22.898775  7687 solver.cpp:287] Solving LeNet
I1127 11:24:22.898779  7687 solver.cpp:288] Learning Rate Policy: inv
I1127 11:24:22.899188  7687 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:24:26.016665  7687 solver.cpp:408]     Test net output #0: accuracy = 0.0987
I1127 11:24:26.016871  7687 solver.cpp:408]     Test net output #1: loss = 2.46773 (* 1 = 2.46773 loss)
I1127 11:24:26.033303  7687 solver.cpp:236] Iteration 0, loss = 2.42143
I1127 11:24:26.033463  7687 solver.cpp:252]     Train net output #0: loss = 2.42143 (* 1 = 2.42143 loss)
I1127 11:24:26.033511  7687 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:24:37.407677  7687 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:24:39.743913  7687 solver.cpp:408]     Test net output #0: accuracy = 0.9729
I1127 11:24:39.743949  7687 solver.cpp:408]     Test net output #1: loss = 0.0814535 (* 1 = 0.0814535 loss)
I1127 11:24:39.772927  7687 solver.cpp:236] Iteration 500, loss = 0.0760298
I1127 11:24:39.772944  7687 solver.cpp:252]     Train net output #0: loss = 0.07603 (* 1 = 0.07603 loss)
I1127 11:24:39.772953  7687 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:24:51.874796  7687 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:24:51.890314  7687 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:24:51.902184  7687 solver.cpp:320] Iteration 1000, loss = 0.0598185
I1127 11:24:51.902254  7687 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:24:54.610607  7687 solver.cpp:408]     Test net output #0: accuracy = 0.9827
I1127 11:24:54.610707  7687 solver.cpp:408]     Test net output #1: loss = 0.052875 (* 1 = 0.052875 loss)
I1127 11:24:54.610721  7687 solver.cpp:325] Optimization Done.
I1127 11:24:54.610725  7687 caffe.cpp:215] Optimization Done.
I1127 11:24:54.676210  7716 caffe.cpp:184] Using GPUs 0
I1127 11:24:55.129899  7716 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:24:55.130199  7716 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:24:55.130676  7716 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:24:55.130709  7716 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:24:55.130866  7716 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:24:55.130954  7716 layer_factory.hpp:76] Creating layer mnist
I1127 11:24:55.131567  7716 net.cpp:106] Creating Layer mnist
I1127 11:24:55.131615  7716 net.cpp:411] mnist -> data
I1127 11:24:55.131674  7716 net.cpp:411] mnist -> label
I1127 11:24:55.133008  7719 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:24:55.143641  7716 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:24:55.176991  7716 net.cpp:150] Setting up mnist
I1127 11:24:55.177073  7716 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:24:55.177088  7716 net.cpp:157] Top shape: 64 (64)
I1127 11:24:55.177095  7716 net.cpp:165] Memory required for data: 200960
I1127 11:24:55.177111  7716 layer_factory.hpp:76] Creating layer conv1
I1127 11:24:55.177148  7716 net.cpp:106] Creating Layer conv1
I1127 11:24:55.177165  7716 net.cpp:454] conv1 <- data
I1127 11:24:55.177189  7716 net.cpp:411] conv1 -> conv1
I1127 11:24:55.178270  7716 net.cpp:150] Setting up conv1
I1127 11:24:55.178306  7716 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:24:55.178313  7716 net.cpp:165] Memory required for data: 3150080
I1127 11:24:55.178344  7716 layer_factory.hpp:76] Creating layer pool1
I1127 11:24:55.178364  7716 net.cpp:106] Creating Layer pool1
I1127 11:24:55.178377  7716 net.cpp:454] pool1 <- conv1
I1127 11:24:55.178393  7716 net.cpp:411] pool1 -> pool1
I1127 11:24:55.178561  7716 net.cpp:150] Setting up pool1
I1127 11:24:55.178581  7716 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:24:55.178587  7716 net.cpp:165] Memory required for data: 3887360
I1127 11:24:55.178596  7716 layer_factory.hpp:76] Creating layer conv2
I1127 11:24:55.178617  7716 net.cpp:106] Creating Layer conv2
I1127 11:24:55.178627  7716 net.cpp:454] conv2 <- pool1
I1127 11:24:55.178637  7716 net.cpp:411] conv2 -> conv2
I1127 11:24:55.179141  7716 net.cpp:150] Setting up conv2
I1127 11:24:55.179163  7716 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:24:55.179172  7716 net.cpp:165] Memory required for data: 4706560
I1127 11:24:55.179188  7716 layer_factory.hpp:76] Creating layer pool2
I1127 11:24:55.179204  7716 net.cpp:106] Creating Layer pool2
I1127 11:24:55.179213  7716 net.cpp:454] pool2 <- conv2
I1127 11:24:55.179224  7716 net.cpp:411] pool2 -> pool2
I1127 11:24:55.179275  7716 net.cpp:150] Setting up pool2
I1127 11:24:55.179293  7716 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:24:55.179303  7716 net.cpp:165] Memory required for data: 4911360
I1127 11:24:55.179311  7716 layer_factory.hpp:76] Creating layer ip1
I1127 11:24:55.179327  7716 net.cpp:106] Creating Layer ip1
I1127 11:24:55.179335  7716 net.cpp:454] ip1 <- pool2
I1127 11:24:55.179347  7716 net.cpp:411] ip1 -> ip1
I1127 11:24:55.183907  7716 net.cpp:150] Setting up ip1
I1127 11:24:55.184020  7716 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:24:55.184058  7716 net.cpp:165] Memory required for data: 5039360
I1127 11:24:55.184111  7716 layer_factory.hpp:76] Creating layer relu1
I1127 11:24:55.184150  7716 net.cpp:106] Creating Layer relu1
I1127 11:24:55.184177  7716 net.cpp:454] relu1 <- ip1
I1127 11:24:55.184216  7716 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:24:55.184265  7716 net.cpp:150] Setting up relu1
I1127 11:24:55.184288  7716 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:24:55.184300  7716 net.cpp:165] Memory required for data: 5167360
I1127 11:24:55.184310  7716 layer_factory.hpp:76] Creating layer ip2
I1127 11:24:55.184336  7716 net.cpp:106] Creating Layer ip2
I1127 11:24:55.184356  7716 net.cpp:454] ip2 <- ip1
I1127 11:24:55.184394  7716 net.cpp:411] ip2 -> ip2
I1127 11:24:55.186125  7716 net.cpp:150] Setting up ip2
I1127 11:24:55.186234  7716 net.cpp:157] Top shape: 64 10 (640)
I1127 11:24:55.186269  7716 net.cpp:165] Memory required for data: 5169920
I1127 11:24:55.186322  7716 layer_factory.hpp:76] Creating layer loss
I1127 11:24:55.186378  7716 net.cpp:106] Creating Layer loss
I1127 11:24:55.186414  7716 net.cpp:454] loss <- ip2
I1127 11:24:55.186450  7716 net.cpp:454] loss <- label
I1127 11:24:55.186486  7716 net.cpp:411] loss -> loss
I1127 11:24:55.186552  7716 layer_factory.hpp:76] Creating layer loss
I1127 11:24:55.186926  7716 net.cpp:150] Setting up loss
I1127 11:24:55.186959  7716 net.cpp:157] Top shape: (1)
I1127 11:24:55.186972  7716 net.cpp:160]     with loss weight 1
I1127 11:24:55.187041  7716 net.cpp:165] Memory required for data: 5169924
I1127 11:24:55.187057  7716 net.cpp:226] loss needs backward computation.
I1127 11:24:55.187069  7716 net.cpp:226] ip2 needs backward computation.
I1127 11:24:55.187080  7716 net.cpp:226] relu1 needs backward computation.
I1127 11:24:55.187088  7716 net.cpp:226] ip1 needs backward computation.
I1127 11:24:55.187100  7716 net.cpp:226] pool2 needs backward computation.
I1127 11:24:55.187113  7716 net.cpp:226] conv2 needs backward computation.
I1127 11:24:55.187125  7716 net.cpp:226] pool1 needs backward computation.
I1127 11:24:55.187137  7716 net.cpp:226] conv1 needs backward computation.
I1127 11:24:55.187150  7716 net.cpp:228] mnist does not need backward computation.
I1127 11:24:55.187160  7716 net.cpp:270] This network produces output loss
I1127 11:24:55.187183  7716 net.cpp:283] Network initialization done.
I1127 11:24:55.187861  7716 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:24:55.187947  7716 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:24:55.188261  7716 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:24:55.188437  7716 layer_factory.hpp:76] Creating layer mnist
I1127 11:24:55.188678  7716 net.cpp:106] Creating Layer mnist
I1127 11:24:55.188705  7716 net.cpp:411] mnist -> data
I1127 11:24:55.188741  7716 net.cpp:411] mnist -> label
I1127 11:24:55.190134  7721 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:24:55.190517  7716 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:24:55.192587  7716 net.cpp:150] Setting up mnist
I1127 11:24:55.192662  7716 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:24:55.192683  7716 net.cpp:157] Top shape: 100 (100)
I1127 11:24:55.192694  7716 net.cpp:165] Memory required for data: 314000
I1127 11:24:55.192716  7716 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:24:55.192757  7716 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:24:55.192776  7716 net.cpp:454] label_mnist_1_split <- label
I1127 11:24:55.192808  7716 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:24:55.192848  7716 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:24:55.192929  7716 net.cpp:150] Setting up label_mnist_1_split
I1127 11:24:55.192946  7716 net.cpp:157] Top shape: 100 (100)
I1127 11:24:55.192956  7716 net.cpp:157] Top shape: 100 (100)
I1127 11:24:55.192963  7716 net.cpp:165] Memory required for data: 314800
I1127 11:24:55.192972  7716 layer_factory.hpp:76] Creating layer conv1
I1127 11:24:55.192997  7716 net.cpp:106] Creating Layer conv1
I1127 11:24:55.193007  7716 net.cpp:454] conv1 <- data
I1127 11:24:55.193020  7716 net.cpp:411] conv1 -> conv1
I1127 11:24:55.193387  7716 net.cpp:150] Setting up conv1
I1127 11:24:55.193408  7716 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:24:55.193434  7716 net.cpp:165] Memory required for data: 4922800
I1127 11:24:55.193455  7716 layer_factory.hpp:76] Creating layer pool1
I1127 11:24:55.193473  7716 net.cpp:106] Creating Layer pool1
I1127 11:24:55.193481  7716 net.cpp:454] pool1 <- conv1
I1127 11:24:55.193513  7716 net.cpp:411] pool1 -> pool1
I1127 11:24:55.193574  7716 net.cpp:150] Setting up pool1
I1127 11:24:55.193588  7716 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:24:55.193594  7716 net.cpp:165] Memory required for data: 6074800
I1127 11:24:55.193603  7716 layer_factory.hpp:76] Creating layer conv2
I1127 11:24:55.193626  7716 net.cpp:106] Creating Layer conv2
I1127 11:24:55.193635  7716 net.cpp:454] conv2 <- pool1
I1127 11:24:55.193649  7716 net.cpp:411] conv2 -> conv2
I1127 11:24:55.194300  7716 net.cpp:150] Setting up conv2
I1127 11:24:55.194356  7716 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:24:55.194367  7716 net.cpp:165] Memory required for data: 7354800
I1127 11:24:55.194393  7716 layer_factory.hpp:76] Creating layer pool2
I1127 11:24:55.194422  7716 net.cpp:106] Creating Layer pool2
I1127 11:24:55.194435  7716 net.cpp:454] pool2 <- conv2
I1127 11:24:55.194455  7716 net.cpp:411] pool2 -> pool2
I1127 11:24:55.194521  7716 net.cpp:150] Setting up pool2
I1127 11:24:55.194535  7716 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:24:55.194545  7716 net.cpp:165] Memory required for data: 7674800
I1127 11:24:55.194555  7716 layer_factory.hpp:76] Creating layer ip1
I1127 11:24:55.194573  7716 net.cpp:106] Creating Layer ip1
I1127 11:24:55.194584  7716 net.cpp:454] ip1 <- pool2
I1127 11:24:55.194600  7716 net.cpp:411] ip1 -> ip1
I1127 11:24:55.199139  7716 net.cpp:150] Setting up ip1
I1127 11:24:55.199239  7716 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:24:55.199249  7716 net.cpp:165] Memory required for data: 7874800
I1127 11:24:55.199285  7716 layer_factory.hpp:76] Creating layer relu1
I1127 11:24:55.199313  7716 net.cpp:106] Creating Layer relu1
I1127 11:24:55.199327  7716 net.cpp:454] relu1 <- ip1
I1127 11:24:55.199344  7716 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:24:55.199370  7716 net.cpp:150] Setting up relu1
I1127 11:24:55.199383  7716 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:24:55.199390  7716 net.cpp:165] Memory required for data: 8074800
I1127 11:24:55.199398  7716 layer_factory.hpp:76] Creating layer ip2
I1127 11:24:55.199424  7716 net.cpp:106] Creating Layer ip2
I1127 11:24:55.199435  7716 net.cpp:454] ip2 <- ip1
I1127 11:24:55.199450  7716 net.cpp:411] ip2 -> ip2
I1127 11:24:55.199672  7716 net.cpp:150] Setting up ip2
I1127 11:24:55.199688  7716 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:24:55.199695  7716 net.cpp:165] Memory required for data: 8078800
I1127 11:24:55.199708  7716 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:24:55.199720  7716 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:24:55.199728  7716 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:24:55.199738  7716 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:24:55.199750  7716 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:24:55.199801  7716 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:24:55.199815  7716 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:24:55.199825  7716 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:24:55.199831  7716 net.cpp:165] Memory required for data: 8086800
I1127 11:24:55.199838  7716 layer_factory.hpp:76] Creating layer accuracy
I1127 11:24:55.199856  7716 net.cpp:106] Creating Layer accuracy
I1127 11:24:55.199863  7716 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:24:55.199875  7716 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:24:55.199884  7716 net.cpp:411] accuracy -> accuracy
I1127 11:24:55.199901  7716 net.cpp:150] Setting up accuracy
I1127 11:24:55.199913  7716 net.cpp:157] Top shape: (1)
I1127 11:24:55.199918  7716 net.cpp:165] Memory required for data: 8086804
I1127 11:24:55.199928  7716 layer_factory.hpp:76] Creating layer loss
I1127 11:24:55.199941  7716 net.cpp:106] Creating Layer loss
I1127 11:24:55.199967  7716 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:24:55.199978  7716 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:24:55.199990  7716 net.cpp:411] loss -> loss
I1127 11:24:55.200009  7716 layer_factory.hpp:76] Creating layer loss
I1127 11:24:55.200181  7716 net.cpp:150] Setting up loss
I1127 11:24:55.200196  7716 net.cpp:157] Top shape: (1)
I1127 11:24:55.200203  7716 net.cpp:160]     with loss weight 1
I1127 11:24:55.200235  7716 net.cpp:165] Memory required for data: 8086808
I1127 11:24:55.200245  7716 net.cpp:226] loss needs backward computation.
I1127 11:24:55.200263  7716 net.cpp:228] accuracy does not need backward computation.
I1127 11:24:55.200273  7716 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:24:55.200284  7716 net.cpp:226] ip2 needs backward computation.
I1127 11:24:55.200294  7716 net.cpp:226] relu1 needs backward computation.
I1127 11:24:55.200304  7716 net.cpp:226] ip1 needs backward computation.
I1127 11:24:55.200314  7716 net.cpp:226] pool2 needs backward computation.
I1127 11:24:55.200325  7716 net.cpp:226] conv2 needs backward computation.
I1127 11:24:55.200335  7716 net.cpp:226] pool1 needs backward computation.
I1127 11:24:55.200345  7716 net.cpp:226] conv1 needs backward computation.
I1127 11:24:55.200356  7716 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:24:55.200366  7716 net.cpp:228] mnist does not need backward computation.
I1127 11:24:55.200376  7716 net.cpp:270] This network produces output accuracy
I1127 11:24:55.200386  7716 net.cpp:270] This network produces output loss
I1127 11:24:55.200410  7716 net.cpp:283] Network initialization done.
I1127 11:24:55.200525  7716 solver.cpp:59] Solver scaffolding done.
I1127 11:24:55.200899  7716 caffe.cpp:212] Starting Optimization
I1127 11:24:55.200918  7716 solver.cpp:287] Solving LeNet
I1127 11:24:55.200927  7716 solver.cpp:288] Learning Rate Policy: inv
I1127 11:24:55.201786  7716 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:24:55.280642  7716 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:24:56.295017  7716 solver.cpp:408]     Test net output #0: accuracy = 0.0502
I1127 11:24:56.295119  7716 solver.cpp:408]     Test net output #1: loss = 2.40698 (* 1 = 2.40698 loss)
I1127 11:24:56.306304  7716 solver.cpp:236] Iteration 0, loss = 2.40097
I1127 11:24:56.306354  7716 solver.cpp:252]     Train net output #0: loss = 2.40097 (* 1 = 2.40097 loss)
I1127 11:24:56.306371  7716 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:25:09.723744  7716 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:25:12.483680  7716 solver.cpp:408]     Test net output #0: accuracy = 0.9733
I1127 11:25:12.483726  7716 solver.cpp:408]     Test net output #1: loss = 0.0839414 (* 1 = 0.0839414 loss)
I1127 11:25:12.516191  7716 solver.cpp:236] Iteration 500, loss = 0.0802879
I1127 11:25:12.516209  7716 solver.cpp:252]     Train net output #0: loss = 0.0802879 (* 1 = 0.0802879 loss)
I1127 11:25:12.516218  7716 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:25:24.289665  7716 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:25:24.305533  7716 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:25:24.317176  7716 solver.cpp:320] Iteration 1000, loss = 0.0401158
I1127 11:25:24.317234  7716 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:25:25.463623  7716 solver.cpp:408]     Test net output #0: accuracy = 0.9801
I1127 11:25:25.463881  7716 solver.cpp:408]     Test net output #1: loss = 0.057242 (* 1 = 0.057242 loss)
I1127 11:25:25.463910  7716 solver.cpp:325] Optimization Done.
I1127 11:25:25.463919  7716 caffe.cpp:215] Optimization Done.
I1127 11:25:25.607538  7741 caffe.cpp:184] Using GPUs 0
I1127 11:25:25.935976  7741 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:25:25.936344  7741 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:25:25.936769  7741 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:25:25.936796  7741 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:25:25.936902  7741 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:25:25.936980  7741 layer_factory.hpp:76] Creating layer mnist
I1127 11:25:25.937489  7741 net.cpp:106] Creating Layer mnist
I1127 11:25:25.937528  7741 net.cpp:411] mnist -> data
I1127 11:25:25.937567  7741 net.cpp:411] mnist -> label
I1127 11:25:25.938891  7744 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:25:25.963209  7741 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:25:25.965237  7741 net.cpp:150] Setting up mnist
I1127 11:25:25.965335  7741 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:25:25.965351  7741 net.cpp:157] Top shape: 64 (64)
I1127 11:25:25.965360  7741 net.cpp:165] Memory required for data: 200960
I1127 11:25:25.965384  7741 layer_factory.hpp:76] Creating layer conv1
I1127 11:25:25.965432  7741 net.cpp:106] Creating Layer conv1
I1127 11:25:25.965450  7741 net.cpp:454] conv1 <- data
I1127 11:25:25.965474  7741 net.cpp:411] conv1 -> conv1
I1127 11:25:25.966660  7741 net.cpp:150] Setting up conv1
I1127 11:25:25.966788  7741 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:25:25.966802  7741 net.cpp:165] Memory required for data: 3150080
I1127 11:25:25.966851  7741 layer_factory.hpp:76] Creating layer pool1
I1127 11:25:25.966902  7741 net.cpp:106] Creating Layer pool1
I1127 11:25:25.966919  7741 net.cpp:454] pool1 <- conv1
I1127 11:25:25.966940  7741 net.cpp:411] pool1 -> pool1
I1127 11:25:25.967115  7741 net.cpp:150] Setting up pool1
I1127 11:25:25.967129  7741 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:25:25.967138  7741 net.cpp:165] Memory required for data: 3887360
I1127 11:25:25.967156  7741 layer_factory.hpp:76] Creating layer conv2
I1127 11:25:25.967188  7741 net.cpp:106] Creating Layer conv2
I1127 11:25:25.967198  7741 net.cpp:454] conv2 <- pool1
I1127 11:25:25.967216  7741 net.cpp:411] conv2 -> conv2
I1127 11:25:25.967754  7741 net.cpp:150] Setting up conv2
I1127 11:25:25.967805  7741 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:25:25.967813  7741 net.cpp:165] Memory required for data: 4706560
I1127 11:25:25.967834  7741 layer_factory.hpp:76] Creating layer pool2
I1127 11:25:25.967860  7741 net.cpp:106] Creating Layer pool2
I1127 11:25:25.967871  7741 net.cpp:454] pool2 <- conv2
I1127 11:25:25.967885  7741 net.cpp:411] pool2 -> pool2
I1127 11:25:25.967939  7741 net.cpp:150] Setting up pool2
I1127 11:25:25.967952  7741 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:25:25.967957  7741 net.cpp:165] Memory required for data: 4911360
I1127 11:25:25.967964  7741 layer_factory.hpp:76] Creating layer ip1
I1127 11:25:25.967985  7741 net.cpp:106] Creating Layer ip1
I1127 11:25:25.967993  7741 net.cpp:454] ip1 <- pool2
I1127 11:25:25.968004  7741 net.cpp:411] ip1 -> ip1
I1127 11:25:25.973958  7741 net.cpp:150] Setting up ip1
I1127 11:25:25.974023  7741 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:25:25.974033  7741 net.cpp:165] Memory required for data: 5039360
I1127 11:25:25.974059  7741 layer_factory.hpp:76] Creating layer relu1
I1127 11:25:25.974081  7741 net.cpp:106] Creating Layer relu1
I1127 11:25:25.974093  7741 net.cpp:454] relu1 <- ip1
I1127 11:25:25.974108  7741 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:25:25.974140  7741 net.cpp:150] Setting up relu1
I1127 11:25:25.974164  7741 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:25:25.974172  7741 net.cpp:165] Memory required for data: 5167360
I1127 11:25:25.974181  7741 layer_factory.hpp:76] Creating layer ip2
I1127 11:25:25.974205  7741 net.cpp:106] Creating Layer ip2
I1127 11:25:25.974212  7741 net.cpp:454] ip2 <- ip1
I1127 11:25:25.974230  7741 net.cpp:411] ip2 -> ip2
I1127 11:25:25.975528  7741 net.cpp:150] Setting up ip2
I1127 11:25:25.975602  7741 net.cpp:157] Top shape: 64 10 (640)
I1127 11:25:25.975617  7741 net.cpp:165] Memory required for data: 5169920
I1127 11:25:25.975641  7741 layer_factory.hpp:76] Creating layer loss
I1127 11:25:25.975675  7741 net.cpp:106] Creating Layer loss
I1127 11:25:25.975690  7741 net.cpp:454] loss <- ip2
I1127 11:25:25.975704  7741 net.cpp:454] loss <- label
I1127 11:25:25.975725  7741 net.cpp:411] loss -> loss
I1127 11:25:25.975761  7741 layer_factory.hpp:76] Creating layer loss
I1127 11:25:25.975904  7741 net.cpp:150] Setting up loss
I1127 11:25:25.975919  7741 net.cpp:157] Top shape: (1)
I1127 11:25:25.975926  7741 net.cpp:160]     with loss weight 1
I1127 11:25:25.975986  7741 net.cpp:165] Memory required for data: 5169924
I1127 11:25:25.975996  7741 net.cpp:226] loss needs backward computation.
I1127 11:25:25.976004  7741 net.cpp:226] ip2 needs backward computation.
I1127 11:25:25.976012  7741 net.cpp:226] relu1 needs backward computation.
I1127 11:25:25.976021  7741 net.cpp:226] ip1 needs backward computation.
I1127 11:25:25.976028  7741 net.cpp:226] pool2 needs backward computation.
I1127 11:25:25.976037  7741 net.cpp:226] conv2 needs backward computation.
I1127 11:25:25.976047  7741 net.cpp:226] pool1 needs backward computation.
I1127 11:25:25.976055  7741 net.cpp:226] conv1 needs backward computation.
I1127 11:25:25.976063  7741 net.cpp:228] mnist does not need backward computation.
I1127 11:25:25.976071  7741 net.cpp:270] This network produces output loss
I1127 11:25:25.976089  7741 net.cpp:283] Network initialization done.
I1127 11:25:25.976582  7741 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:25:25.976670  7741 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:25:25.977000  7741 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:25:25.977177  7741 layer_factory.hpp:76] Creating layer mnist
I1127 11:25:25.977466  7741 net.cpp:106] Creating Layer mnist
I1127 11:25:25.977486  7741 net.cpp:411] mnist -> data
I1127 11:25:25.977526  7741 net.cpp:411] mnist -> label
I1127 11:25:25.979729  7746 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:25:25.981170  7741 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:25:26.030320  7741 net.cpp:150] Setting up mnist
I1127 11:25:26.030446  7741 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:25:26.030468  7741 net.cpp:157] Top shape: 100 (100)
I1127 11:25:26.030481  7741 net.cpp:165] Memory required for data: 314000
I1127 11:25:26.030505  7741 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:25:26.030537  7741 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:25:26.030551  7741 net.cpp:454] label_mnist_1_split <- label
I1127 11:25:26.030575  7741 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:25:26.030612  7741 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:25:26.030745  7741 net.cpp:150] Setting up label_mnist_1_split
I1127 11:25:26.030761  7741 net.cpp:157] Top shape: 100 (100)
I1127 11:25:26.030776  7741 net.cpp:157] Top shape: 100 (100)
I1127 11:25:26.030787  7741 net.cpp:165] Memory required for data: 314800
I1127 11:25:26.030799  7741 layer_factory.hpp:76] Creating layer conv1
I1127 11:25:26.030832  7741 net.cpp:106] Creating Layer conv1
I1127 11:25:26.030848  7741 net.cpp:454] conv1 <- data
I1127 11:25:26.030865  7741 net.cpp:411] conv1 -> conv1
I1127 11:25:26.031296  7741 net.cpp:150] Setting up conv1
I1127 11:25:26.031348  7741 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:25:26.031365  7741 net.cpp:165] Memory required for data: 4922800
I1127 11:25:26.031407  7741 layer_factory.hpp:76] Creating layer pool1
I1127 11:25:26.031440  7741 net.cpp:106] Creating Layer pool1
I1127 11:25:26.031450  7741 net.cpp:454] pool1 <- conv1
I1127 11:25:26.031510  7741 net.cpp:411] pool1 -> pool1
I1127 11:25:26.031653  7741 net.cpp:150] Setting up pool1
I1127 11:25:26.031682  7741 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:25:26.031708  7741 net.cpp:165] Memory required for data: 6074800
I1127 11:25:26.031724  7741 layer_factory.hpp:76] Creating layer conv2
I1127 11:25:26.031755  7741 net.cpp:106] Creating Layer conv2
I1127 11:25:26.031765  7741 net.cpp:454] conv2 <- pool1
I1127 11:25:26.031783  7741 net.cpp:411] conv2 -> conv2
I1127 11:25:26.032415  7741 net.cpp:150] Setting up conv2
I1127 11:25:26.032483  7741 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:25:26.032495  7741 net.cpp:165] Memory required for data: 7354800
I1127 11:25:26.032527  7741 layer_factory.hpp:76] Creating layer pool2
I1127 11:25:26.032558  7741 net.cpp:106] Creating Layer pool2
I1127 11:25:26.032572  7741 net.cpp:454] pool2 <- conv2
I1127 11:25:26.032588  7741 net.cpp:411] pool2 -> pool2
I1127 11:25:26.032668  7741 net.cpp:150] Setting up pool2
I1127 11:25:26.032688  7741 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:25:26.032696  7741 net.cpp:165] Memory required for data: 7674800
I1127 11:25:26.032707  7741 layer_factory.hpp:76] Creating layer ip1
I1127 11:25:26.032732  7741 net.cpp:106] Creating Layer ip1
I1127 11:25:26.032743  7741 net.cpp:454] ip1 <- pool2
I1127 11:25:26.032755  7741 net.cpp:411] ip1 -> ip1
I1127 11:25:26.037562  7741 net.cpp:150] Setting up ip1
I1127 11:25:26.037619  7741 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:25:26.037627  7741 net.cpp:165] Memory required for data: 7874800
I1127 11:25:26.037655  7741 layer_factory.hpp:76] Creating layer relu1
I1127 11:25:26.037683  7741 net.cpp:106] Creating Layer relu1
I1127 11:25:26.037693  7741 net.cpp:454] relu1 <- ip1
I1127 11:25:26.037708  7741 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:25:26.037729  7741 net.cpp:150] Setting up relu1
I1127 11:25:26.037739  7741 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:25:26.037746  7741 net.cpp:165] Memory required for data: 8074800
I1127 11:25:26.037752  7741 layer_factory.hpp:76] Creating layer ip2
I1127 11:25:26.037776  7741 net.cpp:106] Creating Layer ip2
I1127 11:25:26.037783  7741 net.cpp:454] ip2 <- ip1
I1127 11:25:26.037793  7741 net.cpp:411] ip2 -> ip2
I1127 11:25:26.037978  7741 net.cpp:150] Setting up ip2
I1127 11:25:26.037994  7741 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:25:26.038000  7741 net.cpp:165] Memory required for data: 8078800
I1127 11:25:26.038012  7741 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:25:26.038022  7741 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:25:26.038029  7741 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:25:26.038041  7741 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:25:26.038053  7741 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:25:26.038100  7741 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:25:26.038112  7741 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:25:26.038120  7741 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:25:26.038126  7741 net.cpp:165] Memory required for data: 8086800
I1127 11:25:26.038133  7741 layer_factory.hpp:76] Creating layer accuracy
I1127 11:25:26.038157  7741 net.cpp:106] Creating Layer accuracy
I1127 11:25:26.038167  7741 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:25:26.038177  7741 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:25:26.038188  7741 net.cpp:411] accuracy -> accuracy
I1127 11:25:26.038206  7741 net.cpp:150] Setting up accuracy
I1127 11:25:26.038215  7741 net.cpp:157] Top shape: (1)
I1127 11:25:26.038223  7741 net.cpp:165] Memory required for data: 8086804
I1127 11:25:26.038230  7741 layer_factory.hpp:76] Creating layer loss
I1127 11:25:26.038241  7741 net.cpp:106] Creating Layer loss
I1127 11:25:26.038254  7741 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:25:26.038261  7741 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:25:26.038270  7741 net.cpp:411] loss -> loss
I1127 11:25:26.038287  7741 layer_factory.hpp:76] Creating layer loss
I1127 11:25:26.038442  7741 net.cpp:150] Setting up loss
I1127 11:25:26.038458  7741 net.cpp:157] Top shape: (1)
I1127 11:25:26.038465  7741 net.cpp:160]     with loss weight 1
I1127 11:25:26.038494  7741 net.cpp:165] Memory required for data: 8086808
I1127 11:25:26.038509  7741 net.cpp:226] loss needs backward computation.
I1127 11:25:26.038525  7741 net.cpp:228] accuracy does not need backward computation.
I1127 11:25:26.038533  7741 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:25:26.038540  7741 net.cpp:226] ip2 needs backward computation.
I1127 11:25:26.038548  7741 net.cpp:226] relu1 needs backward computation.
I1127 11:25:26.038595  7741 net.cpp:226] ip1 needs backward computation.
I1127 11:25:26.038605  7741 net.cpp:226] pool2 needs backward computation.
I1127 11:25:26.038614  7741 net.cpp:226] conv2 needs backward computation.
I1127 11:25:26.038622  7741 net.cpp:226] pool1 needs backward computation.
I1127 11:25:26.038630  7741 net.cpp:226] conv1 needs backward computation.
I1127 11:25:26.038637  7741 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:25:26.038647  7741 net.cpp:228] mnist does not need backward computation.
I1127 11:25:26.038653  7741 net.cpp:270] This network produces output accuracy
I1127 11:25:26.038661  7741 net.cpp:270] This network produces output loss
I1127 11:25:26.038698  7741 net.cpp:283] Network initialization done.
I1127 11:25:26.038830  7741 solver.cpp:59] Solver scaffolding done.
I1127 11:25:26.039204  7741 caffe.cpp:212] Starting Optimization
I1127 11:25:26.039222  7741 solver.cpp:287] Solving LeNet
I1127 11:25:26.039228  7741 solver.cpp:288] Learning Rate Policy: inv
I1127 11:25:26.040405  7741 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:25:29.121698  7741 solver.cpp:408]     Test net output #0: accuracy = 0.0827
I1127 11:25:29.121731  7741 solver.cpp:408]     Test net output #1: loss = 2.36294 (* 1 = 2.36294 loss)
I1127 11:25:29.146543  7741 solver.cpp:236] Iteration 0, loss = 2.29505
I1127 11:25:29.146564  7741 solver.cpp:252]     Train net output #0: loss = 2.29505 (* 1 = 2.29505 loss)
I1127 11:25:29.146576  7741 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:25:42.523766  7741 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:25:43.600443  7741 solver.cpp:408]     Test net output #0: accuracy = 0.9735
I1127 11:25:43.600494  7741 solver.cpp:408]     Test net output #1: loss = 0.083553 (* 1 = 0.083553 loss)
I1127 11:25:43.609843  7741 solver.cpp:236] Iteration 500, loss = 0.132011
I1127 11:25:43.609897  7741 solver.cpp:252]     Train net output #0: loss = 0.132011 (* 1 = 0.132011 loss)
I1127 11:25:43.609908  7741 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:25:57.101032  7741 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:25:57.123968  7741 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:25:57.137589  7741 solver.cpp:320] Iteration 1000, loss = 0.0869522
I1127 11:25:57.137676  7741 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:25:58.292964  7741 solver.cpp:408]     Test net output #0: accuracy = 0.9806
I1127 11:25:58.293087  7741 solver.cpp:408]     Test net output #1: loss = 0.0591195 (* 1 = 0.0591195 loss)
I1127 11:25:58.293100  7741 solver.cpp:325] Optimization Done.
I1127 11:25:58.293107  7741 caffe.cpp:215] Optimization Done.
I1127 11:25:58.415696  7768 caffe.cpp:184] Using GPUs 0
I1127 11:25:58.765276  7768 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:25:58.765527  7768 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:25:58.765926  7768 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:25:58.765949  7768 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:25:58.766059  7768 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:25:58.766187  7768 layer_factory.hpp:76] Creating layer mnist
I1127 11:25:58.766645  7768 net.cpp:106] Creating Layer mnist
I1127 11:25:58.766666  7768 net.cpp:411] mnist -> data
I1127 11:25:58.766715  7768 net.cpp:411] mnist -> label
I1127 11:25:58.767828  7773 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:25:58.791115  7768 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:25:58.797610  7768 net.cpp:150] Setting up mnist
I1127 11:25:58.797631  7768 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:25:58.797638  7768 net.cpp:157] Top shape: 64 (64)
I1127 11:25:58.797642  7768 net.cpp:165] Memory required for data: 200960
I1127 11:25:58.797652  7768 layer_factory.hpp:76] Creating layer conv1
I1127 11:25:58.797672  7768 net.cpp:106] Creating Layer conv1
I1127 11:25:58.797678  7768 net.cpp:454] conv1 <- data
I1127 11:25:58.797689  7768 net.cpp:411] conv1 -> conv1
I1127 11:25:58.798379  7768 net.cpp:150] Setting up conv1
I1127 11:25:58.798395  7768 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:25:58.798400  7768 net.cpp:165] Memory required for data: 3150080
I1127 11:25:58.798413  7768 layer_factory.hpp:76] Creating layer pool1
I1127 11:25:58.798424  7768 net.cpp:106] Creating Layer pool1
I1127 11:25:58.798429  7768 net.cpp:454] pool1 <- conv1
I1127 11:25:58.798435  7768 net.cpp:411] pool1 -> pool1
I1127 11:25:58.798481  7768 net.cpp:150] Setting up pool1
I1127 11:25:58.798490  7768 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:25:58.798493  7768 net.cpp:165] Memory required for data: 3887360
I1127 11:25:58.798498  7768 layer_factory.hpp:76] Creating layer conv2
I1127 11:25:58.798507  7768 net.cpp:106] Creating Layer conv2
I1127 11:25:58.798512  7768 net.cpp:454] conv2 <- pool1
I1127 11:25:58.798518  7768 net.cpp:411] conv2 -> conv2
I1127 11:25:58.798873  7768 net.cpp:150] Setting up conv2
I1127 11:25:58.798890  7768 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:25:58.798897  7768 net.cpp:165] Memory required for data: 4706560
I1127 11:25:58.798909  7768 layer_factory.hpp:76] Creating layer pool2
I1127 11:25:58.798918  7768 net.cpp:106] Creating Layer pool2
I1127 11:25:58.798928  7768 net.cpp:454] pool2 <- conv2
I1127 11:25:58.798935  7768 net.cpp:411] pool2 -> pool2
I1127 11:25:58.798966  7768 net.cpp:150] Setting up pool2
I1127 11:25:58.798974  7768 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:25:58.798977  7768 net.cpp:165] Memory required for data: 4911360
I1127 11:25:58.798982  7768 layer_factory.hpp:76] Creating layer ip1
I1127 11:25:58.798993  7768 net.cpp:106] Creating Layer ip1
I1127 11:25:58.798997  7768 net.cpp:454] ip1 <- pool2
I1127 11:25:58.799005  7768 net.cpp:411] ip1 -> ip1
I1127 11:25:58.801113  7768 net.cpp:150] Setting up ip1
I1127 11:25:58.801125  7768 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:25:58.801128  7768 net.cpp:165] Memory required for data: 5039360
I1127 11:25:58.801137  7768 layer_factory.hpp:76] Creating layer relu1
I1127 11:25:58.801146  7768 net.cpp:106] Creating Layer relu1
I1127 11:25:58.801149  7768 net.cpp:454] relu1 <- ip1
I1127 11:25:58.801156  7768 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:25:58.801163  7768 net.cpp:150] Setting up relu1
I1127 11:25:58.801169  7768 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:25:58.801173  7768 net.cpp:165] Memory required for data: 5167360
I1127 11:25:58.801177  7768 layer_factory.hpp:76] Creating layer ip2
I1127 11:25:58.801185  7768 net.cpp:106] Creating Layer ip2
I1127 11:25:58.801190  7768 net.cpp:454] ip2 <- ip1
I1127 11:25:58.801198  7768 net.cpp:411] ip2 -> ip2
I1127 11:25:58.801616  7768 net.cpp:150] Setting up ip2
I1127 11:25:58.801627  7768 net.cpp:157] Top shape: 64 10 (640)
I1127 11:25:58.801632  7768 net.cpp:165] Memory required for data: 5169920
I1127 11:25:58.801640  7768 layer_factory.hpp:76] Creating layer loss
I1127 11:25:58.801647  7768 net.cpp:106] Creating Layer loss
I1127 11:25:58.801651  7768 net.cpp:454] loss <- ip2
I1127 11:25:58.801656  7768 net.cpp:454] loss <- label
I1127 11:25:58.801666  7768 net.cpp:411] loss -> loss
I1127 11:25:58.801677  7768 layer_factory.hpp:76] Creating layer loss
I1127 11:25:58.801743  7768 net.cpp:150] Setting up loss
I1127 11:25:58.801750  7768 net.cpp:157] Top shape: (1)
I1127 11:25:58.801755  7768 net.cpp:160]     with loss weight 1
I1127 11:25:58.801770  7768 net.cpp:165] Memory required for data: 5169924
I1127 11:25:58.801775  7768 net.cpp:226] loss needs backward computation.
I1127 11:25:58.801779  7768 net.cpp:226] ip2 needs backward computation.
I1127 11:25:58.801784  7768 net.cpp:226] relu1 needs backward computation.
I1127 11:25:58.801789  7768 net.cpp:226] ip1 needs backward computation.
I1127 11:25:58.801792  7768 net.cpp:226] pool2 needs backward computation.
I1127 11:25:58.801796  7768 net.cpp:226] conv2 needs backward computation.
I1127 11:25:58.801800  7768 net.cpp:226] pool1 needs backward computation.
I1127 11:25:58.801805  7768 net.cpp:226] conv1 needs backward computation.
I1127 11:25:58.801810  7768 net.cpp:228] mnist does not need backward computation.
I1127 11:25:58.801813  7768 net.cpp:270] This network produces output loss
I1127 11:25:58.801821  7768 net.cpp:283] Network initialization done.
I1127 11:25:58.802059  7768 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:25:58.802083  7768 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:25:58.802202  7768 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:25:58.802266  7768 layer_factory.hpp:76] Creating layer mnist
I1127 11:25:58.802351  7768 net.cpp:106] Creating Layer mnist
I1127 11:25:58.802362  7768 net.cpp:411] mnist -> data
I1127 11:25:58.802371  7768 net.cpp:411] mnist -> label
I1127 11:25:58.803088  7776 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:25:58.803309  7768 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:25:58.887145  7768 net.cpp:150] Setting up mnist
I1127 11:25:58.887193  7768 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:25:58.887207  7768 net.cpp:157] Top shape: 100 (100)
I1127 11:25:58.887217  7768 net.cpp:165] Memory required for data: 314000
I1127 11:25:58.887228  7768 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:25:58.887251  7768 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:25:58.887261  7768 net.cpp:454] label_mnist_1_split <- label
I1127 11:25:58.887274  7768 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:25:58.887292  7768 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:25:58.887390  7768 net.cpp:150] Setting up label_mnist_1_split
I1127 11:25:58.887401  7768 net.cpp:157] Top shape: 100 (100)
I1127 11:25:58.887406  7768 net.cpp:157] Top shape: 100 (100)
I1127 11:25:58.887410  7768 net.cpp:165] Memory required for data: 314800
I1127 11:25:58.887415  7768 layer_factory.hpp:76] Creating layer conv1
I1127 11:25:58.887433  7768 net.cpp:106] Creating Layer conv1
I1127 11:25:58.887437  7768 net.cpp:454] conv1 <- data
I1127 11:25:58.887445  7768 net.cpp:411] conv1 -> conv1
I1127 11:25:58.887672  7768 net.cpp:150] Setting up conv1
I1127 11:25:58.887688  7768 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:25:58.887693  7768 net.cpp:165] Memory required for data: 4922800
I1127 11:25:58.887707  7768 layer_factory.hpp:76] Creating layer pool1
I1127 11:25:58.887722  7768 net.cpp:106] Creating Layer pool1
I1127 11:25:58.887730  7768 net.cpp:454] pool1 <- conv1
I1127 11:25:58.887768  7768 net.cpp:411] pool1 -> pool1
I1127 11:25:58.887820  7768 net.cpp:150] Setting up pool1
I1127 11:25:58.887830  7768 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:25:58.887835  7768 net.cpp:165] Memory required for data: 6074800
I1127 11:25:58.887840  7768 layer_factory.hpp:76] Creating layer conv2
I1127 11:25:58.887853  7768 net.cpp:106] Creating Layer conv2
I1127 11:25:58.887858  7768 net.cpp:454] conv2 <- pool1
I1127 11:25:58.887867  7768 net.cpp:411] conv2 -> conv2
I1127 11:25:58.888131  7768 net.cpp:150] Setting up conv2
I1127 11:25:58.888140  7768 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:25:58.888145  7768 net.cpp:165] Memory required for data: 7354800
I1127 11:25:58.888154  7768 layer_factory.hpp:76] Creating layer pool2
I1127 11:25:58.888165  7768 net.cpp:106] Creating Layer pool2
I1127 11:25:58.888170  7768 net.cpp:454] pool2 <- conv2
I1127 11:25:58.888176  7768 net.cpp:411] pool2 -> pool2
I1127 11:25:58.888205  7768 net.cpp:150] Setting up pool2
I1127 11:25:58.888212  7768 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:25:58.888216  7768 net.cpp:165] Memory required for data: 7674800
I1127 11:25:58.888221  7768 layer_factory.hpp:76] Creating layer ip1
I1127 11:25:58.888228  7768 net.cpp:106] Creating Layer ip1
I1127 11:25:58.888233  7768 net.cpp:454] ip1 <- pool2
I1127 11:25:58.888239  7768 net.cpp:411] ip1 -> ip1
I1127 11:25:58.890558  7768 net.cpp:150] Setting up ip1
I1127 11:25:58.890573  7768 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:25:58.890576  7768 net.cpp:165] Memory required for data: 7874800
I1127 11:25:58.890585  7768 layer_factory.hpp:76] Creating layer relu1
I1127 11:25:58.890594  7768 net.cpp:106] Creating Layer relu1
I1127 11:25:58.890599  7768 net.cpp:454] relu1 <- ip1
I1127 11:25:58.890604  7768 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:25:58.890611  7768 net.cpp:150] Setting up relu1
I1127 11:25:58.890616  7768 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:25:58.890620  7768 net.cpp:165] Memory required for data: 8074800
I1127 11:25:58.890625  7768 layer_factory.hpp:76] Creating layer ip2
I1127 11:25:58.890635  7768 net.cpp:106] Creating Layer ip2
I1127 11:25:58.890640  7768 net.cpp:454] ip2 <- ip1
I1127 11:25:58.890645  7768 net.cpp:411] ip2 -> ip2
I1127 11:25:58.890827  7768 net.cpp:150] Setting up ip2
I1127 11:25:58.890837  7768 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:25:58.890841  7768 net.cpp:165] Memory required for data: 8078800
I1127 11:25:58.890848  7768 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:25:58.890856  7768 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:25:58.890861  7768 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:25:58.890866  7768 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:25:58.890872  7768 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:25:58.890900  7768 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:25:58.890908  7768 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:25:58.890913  7768 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:25:58.890916  7768 net.cpp:165] Memory required for data: 8086800
I1127 11:25:58.890920  7768 layer_factory.hpp:76] Creating layer accuracy
I1127 11:25:58.890928  7768 net.cpp:106] Creating Layer accuracy
I1127 11:25:58.890933  7768 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:25:58.890938  7768 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:25:58.890944  7768 net.cpp:411] accuracy -> accuracy
I1127 11:25:58.890954  7768 net.cpp:150] Setting up accuracy
I1127 11:25:58.890959  7768 net.cpp:157] Top shape: (1)
I1127 11:25:58.890962  7768 net.cpp:165] Memory required for data: 8086804
I1127 11:25:58.890966  7768 layer_factory.hpp:76] Creating layer loss
I1127 11:25:58.890974  7768 net.cpp:106] Creating Layer loss
I1127 11:25:58.890977  7768 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:25:58.890982  7768 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:25:58.890990  7768 net.cpp:411] loss -> loss
I1127 11:25:58.890998  7768 layer_factory.hpp:76] Creating layer loss
I1127 11:25:58.891070  7768 net.cpp:150] Setting up loss
I1127 11:25:58.891078  7768 net.cpp:157] Top shape: (1)
I1127 11:25:58.891083  7768 net.cpp:160]     with loss weight 1
I1127 11:25:58.891094  7768 net.cpp:165] Memory required for data: 8086808
I1127 11:25:58.891098  7768 net.cpp:226] loss needs backward computation.
I1127 11:25:58.891108  7768 net.cpp:228] accuracy does not need backward computation.
I1127 11:25:58.891113  7768 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:25:58.891116  7768 net.cpp:226] ip2 needs backward computation.
I1127 11:25:58.891120  7768 net.cpp:226] relu1 needs backward computation.
I1127 11:25:58.891124  7768 net.cpp:226] ip1 needs backward computation.
I1127 11:25:58.891129  7768 net.cpp:226] pool2 needs backward computation.
I1127 11:25:58.891137  7768 net.cpp:226] conv2 needs backward computation.
I1127 11:25:58.891141  7768 net.cpp:226] pool1 needs backward computation.
I1127 11:25:58.891145  7768 net.cpp:226] conv1 needs backward computation.
I1127 11:25:58.891150  7768 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:25:58.891156  7768 net.cpp:228] mnist does not need backward computation.
I1127 11:25:58.891161  7768 net.cpp:270] This network produces output accuracy
I1127 11:25:58.891165  7768 net.cpp:270] This network produces output loss
I1127 11:25:58.891175  7768 net.cpp:283] Network initialization done.
I1127 11:25:58.891218  7768 solver.cpp:59] Solver scaffolding done.
I1127 11:25:58.891454  7768 caffe.cpp:212] Starting Optimization
I1127 11:25:58.891463  7768 solver.cpp:287] Solving LeNet
I1127 11:25:58.891466  7768 solver.cpp:288] Learning Rate Policy: inv
I1127 11:25:58.891830  7768 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:26:01.878657  7768 solver.cpp:408]     Test net output #0: accuracy = 0.0753
I1127 11:26:01.878754  7768 solver.cpp:408]     Test net output #1: loss = 2.39404 (* 1 = 2.39404 loss)
I1127 11:26:01.890390  7768 solver.cpp:236] Iteration 0, loss = 2.40595
I1127 11:26:01.890506  7768 solver.cpp:252]     Train net output #0: loss = 2.40595 (* 1 = 2.40595 loss)
I1127 11:26:01.890549  7768 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:26:14.715996  7768 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:26:16.445086  7768 solver.cpp:408]     Test net output #0: accuracy = 0.9733
I1127 11:26:16.445148  7768 solver.cpp:408]     Test net output #1: loss = 0.0854348 (* 1 = 0.0854348 loss)
I1127 11:26:16.456981  7768 solver.cpp:236] Iteration 500, loss = 0.124791
I1127 11:26:16.457064  7768 solver.cpp:252]     Train net output #0: loss = 0.124791 (* 1 = 0.124791 loss)
I1127 11:26:16.457082  7768 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:26:29.915735  7768 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:26:29.929225  7768 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:26:29.939342  7768 solver.cpp:320] Iteration 1000, loss = 0.0720311
I1127 11:26:29.939411  7768 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:26:31.958678  7768 solver.cpp:408]     Test net output #0: accuracy = 0.9791
I1127 11:26:31.958719  7768 solver.cpp:408]     Test net output #1: loss = 0.0608752 (* 1 = 0.0608752 loss)
I1127 11:26:31.958726  7768 solver.cpp:325] Optimization Done.
I1127 11:26:31.958731  7768 caffe.cpp:215] Optimization Done.
I1127 11:26:32.025673  7799 caffe.cpp:184] Using GPUs 0
I1127 11:26:32.432162  7799 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:26:32.432279  7799 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:26:32.432541  7799 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:26:32.432559  7799 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:26:32.432646  7799 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:26:32.432711  7799 layer_factory.hpp:76] Creating layer mnist
I1127 11:26:32.433029  7799 net.cpp:106] Creating Layer mnist
I1127 11:26:32.433043  7799 net.cpp:411] mnist -> data
I1127 11:26:32.433066  7799 net.cpp:411] mnist -> label
I1127 11:26:32.433830  7802 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:26:32.472878  7799 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:26:32.479182  7799 net.cpp:150] Setting up mnist
I1127 11:26:32.479203  7799 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:26:32.479210  7799 net.cpp:157] Top shape: 64 (64)
I1127 11:26:32.479214  7799 net.cpp:165] Memory required for data: 200960
I1127 11:26:32.479223  7799 layer_factory.hpp:76] Creating layer conv1
I1127 11:26:32.479240  7799 net.cpp:106] Creating Layer conv1
I1127 11:26:32.479248  7799 net.cpp:454] conv1 <- data
I1127 11:26:32.479260  7799 net.cpp:411] conv1 -> conv1
I1127 11:26:32.479893  7799 net.cpp:150] Setting up conv1
I1127 11:26:32.479909  7799 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:26:32.479914  7799 net.cpp:165] Memory required for data: 3150080
I1127 11:26:32.479928  7799 layer_factory.hpp:76] Creating layer pool1
I1127 11:26:32.479936  7799 net.cpp:106] Creating Layer pool1
I1127 11:26:32.479941  7799 net.cpp:454] pool1 <- conv1
I1127 11:26:32.479948  7799 net.cpp:411] pool1 -> pool1
I1127 11:26:32.479995  7799 net.cpp:150] Setting up pool1
I1127 11:26:32.480002  7799 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:26:32.480006  7799 net.cpp:165] Memory required for data: 3887360
I1127 11:26:32.480010  7799 layer_factory.hpp:76] Creating layer conv2
I1127 11:26:32.480020  7799 net.cpp:106] Creating Layer conv2
I1127 11:26:32.480026  7799 net.cpp:454] conv2 <- pool1
I1127 11:26:32.480031  7799 net.cpp:411] conv2 -> conv2
I1127 11:26:32.480402  7799 net.cpp:150] Setting up conv2
I1127 11:26:32.480412  7799 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:26:32.480427  7799 net.cpp:165] Memory required for data: 4706560
I1127 11:26:32.480437  7799 layer_factory.hpp:76] Creating layer pool2
I1127 11:26:32.480443  7799 net.cpp:106] Creating Layer pool2
I1127 11:26:32.480448  7799 net.cpp:454] pool2 <- conv2
I1127 11:26:32.480454  7799 net.cpp:411] pool2 -> pool2
I1127 11:26:32.480484  7799 net.cpp:150] Setting up pool2
I1127 11:26:32.480490  7799 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:26:32.480494  7799 net.cpp:165] Memory required for data: 4911360
I1127 11:26:32.480499  7799 layer_factory.hpp:76] Creating layer ip1
I1127 11:26:32.480509  7799 net.cpp:106] Creating Layer ip1
I1127 11:26:32.480512  7799 net.cpp:454] ip1 <- pool2
I1127 11:26:32.480518  7799 net.cpp:411] ip1 -> ip1
I1127 11:26:32.482656  7799 net.cpp:150] Setting up ip1
I1127 11:26:32.482668  7799 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:26:32.482673  7799 net.cpp:165] Memory required for data: 5039360
I1127 11:26:32.482682  7799 layer_factory.hpp:76] Creating layer relu1
I1127 11:26:32.482691  7799 net.cpp:106] Creating Layer relu1
I1127 11:26:32.482695  7799 net.cpp:454] relu1 <- ip1
I1127 11:26:32.482702  7799 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:26:32.482709  7799 net.cpp:150] Setting up relu1
I1127 11:26:32.482715  7799 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:26:32.482719  7799 net.cpp:165] Memory required for data: 5167360
I1127 11:26:32.482723  7799 layer_factory.hpp:76] Creating layer ip2
I1127 11:26:32.482731  7799 net.cpp:106] Creating Layer ip2
I1127 11:26:32.482736  7799 net.cpp:454] ip2 <- ip1
I1127 11:26:32.482743  7799 net.cpp:411] ip2 -> ip2
I1127 11:26:32.483144  7799 net.cpp:150] Setting up ip2
I1127 11:26:32.483153  7799 net.cpp:157] Top shape: 64 10 (640)
I1127 11:26:32.483158  7799 net.cpp:165] Memory required for data: 5169920
I1127 11:26:32.483165  7799 layer_factory.hpp:76] Creating layer loss
I1127 11:26:32.483175  7799 net.cpp:106] Creating Layer loss
I1127 11:26:32.483180  7799 net.cpp:454] loss <- ip2
I1127 11:26:32.483185  7799 net.cpp:454] loss <- label
I1127 11:26:32.483191  7799 net.cpp:411] loss -> loss
I1127 11:26:32.483202  7799 layer_factory.hpp:76] Creating layer loss
I1127 11:26:32.483268  7799 net.cpp:150] Setting up loss
I1127 11:26:32.483275  7799 net.cpp:157] Top shape: (1)
I1127 11:26:32.483279  7799 net.cpp:160]     with loss weight 1
I1127 11:26:32.483295  7799 net.cpp:165] Memory required for data: 5169924
I1127 11:26:32.483300  7799 net.cpp:226] loss needs backward computation.
I1127 11:26:32.483304  7799 net.cpp:226] ip2 needs backward computation.
I1127 11:26:32.483309  7799 net.cpp:226] relu1 needs backward computation.
I1127 11:26:32.483314  7799 net.cpp:226] ip1 needs backward computation.
I1127 11:26:32.483317  7799 net.cpp:226] pool2 needs backward computation.
I1127 11:26:32.483321  7799 net.cpp:226] conv2 needs backward computation.
I1127 11:26:32.483326  7799 net.cpp:226] pool1 needs backward computation.
I1127 11:26:32.483330  7799 net.cpp:226] conv1 needs backward computation.
I1127 11:26:32.483335  7799 net.cpp:228] mnist does not need backward computation.
I1127 11:26:32.483340  7799 net.cpp:270] This network produces output loss
I1127 11:26:32.483347  7799 net.cpp:283] Network initialization done.
I1127 11:26:32.483584  7799 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:26:32.483606  7799 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:26:32.483714  7799 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:26:32.483777  7799 layer_factory.hpp:76] Creating layer mnist
I1127 11:26:32.526248  7799 net.cpp:106] Creating Layer mnist
I1127 11:26:32.526290  7799 net.cpp:411] mnist -> data
I1127 11:26:32.526306  7799 net.cpp:411] mnist -> label
I1127 11:26:32.527127  7804 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:26:32.527271  7799 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:26:32.531261  7799 net.cpp:150] Setting up mnist
I1127 11:26:32.531277  7799 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:26:32.531285  7799 net.cpp:157] Top shape: 100 (100)
I1127 11:26:32.531288  7799 net.cpp:165] Memory required for data: 314000
I1127 11:26:32.531294  7799 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:26:32.531303  7799 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:26:32.531307  7799 net.cpp:454] label_mnist_1_split <- label
I1127 11:26:32.531314  7799 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:26:32.531323  7799 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:26:32.531358  7799 net.cpp:150] Setting up label_mnist_1_split
I1127 11:26:32.531366  7799 net.cpp:157] Top shape: 100 (100)
I1127 11:26:32.531371  7799 net.cpp:157] Top shape: 100 (100)
I1127 11:26:32.531375  7799 net.cpp:165] Memory required for data: 314800
I1127 11:26:32.531380  7799 layer_factory.hpp:76] Creating layer conv1
I1127 11:26:32.531393  7799 net.cpp:106] Creating Layer conv1
I1127 11:26:32.531397  7799 net.cpp:454] conv1 <- data
I1127 11:26:32.531406  7799 net.cpp:411] conv1 -> conv1
I1127 11:26:32.531564  7799 net.cpp:150] Setting up conv1
I1127 11:26:32.531574  7799 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:26:32.531577  7799 net.cpp:165] Memory required for data: 4922800
I1127 11:26:32.531587  7799 layer_factory.hpp:76] Creating layer pool1
I1127 11:26:32.531596  7799 net.cpp:106] Creating Layer pool1
I1127 11:26:32.531601  7799 net.cpp:454] pool1 <- conv1
I1127 11:26:32.531617  7799 net.cpp:411] pool1 -> pool1
I1127 11:26:32.531646  7799 net.cpp:150] Setting up pool1
I1127 11:26:32.531653  7799 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:26:32.531657  7799 net.cpp:165] Memory required for data: 6074800
I1127 11:26:32.531662  7799 layer_factory.hpp:76] Creating layer conv2
I1127 11:26:32.531672  7799 net.cpp:106] Creating Layer conv2
I1127 11:26:32.531677  7799 net.cpp:454] conv2 <- pool1
I1127 11:26:32.531683  7799 net.cpp:411] conv2 -> conv2
I1127 11:26:32.531935  7799 net.cpp:150] Setting up conv2
I1127 11:26:32.531944  7799 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:26:32.531947  7799 net.cpp:165] Memory required for data: 7354800
I1127 11:26:32.531956  7799 layer_factory.hpp:76] Creating layer pool2
I1127 11:26:32.531965  7799 net.cpp:106] Creating Layer pool2
I1127 11:26:32.531968  7799 net.cpp:454] pool2 <- conv2
I1127 11:26:32.531975  7799 net.cpp:411] pool2 -> pool2
I1127 11:26:32.531999  7799 net.cpp:150] Setting up pool2
I1127 11:26:32.532006  7799 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:26:32.532011  7799 net.cpp:165] Memory required for data: 7674800
I1127 11:26:32.532016  7799 layer_factory.hpp:76] Creating layer ip1
I1127 11:26:32.532024  7799 net.cpp:106] Creating Layer ip1
I1127 11:26:32.532032  7799 net.cpp:454] ip1 <- pool2
I1127 11:26:32.532040  7799 net.cpp:411] ip1 -> ip1
I1127 11:26:32.534694  7799 net.cpp:150] Setting up ip1
I1127 11:26:32.534715  7799 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:26:32.534720  7799 net.cpp:165] Memory required for data: 7874800
I1127 11:26:32.534731  7799 layer_factory.hpp:76] Creating layer relu1
I1127 11:26:32.534740  7799 net.cpp:106] Creating Layer relu1
I1127 11:26:32.534745  7799 net.cpp:454] relu1 <- ip1
I1127 11:26:32.534751  7799 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:26:32.534759  7799 net.cpp:150] Setting up relu1
I1127 11:26:32.534765  7799 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:26:32.534770  7799 net.cpp:165] Memory required for data: 8074800
I1127 11:26:32.534773  7799 layer_factory.hpp:76] Creating layer ip2
I1127 11:26:32.534783  7799 net.cpp:106] Creating Layer ip2
I1127 11:26:32.534788  7799 net.cpp:454] ip2 <- ip1
I1127 11:26:32.534795  7799 net.cpp:411] ip2 -> ip2
I1127 11:26:32.534891  7799 net.cpp:150] Setting up ip2
I1127 11:26:32.534899  7799 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:26:32.534904  7799 net.cpp:165] Memory required for data: 8078800
I1127 11:26:32.534910  7799 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:26:32.534916  7799 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:26:32.534920  7799 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:26:32.534927  7799 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:26:32.534934  7799 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:26:32.534960  7799 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:26:32.534967  7799 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:26:32.534972  7799 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:26:32.534976  7799 net.cpp:165] Memory required for data: 8086800
I1127 11:26:32.534981  7799 layer_factory.hpp:76] Creating layer accuracy
I1127 11:26:32.534989  7799 net.cpp:106] Creating Layer accuracy
I1127 11:26:32.534993  7799 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:26:32.534998  7799 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:26:32.535006  7799 net.cpp:411] accuracy -> accuracy
I1127 11:26:32.535014  7799 net.cpp:150] Setting up accuracy
I1127 11:26:32.535020  7799 net.cpp:157] Top shape: (1)
I1127 11:26:32.535024  7799 net.cpp:165] Memory required for data: 8086804
I1127 11:26:32.535028  7799 layer_factory.hpp:76] Creating layer loss
I1127 11:26:32.535034  7799 net.cpp:106] Creating Layer loss
I1127 11:26:32.535038  7799 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:26:32.535043  7799 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:26:32.535049  7799 net.cpp:411] loss -> loss
I1127 11:26:32.535058  7799 layer_factory.hpp:76] Creating layer loss
I1127 11:26:32.535128  7799 net.cpp:150] Setting up loss
I1127 11:26:32.535135  7799 net.cpp:157] Top shape: (1)
I1127 11:26:32.535140  7799 net.cpp:160]     with loss weight 1
I1127 11:26:32.535151  7799 net.cpp:165] Memory required for data: 8086808
I1127 11:26:32.535154  7799 net.cpp:226] loss needs backward computation.
I1127 11:26:32.535162  7799 net.cpp:228] accuracy does not need backward computation.
I1127 11:26:32.535167  7799 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:26:32.535171  7799 net.cpp:226] ip2 needs backward computation.
I1127 11:26:32.535176  7799 net.cpp:226] relu1 needs backward computation.
I1127 11:26:32.535179  7799 net.cpp:226] ip1 needs backward computation.
I1127 11:26:32.535184  7799 net.cpp:226] pool2 needs backward computation.
I1127 11:26:32.535188  7799 net.cpp:226] conv2 needs backward computation.
I1127 11:26:32.535192  7799 net.cpp:226] pool1 needs backward computation.
I1127 11:26:32.535197  7799 net.cpp:226] conv1 needs backward computation.
I1127 11:26:32.535202  7799 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:26:32.535207  7799 net.cpp:228] mnist does not need backward computation.
I1127 11:26:32.535210  7799 net.cpp:270] This network produces output accuracy
I1127 11:26:32.535214  7799 net.cpp:270] This network produces output loss
I1127 11:26:32.535229  7799 net.cpp:283] Network initialization done.
I1127 11:26:32.535270  7799 solver.cpp:59] Solver scaffolding done.
I1127 11:26:32.535457  7799 caffe.cpp:212] Starting Optimization
I1127 11:26:32.535465  7799 solver.cpp:287] Solving LeNet
I1127 11:26:32.535468  7799 solver.cpp:288] Learning Rate Policy: inv
I1127 11:26:32.535794  7799 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:26:34.160643  7799 solver.cpp:408]     Test net output #0: accuracy = 0.0942
I1127 11:26:34.160691  7799 solver.cpp:408]     Test net output #1: loss = 2.43283 (* 1 = 2.43283 loss)
I1127 11:26:34.175379  7799 solver.cpp:236] Iteration 0, loss = 2.49913
I1127 11:26:34.175530  7799 solver.cpp:252]     Train net output #0: loss = 2.49913 (* 1 = 2.49913 loss)
I1127 11:26:34.175590  7799 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:26:46.426234  7799 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:26:48.794955  7799 solver.cpp:408]     Test net output #0: accuracy = 0.9722
I1127 11:26:48.795008  7799 solver.cpp:408]     Test net output #1: loss = 0.0922367 (* 1 = 0.0922367 loss)
I1127 11:26:48.807340  7799 solver.cpp:236] Iteration 500, loss = 0.0921528
I1127 11:26:48.807456  7799 solver.cpp:252]     Train net output #0: loss = 0.092153 (* 1 = 0.092153 loss)
I1127 11:26:48.807497  7799 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:27:02.215144  7799 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:27:02.229199  7799 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:27:02.239042  7799 solver.cpp:320] Iteration 1000, loss = 0.0740987
I1127 11:27:02.239084  7799 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:27:03.564704  7799 solver.cpp:408]     Test net output #0: accuracy = 0.9797
I1127 11:27:03.564745  7799 solver.cpp:408]     Test net output #1: loss = 0.0602982 (* 1 = 0.0602982 loss)
I1127 11:27:03.564752  7799 solver.cpp:325] Optimization Done.
I1127 11:27:03.564757  7799 caffe.cpp:215] Optimization Done.
I1127 11:27:03.630317  7827 caffe.cpp:184] Using GPUs 0
I1127 11:27:03.944456  7827 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:27:03.944566  7827 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:27:03.944807  7827 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:27:03.944821  7827 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:27:03.944902  7827 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:27:03.944958  7827 layer_factory.hpp:76] Creating layer mnist
I1127 11:27:03.990247  7827 net.cpp:106] Creating Layer mnist
I1127 11:27:03.990288  7827 net.cpp:411] mnist -> data
I1127 11:27:03.990319  7827 net.cpp:411] mnist -> label
I1127 11:27:03.991104  7830 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:27:04.025146  7827 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:27:04.031437  7827 net.cpp:150] Setting up mnist
I1127 11:27:04.031460  7827 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:27:04.031466  7827 net.cpp:157] Top shape: 64 (64)
I1127 11:27:04.031471  7827 net.cpp:165] Memory required for data: 200960
I1127 11:27:04.031479  7827 layer_factory.hpp:76] Creating layer conv1
I1127 11:27:04.031494  7827 net.cpp:106] Creating Layer conv1
I1127 11:27:04.031500  7827 net.cpp:454] conv1 <- data
I1127 11:27:04.031512  7827 net.cpp:411] conv1 -> conv1
I1127 11:27:04.032153  7827 net.cpp:150] Setting up conv1
I1127 11:27:04.032169  7827 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:27:04.032174  7827 net.cpp:165] Memory required for data: 3150080
I1127 11:27:04.032187  7827 layer_factory.hpp:76] Creating layer pool1
I1127 11:27:04.032197  7827 net.cpp:106] Creating Layer pool1
I1127 11:27:04.032202  7827 net.cpp:454] pool1 <- conv1
I1127 11:27:04.032208  7827 net.cpp:411] pool1 -> pool1
I1127 11:27:04.032256  7827 net.cpp:150] Setting up pool1
I1127 11:27:04.032264  7827 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:27:04.032269  7827 net.cpp:165] Memory required for data: 3887360
I1127 11:27:04.032274  7827 layer_factory.hpp:76] Creating layer conv2
I1127 11:27:04.032281  7827 net.cpp:106] Creating Layer conv2
I1127 11:27:04.032286  7827 net.cpp:454] conv2 <- pool1
I1127 11:27:04.032294  7827 net.cpp:411] conv2 -> conv2
I1127 11:27:04.032703  7827 net.cpp:150] Setting up conv2
I1127 11:27:04.032719  7827 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:27:04.032724  7827 net.cpp:165] Memory required for data: 4706560
I1127 11:27:04.032733  7827 layer_factory.hpp:76] Creating layer pool2
I1127 11:27:04.032743  7827 net.cpp:106] Creating Layer pool2
I1127 11:27:04.032749  7827 net.cpp:454] pool2 <- conv2
I1127 11:27:04.032755  7827 net.cpp:411] pool2 -> pool2
I1127 11:27:04.032784  7827 net.cpp:150] Setting up pool2
I1127 11:27:04.032791  7827 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:27:04.032796  7827 net.cpp:165] Memory required for data: 4911360
I1127 11:27:04.032800  7827 layer_factory.hpp:76] Creating layer ip1
I1127 11:27:04.032810  7827 net.cpp:106] Creating Layer ip1
I1127 11:27:04.032815  7827 net.cpp:454] ip1 <- pool2
I1127 11:27:04.032821  7827 net.cpp:411] ip1 -> ip1
I1127 11:27:04.035022  7827 net.cpp:150] Setting up ip1
I1127 11:27:04.035034  7827 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:27:04.035039  7827 net.cpp:165] Memory required for data: 5039360
I1127 11:27:04.035048  7827 layer_factory.hpp:76] Creating layer relu1
I1127 11:27:04.035058  7827 net.cpp:106] Creating Layer relu1
I1127 11:27:04.035063  7827 net.cpp:454] relu1 <- ip1
I1127 11:27:04.035068  7827 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:27:04.035076  7827 net.cpp:150] Setting up relu1
I1127 11:27:04.035082  7827 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:27:04.035090  7827 net.cpp:165] Memory required for data: 5167360
I1127 11:27:04.035094  7827 layer_factory.hpp:76] Creating layer ip2
I1127 11:27:04.035104  7827 net.cpp:106] Creating Layer ip2
I1127 11:27:04.035109  7827 net.cpp:454] ip2 <- ip1
I1127 11:27:04.035115  7827 net.cpp:411] ip2 -> ip2
I1127 11:27:04.035507  7827 net.cpp:150] Setting up ip2
I1127 11:27:04.035517  7827 net.cpp:157] Top shape: 64 10 (640)
I1127 11:27:04.035521  7827 net.cpp:165] Memory required for data: 5169920
I1127 11:27:04.035528  7827 layer_factory.hpp:76] Creating layer loss
I1127 11:27:04.035537  7827 net.cpp:106] Creating Layer loss
I1127 11:27:04.035542  7827 net.cpp:454] loss <- ip2
I1127 11:27:04.035547  7827 net.cpp:454] loss <- label
I1127 11:27:04.035554  7827 net.cpp:411] loss -> loss
I1127 11:27:04.035567  7827 layer_factory.hpp:76] Creating layer loss
I1127 11:27:04.035631  7827 net.cpp:150] Setting up loss
I1127 11:27:04.035639  7827 net.cpp:157] Top shape: (1)
I1127 11:27:04.035642  7827 net.cpp:160]     with loss weight 1
I1127 11:27:04.035657  7827 net.cpp:165] Memory required for data: 5169924
I1127 11:27:04.035661  7827 net.cpp:226] loss needs backward computation.
I1127 11:27:04.035666  7827 net.cpp:226] ip2 needs backward computation.
I1127 11:27:04.035670  7827 net.cpp:226] relu1 needs backward computation.
I1127 11:27:04.035675  7827 net.cpp:226] ip1 needs backward computation.
I1127 11:27:04.035679  7827 net.cpp:226] pool2 needs backward computation.
I1127 11:27:04.035683  7827 net.cpp:226] conv2 needs backward computation.
I1127 11:27:04.035687  7827 net.cpp:226] pool1 needs backward computation.
I1127 11:27:04.035691  7827 net.cpp:226] conv1 needs backward computation.
I1127 11:27:04.035696  7827 net.cpp:228] mnist does not need backward computation.
I1127 11:27:04.035701  7827 net.cpp:270] This network produces output loss
I1127 11:27:04.035708  7827 net.cpp:283] Network initialization done.
I1127 11:27:04.035946  7827 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:27:04.035969  7827 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:27:04.036087  7827 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:27:04.036151  7827 layer_factory.hpp:76] Creating layer mnist
I1127 11:27:04.036239  7827 net.cpp:106] Creating Layer mnist
I1127 11:27:04.036249  7827 net.cpp:411] mnist -> data
I1127 11:27:04.036258  7827 net.cpp:411] mnist -> label
I1127 11:27:04.036973  7832 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:27:04.037086  7827 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:27:04.040328  7827 net.cpp:150] Setting up mnist
I1127 11:27:04.040341  7827 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:27:04.040348  7827 net.cpp:157] Top shape: 100 (100)
I1127 11:27:04.040351  7827 net.cpp:165] Memory required for data: 314000
I1127 11:27:04.040356  7827 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:27:04.040367  7827 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:27:04.040372  7827 net.cpp:454] label_mnist_1_split <- label
I1127 11:27:04.040379  7827 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:27:04.040386  7827 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:27:04.040432  7827 net.cpp:150] Setting up label_mnist_1_split
I1127 11:27:04.040441  7827 net.cpp:157] Top shape: 100 (100)
I1127 11:27:04.040446  7827 net.cpp:157] Top shape: 100 (100)
I1127 11:27:04.040451  7827 net.cpp:165] Memory required for data: 314800
I1127 11:27:04.040455  7827 layer_factory.hpp:76] Creating layer conv1
I1127 11:27:04.040465  7827 net.cpp:106] Creating Layer conv1
I1127 11:27:04.040470  7827 net.cpp:454] conv1 <- data
I1127 11:27:04.040479  7827 net.cpp:411] conv1 -> conv1
I1127 11:27:04.040621  7827 net.cpp:150] Setting up conv1
I1127 11:27:04.040629  7827 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:27:04.040633  7827 net.cpp:165] Memory required for data: 4922800
I1127 11:27:04.040643  7827 layer_factory.hpp:76] Creating layer pool1
I1127 11:27:04.040650  7827 net.cpp:106] Creating Layer pool1
I1127 11:27:04.040655  7827 net.cpp:454] pool1 <- conv1
I1127 11:27:04.040669  7827 net.cpp:411] pool1 -> pool1
I1127 11:27:04.040698  7827 net.cpp:150] Setting up pool1
I1127 11:27:04.040705  7827 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:27:04.040709  7827 net.cpp:165] Memory required for data: 6074800
I1127 11:27:04.040714  7827 layer_factory.hpp:76] Creating layer conv2
I1127 11:27:04.040722  7827 net.cpp:106] Creating Layer conv2
I1127 11:27:04.040727  7827 net.cpp:454] conv2 <- pool1
I1127 11:27:04.040734  7827 net.cpp:411] conv2 -> conv2
I1127 11:27:04.040976  7827 net.cpp:150] Setting up conv2
I1127 11:27:04.040984  7827 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:27:04.040988  7827 net.cpp:165] Memory required for data: 7354800
I1127 11:27:04.040997  7827 layer_factory.hpp:76] Creating layer pool2
I1127 11:27:04.041003  7827 net.cpp:106] Creating Layer pool2
I1127 11:27:04.041007  7827 net.cpp:454] pool2 <- conv2
I1127 11:27:04.041015  7827 net.cpp:411] pool2 -> pool2
I1127 11:27:04.041041  7827 net.cpp:150] Setting up pool2
I1127 11:27:04.041049  7827 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:27:04.041054  7827 net.cpp:165] Memory required for data: 7674800
I1127 11:27:04.041057  7827 layer_factory.hpp:76] Creating layer ip1
I1127 11:27:04.041065  7827 net.cpp:106] Creating Layer ip1
I1127 11:27:04.041070  7827 net.cpp:454] ip1 <- pool2
I1127 11:27:04.041077  7827 net.cpp:411] ip1 -> ip1
I1127 11:27:04.043263  7827 net.cpp:150] Setting up ip1
I1127 11:27:04.043277  7827 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:27:04.043280  7827 net.cpp:165] Memory required for data: 7874800
I1127 11:27:04.043289  7827 layer_factory.hpp:76] Creating layer relu1
I1127 11:27:04.043298  7827 net.cpp:106] Creating Layer relu1
I1127 11:27:04.043303  7827 net.cpp:454] relu1 <- ip1
I1127 11:27:04.043309  7827 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:27:04.043320  7827 net.cpp:150] Setting up relu1
I1127 11:27:04.043326  7827 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:27:04.043330  7827 net.cpp:165] Memory required for data: 8074800
I1127 11:27:04.043334  7827 layer_factory.hpp:76] Creating layer ip2
I1127 11:27:04.043344  7827 net.cpp:106] Creating Layer ip2
I1127 11:27:04.043349  7827 net.cpp:454] ip2 <- ip1
I1127 11:27:04.043354  7827 net.cpp:411] ip2 -> ip2
I1127 11:27:04.043443  7827 net.cpp:150] Setting up ip2
I1127 11:27:04.043452  7827 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:27:04.043455  7827 net.cpp:165] Memory required for data: 8078800
I1127 11:27:04.043462  7827 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:27:04.043467  7827 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:27:04.043473  7827 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:27:04.043478  7827 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:27:04.043485  7827 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:27:04.043511  7827 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:27:04.043519  7827 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:27:04.043524  7827 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:27:04.043527  7827 net.cpp:165] Memory required for data: 8086800
I1127 11:27:04.043531  7827 layer_factory.hpp:76] Creating layer accuracy
I1127 11:27:04.043540  7827 net.cpp:106] Creating Layer accuracy
I1127 11:27:04.043545  7827 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:27:04.043550  7827 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:27:04.043556  7827 net.cpp:411] accuracy -> accuracy
I1127 11:27:04.043565  7827 net.cpp:150] Setting up accuracy
I1127 11:27:04.043571  7827 net.cpp:157] Top shape: (1)
I1127 11:27:04.043576  7827 net.cpp:165] Memory required for data: 8086804
I1127 11:27:04.043579  7827 layer_factory.hpp:76] Creating layer loss
I1127 11:27:04.043584  7827 net.cpp:106] Creating Layer loss
I1127 11:27:04.043589  7827 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:27:04.043594  7827 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:27:04.043599  7827 net.cpp:411] loss -> loss
I1127 11:27:04.043607  7827 layer_factory.hpp:76] Creating layer loss
I1127 11:27:04.043676  7827 net.cpp:150] Setting up loss
I1127 11:27:04.043684  7827 net.cpp:157] Top shape: (1)
I1127 11:27:04.043687  7827 net.cpp:160]     with loss weight 1
I1127 11:27:04.043696  7827 net.cpp:165] Memory required for data: 8086808
I1127 11:27:04.043701  7827 net.cpp:226] loss needs backward computation.
I1127 11:27:04.043707  7827 net.cpp:228] accuracy does not need backward computation.
I1127 11:27:04.043712  7827 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:27:04.043716  7827 net.cpp:226] ip2 needs backward computation.
I1127 11:27:04.043720  7827 net.cpp:226] relu1 needs backward computation.
I1127 11:27:04.043725  7827 net.cpp:226] ip1 needs backward computation.
I1127 11:27:04.043730  7827 net.cpp:226] pool2 needs backward computation.
I1127 11:27:04.043733  7827 net.cpp:226] conv2 needs backward computation.
I1127 11:27:04.043743  7827 net.cpp:226] pool1 needs backward computation.
I1127 11:27:04.043748  7827 net.cpp:226] conv1 needs backward computation.
I1127 11:27:04.043753  7827 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:27:04.043757  7827 net.cpp:228] mnist does not need backward computation.
I1127 11:27:04.043761  7827 net.cpp:270] This network produces output accuracy
I1127 11:27:04.043766  7827 net.cpp:270] This network produces output loss
I1127 11:27:04.043777  7827 net.cpp:283] Network initialization done.
I1127 11:27:04.043809  7827 solver.cpp:59] Solver scaffolding done.
I1127 11:27:04.043992  7827 caffe.cpp:212] Starting Optimization
I1127 11:27:04.043998  7827 solver.cpp:287] Solving LeNet
I1127 11:27:04.044003  7827 solver.cpp:288] Learning Rate Policy: inv
I1127 11:27:04.044307  7827 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:27:06.480577  7827 solver.cpp:408]     Test net output #0: accuracy = 0.0642
I1127 11:27:06.480702  7827 solver.cpp:408]     Test net output #1: loss = 2.34895 (* 1 = 2.34895 loss)
I1127 11:27:06.496554  7827 solver.cpp:236] Iteration 0, loss = 2.38322
I1127 11:27:06.496668  7827 solver.cpp:252]     Train net output #0: loss = 2.38322 (* 1 = 2.38322 loss)
I1127 11:27:06.496723  7827 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:27:18.007128  7827 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:27:21.142880  7827 solver.cpp:408]     Test net output #0: accuracy = 0.9719
I1127 11:27:21.142994  7827 solver.cpp:408]     Test net output #1: loss = 0.0864804 (* 1 = 0.0864804 loss)
I1127 11:27:21.152462  7827 solver.cpp:236] Iteration 500, loss = 0.0815196
I1127 11:27:21.152539  7827 solver.cpp:252]     Train net output #0: loss = 0.0815195 (* 1 = 0.0815195 loss)
I1127 11:27:21.152554  7827 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:27:33.678725  7827 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:27:33.698918  7827 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:27:33.726174  7827 solver.cpp:320] Iteration 1000, loss = 0.0731162
I1127 11:27:33.726210  7827 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:27:35.685833  7827 solver.cpp:408]     Test net output #0: accuracy = 0.9815
I1127 11:27:35.685979  7827 solver.cpp:408]     Test net output #1: loss = 0.0569196 (* 1 = 0.0569196 loss)
I1127 11:27:35.686013  7827 solver.cpp:325] Optimization Done.
I1127 11:27:35.686033  7827 caffe.cpp:215] Optimization Done.
I1127 11:27:35.821563  7854 caffe.cpp:184] Using GPUs 0
I1127 11:27:36.101714  7854 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:27:36.101917  7854 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:27:36.102512  7854 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:27:36.102567  7854 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:27:36.102766  7854 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:27:36.102872  7854 layer_factory.hpp:76] Creating layer mnist
I1127 11:27:36.103487  7854 net.cpp:106] Creating Layer mnist
I1127 11:27:36.103541  7854 net.cpp:411] mnist -> data
I1127 11:27:36.103586  7854 net.cpp:411] mnist -> label
I1127 11:27:36.104938  7857 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:27:36.117444  7854 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:27:36.119295  7854 net.cpp:150] Setting up mnist
I1127 11:27:36.119441  7854 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:27:36.119467  7854 net.cpp:157] Top shape: 64 (64)
I1127 11:27:36.119477  7854 net.cpp:165] Memory required for data: 200960
I1127 11:27:36.119499  7854 layer_factory.hpp:76] Creating layer conv1
I1127 11:27:36.119552  7854 net.cpp:106] Creating Layer conv1
I1127 11:27:36.119577  7854 net.cpp:454] conv1 <- data
I1127 11:27:36.119608  7854 net.cpp:411] conv1 -> conv1
I1127 11:27:36.120607  7854 net.cpp:150] Setting up conv1
I1127 11:27:36.120643  7854 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:27:36.120650  7854 net.cpp:165] Memory required for data: 3150080
I1127 11:27:36.120666  7854 layer_factory.hpp:76] Creating layer pool1
I1127 11:27:36.120682  7854 net.cpp:106] Creating Layer pool1
I1127 11:27:36.120688  7854 net.cpp:454] pool1 <- conv1
I1127 11:27:36.120695  7854 net.cpp:411] pool1 -> pool1
I1127 11:27:36.120818  7854 net.cpp:150] Setting up pool1
I1127 11:27:36.120828  7854 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:27:36.120833  7854 net.cpp:165] Memory required for data: 3887360
I1127 11:27:36.120841  7854 layer_factory.hpp:76] Creating layer conv2
I1127 11:27:36.120857  7854 net.cpp:106] Creating Layer conv2
I1127 11:27:36.120864  7854 net.cpp:454] conv2 <- pool1
I1127 11:27:36.120872  7854 net.cpp:411] conv2 -> conv2
I1127 11:27:36.121155  7854 net.cpp:150] Setting up conv2
I1127 11:27:36.121172  7854 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:27:36.121177  7854 net.cpp:165] Memory required for data: 4706560
I1127 11:27:36.121189  7854 layer_factory.hpp:76] Creating layer pool2
I1127 11:27:36.121198  7854 net.cpp:106] Creating Layer pool2
I1127 11:27:36.121203  7854 net.cpp:454] pool2 <- conv2
I1127 11:27:36.121209  7854 net.cpp:411] pool2 -> pool2
I1127 11:27:36.121240  7854 net.cpp:150] Setting up pool2
I1127 11:27:36.121248  7854 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:27:36.121253  7854 net.cpp:165] Memory required for data: 4911360
I1127 11:27:36.121256  7854 layer_factory.hpp:76] Creating layer ip1
I1127 11:27:36.121268  7854 net.cpp:106] Creating Layer ip1
I1127 11:27:36.121273  7854 net.cpp:454] ip1 <- pool2
I1127 11:27:36.121279  7854 net.cpp:411] ip1 -> ip1
I1127 11:27:36.124367  7854 net.cpp:150] Setting up ip1
I1127 11:27:36.124456  7854 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:27:36.124467  7854 net.cpp:165] Memory required for data: 5039360
I1127 11:27:36.124500  7854 layer_factory.hpp:76] Creating layer relu1
I1127 11:27:36.124534  7854 net.cpp:106] Creating Layer relu1
I1127 11:27:36.124548  7854 net.cpp:454] relu1 <- ip1
I1127 11:27:36.124565  7854 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:27:36.124588  7854 net.cpp:150] Setting up relu1
I1127 11:27:36.124598  7854 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:27:36.124603  7854 net.cpp:165] Memory required for data: 5167360
I1127 11:27:36.124611  7854 layer_factory.hpp:76] Creating layer ip2
I1127 11:27:36.124629  7854 net.cpp:106] Creating Layer ip2
I1127 11:27:36.124637  7854 net.cpp:454] ip2 <- ip1
I1127 11:27:36.124650  7854 net.cpp:411] ip2 -> ip2
I1127 11:27:36.125723  7854 net.cpp:150] Setting up ip2
I1127 11:27:36.125813  7854 net.cpp:157] Top shape: 64 10 (640)
I1127 11:27:36.125823  7854 net.cpp:165] Memory required for data: 5169920
I1127 11:27:36.125844  7854 layer_factory.hpp:76] Creating layer loss
I1127 11:27:36.125885  7854 net.cpp:106] Creating Layer loss
I1127 11:27:36.125895  7854 net.cpp:454] loss <- ip2
I1127 11:27:36.125906  7854 net.cpp:454] loss <- label
I1127 11:27:36.125924  7854 net.cpp:411] loss -> loss
I1127 11:27:36.125957  7854 layer_factory.hpp:76] Creating layer loss
I1127 11:27:36.126134  7854 net.cpp:150] Setting up loss
I1127 11:27:36.126163  7854 net.cpp:157] Top shape: (1)
I1127 11:27:36.126173  7854 net.cpp:160]     with loss weight 1
I1127 11:27:36.126230  7854 net.cpp:165] Memory required for data: 5169924
I1127 11:27:36.126247  7854 net.cpp:226] loss needs backward computation.
I1127 11:27:36.126261  7854 net.cpp:226] ip2 needs backward computation.
I1127 11:27:36.126277  7854 net.cpp:226] relu1 needs backward computation.
I1127 11:27:36.126292  7854 net.cpp:226] ip1 needs backward computation.
I1127 11:27:36.126308  7854 net.cpp:226] pool2 needs backward computation.
I1127 11:27:36.126333  7854 net.cpp:226] conv2 needs backward computation.
I1127 11:27:36.126348  7854 net.cpp:226] pool1 needs backward computation.
I1127 11:27:36.126360  7854 net.cpp:226] conv1 needs backward computation.
I1127 11:27:36.126376  7854 net.cpp:228] mnist does not need backward computation.
I1127 11:27:36.126391  7854 net.cpp:270] This network produces output loss
I1127 11:27:36.126408  7854 net.cpp:283] Network initialization done.
I1127 11:27:36.126931  7854 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:27:36.126978  7854 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:27:36.127172  7854 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:27:36.127316  7854 layer_factory.hpp:76] Creating layer mnist
I1127 11:27:36.170295  7854 net.cpp:106] Creating Layer mnist
I1127 11:27:36.170367  7854 net.cpp:411] mnist -> data
I1127 11:27:36.170390  7854 net.cpp:411] mnist -> label
I1127 11:27:36.171578  7859 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:27:36.171798  7854 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:27:36.173555  7854 net.cpp:150] Setting up mnist
I1127 11:27:36.173655  7854 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:27:36.173671  7854 net.cpp:157] Top shape: 100 (100)
I1127 11:27:36.173679  7854 net.cpp:165] Memory required for data: 314000
I1127 11:27:36.173694  7854 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:27:36.173728  7854 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:27:36.173741  7854 net.cpp:454] label_mnist_1_split <- label
I1127 11:27:36.173758  7854 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:27:36.173785  7854 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:27:36.173877  7854 net.cpp:150] Setting up label_mnist_1_split
I1127 11:27:36.173892  7854 net.cpp:157] Top shape: 100 (100)
I1127 11:27:36.173902  7854 net.cpp:157] Top shape: 100 (100)
I1127 11:27:36.173910  7854 net.cpp:165] Memory required for data: 314800
I1127 11:27:36.173918  7854 layer_factory.hpp:76] Creating layer conv1
I1127 11:27:36.173946  7854 net.cpp:106] Creating Layer conv1
I1127 11:27:36.173966  7854 net.cpp:454] conv1 <- data
I1127 11:27:36.173991  7854 net.cpp:411] conv1 -> conv1
I1127 11:27:36.174408  7854 net.cpp:150] Setting up conv1
I1127 11:27:36.174448  7854 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:27:36.174459  7854 net.cpp:165] Memory required for data: 4922800
I1127 11:27:36.174484  7854 layer_factory.hpp:76] Creating layer pool1
I1127 11:27:36.174501  7854 net.cpp:106] Creating Layer pool1
I1127 11:27:36.174510  7854 net.cpp:454] pool1 <- conv1
I1127 11:27:36.174553  7854 net.cpp:411] pool1 -> pool1
I1127 11:27:36.174618  7854 net.cpp:150] Setting up pool1
I1127 11:27:36.174633  7854 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:27:36.174639  7854 net.cpp:165] Memory required for data: 6074800
I1127 11:27:36.174648  7854 layer_factory.hpp:76] Creating layer conv2
I1127 11:27:36.174670  7854 net.cpp:106] Creating Layer conv2
I1127 11:27:36.174679  7854 net.cpp:454] conv2 <- pool1
I1127 11:27:36.174693  7854 net.cpp:411] conv2 -> conv2
I1127 11:27:36.175389  7854 net.cpp:150] Setting up conv2
I1127 11:27:36.175448  7854 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:27:36.175458  7854 net.cpp:165] Memory required for data: 7354800
I1127 11:27:36.175484  7854 layer_factory.hpp:76] Creating layer pool2
I1127 11:27:36.175511  7854 net.cpp:106] Creating Layer pool2
I1127 11:27:36.175523  7854 net.cpp:454] pool2 <- conv2
I1127 11:27:36.175541  7854 net.cpp:411] pool2 -> pool2
I1127 11:27:36.175632  7854 net.cpp:150] Setting up pool2
I1127 11:27:36.175647  7854 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:27:36.175667  7854 net.cpp:165] Memory required for data: 7674800
I1127 11:27:36.175674  7854 layer_factory.hpp:76] Creating layer ip1
I1127 11:27:36.175695  7854 net.cpp:106] Creating Layer ip1
I1127 11:27:36.175705  7854 net.cpp:454] ip1 <- pool2
I1127 11:27:36.175719  7854 net.cpp:411] ip1 -> ip1
I1127 11:27:36.180582  7854 net.cpp:150] Setting up ip1
I1127 11:27:36.180682  7854 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:27:36.180692  7854 net.cpp:165] Memory required for data: 7874800
I1127 11:27:36.180724  7854 layer_factory.hpp:76] Creating layer relu1
I1127 11:27:36.180754  7854 net.cpp:106] Creating Layer relu1
I1127 11:27:36.180765  7854 net.cpp:454] relu1 <- ip1
I1127 11:27:36.180778  7854 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:27:36.180799  7854 net.cpp:150] Setting up relu1
I1127 11:27:36.180806  7854 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:27:36.180814  7854 net.cpp:165] Memory required for data: 8074800
I1127 11:27:36.180820  7854 layer_factory.hpp:76] Creating layer ip2
I1127 11:27:36.180840  7854 net.cpp:106] Creating Layer ip2
I1127 11:27:36.180846  7854 net.cpp:454] ip2 <- ip1
I1127 11:27:36.180868  7854 net.cpp:411] ip2 -> ip2
I1127 11:27:36.181108  7854 net.cpp:150] Setting up ip2
I1127 11:27:36.181130  7854 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:27:36.181140  7854 net.cpp:165] Memory required for data: 8078800
I1127 11:27:36.181176  7854 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:27:36.181193  7854 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:27:36.181205  7854 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:27:36.181217  7854 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:27:36.181234  7854 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:27:36.181298  7854 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:27:36.181318  7854 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:27:36.181330  7854 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:27:36.181339  7854 net.cpp:165] Memory required for data: 8086800
I1127 11:27:36.181349  7854 layer_factory.hpp:76] Creating layer accuracy
I1127 11:27:36.181370  7854 net.cpp:106] Creating Layer accuracy
I1127 11:27:36.181378  7854 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:27:36.181391  7854 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:27:36.181403  7854 net.cpp:411] accuracy -> accuracy
I1127 11:27:36.181427  7854 net.cpp:150] Setting up accuracy
I1127 11:27:36.181437  7854 net.cpp:157] Top shape: (1)
I1127 11:27:36.181445  7854 net.cpp:165] Memory required for data: 8086804
I1127 11:27:36.181454  7854 layer_factory.hpp:76] Creating layer loss
I1127 11:27:36.181478  7854 net.cpp:106] Creating Layer loss
I1127 11:27:36.181500  7854 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:27:36.181524  7854 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:27:36.181553  7854 net.cpp:411] loss -> loss
I1127 11:27:36.181576  7854 layer_factory.hpp:76] Creating layer loss
I1127 11:27:36.181710  7854 net.cpp:150] Setting up loss
I1127 11:27:36.181720  7854 net.cpp:157] Top shape: (1)
I1127 11:27:36.181726  7854 net.cpp:160]     with loss weight 1
I1127 11:27:36.181746  7854 net.cpp:165] Memory required for data: 8086808
I1127 11:27:36.181754  7854 net.cpp:226] loss needs backward computation.
I1127 11:27:36.181767  7854 net.cpp:228] accuracy does not need backward computation.
I1127 11:27:36.181776  7854 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:27:36.181782  7854 net.cpp:226] ip2 needs backward computation.
I1127 11:27:36.181788  7854 net.cpp:226] relu1 needs backward computation.
I1127 11:27:36.181795  7854 net.cpp:226] ip1 needs backward computation.
I1127 11:27:36.181802  7854 net.cpp:226] pool2 needs backward computation.
I1127 11:27:36.181809  7854 net.cpp:226] conv2 needs backward computation.
I1127 11:27:36.181816  7854 net.cpp:226] pool1 needs backward computation.
I1127 11:27:36.181823  7854 net.cpp:226] conv1 needs backward computation.
I1127 11:27:36.181831  7854 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:27:36.181838  7854 net.cpp:228] mnist does not need backward computation.
I1127 11:27:36.181844  7854 net.cpp:270] This network produces output accuracy
I1127 11:27:36.181851  7854 net.cpp:270] This network produces output loss
I1127 11:27:36.181867  7854 net.cpp:283] Network initialization done.
I1127 11:27:36.181949  7854 solver.cpp:59] Solver scaffolding done.
I1127 11:27:36.182276  7854 caffe.cpp:212] Starting Optimization
I1127 11:27:36.182301  7854 solver.cpp:287] Solving LeNet
I1127 11:27:36.182308  7854 solver.cpp:288] Learning Rate Policy: inv
I1127 11:27:36.183280  7854 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:27:39.323567  7854 solver.cpp:408]     Test net output #0: accuracy = 0.0934
I1127 11:27:39.323698  7854 solver.cpp:408]     Test net output #1: loss = 2.32017 (* 1 = 2.32017 loss)
I1127 11:27:39.335386  7854 solver.cpp:236] Iteration 0, loss = 2.30417
I1127 11:27:39.335469  7854 solver.cpp:252]     Train net output #0: loss = 2.30417 (* 1 = 2.30417 loss)
I1127 11:27:39.335497  7854 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:27:39.378653  7854 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:27:52.006660  7854 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:27:53.936698  7854 solver.cpp:408]     Test net output #0: accuracy = 0.975
I1127 11:27:53.936775  7854 solver.cpp:408]     Test net output #1: loss = 0.0835327 (* 1 = 0.0835327 loss)
I1127 11:27:53.947285  7854 solver.cpp:236] Iteration 500, loss = 0.0908679
I1127 11:27:53.947410  7854 solver.cpp:252]     Train net output #0: loss = 0.0908678 (* 1 = 0.0908678 loss)
I1127 11:27:53.947433  7854 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:28:05.492177  7854 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:28:05.508534  7854 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:28:05.520546  7854 solver.cpp:320] Iteration 1000, loss = 0.0968241
I1127 11:28:05.520661  7854 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:28:08.222584  7854 solver.cpp:408]     Test net output #0: accuracy = 0.9789
I1127 11:28:08.222692  7854 solver.cpp:408]     Test net output #1: loss = 0.0622126 (* 1 = 0.0622126 loss)
I1127 11:28:08.222699  7854 solver.cpp:325] Optimization Done.
I1127 11:28:08.222703  7854 caffe.cpp:215] Optimization Done.
I1127 11:28:08.363173  7931 caffe.cpp:184] Using GPUs 0
I1127 11:28:08.717908  7931 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:28:08.718091  7931 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:28:08.718564  7931 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:28:08.718614  7931 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:28:08.718772  7931 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:28:08.718870  7931 layer_factory.hpp:76] Creating layer mnist
I1127 11:28:08.719522  7931 net.cpp:106] Creating Layer mnist
I1127 11:28:08.719606  7931 net.cpp:411] mnist -> data
I1127 11:28:08.719696  7931 net.cpp:411] mnist -> label
I1127 11:28:08.720943  7934 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:28:08.733022  7931 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:28:08.734393  7931 net.cpp:150] Setting up mnist
I1127 11:28:08.734501  7931 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:28:08.734520  7931 net.cpp:157] Top shape: 64 (64)
I1127 11:28:08.734532  7931 net.cpp:165] Memory required for data: 200960
I1127 11:28:08.734556  7931 layer_factory.hpp:76] Creating layer conv1
I1127 11:28:08.734603  7931 net.cpp:106] Creating Layer conv1
I1127 11:28:08.734623  7931 net.cpp:454] conv1 <- data
I1127 11:28:08.734657  7931 net.cpp:411] conv1 -> conv1
I1127 11:28:08.735571  7931 net.cpp:150] Setting up conv1
I1127 11:28:08.735608  7931 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:28:08.735618  7931 net.cpp:165] Memory required for data: 3150080
I1127 11:28:08.735646  7931 layer_factory.hpp:76] Creating layer pool1
I1127 11:28:08.735667  7931 net.cpp:106] Creating Layer pool1
I1127 11:28:08.735678  7931 net.cpp:454] pool1 <- conv1
I1127 11:28:08.735692  7931 net.cpp:411] pool1 -> pool1
I1127 11:28:08.735791  7931 net.cpp:150] Setting up pool1
I1127 11:28:08.735807  7931 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:28:08.735816  7931 net.cpp:165] Memory required for data: 3887360
I1127 11:28:08.735823  7931 layer_factory.hpp:76] Creating layer conv2
I1127 11:28:08.735843  7931 net.cpp:106] Creating Layer conv2
I1127 11:28:08.735852  7931 net.cpp:454] conv2 <- pool1
I1127 11:28:08.735863  7931 net.cpp:411] conv2 -> conv2
I1127 11:28:08.736292  7931 net.cpp:150] Setting up conv2
I1127 11:28:08.736310  7931 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:28:08.736315  7931 net.cpp:165] Memory required for data: 4706560
I1127 11:28:08.736325  7931 layer_factory.hpp:76] Creating layer pool2
I1127 11:28:08.736335  7931 net.cpp:106] Creating Layer pool2
I1127 11:28:08.736340  7931 net.cpp:454] pool2 <- conv2
I1127 11:28:08.736346  7931 net.cpp:411] pool2 -> pool2
I1127 11:28:08.736377  7931 net.cpp:150] Setting up pool2
I1127 11:28:08.736397  7931 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:28:08.736402  7931 net.cpp:165] Memory required for data: 4911360
I1127 11:28:08.736407  7931 layer_factory.hpp:76] Creating layer ip1
I1127 11:28:08.736415  7931 net.cpp:106] Creating Layer ip1
I1127 11:28:08.736420  7931 net.cpp:454] ip1 <- pool2
I1127 11:28:08.736426  7931 net.cpp:411] ip1 -> ip1
I1127 11:28:08.740300  7931 net.cpp:150] Setting up ip1
I1127 11:28:08.740401  7931 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:28:08.740419  7931 net.cpp:165] Memory required for data: 5039360
I1127 11:28:08.740461  7931 layer_factory.hpp:76] Creating layer relu1
I1127 11:28:08.740489  7931 net.cpp:106] Creating Layer relu1
I1127 11:28:08.740500  7931 net.cpp:454] relu1 <- ip1
I1127 11:28:08.740514  7931 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:28:08.740536  7931 net.cpp:150] Setting up relu1
I1127 11:28:08.740545  7931 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:28:08.740551  7931 net.cpp:165] Memory required for data: 5167360
I1127 11:28:08.740558  7931 layer_factory.hpp:76] Creating layer ip2
I1127 11:28:08.740571  7931 net.cpp:106] Creating Layer ip2
I1127 11:28:08.740578  7931 net.cpp:454] ip2 <- ip1
I1127 11:28:08.740589  7931 net.cpp:411] ip2 -> ip2
I1127 11:28:08.741507  7931 net.cpp:150] Setting up ip2
I1127 11:28:08.741595  7931 net.cpp:157] Top shape: 64 10 (640)
I1127 11:28:08.741602  7931 net.cpp:165] Memory required for data: 5169920
I1127 11:28:08.741618  7931 layer_factory.hpp:76] Creating layer loss
I1127 11:28:08.741642  7931 net.cpp:106] Creating Layer loss
I1127 11:28:08.741652  7931 net.cpp:454] loss <- ip2
I1127 11:28:08.741662  7931 net.cpp:454] loss <- label
I1127 11:28:08.741677  7931 net.cpp:411] loss -> loss
I1127 11:28:08.741705  7931 layer_factory.hpp:76] Creating layer loss
I1127 11:28:08.741814  7931 net.cpp:150] Setting up loss
I1127 11:28:08.741825  7931 net.cpp:157] Top shape: (1)
I1127 11:28:08.741830  7931 net.cpp:160]     with loss weight 1
I1127 11:28:08.741871  7931 net.cpp:165] Memory required for data: 5169924
I1127 11:28:08.741878  7931 net.cpp:226] loss needs backward computation.
I1127 11:28:08.741884  7931 net.cpp:226] ip2 needs backward computation.
I1127 11:28:08.741889  7931 net.cpp:226] relu1 needs backward computation.
I1127 11:28:08.741894  7931 net.cpp:226] ip1 needs backward computation.
I1127 11:28:08.741899  7931 net.cpp:226] pool2 needs backward computation.
I1127 11:28:08.741904  7931 net.cpp:226] conv2 needs backward computation.
I1127 11:28:08.741907  7931 net.cpp:226] pool1 needs backward computation.
I1127 11:28:08.741912  7931 net.cpp:226] conv1 needs backward computation.
I1127 11:28:08.741916  7931 net.cpp:228] mnist does not need backward computation.
I1127 11:28:08.741921  7931 net.cpp:270] This network produces output loss
I1127 11:28:08.741935  7931 net.cpp:283] Network initialization done.
I1127 11:28:08.742292  7931 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:28:08.742338  7931 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:28:08.742480  7931 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:28:08.742554  7931 layer_factory.hpp:76] Creating layer mnist
I1127 11:28:08.742691  7931 net.cpp:106] Creating Layer mnist
I1127 11:28:08.742709  7931 net.cpp:411] mnist -> data
I1127 11:28:08.742729  7931 net.cpp:411] mnist -> label
I1127 11:28:08.743633  7936 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:28:08.743782  7931 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:28:08.744977  7931 net.cpp:150] Setting up mnist
I1127 11:28:08.745003  7931 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:28:08.745009  7931 net.cpp:157] Top shape: 100 (100)
I1127 11:28:08.745014  7931 net.cpp:165] Memory required for data: 314000
I1127 11:28:08.745021  7931 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:28:08.745034  7931 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:28:08.745046  7931 net.cpp:454] label_mnist_1_split <- label
I1127 11:28:08.745054  7931 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:28:08.745064  7931 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:28:08.745102  7931 net.cpp:150] Setting up label_mnist_1_split
I1127 11:28:08.745110  7931 net.cpp:157] Top shape: 100 (100)
I1127 11:28:08.745115  7931 net.cpp:157] Top shape: 100 (100)
I1127 11:28:08.745120  7931 net.cpp:165] Memory required for data: 314800
I1127 11:28:08.745124  7931 layer_factory.hpp:76] Creating layer conv1
I1127 11:28:08.745136  7931 net.cpp:106] Creating Layer conv1
I1127 11:28:08.745139  7931 net.cpp:454] conv1 <- data
I1127 11:28:08.745147  7931 net.cpp:411] conv1 -> conv1
I1127 11:28:08.745308  7931 net.cpp:150] Setting up conv1
I1127 11:28:08.745317  7931 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:28:08.745321  7931 net.cpp:165] Memory required for data: 4922800
I1127 11:28:08.745332  7931 layer_factory.hpp:76] Creating layer pool1
I1127 11:28:08.745342  7931 net.cpp:106] Creating Layer pool1
I1127 11:28:08.745345  7931 net.cpp:454] pool1 <- conv1
I1127 11:28:08.745360  7931 net.cpp:411] pool1 -> pool1
I1127 11:28:08.745390  7931 net.cpp:150] Setting up pool1
I1127 11:28:08.745398  7931 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:28:08.745403  7931 net.cpp:165] Memory required for data: 6074800
I1127 11:28:08.745407  7931 layer_factory.hpp:76] Creating layer conv2
I1127 11:28:08.745416  7931 net.cpp:106] Creating Layer conv2
I1127 11:28:08.745421  7931 net.cpp:454] conv2 <- pool1
I1127 11:28:08.745429  7931 net.cpp:411] conv2 -> conv2
I1127 11:28:08.745698  7931 net.cpp:150] Setting up conv2
I1127 11:28:08.745712  7931 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:28:08.745715  7931 net.cpp:165] Memory required for data: 7354800
I1127 11:28:08.745724  7931 layer_factory.hpp:76] Creating layer pool2
I1127 11:28:08.745734  7931 net.cpp:106] Creating Layer pool2
I1127 11:28:08.745738  7931 net.cpp:454] pool2 <- conv2
I1127 11:28:08.745744  7931 net.cpp:411] pool2 -> pool2
I1127 11:28:08.745776  7931 net.cpp:150] Setting up pool2
I1127 11:28:08.745784  7931 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:28:08.745789  7931 net.cpp:165] Memory required for data: 7674800
I1127 11:28:08.745792  7931 layer_factory.hpp:76] Creating layer ip1
I1127 11:28:08.745805  7931 net.cpp:106] Creating Layer ip1
I1127 11:28:08.745810  7931 net.cpp:454] ip1 <- pool2
I1127 11:28:08.745816  7931 net.cpp:411] ip1 -> ip1
I1127 11:28:08.750000  7931 net.cpp:150] Setting up ip1
I1127 11:28:08.750108  7931 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:28:08.750118  7931 net.cpp:165] Memory required for data: 7874800
I1127 11:28:08.750161  7931 layer_factory.hpp:76] Creating layer relu1
I1127 11:28:08.750187  7931 net.cpp:106] Creating Layer relu1
I1127 11:28:08.750198  7931 net.cpp:454] relu1 <- ip1
I1127 11:28:08.750216  7931 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:28:08.750234  7931 net.cpp:150] Setting up relu1
I1127 11:28:08.750243  7931 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:28:08.750252  7931 net.cpp:165] Memory required for data: 8074800
I1127 11:28:08.750259  7931 layer_factory.hpp:76] Creating layer ip2
I1127 11:28:08.750283  7931 net.cpp:106] Creating Layer ip2
I1127 11:28:08.750293  7931 net.cpp:454] ip2 <- ip1
I1127 11:28:08.750309  7931 net.cpp:411] ip2 -> ip2
I1127 11:28:08.750588  7931 net.cpp:150] Setting up ip2
I1127 11:28:08.750607  7931 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:28:08.750615  7931 net.cpp:165] Memory required for data: 8078800
I1127 11:28:08.750627  7931 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:28:08.750641  7931 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:28:08.750648  7931 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:28:08.750658  7931 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:28:08.750670  7931 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:28:08.750717  7931 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:28:08.750748  7931 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:28:08.750758  7931 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:28:08.750766  7931 net.cpp:165] Memory required for data: 8086800
I1127 11:28:08.750774  7931 layer_factory.hpp:76] Creating layer accuracy
I1127 11:28:08.750792  7931 net.cpp:106] Creating Layer accuracy
I1127 11:28:08.750799  7931 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:28:08.750808  7931 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:28:08.750819  7931 net.cpp:411] accuracy -> accuracy
I1127 11:28:08.750838  7931 net.cpp:150] Setting up accuracy
I1127 11:28:08.750847  7931 net.cpp:157] Top shape: (1)
I1127 11:28:08.750854  7931 net.cpp:165] Memory required for data: 8086804
I1127 11:28:08.750861  7931 layer_factory.hpp:76] Creating layer loss
I1127 11:28:08.750874  7931 net.cpp:106] Creating Layer loss
I1127 11:28:08.750883  7931 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:28:08.750892  7931 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:28:08.750903  7931 net.cpp:411] loss -> loss
I1127 11:28:08.750924  7931 layer_factory.hpp:76] Creating layer loss
I1127 11:28:08.751097  7931 net.cpp:150] Setting up loss
I1127 11:28:08.751117  7931 net.cpp:157] Top shape: (1)
I1127 11:28:08.751126  7931 net.cpp:160]     with loss weight 1
I1127 11:28:08.751158  7931 net.cpp:165] Memory required for data: 8086808
I1127 11:28:08.751168  7931 net.cpp:226] loss needs backward computation.
I1127 11:28:08.751188  7931 net.cpp:228] accuracy does not need backward computation.
I1127 11:28:08.751198  7931 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:28:08.751204  7931 net.cpp:226] ip2 needs backward computation.
I1127 11:28:08.751211  7931 net.cpp:226] relu1 needs backward computation.
I1127 11:28:08.751219  7931 net.cpp:226] ip1 needs backward computation.
I1127 11:28:08.751227  7931 net.cpp:226] pool2 needs backward computation.
I1127 11:28:08.751235  7931 net.cpp:226] conv2 needs backward computation.
I1127 11:28:08.751243  7931 net.cpp:226] pool1 needs backward computation.
I1127 11:28:08.751251  7931 net.cpp:226] conv1 needs backward computation.
I1127 11:28:08.751261  7931 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:28:08.751269  7931 net.cpp:228] mnist does not need backward computation.
I1127 11:28:08.751276  7931 net.cpp:270] This network produces output accuracy
I1127 11:28:08.751284  7931 net.cpp:270] This network produces output loss
I1127 11:28:08.751301  7931 net.cpp:283] Network initialization done.
I1127 11:28:08.751410  7931 solver.cpp:59] Solver scaffolding done.
I1127 11:28:08.751799  7931 caffe.cpp:212] Starting Optimization
I1127 11:28:08.751824  7931 solver.cpp:287] Solving LeNet
I1127 11:28:08.751832  7931 solver.cpp:288] Learning Rate Policy: inv
I1127 11:28:08.752730  7931 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:28:09.865386  7931 solver.cpp:408]     Test net output #0: accuracy = 0.1186
I1127 11:28:09.865471  7931 solver.cpp:408]     Test net output #1: loss = 2.37116 (* 1 = 2.37116 loss)
I1127 11:28:09.882375  7931 solver.cpp:236] Iteration 0, loss = 2.33636
I1127 11:28:09.882571  7931 solver.cpp:252]     Train net output #0: loss = 2.33636 (* 1 = 2.33636 loss)
I1127 11:28:09.882653  7931 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:28:18.960352  7931 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:28:23.311442  7931 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:28:25.846424  7931 solver.cpp:408]     Test net output #0: accuracy = 0.9753
I1127 11:28:25.846529  7931 solver.cpp:408]     Test net output #1: loss = 0.079522 (* 1 = 0.079522 loss)
I1127 11:28:25.877862  7931 solver.cpp:236] Iteration 500, loss = 0.108243
I1127 11:28:25.877986  7931 solver.cpp:252]     Train net output #0: loss = 0.108243 (* 1 = 0.108243 loss)
I1127 11:28:25.878005  7931 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:28:37.958112  7931 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:28:37.974123  7931 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:28:37.985648  7931 solver.cpp:320] Iteration 1000, loss = 0.0727344
I1127 11:28:37.985739  7931 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:28:41.112529  7931 solver.cpp:408]     Test net output #0: accuracy = 0.9835
I1127 11:28:41.112609  7931 solver.cpp:408]     Test net output #1: loss = 0.0517027 (* 1 = 0.0517027 loss)
I1127 11:28:41.112622  7931 solver.cpp:325] Optimization Done.
I1127 11:28:41.112627  7931 caffe.cpp:215] Optimization Done.
I1127 11:28:41.248356  7961 caffe.cpp:184] Using GPUs 0
I1127 11:28:41.584153  7961 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:28:41.584609  7961 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:28:41.585239  7961 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:28:41.585292  7961 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:28:41.585525  7961 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:28:41.585650  7961 layer_factory.hpp:76] Creating layer mnist
I1127 11:28:41.586382  7961 net.cpp:106] Creating Layer mnist
I1127 11:28:41.586468  7961 net.cpp:411] mnist -> data
I1127 11:28:41.586542  7961 net.cpp:411] mnist -> label
I1127 11:28:41.591713  7964 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:28:41.608001  7961 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:28:41.610285  7961 net.cpp:150] Setting up mnist
I1127 11:28:41.610355  7961 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:28:41.610366  7961 net.cpp:157] Top shape: 64 (64)
I1127 11:28:41.610373  7961 net.cpp:165] Memory required for data: 200960
I1127 11:28:41.610391  7961 layer_factory.hpp:76] Creating layer conv1
I1127 11:28:41.610448  7961 net.cpp:106] Creating Layer conv1
I1127 11:28:41.610465  7961 net.cpp:454] conv1 <- data
I1127 11:28:41.610488  7961 net.cpp:411] conv1 -> conv1
I1127 11:28:41.611928  7961 net.cpp:150] Setting up conv1
I1127 11:28:41.612038  7961 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:28:41.612049  7961 net.cpp:165] Memory required for data: 3150080
I1127 11:28:41.612087  7961 layer_factory.hpp:76] Creating layer pool1
I1127 11:28:41.612124  7961 net.cpp:106] Creating Layer pool1
I1127 11:28:41.612138  7961 net.cpp:454] pool1 <- conv1
I1127 11:28:41.612153  7961 net.cpp:411] pool1 -> pool1
I1127 11:28:41.612323  7961 net.cpp:150] Setting up pool1
I1127 11:28:41.612340  7961 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:28:41.612349  7961 net.cpp:165] Memory required for data: 3887360
I1127 11:28:41.612355  7961 layer_factory.hpp:76] Creating layer conv2
I1127 11:28:41.612380  7961 net.cpp:106] Creating Layer conv2
I1127 11:28:41.612390  7961 net.cpp:454] conv2 <- pool1
I1127 11:28:41.612401  7961 net.cpp:411] conv2 -> conv2
I1127 11:28:41.612903  7961 net.cpp:150] Setting up conv2
I1127 11:28:41.612957  7961 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:28:41.612965  7961 net.cpp:165] Memory required for data: 4706560
I1127 11:28:41.612994  7961 layer_factory.hpp:76] Creating layer pool2
I1127 11:28:41.613023  7961 net.cpp:106] Creating Layer pool2
I1127 11:28:41.613036  7961 net.cpp:454] pool2 <- conv2
I1127 11:28:41.613055  7961 net.cpp:411] pool2 -> pool2
I1127 11:28:41.613126  7961 net.cpp:150] Setting up pool2
I1127 11:28:41.613140  7961 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:28:41.613148  7961 net.cpp:165] Memory required for data: 4911360
I1127 11:28:41.613157  7961 layer_factory.hpp:76] Creating layer ip1
I1127 11:28:41.613180  7961 net.cpp:106] Creating Layer ip1
I1127 11:28:41.613189  7961 net.cpp:454] ip1 <- pool2
I1127 11:28:41.613201  7961 net.cpp:411] ip1 -> ip1
I1127 11:28:41.619789  7961 net.cpp:150] Setting up ip1
I1127 11:28:41.619910  7961 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:28:41.619923  7961 net.cpp:165] Memory required for data: 5039360
I1127 11:28:41.619964  7961 layer_factory.hpp:76] Creating layer relu1
I1127 11:28:41.620000  7961 net.cpp:106] Creating Layer relu1
I1127 11:28:41.620018  7961 net.cpp:454] relu1 <- ip1
I1127 11:28:41.620038  7961 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:28:41.620075  7961 net.cpp:150] Setting up relu1
I1127 11:28:41.620088  7961 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:28:41.620096  7961 net.cpp:165] Memory required for data: 5167360
I1127 11:28:41.620105  7961 layer_factory.hpp:76] Creating layer ip2
I1127 11:28:41.620134  7961 net.cpp:106] Creating Layer ip2
I1127 11:28:41.620144  7961 net.cpp:454] ip2 <- ip1
I1127 11:28:41.620160  7961 net.cpp:411] ip2 -> ip2
I1127 11:28:41.621645  7961 net.cpp:150] Setting up ip2
I1127 11:28:41.621737  7961 net.cpp:157] Top shape: 64 10 (640)
I1127 11:28:41.621749  7961 net.cpp:165] Memory required for data: 5169920
I1127 11:28:41.621776  7961 layer_factory.hpp:76] Creating layer loss
I1127 11:28:41.621816  7961 net.cpp:106] Creating Layer loss
I1127 11:28:41.621832  7961 net.cpp:454] loss <- ip2
I1127 11:28:41.621850  7961 net.cpp:454] loss <- label
I1127 11:28:41.621873  7961 net.cpp:411] loss -> loss
I1127 11:28:41.621915  7961 layer_factory.hpp:76] Creating layer loss
I1127 11:28:41.622074  7961 net.cpp:150] Setting up loss
I1127 11:28:41.622097  7961 net.cpp:157] Top shape: (1)
I1127 11:28:41.622112  7961 net.cpp:160]     with loss weight 1
I1127 11:28:41.622220  7961 net.cpp:165] Memory required for data: 5169924
I1127 11:28:41.622236  7961 net.cpp:226] loss needs backward computation.
I1127 11:28:41.622249  7961 net.cpp:226] ip2 needs backward computation.
I1127 11:28:41.622259  7961 net.cpp:226] relu1 needs backward computation.
I1127 11:28:41.622267  7961 net.cpp:226] ip1 needs backward computation.
I1127 11:28:41.622273  7961 net.cpp:226] pool2 needs backward computation.
I1127 11:28:41.622282  7961 net.cpp:226] conv2 needs backward computation.
I1127 11:28:41.622308  7961 net.cpp:226] pool1 needs backward computation.
I1127 11:28:41.622318  7961 net.cpp:226] conv1 needs backward computation.
I1127 11:28:41.622328  7961 net.cpp:228] mnist does not need backward computation.
I1127 11:28:41.622337  7961 net.cpp:270] This network produces output loss
I1127 11:28:41.622356  7961 net.cpp:283] Network initialization done.
I1127 11:28:41.622900  7961 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:28:41.623014  7961 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:28:41.623311  7961 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:28:41.623510  7961 layer_factory.hpp:76] Creating layer mnist
I1127 11:28:41.630329  7961 net.cpp:106] Creating Layer mnist
I1127 11:28:41.630455  7961 net.cpp:411] mnist -> data
I1127 11:28:41.630517  7961 net.cpp:411] mnist -> label
I1127 11:28:41.635727  7966 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:28:41.636067  7961 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:28:41.638854  7961 net.cpp:150] Setting up mnist
I1127 11:28:41.638989  7961 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:28:41.639006  7961 net.cpp:157] Top shape: 100 (100)
I1127 11:28:41.639016  7961 net.cpp:165] Memory required for data: 314000
I1127 11:28:41.639032  7961 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:28:41.639071  7961 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:28:41.639088  7961 net.cpp:454] label_mnist_1_split <- label
I1127 11:28:41.639119  7961 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:28:41.639156  7961 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:28:41.639286  7961 net.cpp:150] Setting up label_mnist_1_split
I1127 11:28:41.639302  7961 net.cpp:157] Top shape: 100 (100)
I1127 11:28:41.639314  7961 net.cpp:157] Top shape: 100 (100)
I1127 11:28:41.639344  7961 net.cpp:165] Memory required for data: 314800
I1127 11:28:41.639356  7961 layer_factory.hpp:76] Creating layer conv1
I1127 11:28:41.639394  7961 net.cpp:106] Creating Layer conv1
I1127 11:28:41.639410  7961 net.cpp:454] conv1 <- data
I1127 11:28:41.639425  7961 net.cpp:411] conv1 -> conv1
I1127 11:28:41.639788  7961 net.cpp:150] Setting up conv1
I1127 11:28:41.639808  7961 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:28:41.639817  7961 net.cpp:165] Memory required for data: 4922800
I1127 11:28:41.639838  7961 layer_factory.hpp:76] Creating layer pool1
I1127 11:28:41.639859  7961 net.cpp:106] Creating Layer pool1
I1127 11:28:41.639869  7961 net.cpp:454] pool1 <- conv1
I1127 11:28:41.639914  7961 net.cpp:411] pool1 -> pool1
I1127 11:28:41.639969  7961 net.cpp:150] Setting up pool1
I1127 11:28:41.639983  7961 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:28:41.639991  7961 net.cpp:165] Memory required for data: 6074800
I1127 11:28:41.639999  7961 layer_factory.hpp:76] Creating layer conv2
I1127 11:28:41.640022  7961 net.cpp:106] Creating Layer conv2
I1127 11:28:41.640030  7961 net.cpp:454] conv2 <- pool1
I1127 11:28:41.640045  7961 net.cpp:411] conv2 -> conv2
I1127 11:28:41.640534  7961 net.cpp:150] Setting up conv2
I1127 11:28:41.640555  7961 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:28:41.640563  7961 net.cpp:165] Memory required for data: 7354800
I1127 11:28:41.640578  7961 layer_factory.hpp:76] Creating layer pool2
I1127 11:28:41.640590  7961 net.cpp:106] Creating Layer pool2
I1127 11:28:41.640597  7961 net.cpp:454] pool2 <- conv2
I1127 11:28:41.640606  7961 net.cpp:411] pool2 -> pool2
I1127 11:28:41.640647  7961 net.cpp:150] Setting up pool2
I1127 11:28:41.640658  7961 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:28:41.640666  7961 net.cpp:165] Memory required for data: 7674800
I1127 11:28:41.640672  7961 layer_factory.hpp:76] Creating layer ip1
I1127 11:28:41.640684  7961 net.cpp:106] Creating Layer ip1
I1127 11:28:41.640691  7961 net.cpp:454] ip1 <- pool2
I1127 11:28:41.640702  7961 net.cpp:411] ip1 -> ip1
I1127 11:28:41.644951  7961 net.cpp:150] Setting up ip1
I1127 11:28:41.645093  7961 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:28:41.645113  7961 net.cpp:165] Memory required for data: 7874800
I1127 11:28:41.645162  7961 layer_factory.hpp:76] Creating layer relu1
I1127 11:28:41.645191  7961 net.cpp:106] Creating Layer relu1
I1127 11:28:41.645205  7961 net.cpp:454] relu1 <- ip1
I1127 11:28:41.645226  7961 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:28:41.645254  7961 net.cpp:150] Setting up relu1
I1127 11:28:41.645268  7961 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:28:41.645277  7961 net.cpp:165] Memory required for data: 8074800
I1127 11:28:41.645287  7961 layer_factory.hpp:76] Creating layer ip2
I1127 11:28:41.645314  7961 net.cpp:106] Creating Layer ip2
I1127 11:28:41.645324  7961 net.cpp:454] ip2 <- ip1
I1127 11:28:41.645344  7961 net.cpp:411] ip2 -> ip2
I1127 11:28:41.645604  7961 net.cpp:150] Setting up ip2
I1127 11:28:41.645628  7961 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:28:41.645637  7961 net.cpp:165] Memory required for data: 8078800
I1127 11:28:41.645653  7961 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:28:41.645670  7961 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:28:41.645680  7961 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:28:41.645690  7961 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:28:41.645706  7961 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:28:41.645799  7961 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:28:41.645839  7961 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:28:41.645853  7961 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:28:41.645860  7961 net.cpp:165] Memory required for data: 8086800
I1127 11:28:41.645870  7961 layer_factory.hpp:76] Creating layer accuracy
I1127 11:28:41.645892  7961 net.cpp:106] Creating Layer accuracy
I1127 11:28:41.645903  7961 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:28:41.645913  7961 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:28:41.645946  7961 net.cpp:411] accuracy -> accuracy
I1127 11:28:41.645968  7961 net.cpp:150] Setting up accuracy
I1127 11:28:41.645978  7961 net.cpp:157] Top shape: (1)
I1127 11:28:41.645985  7961 net.cpp:165] Memory required for data: 8086804
I1127 11:28:41.645992  7961 layer_factory.hpp:76] Creating layer loss
I1127 11:28:41.646008  7961 net.cpp:106] Creating Layer loss
I1127 11:28:41.646015  7961 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:28:41.646024  7961 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:28:41.646034  7961 net.cpp:411] loss -> loss
I1127 11:28:41.646049  7961 layer_factory.hpp:76] Creating layer loss
I1127 11:28:41.654188  7961 net.cpp:150] Setting up loss
I1127 11:28:41.654299  7961 net.cpp:157] Top shape: (1)
I1127 11:28:41.654311  7961 net.cpp:160]     with loss weight 1
I1127 11:28:41.654352  7961 net.cpp:165] Memory required for data: 8086808
I1127 11:28:41.654371  7961 net.cpp:226] loss needs backward computation.
I1127 11:28:41.654400  7961 net.cpp:228] accuracy does not need backward computation.
I1127 11:28:41.654412  7961 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:28:41.654423  7961 net.cpp:226] ip2 needs backward computation.
I1127 11:28:41.654433  7961 net.cpp:226] relu1 needs backward computation.
I1127 11:28:41.654444  7961 net.cpp:226] ip1 needs backward computation.
I1127 11:28:41.654454  7961 net.cpp:226] pool2 needs backward computation.
I1127 11:28:41.654465  7961 net.cpp:226] conv2 needs backward computation.
I1127 11:28:41.654476  7961 net.cpp:226] pool1 needs backward computation.
I1127 11:28:41.654487  7961 net.cpp:226] conv1 needs backward computation.
I1127 11:28:41.654498  7961 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:28:41.654508  7961 net.cpp:228] mnist does not need backward computation.
I1127 11:28:41.654515  7961 net.cpp:270] This network produces output accuracy
I1127 11:28:41.654527  7961 net.cpp:270] This network produces output loss
I1127 11:28:41.654548  7961 net.cpp:283] Network initialization done.
I1127 11:28:41.654707  7961 solver.cpp:59] Solver scaffolding done.
I1127 11:28:41.655144  7961 caffe.cpp:212] Starting Optimization
I1127 11:28:41.655174  7961 solver.cpp:287] Solving LeNet
I1127 11:28:41.655189  7961 solver.cpp:288] Learning Rate Policy: inv
I1127 11:28:41.656430  7961 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:28:42.766808  7961 solver.cpp:408]     Test net output #0: accuracy = 0.1355
I1127 11:28:42.766927  7961 solver.cpp:408]     Test net output #1: loss = 2.31082 (* 1 = 2.31082 loss)
I1127 11:28:42.782685  7961 solver.cpp:236] Iteration 0, loss = 2.2425
I1127 11:28:42.782799  7961 solver.cpp:252]     Train net output #0: loss = 2.2425 (* 1 = 2.2425 loss)
I1127 11:28:42.782858  7961 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:28:56.203754  7961 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:28:58.518252  7961 solver.cpp:408]     Test net output #0: accuracy = 0.9742
I1127 11:28:58.518365  7961 solver.cpp:408]     Test net output #1: loss = 0.0842076 (* 1 = 0.0842076 loss)
I1127 11:28:58.550173  7961 solver.cpp:236] Iteration 500, loss = 0.0997414
I1127 11:28:58.550298  7961 solver.cpp:252]     Train net output #0: loss = 0.0997414 (* 1 = 0.0997414 loss)
I1127 11:28:58.550326  7961 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:29:11.678686  7961 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:29:11.715032  7961 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:29:11.741870  7961 solver.cpp:320] Iteration 1000, loss = 0.117348
I1127 11:29:11.741897  7961 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:29:13.848860  7961 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:29:13.962945  7961 solver.cpp:408]     Test net output #0: accuracy = 0.9823
I1127 11:29:13.963116  7961 solver.cpp:408]     Test net output #1: loss = 0.0547872 (* 1 = 0.0547872 loss)
I1127 11:29:13.963166  7961 solver.cpp:325] Optimization Done.
I1127 11:29:13.963177  7961 caffe.cpp:215] Optimization Done.
I1127 11:29:14.083360  7988 caffe.cpp:184] Using GPUs 0
I1127 11:29:14.469158  7988 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:29:14.469378  7988 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:29:14.469758  7988 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:29:14.469786  7988 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:29:14.469980  7988 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:29:14.470077  7988 layer_factory.hpp:76] Creating layer mnist
I1127 11:29:14.470538  7988 net.cpp:106] Creating Layer mnist
I1127 11:29:14.470563  7988 net.cpp:411] mnist -> data
I1127 11:29:14.470595  7988 net.cpp:411] mnist -> label
I1127 11:29:14.472219  7991 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:29:14.484208  7988 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:29:14.485560  7988 net.cpp:150] Setting up mnist
I1127 11:29:14.485606  7988 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:29:14.485615  7988 net.cpp:157] Top shape: 64 (64)
I1127 11:29:14.485620  7988 net.cpp:165] Memory required for data: 200960
I1127 11:29:14.485632  7988 layer_factory.hpp:76] Creating layer conv1
I1127 11:29:14.485654  7988 net.cpp:106] Creating Layer conv1
I1127 11:29:14.485662  7988 net.cpp:454] conv1 <- data
I1127 11:29:14.485674  7988 net.cpp:411] conv1 -> conv1
I1127 11:29:14.486573  7988 net.cpp:150] Setting up conv1
I1127 11:29:14.486631  7988 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:29:14.486636  7988 net.cpp:165] Memory required for data: 3150080
I1127 11:29:14.486657  7988 layer_factory.hpp:76] Creating layer pool1
I1127 11:29:14.486693  7988 net.cpp:106] Creating Layer pool1
I1127 11:29:14.486701  7988 net.cpp:454] pool1 <- conv1
I1127 11:29:14.486711  7988 net.cpp:411] pool1 -> pool1
I1127 11:29:14.486783  7988 net.cpp:150] Setting up pool1
I1127 11:29:14.486791  7988 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:29:14.486795  7988 net.cpp:165] Memory required for data: 3887360
I1127 11:29:14.486799  7988 layer_factory.hpp:76] Creating layer conv2
I1127 11:29:14.486817  7988 net.cpp:106] Creating Layer conv2
I1127 11:29:14.486822  7988 net.cpp:454] conv2 <- pool1
I1127 11:29:14.486830  7988 net.cpp:411] conv2 -> conv2
I1127 11:29:14.487154  7988 net.cpp:150] Setting up conv2
I1127 11:29:14.487175  7988 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:29:14.487179  7988 net.cpp:165] Memory required for data: 4706560
I1127 11:29:14.487191  7988 layer_factory.hpp:76] Creating layer pool2
I1127 11:29:14.487206  7988 net.cpp:106] Creating Layer pool2
I1127 11:29:14.487212  7988 net.cpp:454] pool2 <- conv2
I1127 11:29:14.487220  7988 net.cpp:411] pool2 -> pool2
I1127 11:29:14.487263  7988 net.cpp:150] Setting up pool2
I1127 11:29:14.487272  7988 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:29:14.487277  7988 net.cpp:165] Memory required for data: 4911360
I1127 11:29:14.487280  7988 layer_factory.hpp:76] Creating layer ip1
I1127 11:29:14.487290  7988 net.cpp:106] Creating Layer ip1
I1127 11:29:14.487294  7988 net.cpp:454] ip1 <- pool2
I1127 11:29:14.487303  7988 net.cpp:411] ip1 -> ip1
I1127 11:29:14.489948  7988 net.cpp:150] Setting up ip1
I1127 11:29:14.490008  7988 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:29:14.490013  7988 net.cpp:165] Memory required for data: 5039360
I1127 11:29:14.490030  7988 layer_factory.hpp:76] Creating layer relu1
I1127 11:29:14.490046  7988 net.cpp:106] Creating Layer relu1
I1127 11:29:14.490052  7988 net.cpp:454] relu1 <- ip1
I1127 11:29:14.490061  7988 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:29:14.490075  7988 net.cpp:150] Setting up relu1
I1127 11:29:14.490082  7988 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:29:14.490085  7988 net.cpp:165] Memory required for data: 5167360
I1127 11:29:14.490090  7988 layer_factory.hpp:76] Creating layer ip2
I1127 11:29:14.490100  7988 net.cpp:106] Creating Layer ip2
I1127 11:29:14.490105  7988 net.cpp:454] ip2 <- ip1
I1127 11:29:14.490113  7988 net.cpp:411] ip2 -> ip2
I1127 11:29:14.491165  7988 net.cpp:150] Setting up ip2
I1127 11:29:14.491240  7988 net.cpp:157] Top shape: 64 10 (640)
I1127 11:29:14.491250  7988 net.cpp:165] Memory required for data: 5169920
I1127 11:29:14.491271  7988 layer_factory.hpp:76] Creating layer loss
I1127 11:29:14.491305  7988 net.cpp:106] Creating Layer loss
I1127 11:29:14.491319  7988 net.cpp:454] loss <- ip2
I1127 11:29:14.491333  7988 net.cpp:454] loss <- label
I1127 11:29:14.491353  7988 net.cpp:411] loss -> loss
I1127 11:29:14.491391  7988 layer_factory.hpp:76] Creating layer loss
I1127 11:29:14.491739  7988 net.cpp:150] Setting up loss
I1127 11:29:14.491778  7988 net.cpp:157] Top shape: (1)
I1127 11:29:14.491787  7988 net.cpp:160]     with loss weight 1
I1127 11:29:14.491832  7988 net.cpp:165] Memory required for data: 5169924
I1127 11:29:14.491843  7988 net.cpp:226] loss needs backward computation.
I1127 11:29:14.491855  7988 net.cpp:226] ip2 needs backward computation.
I1127 11:29:14.491865  7988 net.cpp:226] relu1 needs backward computation.
I1127 11:29:14.491873  7988 net.cpp:226] ip1 needs backward computation.
I1127 11:29:14.491883  7988 net.cpp:226] pool2 needs backward computation.
I1127 11:29:14.491891  7988 net.cpp:226] conv2 needs backward computation.
I1127 11:29:14.491900  7988 net.cpp:226] pool1 needs backward computation.
I1127 11:29:14.491910  7988 net.cpp:226] conv1 needs backward computation.
I1127 11:29:14.491922  7988 net.cpp:228] mnist does not need backward computation.
I1127 11:29:14.491931  7988 net.cpp:270] This network produces output loss
I1127 11:29:14.491951  7988 net.cpp:283] Network initialization done.
I1127 11:29:14.492529  7988 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:29:14.492640  7988 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:29:14.492908  7988 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:29:14.493034  7988 layer_factory.hpp:76] Creating layer mnist
I1127 11:29:14.493224  7988 net.cpp:106] Creating Layer mnist
I1127 11:29:14.493240  7988 net.cpp:411] mnist -> data
I1127 11:29:14.493265  7988 net.cpp:411] mnist -> label
I1127 11:29:14.494684  7993 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:29:14.498397  7988 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:29:14.500176  7988 net.cpp:150] Setting up mnist
I1127 11:29:14.500257  7988 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:29:14.500272  7988 net.cpp:157] Top shape: 100 (100)
I1127 11:29:14.500282  7988 net.cpp:165] Memory required for data: 314000
I1127 11:29:14.500298  7988 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:29:14.500332  7988 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:29:14.500345  7988 net.cpp:454] label_mnist_1_split <- label
I1127 11:29:14.500366  7988 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:29:14.500396  7988 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:29:14.500481  7988 net.cpp:150] Setting up label_mnist_1_split
I1127 11:29:14.500499  7988 net.cpp:157] Top shape: 100 (100)
I1127 11:29:14.500507  7988 net.cpp:157] Top shape: 100 (100)
I1127 11:29:14.500515  7988 net.cpp:165] Memory required for data: 314800
I1127 11:29:14.500524  7988 layer_factory.hpp:76] Creating layer conv1
I1127 11:29:14.500553  7988 net.cpp:106] Creating Layer conv1
I1127 11:29:14.500561  7988 net.cpp:454] conv1 <- data
I1127 11:29:14.500572  7988 net.cpp:411] conv1 -> conv1
I1127 11:29:14.500885  7988 net.cpp:150] Setting up conv1
I1127 11:29:14.500905  7988 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:29:14.500912  7988 net.cpp:165] Memory required for data: 4922800
I1127 11:29:14.500954  7988 layer_factory.hpp:76] Creating layer pool1
I1127 11:29:14.500975  7988 net.cpp:106] Creating Layer pool1
I1127 11:29:14.500983  7988 net.cpp:454] pool1 <- conv1
I1127 11:29:14.501034  7988 net.cpp:411] pool1 -> pool1
I1127 11:29:14.501094  7988 net.cpp:150] Setting up pool1
I1127 11:29:14.501107  7988 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:29:14.501114  7988 net.cpp:165] Memory required for data: 6074800
I1127 11:29:14.501122  7988 layer_factory.hpp:76] Creating layer conv2
I1127 11:29:14.501144  7988 net.cpp:106] Creating Layer conv2
I1127 11:29:14.501152  7988 net.cpp:454] conv2 <- pool1
I1127 11:29:14.501163  7988 net.cpp:411] conv2 -> conv2
I1127 11:29:14.506672  7988 net.cpp:150] Setting up conv2
I1127 11:29:14.506798  7988 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:29:14.506809  7988 net.cpp:165] Memory required for data: 7354800
I1127 11:29:14.506850  7988 layer_factory.hpp:76] Creating layer pool2
I1127 11:29:14.506897  7988 net.cpp:106] Creating Layer pool2
I1127 11:29:14.506918  7988 net.cpp:454] pool2 <- conv2
I1127 11:29:14.506944  7988 net.cpp:411] pool2 -> pool2
I1127 11:29:14.507094  7988 net.cpp:150] Setting up pool2
I1127 11:29:14.507112  7988 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:29:14.507119  7988 net.cpp:165] Memory required for data: 7674800
I1127 11:29:14.507128  7988 layer_factory.hpp:76] Creating layer ip1
I1127 11:29:14.507143  7988 net.cpp:106] Creating Layer ip1
I1127 11:29:14.507151  7988 net.cpp:454] ip1 <- pool2
I1127 11:29:14.507164  7988 net.cpp:411] ip1 -> ip1
I1127 11:29:14.511862  7988 net.cpp:150] Setting up ip1
I1127 11:29:14.511956  7988 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:29:14.511967  7988 net.cpp:165] Memory required for data: 7874800
I1127 11:29:14.512001  7988 layer_factory.hpp:76] Creating layer relu1
I1127 11:29:14.512027  7988 net.cpp:106] Creating Layer relu1
I1127 11:29:14.512040  7988 net.cpp:454] relu1 <- ip1
I1127 11:29:14.512055  7988 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:29:14.512079  7988 net.cpp:150] Setting up relu1
I1127 11:29:14.512087  7988 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:29:14.512094  7988 net.cpp:165] Memory required for data: 8074800
I1127 11:29:14.512101  7988 layer_factory.hpp:76] Creating layer ip2
I1127 11:29:14.512130  7988 net.cpp:106] Creating Layer ip2
I1127 11:29:14.512137  7988 net.cpp:454] ip2 <- ip1
I1127 11:29:14.512151  7988 net.cpp:411] ip2 -> ip2
I1127 11:29:14.512424  7988 net.cpp:150] Setting up ip2
I1127 11:29:14.512451  7988 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:29:14.512459  7988 net.cpp:165] Memory required for data: 8078800
I1127 11:29:14.512470  7988 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:29:14.512486  7988 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:29:14.512495  7988 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:29:14.512506  7988 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:29:14.512517  7988 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:29:14.512580  7988 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:29:14.512593  7988 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:29:14.512601  7988 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:29:14.512609  7988 net.cpp:165] Memory required for data: 8086800
I1127 11:29:14.512617  7988 layer_factory.hpp:76] Creating layer accuracy
I1127 11:29:14.512632  7988 net.cpp:106] Creating Layer accuracy
I1127 11:29:14.512640  7988 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:29:14.512650  7988 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:29:14.512661  7988 net.cpp:411] accuracy -> accuracy
I1127 11:29:14.512678  7988 net.cpp:150] Setting up accuracy
I1127 11:29:14.512688  7988 net.cpp:157] Top shape: (1)
I1127 11:29:14.512696  7988 net.cpp:165] Memory required for data: 8086804
I1127 11:29:14.512702  7988 layer_factory.hpp:76] Creating layer loss
I1127 11:29:14.512717  7988 net.cpp:106] Creating Layer loss
I1127 11:29:14.512724  7988 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:29:14.512748  7988 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:29:14.512758  7988 net.cpp:411] loss -> loss
I1127 11:29:14.512774  7988 layer_factory.hpp:76] Creating layer loss
I1127 11:29:14.512938  7988 net.cpp:150] Setting up loss
I1127 11:29:14.512954  7988 net.cpp:157] Top shape: (1)
I1127 11:29:14.512962  7988 net.cpp:160]     with loss weight 1
I1127 11:29:14.512989  7988 net.cpp:165] Memory required for data: 8086808
I1127 11:29:14.512996  7988 net.cpp:226] loss needs backward computation.
I1127 11:29:14.513013  7988 net.cpp:228] accuracy does not need backward computation.
I1127 11:29:14.513022  7988 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:29:14.513031  7988 net.cpp:226] ip2 needs backward computation.
I1127 11:29:14.513039  7988 net.cpp:226] relu1 needs backward computation.
I1127 11:29:14.513047  7988 net.cpp:226] ip1 needs backward computation.
I1127 11:29:14.513054  7988 net.cpp:226] pool2 needs backward computation.
I1127 11:29:14.513064  7988 net.cpp:226] conv2 needs backward computation.
I1127 11:29:14.513072  7988 net.cpp:226] pool1 needs backward computation.
I1127 11:29:14.513082  7988 net.cpp:226] conv1 needs backward computation.
I1127 11:29:14.513089  7988 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:29:14.513098  7988 net.cpp:228] mnist does not need backward computation.
I1127 11:29:14.513106  7988 net.cpp:270] This network produces output accuracy
I1127 11:29:14.513114  7988 net.cpp:270] This network produces output loss
I1127 11:29:14.513146  7988 net.cpp:283] Network initialization done.
I1127 11:29:14.513326  7988 solver.cpp:59] Solver scaffolding done.
I1127 11:29:14.513833  7988 caffe.cpp:212] Starting Optimization
I1127 11:29:14.513867  7988 solver.cpp:287] Solving LeNet
I1127 11:29:14.513880  7988 solver.cpp:288] Learning Rate Policy: inv
I1127 11:29:14.515105  7988 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:29:15.940552  7988 solver.cpp:408]     Test net output #0: accuracy = 0.0944
I1127 11:29:15.940596  7988 solver.cpp:408]     Test net output #1: loss = 2.34566 (* 1 = 2.34566 loss)
I1127 11:29:15.971963  7988 solver.cpp:236] Iteration 0, loss = 2.34158
I1127 11:29:15.971983  7988 solver.cpp:252]     Train net output #0: loss = 2.34158 (* 1 = 2.34158 loss)
I1127 11:29:15.972000  7988 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:29:29.114951  7988 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:29:30.319984  7988 solver.cpp:408]     Test net output #0: accuracy = 0.9718
I1127 11:29:30.320034  7988 solver.cpp:408]     Test net output #1: loss = 0.0864314 (* 1 = 0.0864314 loss)
I1127 11:29:30.349198  7988 solver.cpp:236] Iteration 500, loss = 0.098831
I1127 11:29:30.349263  7988 solver.cpp:252]     Train net output #0: loss = 0.0988311 (* 1 = 0.0988311 loss)
I1127 11:29:30.349279  7988 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:29:44.180384  7988 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:29:44.201007  7988 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:29:44.228565  7988 solver.cpp:320] Iteration 1000, loss = 0.0897486
I1127 11:29:44.228587  7988 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:29:46.747864  7988 solver.cpp:408]     Test net output #0: accuracy = 0.98
I1127 11:29:46.747956  7988 solver.cpp:408]     Test net output #1: loss = 0.059907 (* 1 = 0.059907 loss)
I1127 11:29:46.747967  7988 solver.cpp:325] Optimization Done.
I1127 11:29:46.747972  7988 caffe.cpp:215] Optimization Done.
I1127 11:29:46.842314  8017 caffe.cpp:184] Using GPUs 0
I1127 11:29:47.128192  8017 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:29:47.128401  8017 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:29:47.128772  8017 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:29:47.128803  8017 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:29:47.128926  8017 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:29:47.129000  8017 layer_factory.hpp:76] Creating layer mnist
I1127 11:29:47.129456  8017 net.cpp:106] Creating Layer mnist
I1127 11:29:47.129490  8017 net.cpp:411] mnist -> data
I1127 11:29:47.129529  8017 net.cpp:411] mnist -> label
I1127 11:29:47.130739  8020 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:29:47.138898  8017 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:29:47.140158  8017 net.cpp:150] Setting up mnist
I1127 11:29:47.140192  8017 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:29:47.140202  8017 net.cpp:157] Top shape: 64 (64)
I1127 11:29:47.140208  8017 net.cpp:165] Memory required for data: 200960
I1127 11:29:47.140226  8017 layer_factory.hpp:76] Creating layer conv1
I1127 11:29:47.140247  8017 net.cpp:106] Creating Layer conv1
I1127 11:29:47.140257  8017 net.cpp:454] conv1 <- data
I1127 11:29:47.140274  8017 net.cpp:411] conv1 -> conv1
I1127 11:29:47.141047  8017 net.cpp:150] Setting up conv1
I1127 11:29:47.141072  8017 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:29:47.141078  8017 net.cpp:165] Memory required for data: 3150080
I1127 11:29:47.141098  8017 layer_factory.hpp:76] Creating layer pool1
I1127 11:29:47.141113  8017 net.cpp:106] Creating Layer pool1
I1127 11:29:47.141119  8017 net.cpp:454] pool1 <- conv1
I1127 11:29:47.141129  8017 net.cpp:411] pool1 -> pool1
I1127 11:29:47.141226  8017 net.cpp:150] Setting up pool1
I1127 11:29:47.141240  8017 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:29:47.141244  8017 net.cpp:165] Memory required for data: 3887360
I1127 11:29:47.141252  8017 layer_factory.hpp:76] Creating layer conv2
I1127 11:29:47.141270  8017 net.cpp:106] Creating Layer conv2
I1127 11:29:47.141278  8017 net.cpp:454] conv2 <- pool1
I1127 11:29:47.141297  8017 net.cpp:411] conv2 -> conv2
I1127 11:29:47.141795  8017 net.cpp:150] Setting up conv2
I1127 11:29:47.141823  8017 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:29:47.141829  8017 net.cpp:165] Memory required for data: 4706560
I1127 11:29:47.141841  8017 layer_factory.hpp:76] Creating layer pool2
I1127 11:29:47.141854  8017 net.cpp:106] Creating Layer pool2
I1127 11:29:47.141860  8017 net.cpp:454] pool2 <- conv2
I1127 11:29:47.141866  8017 net.cpp:411] pool2 -> pool2
I1127 11:29:47.141906  8017 net.cpp:150] Setting up pool2
I1127 11:29:47.141914  8017 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:29:47.141918  8017 net.cpp:165] Memory required for data: 4911360
I1127 11:29:47.141923  8017 layer_factory.hpp:76] Creating layer ip1
I1127 11:29:47.141937  8017 net.cpp:106] Creating Layer ip1
I1127 11:29:47.141943  8017 net.cpp:454] ip1 <- pool2
I1127 11:29:47.141950  8017 net.cpp:411] ip1 -> ip1
I1127 11:29:47.145866  8017 net.cpp:150] Setting up ip1
I1127 11:29:47.145927  8017 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:29:47.145934  8017 net.cpp:165] Memory required for data: 5039360
I1127 11:29:47.145956  8017 layer_factory.hpp:76] Creating layer relu1
I1127 11:29:47.145972  8017 net.cpp:106] Creating Layer relu1
I1127 11:29:47.145980  8017 net.cpp:454] relu1 <- ip1
I1127 11:29:47.145992  8017 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:29:47.146010  8017 net.cpp:150] Setting up relu1
I1127 11:29:47.146019  8017 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:29:47.146025  8017 net.cpp:165] Memory required for data: 5167360
I1127 11:29:47.146033  8017 layer_factory.hpp:76] Creating layer ip2
I1127 11:29:47.146047  8017 net.cpp:106] Creating Layer ip2
I1127 11:29:47.146054  8017 net.cpp:454] ip2 <- ip1
I1127 11:29:47.146064  8017 net.cpp:411] ip2 -> ip2
I1127 11:29:47.146917  8017 net.cpp:150] Setting up ip2
I1127 11:29:47.146962  8017 net.cpp:157] Top shape: 64 10 (640)
I1127 11:29:47.146970  8017 net.cpp:165] Memory required for data: 5169920
I1127 11:29:47.146984  8017 layer_factory.hpp:76] Creating layer loss
I1127 11:29:47.146999  8017 net.cpp:106] Creating Layer loss
I1127 11:29:47.147009  8017 net.cpp:454] loss <- ip2
I1127 11:29:47.147018  8017 net.cpp:454] loss <- label
I1127 11:29:47.147032  8017 net.cpp:411] loss -> loss
I1127 11:29:47.147053  8017 layer_factory.hpp:76] Creating layer loss
I1127 11:29:47.147155  8017 net.cpp:150] Setting up loss
I1127 11:29:47.147166  8017 net.cpp:157] Top shape: (1)
I1127 11:29:47.147171  8017 net.cpp:160]     with loss weight 1
I1127 11:29:47.147202  8017 net.cpp:165] Memory required for data: 5169924
I1127 11:29:47.147208  8017 net.cpp:226] loss needs backward computation.
I1127 11:29:47.147215  8017 net.cpp:226] ip2 needs backward computation.
I1127 11:29:47.147222  8017 net.cpp:226] relu1 needs backward computation.
I1127 11:29:47.147228  8017 net.cpp:226] ip1 needs backward computation.
I1127 11:29:47.147233  8017 net.cpp:226] pool2 needs backward computation.
I1127 11:29:47.147239  8017 net.cpp:226] conv2 needs backward computation.
I1127 11:29:47.147248  8017 net.cpp:226] pool1 needs backward computation.
I1127 11:29:47.147253  8017 net.cpp:226] conv1 needs backward computation.
I1127 11:29:47.147261  8017 net.cpp:228] mnist does not need backward computation.
I1127 11:29:47.147267  8017 net.cpp:270] This network produces output loss
I1127 11:29:47.147279  8017 net.cpp:283] Network initialization done.
I1127 11:29:47.147711  8017 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:29:47.147780  8017 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:29:47.148003  8017 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:29:47.148140  8017 layer_factory.hpp:76] Creating layer mnist
I1127 11:29:47.194254  8017 net.cpp:106] Creating Layer mnist
I1127 11:29:47.194300  8017 net.cpp:411] mnist -> data
I1127 11:29:47.194319  8017 net.cpp:411] mnist -> label
I1127 11:29:47.195104  8022 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:29:47.195258  8017 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:29:47.196596  8017 net.cpp:150] Setting up mnist
I1127 11:29:47.196636  8017 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:29:47.196646  8017 net.cpp:157] Top shape: 100 (100)
I1127 11:29:47.196652  8017 net.cpp:165] Memory required for data: 314000
I1127 11:29:47.196662  8017 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:29:47.196681  8017 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:29:47.196688  8017 net.cpp:454] label_mnist_1_split <- label
I1127 11:29:47.196698  8017 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:29:47.196712  8017 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:29:47.196760  8017 net.cpp:150] Setting up label_mnist_1_split
I1127 11:29:47.196770  8017 net.cpp:157] Top shape: 100 (100)
I1127 11:29:47.196779  8017 net.cpp:157] Top shape: 100 (100)
I1127 11:29:47.196784  8017 net.cpp:165] Memory required for data: 314800
I1127 11:29:47.196791  8017 layer_factory.hpp:76] Creating layer conv1
I1127 11:29:47.196810  8017 net.cpp:106] Creating Layer conv1
I1127 11:29:47.196817  8017 net.cpp:454] conv1 <- data
I1127 11:29:47.196828  8017 net.cpp:411] conv1 -> conv1
I1127 11:29:47.197094  8017 net.cpp:150] Setting up conv1
I1127 11:29:47.197116  8017 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:29:47.197123  8017 net.cpp:165] Memory required for data: 4922800
I1127 11:29:47.197137  8017 layer_factory.hpp:76] Creating layer pool1
I1127 11:29:47.197150  8017 net.cpp:106] Creating Layer pool1
I1127 11:29:47.197157  8017 net.cpp:454] pool1 <- conv1
I1127 11:29:47.197186  8017 net.cpp:411] pool1 -> pool1
I1127 11:29:47.197232  8017 net.cpp:150] Setting up pool1
I1127 11:29:47.197245  8017 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:29:47.197254  8017 net.cpp:165] Memory required for data: 6074800
I1127 11:29:47.197264  8017 layer_factory.hpp:76] Creating layer conv2
I1127 11:29:47.197298  8017 net.cpp:106] Creating Layer conv2
I1127 11:29:47.197309  8017 net.cpp:454] conv2 <- pool1
I1127 11:29:47.197324  8017 net.cpp:411] conv2 -> conv2
I1127 11:29:47.197922  8017 net.cpp:150] Setting up conv2
I1127 11:29:47.197942  8017 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:29:47.197949  8017 net.cpp:165] Memory required for data: 7354800
I1127 11:29:47.197967  8017 layer_factory.hpp:76] Creating layer pool2
I1127 11:29:47.197978  8017 net.cpp:106] Creating Layer pool2
I1127 11:29:47.197985  8017 net.cpp:454] pool2 <- conv2
I1127 11:29:47.197996  8017 net.cpp:411] pool2 -> pool2
I1127 11:29:47.198038  8017 net.cpp:150] Setting up pool2
I1127 11:29:47.198055  8017 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:29:47.198061  8017 net.cpp:165] Memory required for data: 7674800
I1127 11:29:47.198068  8017 layer_factory.hpp:76] Creating layer ip1
I1127 11:29:47.198082  8017 net.cpp:106] Creating Layer ip1
I1127 11:29:47.198088  8017 net.cpp:454] ip1 <- pool2
I1127 11:29:47.198099  8017 net.cpp:411] ip1 -> ip1
I1127 11:29:47.202467  8017 net.cpp:150] Setting up ip1
I1127 11:29:47.202574  8017 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:29:47.202585  8017 net.cpp:165] Memory required for data: 7874800
I1127 11:29:47.202616  8017 layer_factory.hpp:76] Creating layer relu1
I1127 11:29:47.202642  8017 net.cpp:106] Creating Layer relu1
I1127 11:29:47.202654  8017 net.cpp:454] relu1 <- ip1
I1127 11:29:47.202671  8017 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:29:47.202694  8017 net.cpp:150] Setting up relu1
I1127 11:29:47.202705  8017 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:29:47.202713  8017 net.cpp:165] Memory required for data: 8074800
I1127 11:29:47.202719  8017 layer_factory.hpp:76] Creating layer ip2
I1127 11:29:47.202745  8017 net.cpp:106] Creating Layer ip2
I1127 11:29:47.202757  8017 net.cpp:454] ip2 <- ip1
I1127 11:29:47.202771  8017 net.cpp:411] ip2 -> ip2
I1127 11:29:47.203018  8017 net.cpp:150] Setting up ip2
I1127 11:29:47.203038  8017 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:29:47.203047  8017 net.cpp:165] Memory required for data: 8078800
I1127 11:29:47.203060  8017 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:29:47.203076  8017 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:29:47.203085  8017 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:29:47.203101  8017 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:29:47.203116  8017 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:29:47.203171  8017 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:29:47.203186  8017 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:29:47.203196  8017 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:29:47.203203  8017 net.cpp:165] Memory required for data: 8086800
I1127 11:29:47.203210  8017 layer_factory.hpp:76] Creating layer accuracy
I1127 11:29:47.203229  8017 net.cpp:106] Creating Layer accuracy
I1127 11:29:47.203239  8017 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:29:47.203250  8017 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:29:47.203266  8017 net.cpp:411] accuracy -> accuracy
I1127 11:29:47.203284  8017 net.cpp:150] Setting up accuracy
I1127 11:29:47.203297  8017 net.cpp:157] Top shape: (1)
I1127 11:29:47.203306  8017 net.cpp:165] Memory required for data: 8086804
I1127 11:29:47.203315  8017 layer_factory.hpp:76] Creating layer loss
I1127 11:29:47.203328  8017 net.cpp:106] Creating Layer loss
I1127 11:29:47.203338  8017 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:29:47.203347  8017 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:29:47.203359  8017 net.cpp:411] loss -> loss
I1127 11:29:47.203378  8017 layer_factory.hpp:76] Creating layer loss
I1127 11:29:47.203563  8017 net.cpp:150] Setting up loss
I1127 11:29:47.203579  8017 net.cpp:157] Top shape: (1)
I1127 11:29:47.203588  8017 net.cpp:160]     with loss weight 1
I1127 11:29:47.203616  8017 net.cpp:165] Memory required for data: 8086808
I1127 11:29:47.203625  8017 net.cpp:226] loss needs backward computation.
I1127 11:29:47.203641  8017 net.cpp:228] accuracy does not need backward computation.
I1127 11:29:47.203658  8017 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:29:47.203666  8017 net.cpp:226] ip2 needs backward computation.
I1127 11:29:47.203677  8017 net.cpp:226] relu1 needs backward computation.
I1127 11:29:47.203685  8017 net.cpp:226] ip1 needs backward computation.
I1127 11:29:47.203692  8017 net.cpp:226] pool2 needs backward computation.
I1127 11:29:47.203701  8017 net.cpp:226] conv2 needs backward computation.
I1127 11:29:47.203708  8017 net.cpp:226] pool1 needs backward computation.
I1127 11:29:47.203717  8017 net.cpp:226] conv1 needs backward computation.
I1127 11:29:47.203726  8017 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:29:47.203733  8017 net.cpp:228] mnist does not need backward computation.
I1127 11:29:47.203740  8017 net.cpp:270] This network produces output accuracy
I1127 11:29:47.203748  8017 net.cpp:270] This network produces output loss
I1127 11:29:47.203771  8017 net.cpp:283] Network initialization done.
I1127 11:29:47.203896  8017 solver.cpp:59] Solver scaffolding done.
I1127 11:29:47.204264  8017 caffe.cpp:212] Starting Optimization
I1127 11:29:47.204283  8017 solver.cpp:287] Solving LeNet
I1127 11:29:47.204289  8017 solver.cpp:288] Learning Rate Policy: inv
I1127 11:29:47.205006  8017 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:29:47.205961  8017 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:29:49.285501  8017 solver.cpp:408]     Test net output #0: accuracy = 0.0948
I1127 11:29:49.285540  8017 solver.cpp:408]     Test net output #1: loss = 2.35852 (* 1 = 2.35852 loss)
I1127 11:29:49.316668  8017 solver.cpp:236] Iteration 0, loss = 2.36842
I1127 11:29:49.316704  8017 solver.cpp:252]     Train net output #0: loss = 2.36842 (* 1 = 2.36842 loss)
I1127 11:29:49.316715  8017 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:30:01.852864  8017 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:30:02.967648  8017 solver.cpp:408]     Test net output #0: accuracy = 0.9714
I1127 11:30:02.967782  8017 solver.cpp:408]     Test net output #1: loss = 0.0877849 (* 1 = 0.0877849 loss)
I1127 11:30:02.981217  8017 solver.cpp:236] Iteration 500, loss = 0.0893159
I1127 11:30:02.981300  8017 solver.cpp:252]     Train net output #0: loss = 0.0893159 (* 1 = 0.0893159 loss)
I1127 11:30:02.981318  8017 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:30:16.416592  8017 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:30:16.431282  8017 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:30:16.443110  8017 solver.cpp:320] Iteration 1000, loss = 0.0807101
I1127 11:30:16.443187  8017 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:30:18.800377  8017 solver.cpp:408]     Test net output #0: accuracy = 0.9844
I1127 11:30:18.800516  8017 solver.cpp:408]     Test net output #1: loss = 0.0524917 (* 1 = 0.0524917 loss)
I1127 11:30:18.800524  8017 solver.cpp:325] Optimization Done.
I1127 11:30:18.800529  8017 caffe.cpp:215] Optimization Done.
I1127 11:30:18.867202  8045 caffe.cpp:184] Using GPUs 0
I1127 11:30:19.302788  8045 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:30:19.302898  8045 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:30:19.303151  8045 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:30:19.303167  8045 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:30:19.303251  8045 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:30:19.303313  8045 layer_factory.hpp:76] Creating layer mnist
I1127 11:30:19.303625  8045 net.cpp:106] Creating Layer mnist
I1127 11:30:19.303637  8045 net.cpp:411] mnist -> data
I1127 11:30:19.303658  8045 net.cpp:411] mnist -> label
I1127 11:30:19.304397  8048 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:30:19.340566  8045 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:30:19.347146  8045 net.cpp:150] Setting up mnist
I1127 11:30:19.347165  8045 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:30:19.347172  8045 net.cpp:157] Top shape: 64 (64)
I1127 11:30:19.347177  8045 net.cpp:165] Memory required for data: 200960
I1127 11:30:19.347187  8045 layer_factory.hpp:76] Creating layer conv1
I1127 11:30:19.347204  8045 net.cpp:106] Creating Layer conv1
I1127 11:30:19.347210  8045 net.cpp:454] conv1 <- data
I1127 11:30:19.347221  8045 net.cpp:411] conv1 -> conv1
I1127 11:30:19.347812  8045 net.cpp:150] Setting up conv1
I1127 11:30:19.347823  8045 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:30:19.347827  8045 net.cpp:165] Memory required for data: 3150080
I1127 11:30:19.347839  8045 layer_factory.hpp:76] Creating layer pool1
I1127 11:30:19.347851  8045 net.cpp:106] Creating Layer pool1
I1127 11:30:19.347856  8045 net.cpp:454] pool1 <- conv1
I1127 11:30:19.347863  8045 net.cpp:411] pool1 -> pool1
I1127 11:30:19.347911  8045 net.cpp:150] Setting up pool1
I1127 11:30:19.347918  8045 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:30:19.347923  8045 net.cpp:165] Memory required for data: 3887360
I1127 11:30:19.347928  8045 layer_factory.hpp:76] Creating layer conv2
I1127 11:30:19.347936  8045 net.cpp:106] Creating Layer conv2
I1127 11:30:19.347940  8045 net.cpp:454] conv2 <- pool1
I1127 11:30:19.347947  8045 net.cpp:411] conv2 -> conv2
I1127 11:30:19.348194  8045 net.cpp:150] Setting up conv2
I1127 11:30:19.348203  8045 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:30:19.348207  8045 net.cpp:165] Memory required for data: 4706560
I1127 11:30:19.348215  8045 layer_factory.hpp:76] Creating layer pool2
I1127 11:30:19.348224  8045 net.cpp:106] Creating Layer pool2
I1127 11:30:19.348228  8045 net.cpp:454] pool2 <- conv2
I1127 11:30:19.348234  8045 net.cpp:411] pool2 -> pool2
I1127 11:30:19.348389  8045 net.cpp:150] Setting up pool2
I1127 11:30:19.348403  8045 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:30:19.348410  8045 net.cpp:165] Memory required for data: 4911360
I1127 11:30:19.348419  8045 layer_factory.hpp:76] Creating layer ip1
I1127 11:30:19.348433  8045 net.cpp:106] Creating Layer ip1
I1127 11:30:19.348439  8045 net.cpp:454] ip1 <- pool2
I1127 11:30:19.348448  8045 net.cpp:411] ip1 -> ip1
I1127 11:30:19.350543  8045 net.cpp:150] Setting up ip1
I1127 11:30:19.350553  8045 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:30:19.350558  8045 net.cpp:165] Memory required for data: 5039360
I1127 11:30:19.350566  8045 layer_factory.hpp:76] Creating layer relu1
I1127 11:30:19.350574  8045 net.cpp:106] Creating Layer relu1
I1127 11:30:19.350577  8045 net.cpp:454] relu1 <- ip1
I1127 11:30:19.350584  8045 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:30:19.350594  8045 net.cpp:150] Setting up relu1
I1127 11:30:19.350599  8045 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:30:19.350602  8045 net.cpp:165] Memory required for data: 5167360
I1127 11:30:19.350606  8045 layer_factory.hpp:76] Creating layer ip2
I1127 11:30:19.350613  8045 net.cpp:106] Creating Layer ip2
I1127 11:30:19.350617  8045 net.cpp:454] ip2 <- ip1
I1127 11:30:19.350625  8045 net.cpp:411] ip2 -> ip2
I1127 11:30:19.351003  8045 net.cpp:150] Setting up ip2
I1127 11:30:19.351013  8045 net.cpp:157] Top shape: 64 10 (640)
I1127 11:30:19.351017  8045 net.cpp:165] Memory required for data: 5169920
I1127 11:30:19.351024  8045 layer_factory.hpp:76] Creating layer loss
I1127 11:30:19.351033  8045 net.cpp:106] Creating Layer loss
I1127 11:30:19.351038  8045 net.cpp:454] loss <- ip2
I1127 11:30:19.351043  8045 net.cpp:454] loss <- label
I1127 11:30:19.351050  8045 net.cpp:411] loss -> loss
I1127 11:30:19.351063  8045 layer_factory.hpp:76] Creating layer loss
I1127 11:30:19.351127  8045 net.cpp:150] Setting up loss
I1127 11:30:19.351135  8045 net.cpp:157] Top shape: (1)
I1127 11:30:19.351140  8045 net.cpp:160]     with loss weight 1
I1127 11:30:19.351155  8045 net.cpp:165] Memory required for data: 5169924
I1127 11:30:19.351160  8045 net.cpp:226] loss needs backward computation.
I1127 11:30:19.351163  8045 net.cpp:226] ip2 needs backward computation.
I1127 11:30:19.351167  8045 net.cpp:226] relu1 needs backward computation.
I1127 11:30:19.351171  8045 net.cpp:226] ip1 needs backward computation.
I1127 11:30:19.351176  8045 net.cpp:226] pool2 needs backward computation.
I1127 11:30:19.351181  8045 net.cpp:226] conv2 needs backward computation.
I1127 11:30:19.351184  8045 net.cpp:226] pool1 needs backward computation.
I1127 11:30:19.351188  8045 net.cpp:226] conv1 needs backward computation.
I1127 11:30:19.351193  8045 net.cpp:228] mnist does not need backward computation.
I1127 11:30:19.351197  8045 net.cpp:270] This network produces output loss
I1127 11:30:19.351205  8045 net.cpp:283] Network initialization done.
I1127 11:30:19.351436  8045 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:30:19.351459  8045 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:30:19.351567  8045 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:30:19.351627  8045 layer_factory.hpp:76] Creating layer mnist
I1127 11:30:19.351711  8045 net.cpp:106] Creating Layer mnist
I1127 11:30:19.351721  8045 net.cpp:411] mnist -> data
I1127 11:30:19.351730  8045 net.cpp:411] mnist -> label
I1127 11:30:19.352448  8050 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:30:19.352557  8045 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:30:19.442055  8045 net.cpp:150] Setting up mnist
I1127 11:30:19.442096  8045 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:30:19.442103  8045 net.cpp:157] Top shape: 100 (100)
I1127 11:30:19.442108  8045 net.cpp:165] Memory required for data: 314000
I1127 11:30:19.442116  8045 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:30:19.442131  8045 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:30:19.442138  8045 net.cpp:454] label_mnist_1_split <- label
I1127 11:30:19.442154  8045 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:30:19.442167  8045 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:30:19.442328  8045 net.cpp:150] Setting up label_mnist_1_split
I1127 11:30:19.442345  8045 net.cpp:157] Top shape: 100 (100)
I1127 11:30:19.442350  8045 net.cpp:157] Top shape: 100 (100)
I1127 11:30:19.442355  8045 net.cpp:165] Memory required for data: 314800
I1127 11:30:19.442360  8045 layer_factory.hpp:76] Creating layer conv1
I1127 11:30:19.442378  8045 net.cpp:106] Creating Layer conv1
I1127 11:30:19.442384  8045 net.cpp:454] conv1 <- data
I1127 11:30:19.442390  8045 net.cpp:411] conv1 -> conv1
I1127 11:30:19.442560  8045 net.cpp:150] Setting up conv1
I1127 11:30:19.442569  8045 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:30:19.442574  8045 net.cpp:165] Memory required for data: 4922800
I1127 11:30:19.442584  8045 layer_factory.hpp:76] Creating layer pool1
I1127 11:30:19.442591  8045 net.cpp:106] Creating Layer pool1
I1127 11:30:19.442595  8045 net.cpp:454] pool1 <- conv1
I1127 11:30:19.442615  8045 net.cpp:411] pool1 -> pool1
I1127 11:30:19.442646  8045 net.cpp:150] Setting up pool1
I1127 11:30:19.442652  8045 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:30:19.442656  8045 net.cpp:165] Memory required for data: 6074800
I1127 11:30:19.442661  8045 layer_factory.hpp:76] Creating layer conv2
I1127 11:30:19.442672  8045 net.cpp:106] Creating Layer conv2
I1127 11:30:19.442677  8045 net.cpp:454] conv2 <- pool1
I1127 11:30:19.442683  8045 net.cpp:411] conv2 -> conv2
I1127 11:30:19.442942  8045 net.cpp:150] Setting up conv2
I1127 11:30:19.442951  8045 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:30:19.442955  8045 net.cpp:165] Memory required for data: 7354800
I1127 11:30:19.442963  8045 layer_factory.hpp:76] Creating layer pool2
I1127 11:30:19.442971  8045 net.cpp:106] Creating Layer pool2
I1127 11:30:19.442980  8045 net.cpp:454] pool2 <- conv2
I1127 11:30:19.442984  8045 net.cpp:411] pool2 -> pool2
I1127 11:30:19.443018  8045 net.cpp:150] Setting up pool2
I1127 11:30:19.443025  8045 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:30:19.443029  8045 net.cpp:165] Memory required for data: 7674800
I1127 11:30:19.443034  8045 layer_factory.hpp:76] Creating layer ip1
I1127 11:30:19.443043  8045 net.cpp:106] Creating Layer ip1
I1127 11:30:19.443048  8045 net.cpp:454] ip1 <- pool2
I1127 11:30:19.443054  8045 net.cpp:411] ip1 -> ip1
I1127 11:30:19.445327  8045 net.cpp:150] Setting up ip1
I1127 11:30:19.445340  8045 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:30:19.445344  8045 net.cpp:165] Memory required for data: 7874800
I1127 11:30:19.445353  8045 layer_factory.hpp:76] Creating layer relu1
I1127 11:30:19.445360  8045 net.cpp:106] Creating Layer relu1
I1127 11:30:19.445365  8045 net.cpp:454] relu1 <- ip1
I1127 11:30:19.445371  8045 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:30:19.445379  8045 net.cpp:150] Setting up relu1
I1127 11:30:19.445384  8045 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:30:19.445389  8045 net.cpp:165] Memory required for data: 8074800
I1127 11:30:19.445392  8045 layer_factory.hpp:76] Creating layer ip2
I1127 11:30:19.445404  8045 net.cpp:106] Creating Layer ip2
I1127 11:30:19.445407  8045 net.cpp:454] ip2 <- ip1
I1127 11:30:19.445415  8045 net.cpp:411] ip2 -> ip2
I1127 11:30:19.445516  8045 net.cpp:150] Setting up ip2
I1127 11:30:19.445525  8045 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:30:19.445529  8045 net.cpp:165] Memory required for data: 8078800
I1127 11:30:19.445535  8045 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:30:19.445543  8045 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:30:19.445546  8045 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:30:19.445552  8045 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:30:19.445559  8045 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:30:19.445588  8045 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:30:19.445595  8045 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:30:19.445600  8045 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:30:19.445605  8045 net.cpp:165] Memory required for data: 8086800
I1127 11:30:19.445608  8045 layer_factory.hpp:76] Creating layer accuracy
I1127 11:30:19.445616  8045 net.cpp:106] Creating Layer accuracy
I1127 11:30:19.445621  8045 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:30:19.445626  8045 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:30:19.445632  8045 net.cpp:411] accuracy -> accuracy
I1127 11:30:19.445641  8045 net.cpp:150] Setting up accuracy
I1127 11:30:19.445647  8045 net.cpp:157] Top shape: (1)
I1127 11:30:19.445652  8045 net.cpp:165] Memory required for data: 8086804
I1127 11:30:19.445655  8045 layer_factory.hpp:76] Creating layer loss
I1127 11:30:19.445662  8045 net.cpp:106] Creating Layer loss
I1127 11:30:19.445667  8045 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:30:19.445672  8045 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:30:19.445678  8045 net.cpp:411] loss -> loss
I1127 11:30:19.445686  8045 layer_factory.hpp:76] Creating layer loss
I1127 11:30:19.445758  8045 net.cpp:150] Setting up loss
I1127 11:30:19.445771  8045 net.cpp:157] Top shape: (1)
I1127 11:30:19.445775  8045 net.cpp:160]     with loss weight 1
I1127 11:30:19.445787  8045 net.cpp:165] Memory required for data: 8086808
I1127 11:30:19.445791  8045 net.cpp:226] loss needs backward computation.
I1127 11:30:19.445799  8045 net.cpp:228] accuracy does not need backward computation.
I1127 11:30:19.445803  8045 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:30:19.445808  8045 net.cpp:226] ip2 needs backward computation.
I1127 11:30:19.445812  8045 net.cpp:226] relu1 needs backward computation.
I1127 11:30:19.445816  8045 net.cpp:226] ip1 needs backward computation.
I1127 11:30:19.445821  8045 net.cpp:226] pool2 needs backward computation.
I1127 11:30:19.445824  8045 net.cpp:226] conv2 needs backward computation.
I1127 11:30:19.445832  8045 net.cpp:226] pool1 needs backward computation.
I1127 11:30:19.445837  8045 net.cpp:226] conv1 needs backward computation.
I1127 11:30:19.445842  8045 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:30:19.445847  8045 net.cpp:228] mnist does not need backward computation.
I1127 11:30:19.445850  8045 net.cpp:270] This network produces output accuracy
I1127 11:30:19.445855  8045 net.cpp:270] This network produces output loss
I1127 11:30:19.445864  8045 net.cpp:283] Network initialization done.
I1127 11:30:19.445909  8045 solver.cpp:59] Solver scaffolding done.
I1127 11:30:19.446113  8045 caffe.cpp:212] Starting Optimization
I1127 11:30:19.446120  8045 solver.cpp:287] Solving LeNet
I1127 11:30:19.446125  8045 solver.cpp:288] Learning Rate Policy: inv
I1127 11:30:19.446481  8045 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:30:20.650063  8045 solver.cpp:408]     Test net output #0: accuracy = 0.1112
I1127 11:30:20.650111  8045 solver.cpp:408]     Test net output #1: loss = 2.32226 (* 1 = 2.32226 loss)
I1127 11:30:20.660908  8045 solver.cpp:236] Iteration 0, loss = 2.33783
I1127 11:30:20.661022  8045 solver.cpp:252]     Train net output #0: loss = 2.33783 (* 1 = 2.33783 loss)
I1127 11:30:20.661068  8045 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:30:34.123725  8045 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:30:35.016142  8045 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:30:35.409971  8045 solver.cpp:408]     Test net output #0: accuracy = 0.9702
I1127 11:30:35.410014  8045 solver.cpp:408]     Test net output #1: loss = 0.0876749 (* 1 = 0.0876749 loss)
I1127 11:30:35.439977  8045 solver.cpp:236] Iteration 500, loss = 0.105958
I1127 11:30:35.439995  8045 solver.cpp:252]     Train net output #0: loss = 0.105958 (* 1 = 0.105958 loss)
I1127 11:30:35.440003  8045 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:30:47.743249  8045 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:30:47.763278  8045 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:30:47.789633  8045 solver.cpp:320] Iteration 1000, loss = 0.0814022
I1127 11:30:47.789654  8045 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:30:49.936148  8045 solver.cpp:408]     Test net output #0: accuracy = 0.9823
I1127 11:30:49.936363  8045 solver.cpp:408]     Test net output #1: loss = 0.0538684 (* 1 = 0.0538684 loss)
I1127 11:30:49.936372  8045 solver.cpp:325] Optimization Done.
I1127 11:30:49.936378  8045 caffe.cpp:215] Optimization Done.
I1127 11:30:50.046635  8101 caffe.cpp:184] Using GPUs 0
I1127 11:30:50.393044  8101 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:30:50.393465  8101 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:30:50.394121  8101 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:30:50.394186  8101 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:30:50.394340  8101 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:30:50.394448  8101 layer_factory.hpp:76] Creating layer mnist
I1127 11:30:50.394840  8101 net.cpp:106] Creating Layer mnist
I1127 11:30:50.394860  8101 net.cpp:411] mnist -> data
I1127 11:30:50.394896  8101 net.cpp:411] mnist -> label
I1127 11:30:50.395993  8104 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:30:50.413573  8101 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:30:50.416118  8101 net.cpp:150] Setting up mnist
I1127 11:30:50.416263  8101 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:30:50.416277  8101 net.cpp:157] Top shape: 64 (64)
I1127 11:30:50.416283  8101 net.cpp:165] Memory required for data: 200960
I1127 11:30:50.416308  8101 layer_factory.hpp:76] Creating layer conv1
I1127 11:30:50.416363  8101 net.cpp:106] Creating Layer conv1
I1127 11:30:50.416383  8101 net.cpp:454] conv1 <- data
I1127 11:30:50.416411  8101 net.cpp:411] conv1 -> conv1
I1127 11:30:50.422407  8101 net.cpp:150] Setting up conv1
I1127 11:30:50.422487  8101 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:30:50.422495  8101 net.cpp:165] Memory required for data: 3150080
I1127 11:30:50.422550  8101 layer_factory.hpp:76] Creating layer pool1
I1127 11:30:50.422597  8101 net.cpp:106] Creating Layer pool1
I1127 11:30:50.422617  8101 net.cpp:454] pool1 <- conv1
I1127 11:30:50.422641  8101 net.cpp:411] pool1 -> pool1
I1127 11:30:50.422797  8101 net.cpp:150] Setting up pool1
I1127 11:30:50.422814  8101 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:30:50.422822  8101 net.cpp:165] Memory required for data: 3887360
I1127 11:30:50.422832  8101 layer_factory.hpp:76] Creating layer conv2
I1127 11:30:50.422857  8101 net.cpp:106] Creating Layer conv2
I1127 11:30:50.422866  8101 net.cpp:454] conv2 <- pool1
I1127 11:30:50.422885  8101 net.cpp:411] conv2 -> conv2
I1127 11:30:50.423414  8101 net.cpp:150] Setting up conv2
I1127 11:30:50.423452  8101 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:30:50.423460  8101 net.cpp:165] Memory required for data: 4706560
I1127 11:30:50.423480  8101 layer_factory.hpp:76] Creating layer pool2
I1127 11:30:50.423501  8101 net.cpp:106] Creating Layer pool2
I1127 11:30:50.423511  8101 net.cpp:454] pool2 <- conv2
I1127 11:30:50.423522  8101 net.cpp:411] pool2 -> pool2
I1127 11:30:50.423578  8101 net.cpp:150] Setting up pool2
I1127 11:30:50.423590  8101 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:30:50.423598  8101 net.cpp:165] Memory required for data: 4911360
I1127 11:30:50.423607  8101 layer_factory.hpp:76] Creating layer ip1
I1127 11:30:50.423622  8101 net.cpp:106] Creating Layer ip1
I1127 11:30:50.423631  8101 net.cpp:454] ip1 <- pool2
I1127 11:30:50.423645  8101 net.cpp:411] ip1 -> ip1
I1127 11:30:50.428398  8101 net.cpp:150] Setting up ip1
I1127 11:30:50.428459  8101 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:30:50.428467  8101 net.cpp:165] Memory required for data: 5039360
I1127 11:30:50.428488  8101 layer_factory.hpp:76] Creating layer relu1
I1127 11:30:50.428504  8101 net.cpp:106] Creating Layer relu1
I1127 11:30:50.428514  8101 net.cpp:454] relu1 <- ip1
I1127 11:30:50.428524  8101 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:30:50.428545  8101 net.cpp:150] Setting up relu1
I1127 11:30:50.428555  8101 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:30:50.428563  8101 net.cpp:165] Memory required for data: 5167360
I1127 11:30:50.428571  8101 layer_factory.hpp:76] Creating layer ip2
I1127 11:30:50.428591  8101 net.cpp:106] Creating Layer ip2
I1127 11:30:50.428599  8101 net.cpp:454] ip2 <- ip1
I1127 11:30:50.428612  8101 net.cpp:411] ip2 -> ip2
I1127 11:30:50.430166  8101 net.cpp:150] Setting up ip2
I1127 11:30:50.430251  8101 net.cpp:157] Top shape: 64 10 (640)
I1127 11:30:50.430260  8101 net.cpp:165] Memory required for data: 5169920
I1127 11:30:50.430279  8101 layer_factory.hpp:76] Creating layer loss
I1127 11:30:50.430306  8101 net.cpp:106] Creating Layer loss
I1127 11:30:50.430316  8101 net.cpp:454] loss <- ip2
I1127 11:30:50.430331  8101 net.cpp:454] loss <- label
I1127 11:30:50.430353  8101 net.cpp:411] loss -> loss
I1127 11:30:50.430392  8101 layer_factory.hpp:76] Creating layer loss
I1127 11:30:50.430538  8101 net.cpp:150] Setting up loss
I1127 11:30:50.430553  8101 net.cpp:157] Top shape: (1)
I1127 11:30:50.430560  8101 net.cpp:160]     with loss weight 1
I1127 11:30:50.430608  8101 net.cpp:165] Memory required for data: 5169924
I1127 11:30:50.430616  8101 net.cpp:226] loss needs backward computation.
I1127 11:30:50.430624  8101 net.cpp:226] ip2 needs backward computation.
I1127 11:30:50.430635  8101 net.cpp:226] relu1 needs backward computation.
I1127 11:30:50.430644  8101 net.cpp:226] ip1 needs backward computation.
I1127 11:30:50.430651  8101 net.cpp:226] pool2 needs backward computation.
I1127 11:30:50.430660  8101 net.cpp:226] conv2 needs backward computation.
I1127 11:30:50.430672  8101 net.cpp:226] pool1 needs backward computation.
I1127 11:30:50.430681  8101 net.cpp:226] conv1 needs backward computation.
I1127 11:30:50.430688  8101 net.cpp:228] mnist does not need backward computation.
I1127 11:30:50.430696  8101 net.cpp:270] This network produces output loss
I1127 11:30:50.430716  8101 net.cpp:283] Network initialization done.
I1127 11:30:50.431231  8101 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:30:50.431309  8101 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:30:50.431566  8101 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:30:50.431715  8101 layer_factory.hpp:76] Creating layer mnist
I1127 11:30:50.431991  8101 net.cpp:106] Creating Layer mnist
I1127 11:30:50.432013  8101 net.cpp:411] mnist -> data
I1127 11:30:50.432054  8101 net.cpp:411] mnist -> label
I1127 11:30:50.435703  8106 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:30:50.438458  8101 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:30:50.441499  8101 net.cpp:150] Setting up mnist
I1127 11:30:50.441612  8101 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:30:50.441627  8101 net.cpp:157] Top shape: 100 (100)
I1127 11:30:50.441634  8101 net.cpp:165] Memory required for data: 314000
I1127 11:30:50.441649  8101 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:30:50.441681  8101 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:30:50.441691  8101 net.cpp:454] label_mnist_1_split <- label
I1127 11:30:50.441707  8101 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:30:50.441730  8101 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:30:50.441836  8101 net.cpp:150] Setting up label_mnist_1_split
I1127 11:30:50.441854  8101 net.cpp:157] Top shape: 100 (100)
I1127 11:30:50.441861  8101 net.cpp:157] Top shape: 100 (100)
I1127 11:30:50.441869  8101 net.cpp:165] Memory required for data: 314800
I1127 11:30:50.441876  8101 layer_factory.hpp:76] Creating layer conv1
I1127 11:30:50.441910  8101 net.cpp:106] Creating Layer conv1
I1127 11:30:50.441920  8101 net.cpp:454] conv1 <- data
I1127 11:30:50.441932  8101 net.cpp:411] conv1 -> conv1
I1127 11:30:50.442260  8101 net.cpp:150] Setting up conv1
I1127 11:30:50.442279  8101 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:30:50.442286  8101 net.cpp:165] Memory required for data: 4922800
I1127 11:30:50.442303  8101 layer_factory.hpp:76] Creating layer pool1
I1127 11:30:50.442319  8101 net.cpp:106] Creating Layer pool1
I1127 11:30:50.442325  8101 net.cpp:454] pool1 <- conv1
I1127 11:30:50.442373  8101 net.cpp:411] pool1 -> pool1
I1127 11:30:50.442442  8101 net.cpp:150] Setting up pool1
I1127 11:30:50.442458  8101 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:30:50.442466  8101 net.cpp:165] Memory required for data: 6074800
I1127 11:30:50.442473  8101 layer_factory.hpp:76] Creating layer conv2
I1127 11:30:50.442494  8101 net.cpp:106] Creating Layer conv2
I1127 11:30:50.442505  8101 net.cpp:454] conv2 <- pool1
I1127 11:30:50.442520  8101 net.cpp:411] conv2 -> conv2
I1127 11:30:50.443193  8101 net.cpp:150] Setting up conv2
I1127 11:30:50.443271  8101 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:30:50.443291  8101 net.cpp:165] Memory required for data: 7354800
I1127 11:30:50.443346  8101 layer_factory.hpp:76] Creating layer pool2
I1127 11:30:50.443395  8101 net.cpp:106] Creating Layer pool2
I1127 11:30:50.443420  8101 net.cpp:454] pool2 <- conv2
I1127 11:30:50.443441  8101 net.cpp:411] pool2 -> pool2
I1127 11:30:50.443541  8101 net.cpp:150] Setting up pool2
I1127 11:30:50.443564  8101 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:30:50.443577  8101 net.cpp:165] Memory required for data: 7674800
I1127 11:30:50.443590  8101 layer_factory.hpp:76] Creating layer ip1
I1127 11:30:50.443620  8101 net.cpp:106] Creating Layer ip1
I1127 11:30:50.443636  8101 net.cpp:454] ip1 <- pool2
I1127 11:30:50.443691  8101 net.cpp:411] ip1 -> ip1
I1127 11:30:50.448670  8101 net.cpp:150] Setting up ip1
I1127 11:30:50.448758  8101 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:30:50.448788  8101 net.cpp:165] Memory required for data: 7874800
I1127 11:30:50.448822  8101 layer_factory.hpp:76] Creating layer relu1
I1127 11:30:50.448849  8101 net.cpp:106] Creating Layer relu1
I1127 11:30:50.448863  8101 net.cpp:454] relu1 <- ip1
I1127 11:30:50.448884  8101 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:30:50.448911  8101 net.cpp:150] Setting up relu1
I1127 11:30:50.448922  8101 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:30:50.448928  8101 net.cpp:165] Memory required for data: 8074800
I1127 11:30:50.448935  8101 layer_factory.hpp:76] Creating layer ip2
I1127 11:30:50.448963  8101 net.cpp:106] Creating Layer ip2
I1127 11:30:50.448976  8101 net.cpp:454] ip2 <- ip1
I1127 11:30:50.449007  8101 net.cpp:411] ip2 -> ip2
I1127 11:30:50.449276  8101 net.cpp:150] Setting up ip2
I1127 11:30:50.449298  8101 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:30:50.449306  8101 net.cpp:165] Memory required for data: 8078800
I1127 11:30:50.449319  8101 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:30:50.449334  8101 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:30:50.449343  8101 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:30:50.449353  8101 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:30:50.449368  8101 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:30:50.449419  8101 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:30:50.449434  8101 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:30:50.449445  8101 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:30:50.449450  8101 net.cpp:165] Memory required for data: 8086800
I1127 11:30:50.449457  8101 layer_factory.hpp:76] Creating layer accuracy
I1127 11:30:50.449472  8101 net.cpp:106] Creating Layer accuracy
I1127 11:30:50.449482  8101 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:30:50.449492  8101 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:30:50.449504  8101 net.cpp:411] accuracy -> accuracy
I1127 11:30:50.449522  8101 net.cpp:150] Setting up accuracy
I1127 11:30:50.449532  8101 net.cpp:157] Top shape: (1)
I1127 11:30:50.449538  8101 net.cpp:165] Memory required for data: 8086804
I1127 11:30:50.449545  8101 layer_factory.hpp:76] Creating layer loss
I1127 11:30:50.449561  8101 net.cpp:106] Creating Layer loss
I1127 11:30:50.449571  8101 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:30:50.449581  8101 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:30:50.449594  8101 net.cpp:411] loss -> loss
I1127 11:30:50.449612  8101 layer_factory.hpp:76] Creating layer loss
I1127 11:30:50.449790  8101 net.cpp:150] Setting up loss
I1127 11:30:50.449807  8101 net.cpp:157] Top shape: (1)
I1127 11:30:50.449813  8101 net.cpp:160]     with loss weight 1
I1127 11:30:50.449841  8101 net.cpp:165] Memory required for data: 8086808
I1127 11:30:50.449851  8101 net.cpp:226] loss needs backward computation.
I1127 11:30:50.449872  8101 net.cpp:228] accuracy does not need backward computation.
I1127 11:30:50.449882  8101 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:30:50.449890  8101 net.cpp:226] ip2 needs backward computation.
I1127 11:30:50.449898  8101 net.cpp:226] relu1 needs backward computation.
I1127 11:30:50.449904  8101 net.cpp:226] ip1 needs backward computation.
I1127 11:30:50.449911  8101 net.cpp:226] pool2 needs backward computation.
I1127 11:30:50.449919  8101 net.cpp:226] conv2 needs backward computation.
I1127 11:30:50.449928  8101 net.cpp:226] pool1 needs backward computation.
I1127 11:30:50.449935  8101 net.cpp:226] conv1 needs backward computation.
I1127 11:30:50.449944  8101 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:30:50.449956  8101 net.cpp:228] mnist does not need backward computation.
I1127 11:30:50.449965  8101 net.cpp:270] This network produces output accuracy
I1127 11:30:50.449975  8101 net.cpp:270] This network produces output loss
I1127 11:30:50.450006  8101 net.cpp:283] Network initialization done.
I1127 11:30:50.458374  8101 solver.cpp:59] Solver scaffolding done.
I1127 11:30:50.459074  8101 caffe.cpp:212] Starting Optimization
I1127 11:30:50.459113  8101 solver.cpp:287] Solving LeNet
I1127 11:30:50.459122  8101 solver.cpp:288] Learning Rate Policy: inv
I1127 11:30:50.460223  8101 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:30:52.201558  8101 solver.cpp:408]     Test net output #0: accuracy = 0.0861
I1127 11:30:52.201630  8101 solver.cpp:408]     Test net output #1: loss = 2.39309 (* 1 = 2.39309 loss)
I1127 11:30:52.234979  8101 solver.cpp:236] Iteration 0, loss = 2.37973
I1127 11:30:52.235028  8101 solver.cpp:252]     Train net output #0: loss = 2.37973 (* 1 = 2.37973 loss)
I1127 11:30:52.235049  8101 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:31:03.454049  8101 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:31:05.569563  8101 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:31:08.278065  8101 solver.cpp:408]     Test net output #0: accuracy = 0.9712
I1127 11:31:08.278182  8101 solver.cpp:408]     Test net output #1: loss = 0.0924149 (* 1 = 0.0924149 loss)
I1127 11:31:08.287911  8101 solver.cpp:236] Iteration 500, loss = 0.118002
I1127 11:31:08.287977  8101 solver.cpp:252]     Train net output #0: loss = 0.118002 (* 1 = 0.118002 loss)
I1127 11:31:08.287992  8101 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:31:21.359911  8101 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:31:21.381111  8101 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:31:21.410373  8101 solver.cpp:320] Iteration 1000, loss = 0.0778577
I1127 11:31:21.410400  8101 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:31:23.151489  8101 solver.cpp:408]     Test net output #0: accuracy = 0.9817
I1127 11:31:23.151566  8101 solver.cpp:408]     Test net output #1: loss = 0.057031 (* 1 = 0.057031 loss)
I1127 11:31:23.151574  8101 solver.cpp:325] Optimization Done.
I1127 11:31:23.151581  8101 caffe.cpp:215] Optimization Done.
I1127 11:31:23.264094  8149 caffe.cpp:184] Using GPUs 0
I1127 11:31:23.710254  8149 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:31:23.710628  8149 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:31:23.711133  8149 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:31:23.711169  8149 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:31:23.711335  8149 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:31:23.711490  8149 layer_factory.hpp:76] Creating layer mnist
I1127 11:31:23.712085  8149 net.cpp:106] Creating Layer mnist
I1127 11:31:23.712126  8149 net.cpp:411] mnist -> data
I1127 11:31:23.712182  8149 net.cpp:411] mnist -> label
I1127 11:31:23.713723  8154 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:31:23.743778  8149 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:31:23.752148  8149 net.cpp:150] Setting up mnist
I1127 11:31:23.752275  8149 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:31:23.752293  8149 net.cpp:157] Top shape: 64 (64)
I1127 11:31:23.752303  8149 net.cpp:165] Memory required for data: 200960
I1127 11:31:23.752328  8149 layer_factory.hpp:76] Creating layer conv1
I1127 11:31:23.752385  8149 net.cpp:106] Creating Layer conv1
I1127 11:31:23.752405  8149 net.cpp:454] conv1 <- data
I1127 11:31:23.752435  8149 net.cpp:411] conv1 -> conv1
I1127 11:31:23.753877  8149 net.cpp:150] Setting up conv1
I1127 11:31:23.753933  8149 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:31:23.753945  8149 net.cpp:165] Memory required for data: 3150080
I1127 11:31:23.753976  8149 layer_factory.hpp:76] Creating layer pool1
I1127 11:31:23.754006  8149 net.cpp:106] Creating Layer pool1
I1127 11:31:23.754019  8149 net.cpp:454] pool1 <- conv1
I1127 11:31:23.754040  8149 net.cpp:411] pool1 -> pool1
I1127 11:31:23.754139  8149 net.cpp:150] Setting up pool1
I1127 11:31:23.754178  8149 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:31:23.754189  8149 net.cpp:165] Memory required for data: 3887360
I1127 11:31:23.754199  8149 layer_factory.hpp:76] Creating layer conv2
I1127 11:31:23.754223  8149 net.cpp:106] Creating Layer conv2
I1127 11:31:23.754232  8149 net.cpp:454] conv2 <- pool1
I1127 11:31:23.754245  8149 net.cpp:411] conv2 -> conv2
I1127 11:31:23.754683  8149 net.cpp:150] Setting up conv2
I1127 11:31:23.754703  8149 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:31:23.754709  8149 net.cpp:165] Memory required for data: 4706560
I1127 11:31:23.754720  8149 layer_factory.hpp:76] Creating layer pool2
I1127 11:31:23.754732  8149 net.cpp:106] Creating Layer pool2
I1127 11:31:23.754737  8149 net.cpp:454] pool2 <- conv2
I1127 11:31:23.754744  8149 net.cpp:411] pool2 -> pool2
I1127 11:31:23.754776  8149 net.cpp:150] Setting up pool2
I1127 11:31:23.754783  8149 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:31:23.754787  8149 net.cpp:165] Memory required for data: 4911360
I1127 11:31:23.754791  8149 layer_factory.hpp:76] Creating layer ip1
I1127 11:31:23.754801  8149 net.cpp:106] Creating Layer ip1
I1127 11:31:23.754806  8149 net.cpp:454] ip1 <- pool2
I1127 11:31:23.754813  8149 net.cpp:411] ip1 -> ip1
I1127 11:31:23.757336  8149 net.cpp:150] Setting up ip1
I1127 11:31:23.757381  8149 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:31:23.757386  8149 net.cpp:165] Memory required for data: 5039360
I1127 11:31:23.757410  8149 layer_factory.hpp:76] Creating layer relu1
I1127 11:31:23.757428  8149 net.cpp:106] Creating Layer relu1
I1127 11:31:23.757436  8149 net.cpp:454] relu1 <- ip1
I1127 11:31:23.757444  8149 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:31:23.757464  8149 net.cpp:150] Setting up relu1
I1127 11:31:23.757483  8149 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:31:23.757486  8149 net.cpp:165] Memory required for data: 5167360
I1127 11:31:23.757491  8149 layer_factory.hpp:76] Creating layer ip2
I1127 11:31:23.757505  8149 net.cpp:106] Creating Layer ip2
I1127 11:31:23.757510  8149 net.cpp:454] ip2 <- ip1
I1127 11:31:23.757518  8149 net.cpp:411] ip2 -> ip2
I1127 11:31:23.758258  8149 net.cpp:150] Setting up ip2
I1127 11:31:23.758312  8149 net.cpp:157] Top shape: 64 10 (640)
I1127 11:31:23.758317  8149 net.cpp:165] Memory required for data: 5169920
I1127 11:31:23.758332  8149 layer_factory.hpp:76] Creating layer loss
I1127 11:31:23.758349  8149 net.cpp:106] Creating Layer loss
I1127 11:31:23.758354  8149 net.cpp:454] loss <- ip2
I1127 11:31:23.758361  8149 net.cpp:454] loss <- label
I1127 11:31:23.758371  8149 net.cpp:411] loss -> loss
I1127 11:31:23.758388  8149 layer_factory.hpp:76] Creating layer loss
I1127 11:31:23.758461  8149 net.cpp:150] Setting up loss
I1127 11:31:23.758469  8149 net.cpp:157] Top shape: (1)
I1127 11:31:23.758473  8149 net.cpp:160]     with loss weight 1
I1127 11:31:23.758494  8149 net.cpp:165] Memory required for data: 5169924
I1127 11:31:23.758498  8149 net.cpp:226] loss needs backward computation.
I1127 11:31:23.758503  8149 net.cpp:226] ip2 needs backward computation.
I1127 11:31:23.758509  8149 net.cpp:226] relu1 needs backward computation.
I1127 11:31:23.758513  8149 net.cpp:226] ip1 needs backward computation.
I1127 11:31:23.758517  8149 net.cpp:226] pool2 needs backward computation.
I1127 11:31:23.758522  8149 net.cpp:226] conv2 needs backward computation.
I1127 11:31:23.758527  8149 net.cpp:226] pool1 needs backward computation.
I1127 11:31:23.758532  8149 net.cpp:226] conv1 needs backward computation.
I1127 11:31:23.758535  8149 net.cpp:228] mnist does not need backward computation.
I1127 11:31:23.758539  8149 net.cpp:270] This network produces output loss
I1127 11:31:23.758549  8149 net.cpp:283] Network initialization done.
I1127 11:31:23.758821  8149 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:31:23.758860  8149 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:31:23.759013  8149 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:31:23.759089  8149 layer_factory.hpp:76] Creating layer mnist
I1127 11:31:23.759198  8149 net.cpp:106] Creating Layer mnist
I1127 11:31:23.759209  8149 net.cpp:411] mnist -> data
I1127 11:31:23.759220  8149 net.cpp:411] mnist -> label
I1127 11:31:23.762346  8156 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:31:23.762742  8149 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:31:23.768111  8149 net.cpp:150] Setting up mnist
I1127 11:31:23.768251  8149 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:31:23.768265  8149 net.cpp:157] Top shape: 100 (100)
I1127 11:31:23.768273  8149 net.cpp:165] Memory required for data: 314000
I1127 11:31:23.768288  8149 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:31:23.768318  8149 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:31:23.768334  8149 net.cpp:454] label_mnist_1_split <- label
I1127 11:31:23.768357  8149 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:31:23.768390  8149 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:31:23.768560  8149 net.cpp:150] Setting up label_mnist_1_split
I1127 11:31:23.768589  8149 net.cpp:157] Top shape: 100 (100)
I1127 11:31:23.768604  8149 net.cpp:157] Top shape: 100 (100)
I1127 11:31:23.768615  8149 net.cpp:165] Memory required for data: 314800
I1127 11:31:23.768627  8149 layer_factory.hpp:76] Creating layer conv1
I1127 11:31:23.768663  8149 net.cpp:106] Creating Layer conv1
I1127 11:31:23.768679  8149 net.cpp:454] conv1 <- data
I1127 11:31:23.768698  8149 net.cpp:411] conv1 -> conv1
I1127 11:31:23.769124  8149 net.cpp:150] Setting up conv1
I1127 11:31:23.769157  8149 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:31:23.769167  8149 net.cpp:165] Memory required for data: 4922800
I1127 11:31:23.769196  8149 layer_factory.hpp:76] Creating layer pool1
I1127 11:31:23.769227  8149 net.cpp:106] Creating Layer pool1
I1127 11:31:23.769242  8149 net.cpp:454] pool1 <- conv1
I1127 11:31:23.769317  8149 net.cpp:411] pool1 -> pool1
I1127 11:31:23.769423  8149 net.cpp:150] Setting up pool1
I1127 11:31:23.769446  8149 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:31:23.769459  8149 net.cpp:165] Memory required for data: 6074800
I1127 11:31:23.769472  8149 layer_factory.hpp:76] Creating layer conv2
I1127 11:31:23.769506  8149 net.cpp:106] Creating Layer conv2
I1127 11:31:23.769521  8149 net.cpp:454] conv2 <- pool1
I1127 11:31:23.769549  8149 net.cpp:411] conv2 -> conv2
I1127 11:31:23.770128  8149 net.cpp:150] Setting up conv2
I1127 11:31:23.770167  8149 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:31:23.770175  8149 net.cpp:165] Memory required for data: 7354800
I1127 11:31:23.770197  8149 layer_factory.hpp:76] Creating layer pool2
I1127 11:31:23.770213  8149 net.cpp:106] Creating Layer pool2
I1127 11:31:23.770222  8149 net.cpp:454] pool2 <- conv2
I1127 11:31:23.770236  8149 net.cpp:411] pool2 -> pool2
I1127 11:31:23.770301  8149 net.cpp:150] Setting up pool2
I1127 11:31:23.770316  8149 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:31:23.770323  8149 net.cpp:165] Memory required for data: 7674800
I1127 11:31:23.770331  8149 layer_factory.hpp:76] Creating layer ip1
I1127 11:31:23.770349  8149 net.cpp:106] Creating Layer ip1
I1127 11:31:23.770355  8149 net.cpp:454] ip1 <- pool2
I1127 11:31:23.770370  8149 net.cpp:411] ip1 -> ip1
I1127 11:31:23.773430  8149 net.cpp:150] Setting up ip1
I1127 11:31:23.773478  8149 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:31:23.773486  8149 net.cpp:165] Memory required for data: 7874800
I1127 11:31:23.773507  8149 layer_factory.hpp:76] Creating layer relu1
I1127 11:31:23.773524  8149 net.cpp:106] Creating Layer relu1
I1127 11:31:23.773536  8149 net.cpp:454] relu1 <- ip1
I1127 11:31:23.773558  8149 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:31:23.773578  8149 net.cpp:150] Setting up relu1
I1127 11:31:23.773588  8149 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:31:23.773596  8149 net.cpp:165] Memory required for data: 8074800
I1127 11:31:23.773602  8149 layer_factory.hpp:76] Creating layer ip2
I1127 11:31:23.773627  8149 net.cpp:106] Creating Layer ip2
I1127 11:31:23.773636  8149 net.cpp:454] ip2 <- ip1
I1127 11:31:23.773649  8149 net.cpp:411] ip2 -> ip2
I1127 11:31:23.773823  8149 net.cpp:150] Setting up ip2
I1127 11:31:23.773838  8149 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:31:23.773845  8149 net.cpp:165] Memory required for data: 8078800
I1127 11:31:23.773857  8149 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:31:23.773869  8149 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:31:23.773876  8149 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:31:23.773885  8149 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:31:23.773896  8149 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:31:23.773951  8149 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:31:23.773962  8149 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:31:23.773972  8149 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:31:23.773978  8149 net.cpp:165] Memory required for data: 8086800
I1127 11:31:23.773986  8149 layer_factory.hpp:76] Creating layer accuracy
I1127 11:31:23.773998  8149 net.cpp:106] Creating Layer accuracy
I1127 11:31:23.774006  8149 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:31:23.774015  8149 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:31:23.774029  8149 net.cpp:411] accuracy -> accuracy
I1127 11:31:23.774046  8149 net.cpp:150] Setting up accuracy
I1127 11:31:23.774057  8149 net.cpp:157] Top shape: (1)
I1127 11:31:23.774065  8149 net.cpp:165] Memory required for data: 8086804
I1127 11:31:23.774071  8149 layer_factory.hpp:76] Creating layer loss
I1127 11:31:23.774083  8149 net.cpp:106] Creating Layer loss
I1127 11:31:23.774091  8149 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:31:23.774101  8149 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:31:23.774116  8149 net.cpp:411] loss -> loss
I1127 11:31:23.774134  8149 layer_factory.hpp:76] Creating layer loss
I1127 11:31:23.774466  8149 net.cpp:150] Setting up loss
I1127 11:31:23.774485  8149 net.cpp:157] Top shape: (1)
I1127 11:31:23.774492  8149 net.cpp:160]     with loss weight 1
I1127 11:31:23.774508  8149 net.cpp:165] Memory required for data: 8086808
I1127 11:31:23.774515  8149 net.cpp:226] loss needs backward computation.
I1127 11:31:23.774528  8149 net.cpp:228] accuracy does not need backward computation.
I1127 11:31:23.774536  8149 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:31:23.774544  8149 net.cpp:226] ip2 needs backward computation.
I1127 11:31:23.774550  8149 net.cpp:226] relu1 needs backward computation.
I1127 11:31:23.774557  8149 net.cpp:226] ip1 needs backward computation.
I1127 11:31:23.774564  8149 net.cpp:226] pool2 needs backward computation.
I1127 11:31:23.774572  8149 net.cpp:226] conv2 needs backward computation.
I1127 11:31:23.774580  8149 net.cpp:226] pool1 needs backward computation.
I1127 11:31:23.774587  8149 net.cpp:226] conv1 needs backward computation.
I1127 11:31:23.774595  8149 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:31:23.774602  8149 net.cpp:228] mnist does not need backward computation.
I1127 11:31:23.774610  8149 net.cpp:270] This network produces output accuracy
I1127 11:31:23.774617  8149 net.cpp:270] This network produces output loss
I1127 11:31:23.774633  8149 net.cpp:283] Network initialization done.
I1127 11:31:23.774705  8149 solver.cpp:59] Solver scaffolding done.
I1127 11:31:23.775013  8149 caffe.cpp:212] Starting Optimization
I1127 11:31:23.775023  8149 solver.cpp:287] Solving LeNet
I1127 11:31:23.775030  8149 solver.cpp:288] Learning Rate Policy: inv
I1127 11:31:23.776470  8149 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:31:25.015547  8149 solver.cpp:408]     Test net output #0: accuracy = 0.0911
I1127 11:31:25.015727  8149 solver.cpp:408]     Test net output #1: loss = 2.37777 (* 1 = 2.37777 loss)
I1127 11:31:25.030299  8149 solver.cpp:236] Iteration 0, loss = 2.42127
I1127 11:31:25.030375  8149 solver.cpp:252]     Train net output #0: loss = 2.42127 (* 1 = 2.42127 loss)
I1127 11:31:25.030413  8149 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:31:34.432693  8149 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:31:35.829567  8149 solver.cpp:408]     Test net output #0: accuracy = 0.9743
I1127 11:31:35.829624  8149 solver.cpp:408]     Test net output #1: loss = 0.0861382 (* 1 = 0.0861382 loss)
I1127 11:31:35.838656  8149 solver.cpp:236] Iteration 500, loss = 0.116498
I1127 11:31:35.838716  8149 solver.cpp:252]     Train net output #0: loss = 0.116498 (* 1 = 0.116498 loss)
I1127 11:31:35.838732  8149 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:31:45.201033  8149 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:31:45.222050  8149 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:31:45.242934  8149 solver.cpp:320] Iteration 1000, loss = 0.0907978
I1127 11:31:45.242974  8149 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:31:46.629650  8149 solver.cpp:408]     Test net output #0: accuracy = 0.9818
I1127 11:31:46.629709  8149 solver.cpp:408]     Test net output #1: loss = 0.0571798 (* 1 = 0.0571798 loss)
I1127 11:31:46.629716  8149 solver.cpp:325] Optimization Done.
I1127 11:31:46.629720  8149 caffe.cpp:215] Optimization Done.
I1127 11:31:46.698798  8217 caffe.cpp:184] Using GPUs 0
I1127 11:31:47.024977  8217 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:31:47.025207  8217 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:31:47.025694  8217 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:31:47.025735  8217 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:31:47.025879  8217 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:31:47.025997  8217 layer_factory.hpp:76] Creating layer mnist
I1127 11:31:47.026557  8217 net.cpp:106] Creating Layer mnist
I1127 11:31:47.026583  8217 net.cpp:411] mnist -> data
I1127 11:31:47.026631  8217 net.cpp:411] mnist -> label
I1127 11:31:47.027648  8220 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:31:47.038234  8217 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:31:47.039628  8217 net.cpp:150] Setting up mnist
I1127 11:31:47.039686  8217 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:31:47.039700  8217 net.cpp:157] Top shape: 64 (64)
I1127 11:31:47.039707  8217 net.cpp:165] Memory required for data: 200960
I1127 11:31:47.039727  8217 layer_factory.hpp:76] Creating layer conv1
I1127 11:31:47.039768  8217 net.cpp:106] Creating Layer conv1
I1127 11:31:47.039784  8217 net.cpp:454] conv1 <- data
I1127 11:31:47.039804  8217 net.cpp:411] conv1 -> conv1
I1127 11:31:47.040822  8217 net.cpp:150] Setting up conv1
I1127 11:31:47.040848  8217 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:31:47.040858  8217 net.cpp:165] Memory required for data: 3150080
I1127 11:31:47.040882  8217 layer_factory.hpp:76] Creating layer pool1
I1127 11:31:47.040901  8217 net.cpp:106] Creating Layer pool1
I1127 11:31:47.040911  8217 net.cpp:454] pool1 <- conv1
I1127 11:31:47.040925  8217 net.cpp:411] pool1 -> pool1
I1127 11:31:47.041019  8217 net.cpp:150] Setting up pool1
I1127 11:31:47.041033  8217 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:31:47.041041  8217 net.cpp:165] Memory required for data: 3887360
I1127 11:31:47.041049  8217 layer_factory.hpp:76] Creating layer conv2
I1127 11:31:47.041070  8217 net.cpp:106] Creating Layer conv2
I1127 11:31:47.041079  8217 net.cpp:454] conv2 <- pool1
I1127 11:31:47.041091  8217 net.cpp:411] conv2 -> conv2
I1127 11:31:47.041538  8217 net.cpp:150] Setting up conv2
I1127 11:31:47.041554  8217 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:31:47.041561  8217 net.cpp:165] Memory required for data: 4706560
I1127 11:31:47.041581  8217 layer_factory.hpp:76] Creating layer pool2
I1127 11:31:47.041595  8217 net.cpp:106] Creating Layer pool2
I1127 11:31:47.041604  8217 net.cpp:454] pool2 <- conv2
I1127 11:31:47.041618  8217 net.cpp:411] pool2 -> pool2
I1127 11:31:47.041683  8217 net.cpp:150] Setting up pool2
I1127 11:31:47.041697  8217 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:31:47.041704  8217 net.cpp:165] Memory required for data: 4911360
I1127 11:31:47.041712  8217 layer_factory.hpp:76] Creating layer ip1
I1127 11:31:47.041726  8217 net.cpp:106] Creating Layer ip1
I1127 11:31:47.041734  8217 net.cpp:454] ip1 <- pool2
I1127 11:31:47.041748  8217 net.cpp:411] ip1 -> ip1
I1127 11:31:47.045475  8217 net.cpp:150] Setting up ip1
I1127 11:31:47.045495  8217 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:31:47.045502  8217 net.cpp:165] Memory required for data: 5039360
I1127 11:31:47.045517  8217 layer_factory.hpp:76] Creating layer relu1
I1127 11:31:47.045528  8217 net.cpp:106] Creating Layer relu1
I1127 11:31:47.045536  8217 net.cpp:454] relu1 <- ip1
I1127 11:31:47.045545  8217 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:31:47.045558  8217 net.cpp:150] Setting up relu1
I1127 11:31:47.045567  8217 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:31:47.045574  8217 net.cpp:165] Memory required for data: 5167360
I1127 11:31:47.045581  8217 layer_factory.hpp:76] Creating layer ip2
I1127 11:31:47.045593  8217 net.cpp:106] Creating Layer ip2
I1127 11:31:47.045600  8217 net.cpp:454] ip2 <- ip1
I1127 11:31:47.045609  8217 net.cpp:411] ip2 -> ip2
I1127 11:31:47.046211  8217 net.cpp:150] Setting up ip2
I1127 11:31:47.046226  8217 net.cpp:157] Top shape: 64 10 (640)
I1127 11:31:47.046232  8217 net.cpp:165] Memory required for data: 5169920
I1127 11:31:47.046252  8217 layer_factory.hpp:76] Creating layer loss
I1127 11:31:47.046263  8217 net.cpp:106] Creating Layer loss
I1127 11:31:47.046270  8217 net.cpp:454] loss <- ip2
I1127 11:31:47.046279  8217 net.cpp:454] loss <- label
I1127 11:31:47.046291  8217 net.cpp:411] loss -> loss
I1127 11:31:47.046308  8217 layer_factory.hpp:76] Creating layer loss
I1127 11:31:47.046407  8217 net.cpp:150] Setting up loss
I1127 11:31:47.046418  8217 net.cpp:157] Top shape: (1)
I1127 11:31:47.046424  8217 net.cpp:160]     with loss weight 1
I1127 11:31:47.046447  8217 net.cpp:165] Memory required for data: 5169924
I1127 11:31:47.046454  8217 net.cpp:226] loss needs backward computation.
I1127 11:31:47.046461  8217 net.cpp:226] ip2 needs backward computation.
I1127 11:31:47.046470  8217 net.cpp:226] relu1 needs backward computation.
I1127 11:31:47.046478  8217 net.cpp:226] ip1 needs backward computation.
I1127 11:31:47.046484  8217 net.cpp:226] pool2 needs backward computation.
I1127 11:31:47.046490  8217 net.cpp:226] conv2 needs backward computation.
I1127 11:31:47.046499  8217 net.cpp:226] pool1 needs backward computation.
I1127 11:31:47.046505  8217 net.cpp:226] conv1 needs backward computation.
I1127 11:31:47.046512  8217 net.cpp:228] mnist does not need backward computation.
I1127 11:31:47.046519  8217 net.cpp:270] This network produces output loss
I1127 11:31:47.046532  8217 net.cpp:283] Network initialization done.
I1127 11:31:47.046890  8217 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:31:47.046922  8217 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:31:47.047090  8217 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:31:47.047175  8217 layer_factory.hpp:76] Creating layer mnist
I1127 11:31:47.047310  8217 net.cpp:106] Creating Layer mnist
I1127 11:31:47.047325  8217 net.cpp:411] mnist -> data
I1127 11:31:47.047338  8217 net.cpp:411] mnist -> label
I1127 11:31:47.048070  8223 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:31:47.048171  8217 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:31:47.051666  8217 net.cpp:150] Setting up mnist
I1127 11:31:47.051683  8217 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:31:47.051692  8217 net.cpp:157] Top shape: 100 (100)
I1127 11:31:47.051698  8217 net.cpp:165] Memory required for data: 314000
I1127 11:31:47.051707  8217 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:31:47.051717  8217 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:31:47.051724  8217 net.cpp:454] label_mnist_1_split <- label
I1127 11:31:47.051733  8217 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:31:47.051744  8217 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:31:47.051789  8217 net.cpp:150] Setting up label_mnist_1_split
I1127 11:31:47.051800  8217 net.cpp:157] Top shape: 100 (100)
I1127 11:31:47.051808  8217 net.cpp:157] Top shape: 100 (100)
I1127 11:31:47.051815  8217 net.cpp:165] Memory required for data: 314800
I1127 11:31:47.051821  8217 layer_factory.hpp:76] Creating layer conv1
I1127 11:31:47.051836  8217 net.cpp:106] Creating Layer conv1
I1127 11:31:47.051842  8217 net.cpp:454] conv1 <- data
I1127 11:31:47.051854  8217 net.cpp:411] conv1 -> conv1
I1127 11:31:47.052072  8217 net.cpp:150] Setting up conv1
I1127 11:31:47.052083  8217 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:31:47.052090  8217 net.cpp:165] Memory required for data: 4922800
I1127 11:31:47.052104  8217 layer_factory.hpp:76] Creating layer pool1
I1127 11:31:47.052114  8217 net.cpp:106] Creating Layer pool1
I1127 11:31:47.052121  8217 net.cpp:454] pool1 <- conv1
I1127 11:31:47.052139  8217 net.cpp:411] pool1 -> pool1
I1127 11:31:47.052182  8217 net.cpp:150] Setting up pool1
I1127 11:31:47.052193  8217 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:31:47.052201  8217 net.cpp:165] Memory required for data: 6074800
I1127 11:31:47.052207  8217 layer_factory.hpp:76] Creating layer conv2
I1127 11:31:47.052219  8217 net.cpp:106] Creating Layer conv2
I1127 11:31:47.052227  8217 net.cpp:454] conv2 <- pool1
I1127 11:31:47.052238  8217 net.cpp:411] conv2 -> conv2
I1127 11:31:47.052958  8217 net.cpp:150] Setting up conv2
I1127 11:31:47.052970  8217 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:31:47.052978  8217 net.cpp:165] Memory required for data: 7354800
I1127 11:31:47.052991  8217 layer_factory.hpp:76] Creating layer pool2
I1127 11:31:47.053000  8217 net.cpp:106] Creating Layer pool2
I1127 11:31:47.053011  8217 net.cpp:454] pool2 <- conv2
I1127 11:31:47.053021  8217 net.cpp:411] pool2 -> pool2
I1127 11:31:47.053064  8217 net.cpp:150] Setting up pool2
I1127 11:31:47.053076  8217 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:31:47.053086  8217 net.cpp:165] Memory required for data: 7674800
I1127 11:31:47.053093  8217 layer_factory.hpp:76] Creating layer ip1
I1127 11:31:47.053104  8217 net.cpp:106] Creating Layer ip1
I1127 11:31:47.053112  8217 net.cpp:454] ip1 <- pool2
I1127 11:31:47.053123  8217 net.cpp:411] ip1 -> ip1
I1127 11:31:47.056726  8217 net.cpp:150] Setting up ip1
I1127 11:31:47.056742  8217 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:31:47.056749  8217 net.cpp:165] Memory required for data: 7874800
I1127 11:31:47.056761  8217 layer_factory.hpp:76] Creating layer relu1
I1127 11:31:47.056773  8217 net.cpp:106] Creating Layer relu1
I1127 11:31:47.056782  8217 net.cpp:454] relu1 <- ip1
I1127 11:31:47.056789  8217 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:31:47.056800  8217 net.cpp:150] Setting up relu1
I1127 11:31:47.056809  8217 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:31:47.056815  8217 net.cpp:165] Memory required for data: 8074800
I1127 11:31:47.056823  8217 layer_factory.hpp:76] Creating layer ip2
I1127 11:31:47.056833  8217 net.cpp:106] Creating Layer ip2
I1127 11:31:47.056839  8217 net.cpp:454] ip2 <- ip1
I1127 11:31:47.056852  8217 net.cpp:411] ip2 -> ip2
I1127 11:31:47.056995  8217 net.cpp:150] Setting up ip2
I1127 11:31:47.057006  8217 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:31:47.057018  8217 net.cpp:165] Memory required for data: 8078800
I1127 11:31:47.057029  8217 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:31:47.057036  8217 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:31:47.057044  8217 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:31:47.057054  8217 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:31:47.057063  8217 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:31:47.057101  8217 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:31:47.057112  8217 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:31:47.057121  8217 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:31:47.057126  8217 net.cpp:165] Memory required for data: 8086800
I1127 11:31:47.057133  8217 layer_factory.hpp:76] Creating layer accuracy
I1127 11:31:47.057143  8217 net.cpp:106] Creating Layer accuracy
I1127 11:31:47.057149  8217 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:31:47.057157  8217 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:31:47.057167  8217 net.cpp:411] accuracy -> accuracy
I1127 11:31:47.057178  8217 net.cpp:150] Setting up accuracy
I1127 11:31:47.057186  8217 net.cpp:157] Top shape: (1)
I1127 11:31:47.057193  8217 net.cpp:165] Memory required for data: 8086804
I1127 11:31:47.057199  8217 layer_factory.hpp:76] Creating layer loss
I1127 11:31:47.057209  8217 net.cpp:106] Creating Layer loss
I1127 11:31:47.057216  8217 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:31:47.057224  8217 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:31:47.057234  8217 net.cpp:411] loss -> loss
I1127 11:31:47.057245  8217 layer_factory.hpp:76] Creating layer loss
I1127 11:31:47.057340  8217 net.cpp:150] Setting up loss
I1127 11:31:47.057350  8217 net.cpp:157] Top shape: (1)
I1127 11:31:47.057358  8217 net.cpp:160]     with loss weight 1
I1127 11:31:47.057368  8217 net.cpp:165] Memory required for data: 8086808
I1127 11:31:47.057375  8217 net.cpp:226] loss needs backward computation.
I1127 11:31:47.057384  8217 net.cpp:228] accuracy does not need backward computation.
I1127 11:31:47.057392  8217 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:31:47.057399  8217 net.cpp:226] ip2 needs backward computation.
I1127 11:31:47.057405  8217 net.cpp:226] relu1 needs backward computation.
I1127 11:31:47.057411  8217 net.cpp:226] ip1 needs backward computation.
I1127 11:31:47.057418  8217 net.cpp:226] pool2 needs backward computation.
I1127 11:31:47.057425  8217 net.cpp:226] conv2 needs backward computation.
I1127 11:31:47.057431  8217 net.cpp:226] pool1 needs backward computation.
I1127 11:31:47.057438  8217 net.cpp:226] conv1 needs backward computation.
I1127 11:31:47.057446  8217 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:31:47.057452  8217 net.cpp:228] mnist does not need backward computation.
I1127 11:31:47.057458  8217 net.cpp:270] This network produces output accuracy
I1127 11:31:47.057466  8217 net.cpp:270] This network produces output loss
I1127 11:31:47.057482  8217 net.cpp:283] Network initialization done.
I1127 11:31:47.057530  8217 solver.cpp:59] Solver scaffolding done.
I1127 11:31:47.057811  8217 caffe.cpp:212] Starting Optimization
I1127 11:31:47.057821  8217 solver.cpp:287] Solving LeNet
I1127 11:31:47.057826  8217 solver.cpp:288] Learning Rate Policy: inv
I1127 11:31:47.058239  8217 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:31:48.533665  8217 solver.cpp:408]     Test net output #0: accuracy = 0.1911
I1127 11:31:48.533772  8217 solver.cpp:408]     Test net output #1: loss = 2.34675 (* 1 = 2.34675 loss)
I1127 11:31:48.547055  8217 solver.cpp:236] Iteration 0, loss = 2.31707
I1127 11:31:48.547109  8217 solver.cpp:252]     Train net output #0: loss = 2.31707 (* 1 = 2.31707 loss)
I1127 11:31:48.547135  8217 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:31:54.739493  8217 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:31:58.017710  8217 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:31:59.603375  8217 solver.cpp:408]     Test net output #0: accuracy = 0.9762
I1127 11:31:59.603507  8217 solver.cpp:408]     Test net output #1: loss = 0.0798311 (* 1 = 0.0798311 loss)
I1127 11:31:59.613529  8217 solver.cpp:236] Iteration 500, loss = 0.0815192
I1127 11:31:59.613616  8217 solver.cpp:252]     Train net output #0: loss = 0.0815192 (* 1 = 0.0815192 loss)
I1127 11:31:59.613631  8217 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:32:09.342841  8217 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:32:09.356204  8217 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:32:09.368147  8217 solver.cpp:320] Iteration 1000, loss = 0.098342
I1127 11:32:09.368197  8217 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:32:10.847404  8217 solver.cpp:408]     Test net output #0: accuracy = 0.983
I1127 11:32:10.847574  8217 solver.cpp:408]     Test net output #1: loss = 0.0531944 (* 1 = 0.0531944 loss)
I1127 11:32:10.847616  8217 solver.cpp:325] Optimization Done.
I1127 11:32:10.847635  8217 caffe.cpp:215] Optimization Done.
I1127 11:32:10.997128  8285 caffe.cpp:184] Using GPUs 0
I1127 11:32:11.379269  8285 solver.cpp:47] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I1127 11:32:11.379518  8285 solver.cpp:90] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1127 11:32:11.379919  8285 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1127 11:32:11.379958  8285 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1127 11:32:11.380090  8285 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:32:11.380190  8285 layer_factory.hpp:76] Creating layer mnist
I1127 11:32:11.380736  8285 net.cpp:106] Creating Layer mnist
I1127 11:32:11.380808  8285 net.cpp:411] mnist -> data
I1127 11:32:11.380887  8285 net.cpp:411] mnist -> label
I1127 11:32:11.383370  8288 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I1127 11:32:11.394433  8285 data_layer.cpp:41] output data size: 64,1,28,28
I1127 11:32:11.396124  8285 net.cpp:150] Setting up mnist
I1127 11:32:11.396224  8285 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1127 11:32:11.396240  8285 net.cpp:157] Top shape: 64 (64)
I1127 11:32:11.396248  8285 net.cpp:165] Memory required for data: 200960
I1127 11:32:11.396271  8285 layer_factory.hpp:76] Creating layer conv1
I1127 11:32:11.396322  8285 net.cpp:106] Creating Layer conv1
I1127 11:32:11.396338  8285 net.cpp:454] conv1 <- data
I1127 11:32:11.396365  8285 net.cpp:411] conv1 -> conv1
I1127 11:32:11.397781  8285 net.cpp:150] Setting up conv1
I1127 11:32:11.397845  8285 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1127 11:32:11.397856  8285 net.cpp:165] Memory required for data: 3150080
I1127 11:32:11.397892  8285 layer_factory.hpp:76] Creating layer pool1
I1127 11:32:11.397923  8285 net.cpp:106] Creating Layer pool1
I1127 11:32:11.397934  8285 net.cpp:454] pool1 <- conv1
I1127 11:32:11.397949  8285 net.cpp:411] pool1 -> pool1
I1127 11:32:11.398056  8285 net.cpp:150] Setting up pool1
I1127 11:32:11.398072  8285 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1127 11:32:11.398080  8285 net.cpp:165] Memory required for data: 3887360
I1127 11:32:11.398088  8285 layer_factory.hpp:76] Creating layer conv2
I1127 11:32:11.398111  8285 net.cpp:106] Creating Layer conv2
I1127 11:32:11.398120  8285 net.cpp:454] conv2 <- pool1
I1127 11:32:11.398135  8285 net.cpp:411] conv2 -> conv2
I1127 11:32:11.398640  8285 net.cpp:150] Setting up conv2
I1127 11:32:11.398681  8285 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1127 11:32:11.398690  8285 net.cpp:165] Memory required for data: 4706560
I1127 11:32:11.398711  8285 layer_factory.hpp:76] Creating layer pool2
I1127 11:32:11.398732  8285 net.cpp:106] Creating Layer pool2
I1127 11:32:11.398742  8285 net.cpp:454] pool2 <- conv2
I1127 11:32:11.398756  8285 net.cpp:411] pool2 -> pool2
I1127 11:32:11.398830  8285 net.cpp:150] Setting up pool2
I1127 11:32:11.398847  8285 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1127 11:32:11.398856  8285 net.cpp:165] Memory required for data: 4911360
I1127 11:32:11.398864  8285 layer_factory.hpp:76] Creating layer ip1
I1127 11:32:11.398890  8285 net.cpp:106] Creating Layer ip1
I1127 11:32:11.398900  8285 net.cpp:454] ip1 <- pool2
I1127 11:32:11.398911  8285 net.cpp:411] ip1 -> ip1
I1127 11:32:11.402485  8285 net.cpp:150] Setting up ip1
I1127 11:32:11.402561  8285 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:32:11.402568  8285 net.cpp:165] Memory required for data: 5039360
I1127 11:32:11.402590  8285 layer_factory.hpp:76] Creating layer relu1
I1127 11:32:11.402607  8285 net.cpp:106] Creating Layer relu1
I1127 11:32:11.402613  8285 net.cpp:454] relu1 <- ip1
I1127 11:32:11.402624  8285 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:32:11.402645  8285 net.cpp:150] Setting up relu1
I1127 11:32:11.402652  8285 net.cpp:157] Top shape: 64 500 (32000)
I1127 11:32:11.402657  8285 net.cpp:165] Memory required for data: 5167360
I1127 11:32:11.402660  8285 layer_factory.hpp:76] Creating layer ip2
I1127 11:32:11.402672  8285 net.cpp:106] Creating Layer ip2
I1127 11:32:11.402675  8285 net.cpp:454] ip2 <- ip1
I1127 11:32:11.402683  8285 net.cpp:411] ip2 -> ip2
I1127 11:32:11.403344  8285 net.cpp:150] Setting up ip2
I1127 11:32:11.403378  8285 net.cpp:157] Top shape: 64 10 (640)
I1127 11:32:11.403383  8285 net.cpp:165] Memory required for data: 5169920
I1127 11:32:11.403393  8285 layer_factory.hpp:76] Creating layer loss
I1127 11:32:11.403406  8285 net.cpp:106] Creating Layer loss
I1127 11:32:11.403411  8285 net.cpp:454] loss <- ip2
I1127 11:32:11.403419  8285 net.cpp:454] loss <- label
I1127 11:32:11.403429  8285 net.cpp:411] loss -> loss
I1127 11:32:11.403445  8285 layer_factory.hpp:76] Creating layer loss
I1127 11:32:11.403517  8285 net.cpp:150] Setting up loss
I1127 11:32:11.403525  8285 net.cpp:157] Top shape: (1)
I1127 11:32:11.403538  8285 net.cpp:160]     with loss weight 1
I1127 11:32:11.403556  8285 net.cpp:165] Memory required for data: 5169924
I1127 11:32:11.403561  8285 net.cpp:226] loss needs backward computation.
I1127 11:32:11.403565  8285 net.cpp:226] ip2 needs backward computation.
I1127 11:32:11.403570  8285 net.cpp:226] relu1 needs backward computation.
I1127 11:32:11.403574  8285 net.cpp:226] ip1 needs backward computation.
I1127 11:32:11.403578  8285 net.cpp:226] pool2 needs backward computation.
I1127 11:32:11.403584  8285 net.cpp:226] conv2 needs backward computation.
I1127 11:32:11.403589  8285 net.cpp:226] pool1 needs backward computation.
I1127 11:32:11.403592  8285 net.cpp:226] conv1 needs backward computation.
I1127 11:32:11.403596  8285 net.cpp:228] mnist does not need backward computation.
I1127 11:32:11.403600  8285 net.cpp:270] This network produces output loss
I1127 11:32:11.403611  8285 net.cpp:283] Network initialization done.
I1127 11:32:11.403908  8285 solver.cpp:180] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1127 11:32:11.403966  8285 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1127 11:32:11.404155  8285 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1127 11:32:11.404260  8285 layer_factory.hpp:76] Creating layer mnist
I1127 11:32:11.404498  8285 net.cpp:106] Creating Layer mnist
I1127 11:32:11.404527  8285 net.cpp:411] mnist -> data
I1127 11:32:11.404556  8285 net.cpp:411] mnist -> label
I1127 11:32:11.405407  8290 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I1127 11:32:11.405537  8285 data_layer.cpp:41] output data size: 100,1,28,28
I1127 11:32:11.406744  8285 net.cpp:150] Setting up mnist
I1127 11:32:11.406792  8285 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1127 11:32:11.406803  8285 net.cpp:157] Top shape: 100 (100)
I1127 11:32:11.406811  8285 net.cpp:165] Memory required for data: 314000
I1127 11:32:11.406822  8285 layer_factory.hpp:76] Creating layer label_mnist_1_split
I1127 11:32:11.406854  8285 net.cpp:106] Creating Layer label_mnist_1_split
I1127 11:32:11.406865  8285 net.cpp:454] label_mnist_1_split <- label
I1127 11:32:11.406883  8285 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I1127 11:32:11.406903  8285 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I1127 11:32:11.406951  8285 net.cpp:150] Setting up label_mnist_1_split
I1127 11:32:11.406965  8285 net.cpp:157] Top shape: 100 (100)
I1127 11:32:11.406976  8285 net.cpp:157] Top shape: 100 (100)
I1127 11:32:11.406982  8285 net.cpp:165] Memory required for data: 314800
I1127 11:32:11.406990  8285 layer_factory.hpp:76] Creating layer conv1
I1127 11:32:11.407011  8285 net.cpp:106] Creating Layer conv1
I1127 11:32:11.407019  8285 net.cpp:454] conv1 <- data
I1127 11:32:11.407030  8285 net.cpp:411] conv1 -> conv1
I1127 11:32:11.407261  8285 net.cpp:150] Setting up conv1
I1127 11:32:11.407277  8285 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1127 11:32:11.407284  8285 net.cpp:165] Memory required for data: 4922800
I1127 11:32:11.407301  8285 layer_factory.hpp:76] Creating layer pool1
I1127 11:32:11.407315  8285 net.cpp:106] Creating Layer pool1
I1127 11:32:11.407322  8285 net.cpp:454] pool1 <- conv1
I1127 11:32:11.407344  8285 net.cpp:411] pool1 -> pool1
I1127 11:32:11.407392  8285 net.cpp:150] Setting up pool1
I1127 11:32:11.407404  8285 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1127 11:32:11.407410  8285 net.cpp:165] Memory required for data: 6074800
I1127 11:32:11.407418  8285 layer_factory.hpp:76] Creating layer conv2
I1127 11:32:11.407436  8285 net.cpp:106] Creating Layer conv2
I1127 11:32:11.407445  8285 net.cpp:454] conv2 <- pool1
I1127 11:32:11.407457  8285 net.cpp:411] conv2 -> conv2
I1127 11:32:11.408004  8285 net.cpp:150] Setting up conv2
I1127 11:32:11.408035  8285 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1127 11:32:11.408042  8285 net.cpp:165] Memory required for data: 7354800
I1127 11:32:11.408061  8285 layer_factory.hpp:76] Creating layer pool2
I1127 11:32:11.408078  8285 net.cpp:106] Creating Layer pool2
I1127 11:32:11.408087  8285 net.cpp:454] pool2 <- conv2
I1127 11:32:11.408098  8285 net.cpp:411] pool2 -> pool2
I1127 11:32:11.408149  8285 net.cpp:150] Setting up pool2
I1127 11:32:11.408160  8285 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1127 11:32:11.408167  8285 net.cpp:165] Memory required for data: 7674800
I1127 11:32:11.408174  8285 layer_factory.hpp:76] Creating layer ip1
I1127 11:32:11.408190  8285 net.cpp:106] Creating Layer ip1
I1127 11:32:11.408198  8285 net.cpp:454] ip1 <- pool2
I1127 11:32:11.408208  8285 net.cpp:411] ip1 -> ip1
I1127 11:32:11.412530  8285 net.cpp:150] Setting up ip1
I1127 11:32:11.412590  8285 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:32:11.412600  8285 net.cpp:165] Memory required for data: 7874800
I1127 11:32:11.412628  8285 layer_factory.hpp:76] Creating layer relu1
I1127 11:32:11.412647  8285 net.cpp:106] Creating Layer relu1
I1127 11:32:11.412654  8285 net.cpp:454] relu1 <- ip1
I1127 11:32:11.412663  8285 net.cpp:397] relu1 -> ip1 (in-place)
I1127 11:32:11.412675  8285 net.cpp:150] Setting up relu1
I1127 11:32:11.412680  8285 net.cpp:157] Top shape: 100 500 (50000)
I1127 11:32:11.412684  8285 net.cpp:165] Memory required for data: 8074800
I1127 11:32:11.412688  8285 layer_factory.hpp:76] Creating layer ip2
I1127 11:32:11.412708  8285 net.cpp:106] Creating Layer ip2
I1127 11:32:11.412714  8285 net.cpp:454] ip2 <- ip1
I1127 11:32:11.412720  8285 net.cpp:411] ip2 -> ip2
I1127 11:32:11.412839  8285 net.cpp:150] Setting up ip2
I1127 11:32:11.412847  8285 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:32:11.412853  8285 net.cpp:165] Memory required for data: 8078800
I1127 11:32:11.412858  8285 layer_factory.hpp:76] Creating layer ip2_ip2_0_split
I1127 11:32:11.412865  8285 net.cpp:106] Creating Layer ip2_ip2_0_split
I1127 11:32:11.412870  8285 net.cpp:454] ip2_ip2_0_split <- ip2
I1127 11:32:11.412875  8285 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1127 11:32:11.412883  8285 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1127 11:32:11.412924  8285 net.cpp:150] Setting up ip2_ip2_0_split
I1127 11:32:11.412931  8285 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:32:11.412936  8285 net.cpp:157] Top shape: 100 10 (1000)
I1127 11:32:11.412941  8285 net.cpp:165] Memory required for data: 8086800
I1127 11:32:11.412945  8285 layer_factory.hpp:76] Creating layer accuracy
I1127 11:32:11.412955  8285 net.cpp:106] Creating Layer accuracy
I1127 11:32:11.412960  8285 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I1127 11:32:11.412964  8285 net.cpp:454] accuracy <- label_mnist_1_split_0
I1127 11:32:11.412972  8285 net.cpp:411] accuracy -> accuracy
I1127 11:32:11.412983  8285 net.cpp:150] Setting up accuracy
I1127 11:32:11.412989  8285 net.cpp:157] Top shape: (1)
I1127 11:32:11.412993  8285 net.cpp:165] Memory required for data: 8086804
I1127 11:32:11.412998  8285 layer_factory.hpp:76] Creating layer loss
I1127 11:32:11.413005  8285 net.cpp:106] Creating Layer loss
I1127 11:32:11.413010  8285 net.cpp:454] loss <- ip2_ip2_0_split_1
I1127 11:32:11.413015  8285 net.cpp:454] loss <- label_mnist_1_split_1
I1127 11:32:11.413022  8285 net.cpp:411] loss -> loss
I1127 11:32:11.413031  8285 layer_factory.hpp:76] Creating layer loss
I1127 11:32:11.413121  8285 net.cpp:150] Setting up loss
I1127 11:32:11.413132  8285 net.cpp:157] Top shape: (1)
I1127 11:32:11.413137  8285 net.cpp:160]     with loss weight 1
I1127 11:32:11.413156  8285 net.cpp:165] Memory required for data: 8086808
I1127 11:32:11.413161  8285 net.cpp:226] loss needs backward computation.
I1127 11:32:11.413172  8285 net.cpp:228] accuracy does not need backward computation.
I1127 11:32:11.413177  8285 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1127 11:32:11.413182  8285 net.cpp:226] ip2 needs backward computation.
I1127 11:32:11.413187  8285 net.cpp:226] relu1 needs backward computation.
I1127 11:32:11.413192  8285 net.cpp:226] ip1 needs backward computation.
I1127 11:32:11.413195  8285 net.cpp:226] pool2 needs backward computation.
I1127 11:32:11.413200  8285 net.cpp:226] conv2 needs backward computation.
I1127 11:32:11.413205  8285 net.cpp:226] pool1 needs backward computation.
I1127 11:32:11.413210  8285 net.cpp:226] conv1 needs backward computation.
I1127 11:32:11.413215  8285 net.cpp:228] label_mnist_1_split does not need backward computation.
I1127 11:32:11.413220  8285 net.cpp:228] mnist does not need backward computation.
I1127 11:32:11.413224  8285 net.cpp:270] This network produces output accuracy
I1127 11:32:11.413228  8285 net.cpp:270] This network produces output loss
I1127 11:32:11.413239  8285 net.cpp:283] Network initialization done.
I1127 11:32:11.413311  8285 solver.cpp:59] Solver scaffolding done.
I1127 11:32:11.413534  8285 caffe.cpp:212] Starting Optimization
I1127 11:32:11.413548  8285 solver.cpp:287] Solving LeNet
I1127 11:32:11.413553  8285 solver.cpp:288] Learning Rate Policy: inv
I1127 11:32:11.414314  8285 solver.cpp:340] Iteration 0, Testing net (#0)
I1127 11:32:11.759187  8285 blocking_queue.cpp:50] Data layer prefetch queue empty
I1127 11:32:13.442381  8285 solver.cpp:408]     Test net output #0: accuracy = 0.08
I1127 11:32:13.442425  8285 solver.cpp:408]     Test net output #1: loss = 2.41995 (* 1 = 2.41995 loss)
I1127 11:32:13.476197  8285 solver.cpp:236] Iteration 0, loss = 2.37733
I1127 11:32:13.476213  8285 solver.cpp:252]     Train net output #0: loss = 2.37733 (* 1 = 2.37733 loss)
I1127 11:32:13.476224  8285 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1127 11:32:26.877698  8285 solver.cpp:340] Iteration 500, Testing net (#0)
I1127 11:32:28.069977  8285 solver.cpp:408]     Test net output #0: accuracy = 0.9698
I1127 11:32:28.070071  8285 solver.cpp:408]     Test net output #1: loss = 0.09306 (* 1 = 0.09306 loss)
I1127 11:32:28.082505  8285 solver.cpp:236] Iteration 500, loss = 0.127571
I1127 11:32:28.082617  8285 solver.cpp:252]     Train net output #0: loss = 0.127571 (* 1 = 0.127571 loss)
I1127 11:32:28.082635  8285 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1127 11:32:41.978354  8285 solver.cpp:461] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1127 11:32:42.043105  8285 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1127 11:32:42.054467  8285 solver.cpp:320] Iteration 1000, loss = 0.0675165
I1127 11:32:42.054517  8285 solver.cpp:340] Iteration 1000, Testing net (#0)
I1127 11:32:44.826019  8285 solver.cpp:408]     Test net output #0: accuracy = 0.9824
I1127 11:32:44.826190  8285 solver.cpp:408]     Test net output #1: loss = 0.0555335 (* 1 = 0.0555335 loss)
I1127 11:32:44.826208  8285 solver.cpp:325] Optimization Done.
I1127 11:32:44.826220  8285 caffe.cpp:215] Optimization Done.
